---
title: "TICS-411 Minería de Datos"
subtitle: "Clase 8: Evaluación de Modelos"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo.jpg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---


# Evaluación de Modelos Supervisados 


## Intuición {.smaller}

#### Supongamos que tengo que estudiar para la prueba de `Minería de Datos` y tengo que aprender a calcular el Coeficiente de Silueta.

::: {.incremental}
* Qué pasa si sólo les entrego una pregunta para estudiar y no tiene respuesta.
* ¿Qué pasa si ahora les doy la respuesta?
* ¿Qué pasa si te doy más ejercicios?
* ¿Qué pasa luego de que haces muchos ejercicios?
:::

::: {.callout-tip .fragment}
Voy `aprendiendo mejor la tarea` de calcular el coeficiente de Silueta. **Lo mismo pasa con los modelos**.
:::

::: {.callout-important .fragment}
Pero no puedo medir qué tan bien aprendiste en los ejercicios que yo ya entregué para practicar. **Tengo que hacer una prueba que tú no hayas visto, para ver si realmente aprendiste**.
:::

## Uso de un Modelo {.smaller}

![](img/clase-9/model_usage.png){.lightbox fig-align="center" width="85%"}


::: {.callout-warning .fragment}
**¿Cómo saber que el modelo está funcionando como esperamos?**
:::

# Evaluación de un Modelo

## Métricas

El Rendimieto de un Modelo de Clasificación permite evaluar el `error` asociado al proceso de predicción.

::: {.callout}

Clase Positiva
: Corresponde a la clase/evento de interés. Ej: Tiene cancer, va a pagar su deuda, es un gato. Normalmente se denota como la `Clase 1`.

Clase Negativa
: Corresponde a la clase/evento contrario al de interés. Ej: No tiene cancer, no va a pagar su deuda, no es un gato. Normalmente se denota como la `Clase 0`. 
:::

::: {.callout-tip}
`Scikit-Learn` usa la siguiente convención:

* Si se llama `*_score` un mayor puntaje es mejor.
* Si se llama `*_error` o `*_loss` un mejor puntaje es mejor.
:::

## Métricas: Matriz de Confusión {.smaller}

> La Matriz de Confusión ordena los valores correctamente predichos y también los distintos errores que el modelo puede cometer.

::: {.columns}
::: {.column width="40%"}
![](img/clase-9/conf_mat.png){.lightbox fig-align="center"}
:::
::: {.column width="60%"}
TP (Verdaderos Positivos)
: Corresponde a valores reales de la clase 1 que fueron correctamente predichos como clase 1.

TN (Verdaderos Negativos)
: Corresponde a valores reales de la clase 0 que fueron correctamente predichos como clase 0.

FP (Falsos Positivos)
: Corresponde a valores reales de la clase 0 que fueron incorrectamente predichos como clase 1.

FN (Falsos Negativos)
: Corresponde a valores reales de la clase 1 que fueron incorrectamente predichos como clase 0.
:::
::: 

## Métricas: A partir de la Matriz de Confusión {.smaller}

::: {.columns}
::: {.column}
::: {.callout appearance="default"}
## Accuracy
$$\frac{TP + TN}{TP + TN + FP + FN}$$
:::

::: {.callout appearance="default"}
## Precision
$$\frac{TP}{TP + FP}$$
:::

:::
::: {.column}
::: {.callout appearance="default"}
## Recall
$$\frac{TP}{TP + FN}$$
:::
::: {.callout appearance="default"}
## F1-Score
$$\frac{2\cdot Precision \cdot Recall}{Precision + Recall} = \frac{2 \cdot TP}{2\cdot TP + FP + FN}$$
:::
:::
::: 

::: {.callout-tip}
* **Accuracy** es probablemente la métrica más sencilla y más utilizada.
* **Precision** y **Recall** ponderarán distintos errores (FP y FN respectivamente) con mayor severidad. Ambas métricas son Antagonistas. 
* **F1-Score** corresponde a la media armónica del **Precision** y **Recall,** y tiende a ponderar los errores de manera más balanceada. 
:::

### ¿Cuándo utilizar cada tipo de error?

## Curva ROC {.smaller}

La curva ROC fue desarrollada en 1950 para analizar señales ruidosas. La curva ROC permite al operador contrapesar la tasa de verdaderos positivos (Eje $y$) versus los falsos positivos (Eje x).

> El área bajo la curva representa la `calidad del modelo`. Una manera de interpretarla es como la probabilidad de que una predicción de la clase positiva tenga mayor probabilidad que una de clase negativa. En otras palabras, **mide que las probabilidades se encuentren correctamente ordenadas**. Por lo tanto varía entre 0.5 y 1. 


::: {.columns}
::: {.column}
![](img/clase-9/roc.png){.lightbox fig-align="center" width="80%"}
:::
::: {.column}

##### ROC $\sim$ 0.5
![](img/clase-9/roc_ex_1.png){.lightbox fig-align="center" width="90%"}

##### ROC $\sim$ 1
![](img/clase-9/roc_ex_2.png){.lightbox fig-align="center" width="90%"}
:::
::: 

## Implementación en Python

```{.python code-line-numbers="|1|3-6|7|"}
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

accuracy_score(y_true, y_pred)
precision_score(y_true, y_pred)
recall_score(y_true, y_pred)
f1_score(y_true, y_pred)
roc_auc_score(y_true, y_proba)
```

* **y_true**: Corresponde a las etiquetas reales del Dataset.
* **y_pred**: Corresponde a las predicciones realizadas por el modelo. 
* **y_proba**: Corresponden a las probabilidades predichas por el modelo (si es que el modelo lo permite). 


## Implementación en Python: Matriz de Confusión

```{.python code-line-numbers="|1|3|"}
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_true, y_pred)
```

![](img/clase-9/conf_mat_sklearn.png){.lightbox fig-align="center" }

## Implementación en Python: Curva ROC

```{.python code-line-numbers="|1|3|"}
from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_predictions(y_true, y_proba)
```

![](img/clase-9/roc_curve_sklearn.png){.lightbox fig-align="center" }


# Sobre el aprendizaje de un Modelo

## Curva de Aprendizaje: Training {.smaller}

![](img/clase-9/training_curve.png){.lightbox fig-align="center"}

::: {.callout-note}
¿Qué sería la Complejidad del Modelo?
:::

## Curva de Aprendizaje: Validación {.smaller}

![](img/clase-9/validation_curve.png){.lightbox fig-align="center"}

::: {.callout-caution}
¿Por qué el modelo pierde rendimiento cuando aumenta su Complejidad?
:::

## Curva de Aprendizaje: Mejor Ajuste {.smaller}

::: {.columns}
::: {.column width="65%"}
![](img/clase-9/learning_curve.png){.lightbox fig-align="center"}
:::
::: {.column width="35%"}
::: {.callout-important}
## **Overfitting**: 
Gran diferencia entre Training y Validation Score.

:::
::: {.callout-caution appearance="default"}
## **Underfitting**: 
Poca diferencia entre Training y Validation Score, pero con ambos puntajes *"relativamente bajos"*. 
:::
::: {.callout-note}
## **Proper fitting**:
Corresponde al mejor puntaje en el set de Validación. Donde también la distancia entre Train y Test es ***poca***.
:::
:::
::: 


## Evaluación {.smaller}

> La evaluación de modelos supervisados es fundamental. De no hacerlo de forma correcta podemos quedarnos con una idea muy equivocada del rendimiento del modelo.

::: {.callout-caution}
Un modelo con un rendimiento incorrecto puede entregar predicciones completamente inútiles. 
:::

Cross Validation (Validación Cruzada)
: > Se debe evaluar el rendimiento de un modelo en un `dataset diferente` al que fue entrenado. Esta es la única manera en la que se puede medir el poder de generalización.

Generalización
: > Corresponde a la habilidad de un modelo de adaptarse apropiadamente a datos no vistos previamente. 

::: {.callout-important}
Para esto se asume que todos los datos son i.i.d (`independent and identically distributed`). De no lograr esto, lograr buenos rendimientos es más difícil.
:::

## Complejidad de un Modelo

#### ¿Qué modelo es un mejor clasificador?

![](img/clase-9/overfitting.png){.lightbox fig-align="center" width="70%"}

## Complejidad de un Modelo {.smaller}

::: {.columns}
::: {.column}

### Overfitting
![](img/clase-9/overf.png){.lightbox fig-align="center" width="80%"}
:::
::: {.column}

### Underfitting
![](img/clase-9/underf.png){.lightbox fig-align="center" width="80%"}
:::
::: 

::: {.callout-tip .fragment}
**Regularización**: Se refiere a una penalización que damos al modelo para que `disminuya su complejidad` de modo tal que evite el ***Overfitting***. En el caso de KNN, $K$ es un parámetro de regularización.
:::

# Esquemas de Validación

## Validación Cruzada: Holdout {.smaller}

> También es conocido como `Train Test Split` o simplemente `Split`. Corresponde a la separacion de nuestra data cuando con el proposito de aislar observaciones que el modelo no vea para una correcta evaluación.

::: {.columns}
::: {.column width="30%"}
![](img/clase-9/dataset.png){.lightbox fig-align="center" width="85%"}
:::
::: {.column width="30%"}

::: {.r-stack}

![](img/clase-9/trainset.png){.lightbox fig-align="center" width="85%" .fragment fragment-index=1}

![](img/clase-9/split.png){.lightbox fig-align="center" width="85%" .fragment fragment-index=2}
::: 
:::
::: {.column width="40%"}

::: {.callout-caution .fragment fragment-index=1}
* El `train set` es la porción de los datos que se utilizará `exclusivamente` para entrenar los datos. 
:::

::: {.callout-tip .fragment fragment-index=2}
* El `test set` es la porción de los datos que se utilizará exclusivamente para validar los datos. 
* El `test set` simula los datos que eventualmente entrarán el modelo para obtener una predicción. 
:::

::: {.callout-note .fragment fragment-index=3}
* Normalmente se utilizan splits del tipo 70/30, 80/20 o 90/10.
:::

::: {.callout-important .fragment fragment-index=4}
* ¿Cuál es el problema con este tipo de validación?
:::

:::
::: 

## Variante Holdout

::: {.columns}
::: {.column}
::: {.r-stack}
![](img/clase-9/trainset.png){.lightbox fig-align="center" .fragment fragment-index=1}

![](img/clase-9/train-val.png){.lightbox fig-align="center" .fragment fragment-index=2}

![](img/clase-9/full-holdout.png){.lightbox fig-align="center" .fragment fragment-index=3}
:::
:::
::: {.column}
::: {.callout-warning .fragment fragment-index=2}
* Se agrega un `validation set` el cuál se utilizará para escoger los `hiperparámetros` que muestren un mejor poder de generalización.
:::

::: {.callout-tip .fragment fragment-index=3}
* El `train set` y el `test set` cumplen la misma función que tenían antes.
:::
:::
::: 

## Variante Holdout: Procedimiento {.smaller}

::: {.columns}
::: {.column width="30%"}
![](img/clase-9/full-holdout.png){.lightbox fig-align="center" }
:::
::: {.column width="70%" .fragment}

#### Procedimiento

* Repetir para cada `Modelo` a probar.

::: {.callout-important}
* Vamos a entender un modelo como la combinación de un `Algoritmo de Aprendizaje` + `Hiperparámetros` + `Preprocesamiento`.
:::

::: {.incremental}
1. Se entrena cada `Modelo` en el `train set`. Se mide una métrica de Evaluación apropiada utilizando el `Validation Set`. La llamaremos ***métrica de Validación***.
2. Se escoge el mejor `Modelo` como el que tenga la mejor ***métrica de Validación***.
3. Se reentrena el ***modelo escogido*** pero ahora en un *"nuevo set"* compuesto por el `Train set` + el `Validation set`. 
4. Se reporta el rendimiento final del ***mejor modelo*** (al momento del diseño) utilizando métricas medidas en el `Test Set`.
:::

:::
::: 

## K-Fold CV {.smaller}

::: {.callout-caution}
* El proceso de Holdout podría llevar a un proceso de overfitting del Test Set si el modelo no es lo suficientemente robusto. 
* Definiremos la robustez más adelante.
:::


::: {.columns}
::: {.column width="40%"}

::: {.callout-note }
El K-Fold CV se aplica sólo al `Train Set` y la métrica final que se reporta utilizando el `Test Set`.
:::

![](img/clase-9/K-Fold.png){.lightbox fig-align="center" }
:::
::: {.column width="60%"}
Fold
: Entenderemos Folds como divisiones que haremos a nuestro dataset. (En el ejemplo se divide el dataset en 5 Folds). 

Split
: Entenderemos Splits, como iteraciones. En cada iteración utilizaremos un Fold como `Validation Set` y todos los Folds restantes como `Train Set`. 

::: {.callout-important}
* La métrica final se calculará como el promedio de las ***Métricas de Validación*** para cada `Split`. 
* A veces la variabilidad (medido a través de la Desviación Estándar) también es usado como criterio para elegir el mejor modelo. 
:::

:::
::: 

## Bootstrap {.smaller}

Consiste en generar subgrupos aleatorios con repetición. Normalmente requiere específicar el tamaño de la muestra de entrenamiento. Y la cantidad de repeticiones que del proceso. Los sets de validación (en morado) acá se denominan ***out-of-bag*** samples.

::: {.columns}
::: {.column width="40%"}
![](img/clase-9/data_boot.png){.lightbox fig-align="center" }

::: {.callout-tip}
* La métrica final a reportar se mide como el promedio de los ***out-of-bag*** samples.
:::
:::
::: {.column width="60%"}
![](img/clase-9/iter_boot.png){.lightbox fig-align="center" width="50%"}
:::
::: 

## Variantes y Consejos {.smaller}

Stratified K-Fold
: Es la variante más utilizada de K-Fold el cual genera los folds considerando que se mantenga la proporción de etiquetas en cada Fold.

Leave One Out
: Sería una variante con $K=n$. Por lo tanto, el `Validation Set` tiene sólo una observación. 

::: {.callout-tip appearance="default" style="font-size: 130%;"}

#### ¿Cuando usar cada uno?

* Si se tiene una cantidad de datos suficiente (normalmente tamaños muy grandes se prefiere) el **Holdout.** 
  * Entre más registros, menos % de `Validation Set` se deja.
* Si se requiere robustez, o hay Test sets que son muy variables se prefiere **K-Fold.**
  * Si es que hay desbalance de clases, se prefiere la versión **Stratified**.  
* Si se tienen ***muy pocos*** datos, entonces utilizar **Leave-One-Out**.
* **Bootstrap** también es utilizado cuando se tengan pocos datos. Aunque suele ser un approach más estadístico.
:::

## Baseline 

> Un modelo `Baseline` es un modelo simple, normalmente sin aprendizaje asociado o con poder de aprendizaje más limitado, el cuál será utilizado como medida de referencia para ver si algoritmos más complejos efectivamente están aprendiendo.

::: {.callout-note}
Si estamos probando un nuevo modelo y éste es capaz de **superar el rendimiento de un Baseline**, se considera como que estamos aprendiendo algo nuevo.
:::

::: {.callout-caution}
Modelos que **no superaron** el puntaje de un modelo Baseline normalmente son deshechados.
:::

## Implementación en Python: Baselines

```{.python code-line-numbers="|1|3|4|5|"}
from sklearn.dummy import DummyClassifier

dc = DummyClassifier(strategy="prior", random_state = 42, constant=None)
dc.fit(X_train,y_train)
y_pred = dc.predict(X_test)
```

<br>

::: {style="font-size: 75%;"}
* **strategy**: Corresponde a estrategias *"dummy"* con las cuales generar predicciones.
    * *"prior"*: predice siempre la clase más frecuente observada en el entrenamiento. Si se predice la probabilidad, se devuelve la probabilidad empírica. 
    * *"constant"*: Devuelve un valor constante provisto por el usuario.
    * *"uniform"*: Predice probabilidades aleatorios obtenidas mediante una distribución uniforme. 

:::

## Data Leakage {.smaller}

Fuga de Datos
: > Se refiere al proceso donde el modelo por alguna razón conoce información que no debería conocer. Puede ser información del `Test Set` o variables que revelan información primordial sobre la etiqueta.

::: {.callout-important}
Cuando existe `Data Leakage` es posible que los resultados del modelo no reflejen correctamente su rendimiento dando una falsa sensación de optimismo.
:::

### Ejemplos

* Estandarizar o aplicar preprocesamientos antes del Split de la Data.
* Utilizar variables que tienen directa relación con el Target. 

::: {.callout-tip .fragment}
Se recomienda siempre que sea posible utilizar `Pipelines` para poder evitar el **Data Leakage**. 
:::

# Arrivederci!


::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-411 Minería de Datos</span> está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::