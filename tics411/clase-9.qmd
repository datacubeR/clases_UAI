---
title: "TICS-411 Minería de Datos"
subtitle: "Clase 8: Evaluación de Modelos"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo.jpg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---


# Evaluación de Modelos Supervisados 


## Intuición {.smaller}

#### Si tengo que estudiar para una prueba donde tengo que calcular el Coeficiente de Silueta.

::: {.incremental}
* Qué pasa si sólo les entrego una pregunta para estudiar y no tiene respuesta.
* ¿Qué pasa si ahora les doy la respuesta?
* ¿Qué pasa si te doy más ejercicios?
* ¿Qué pasa luego de que haces muchos ejercicios?
:::

::: {.callout-tip .fragment}
Voy `aprendiendo mejor` la `tarea` de calcular el coeficiente de Silueta. **Lo mismo pasa con los modelos**.
:::

::: {.callout-important .fragment}
Pero no puedo medir qué tan bien aprendiste en los ejercicios que yo ya entregué para practicar. **Tengo que hacer una prueba que tú no hayas visto, para ver si realmente aprendiste**.
:::

## Uso de un Modelo {.smaller}

![](img/clase-9/model_usage.png){.lightbox fig-align="center" width="85%"}


::: {.callout-warning .fragment}
**¿Cómo saber que el modelo está funcionando como esperamos?**
:::

## Métricas

El Rendimieto de un Modelo de Clasificación permite evaluar el `error` asociado al proceso de predicción.

::: {.callout}

Clase Positiva
: Corresponde a la clase/evento de interés. Ej: Tiene cancer, va a pagar su deuda, es un gato. Normalmente se denota como la `Clase 1`.

Clase Negativa
: Corresponde a la clase/evento contrario al de interés. Ej: No tiene cancer, no va a pagar su deuda, no es un gato. Normalmente se denota como la `Clase 0`. 
:::

## Métricas: Matriz de Confusión {.smaller}

> La Matriz de Confusión ordena los valores correctamente predichos y también los distintos errores que el modelo puede cometer.

::: {.columns}
::: {.column width="40%"}
![](img/clase-9/conf_mat.png){.lightbox fig-align="center"}
:::
::: {.column width="60%"}
TP (Verdaderos Positivos)
: Corresponde a valores reales de la clase 1 que fueron correctamente predichos como clase 1.

TN (Verdaderos Negativos)
: Corresponde a valores reales de la clase 0 que fueron correctamente predichos como clase 0.

FP (Falsos Positivos)
: Corresponde a valores reales de la clase 0 que fueron incorrectamente predichos como clase 1.

FN (Falsos Negativos)
: Corresponde a valores reales de la clase 1 que fueron incorrectamente predichos como clase 0.
:::
::: 

## Métricas: A partir de la Matriz de Confusión {.smaller}

::: {.columns}
::: {.column}
::: {.callout appearance="default"}
## Accuracy
$$\frac{TP + TN}{TP + TN + FP + FN}$$
:::

::: {.callout appearance="default"}
## Precision
$$\frac{TP}{TP + FP}$$
:::

:::
::: {.column}
::: {.callout appearance="default"}
## Recall
$$\frac{TP}{TP + FN}$$
:::
::: {.callout appearance="default"}
## F1-Score
$$\frac{2\cdot Precision \cdot Recall}{Precision + Recall} = \frac{2 \cdot TP}{2\cdot TP + FP + FN}$$
:::
:::
::: 

::: {.callout-tip}
* **Accuracy** es probablemente la métrica más sencilla y más utilizada.
* **Precision** y **Recall** ponderarán distintos errores (FP y FN respectivamente) con mayor severidad. Ambas métricas son Antagonistas. 
* **F1-Score** corresponde a la media armónica del **Precision** y **Recall,** y tiende a ponderar los errores de manera más balanceada. 
:::

### ¿Cuándo utilizar cada tipo de error? (Pendiente de ejemplos). 

## Curva ROC {.smaller}

La curva ROC fue desarrollada en 1950 para analizar señales ruidosas. La curva ROC permite al operador contrapesar la tasa de verdaderos positivos (Eje $y$) versus los falsos positivos (Eje x).

> El área bajo la curva representa la `calidad del modelo`. Una manera de interpretarla es como la probabilidad de que una predicción de la clase positiva tenga mayor probabilidad que una de clase negativa. En otras palabras, que las probabilidades se encuentren correctamente ordenadas. Por lo tanto varía entre 0.5 y 1. 


::: {.columns}
::: {.column}
![](img/clase-9/roc.png){.lightbox fig-align="center" width="80%"}
:::
::: {.column}

##### ROC $\sim$ 0.5
![](img/clase-9/roc_ex_1.png){.lightbox fig-align="center" width="90%"}

##### ROC $\sim$ 1
![](img/clase-9/roc_ex_2.png){.lightbox fig-align="center" width="90%"}
:::
::: 

## Evaluación {.smaller}

> La evaluación de modelos supervisados es fundamental. De no hacerlo de forma correcta podemos quedarnos con una idea muy equivocada del rendimiento del modelo.

* Esto es importante ya que un modelo con un rendimiento incorrecto puede entregar predicciones completamente inútiles. 

Cross Validation (Validación Cruzada)
: > Se debe evaluar el rendimiento de un modelo en un `dataset diferente` al que fue entrenado. Esta es la única manera en la que se puede medir el poder de generalización.

Generalización
: > Corresponde a la habilidad de un modelo de adaptarse apropiadamente a datos no vistos previamente. 

::: {.callout-important}
Para esto se asume que todos los datos son i.i.d (`idependent and identically distributed`). De no lograr esto, lograr buenos rendimientos es más difícil.
:::

## Validación Cruzada: Holdout {.smaller}

> También es conocido como `Train Test Split` o simplemente `Split`. Corresponde a la separacion de nuestra data cuando con el proposito de aislar observaciones que el modelo no vea para una correcta evaluación.

::: {.columns}
::: {.column width="30%"}
![](img/clase-9/dataset.png){.lightbox fig-align="center" width="85%"}
:::
::: {.column width="30%"}

::: {.r-stack}

![](img/clase-9/trainset.png){.lightbox fig-align="center" width="85%" .fragment fragment-index=1}

![](img/clase-9/split.png){.lightbox fig-align="center" width="85%" .fragment fragment-index=2}
::: 
:::
::: {.column width="40%"}

::: {.callout-caution .fragment fragment-index=1}
* El `train set` es la porción de los datos que se utilizará `exclusivamente` para entrenar los datos. 
:::

::: {.callout-tip .fragment fragment-index=2}
* El `test set` es la porción de los datos que se utilizará exclusivamente para validar los datos. 
* El `test set` simula los datos que eventualmente entrarán el modelo para obtener una predicción. 
:::

::: {.callout-note .fragment fragment-index=3}
* Normalmente se utilizan splits del tipo 70/30, 80/20 o 90/10.
:::

::: {.callout-important .fragment fragment-index=4}
* ¿Cuál es el problema con este tipo de validación?
:::

:::
::: 

## Variante Holdout

::: {.columns}
::: {.column}
::: {.r-stack}
![](img/clase-9/trainset.png){.lightbox fig-align="center" .fragment fragment-index=1}

![](img/clase-9/train-val.png){.lightbox fig-align="center" .fragment fragment-index=2}

![](img/clase-9/full-holdout.png){.lightbox fig-align="center" .fragment fragment-index=3}
:::
:::
::: {.column}
::: {.callout-warning .fragment fragment-index=2}
* Se agrega un `validation set` el cuál se utilizará para escoger los `hiperparámetros` que muestren un mejor poder de generalización.
:::

::: {.callout-tip .fragment fragment-index=3}
* El `train set` y el `test set` cumplen la misma función que tenían antes.
:::
:::
::: 

## Variante Holdout: Procedimiento {.smaller}

::: {.columns}
::: {.column width="30%"}
![](img/clase-9/full-holdout.png){.lightbox fig-align="center" }
:::
::: {.column width="70%" .fragment}

#### Procedimiento

* Repetir para cada `Modelo` a probar.

::: {.callout-important}
* Vamos a entender un modelo como la combinación de un `Algoritmo de Aprendizaje` + `Hiperparámetros` + `Preprocesamiento`.
:::

::: {.incremental}
1. Se entrena cada `Modelo` en el `train set`. Se mide una métrica de Evaluación apropiada utilizando el `Validation Set`. La llamaremos ***métrica de Validación***.
2. Se escoge el mejor `Modelo` como el que tenga la mejor ***métrica de Validación***.
3. Se reentrena el ***modelo escogido*** pero ahora en un *"nuevo set"* compuesto por el `Train set` + el `Validation set`. 
4. Se reporta el rendimiento final del ***mejor modelo*** (al momento del diseño) utilizando métricas medidas en el `Test Set`.
:::

:::
::: 

## K-Fold CV {.smaller}

::: {.callout-caution}
* El proceso de Holdout podría llevar a un proceso de overfitting del Test Set si el modelo no es lo suficientemente robusto. 
* Definiremos la robustez más adelante.
:::


::: {.columns}
::: {.column width="40%"}

::: {.callout-note }
El K-Fold CV se aplica sólo al `Train Set` y la métrica final que se reporta utilizando el `Test Set`.
:::

![](img/clase-9/K-Fold.png){.lightbox fig-align="center" }
:::
::: {.column width="60%"}
Fold
: Entenderemos Folds como divisiones que haremos a nuestro dataset. (En el ejemplo se divide el dataset en 5 Folds). 

Split
: Entenderemos Splits, como iteraciones. En cada iteración utilizaremos un Fold como `Validation Set` y todos los Folds restantes como `Train Set`. 

::: {.callout-important}
* La métrica final se calculará como el promedio de las ***Métricas de Validación*** para cada `Split`. 
* A veces la variabilidad (medido a través de la Desviación Estándar) también es usado como criterio para elegir el mejor modelo. 
:::

:::
::: 

## Bootstrap {.smaller}

Consiste en generar subgrupos aleatorios con repetición. Normalmente requiere específicar el tamaño de la muestra de entrenamiento. Y la cantidad de repeticiones que del proceso. Los sets de validación (en morado) acá se denominan ***out-of-bag*** samples.

::: {.columns}
::: {.column width="40%"}
![](img/clase-9/data_boot.png){.lightbox fig-align="center" }

::: {.callout-tip}
* La métrica final a reportar se mide como el promedio de los ***out-of-bag*** samples.
:::
:::
::: {.column width="60%"}
![](img/clase-9/iter_boot.png){.lightbox fig-align="center" width="50%"}
:::
::: 

## Variantes y Consejos {.smaller}

Stratified K-Fold
: Es la variante más utilizada de K-Fold el cual genera los folds considerando que se mantenga la proporción de etiquetas en cada Fold.

Leave One Out
: Sería una variante con $K=n$. Por lo tanto, el `Validation Set` tiene sólo una observación. 

::: {.callout-tip appearance="default" style="font-size: 130%;"}

#### ¿Cuando usar cada uno?

* Si se tiene una cantidad de datos suficiente (normalmente tamaños muy grandes se prefiere) el **Holdout.** 
  * Entre más registros, menos % de `Validation Set` se deja.
* Si se requiere robustez, o hay Test sets que son muy variables se prefiere **K-Fold.**
  * Si es que hay desbalance de clases, se prefiere la versión **Stratified**.  
* Si se tienen ***muy pocos*** datos, entonces utilizar **Leave-One-Out**.
* **Bootstrap** también es utilizado cuando se tengan pocos datos. Aunque suele ser un approach más estadístico.
:::

## Data Leakage

Fuga de Datos
: Se refiere al proceso donde el modelo por alguna razón conoce información que no debería conocer. Puede ser información del `Test Set` o variables que revelan información primordial sobre la etiqueta.

::: {.callout-tip}
Se recomienda siempre que sea posible utilizar `Pipelines` para poder evitar el **Data Leakage**. 
:::


## Baseline
TODO: Crear baselines...
# Arrivederci!


::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-411 Minería de Datos</span> está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::