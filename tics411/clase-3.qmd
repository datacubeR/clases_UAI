---
title: "TICS-411 Minería de Datos"
subtitle: "Modelación Descriptiva y K-Means"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo.jpg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
---

# Modelación Descriptiva (Aprendizaje no Supervisado)

## Definiciones {.smaller}

Aprendizaje No supervisado
: > Es un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender. 

::: {.callout-note}
En nuestro caso nos enfocaremos en un caso particular de Modelación Descriptiva llamada Clustering.
:::

Clustering
: > Consiste en agrupar los datos en un menor número de `entidades/grupos`. A estos grupos se les conoce como `clusters` y pueden ser agrupados de manera global, o modelando las principales características de los datos. 


## Intuición {.smaller}
#### ¿Cuántos clusters se pueden apreciar?


::: {.r-stack}
![](img/clase-3/0_clusters.png){.lightbox fig-align="center" .fragment}

![](img/clase-3/2_clusters.png){.lightbox fig-align="center" .fragment}

![](img/clase-3/3_clusters.png){.lightbox fig-align="center" .fragment}
:::


# Clustering

## Clustering: Introducción

::: {.callout-note .fragment}
Clustering: Consiste en buscar grupos de objetos tales que la similaridad `intra-grupo` sea alta, mientras que la similaridad `inter-grupos` sea baja. Normalmente la distancia es usada para determinar **qué tan similares** son estos grupos. 
:::

::: {.columns}
::: {.column}
![](img/clase-3/intra_grupo.png){.lightbox fig-align="center" .fragment width="70%"}
:::
::: {.column}
![](img/clase-3/inter_grupo.png){.lightbox fig-align="center" .fragment width="70%"}
:::
::: 

## Clustering: Evaluación {.smaller}

::: {.callout-important}
* Evaluar el nivel del éxito o logro del Clustering es complicado. **¿Por qué?**
:::


![](img/clase-3/ambiguo.png){.lightbox fig-align="center" width="60%"}

## Clustering: Tipos

<br>

::: {.columns}
::: {.column}
![](img/clase-3/clust_tipo1.png){.lightbox fig-align="center" }
:::
::: {.column}
![](img/clase-3/clust_tipo2.png){.lightbox fig-align="center" }
:::
::: 


## Clustering: Tipos

<br>

![](img/clase-3/clust_tipo3.png){.lightbox fig-align="center" }


## Clustering: Partición

> Los datos son separados en `K` clusters, donde cada punto pertenece exclusivamente a un `único` cluster.

![](img/clase-3/particion.png){.lightbox fig-align="center" }

## Clustering: Densidad

> Se basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral.

![](img/clase-3/densidad.png){.lightbox fig-align="center" }

## Clustering: Jerarquía {.smaller}

> Los algoritmos basados en jerarquía pueden seguir 2 estrategias: 

* **Aglomerativos*n: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. `Para n objetos se realizan n-1 uniones`. 

* **Divisionales**: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad. 


![](img/clase-3/jerarquia.png){.lightbox fig-align="center" width="70%"}


## Clustering: Probabilístico {.smaller}

Se ajusta cada punto a una distribución de probabilidades que indica cuál es la probabilidad de pertenencia a dicho cluster. 

::: {.columns}
::: {.column}
![](img/clase-3/gmm_1.png){.lightbox fig-align="center" width="80%"}
:::
::: {.column}
![](img/clase-3/gmm_2.png){.lightbox fig-align="center" }
:::
::: 

# Métodos Basados en Partición

## Partición

> Los datos son separados en K Clusters, donde cada punto pertenece exclusivamente a un único cluster. 

::: {.callout-tip}
* Cluster Compactos: Minimizar la `distancia intra-cluster` (within cluster). 
* Clusters bien separados: Maximizar la `distancia inter-cluster` (between cluster).
:::

$$ Score (C,D) = f(wc(C),bc(C))$$

El puntaje/score mide la calidad del clustering $C$ para el Dataset $D$. 

## Score

$$ Score (C,D) = f(wc(C),bc(C))$$

<br>

::: {.columns style="font-size: 70%;"}
::: {.column}
* Distancia Between-Cluster:
  $$bc(C) = \sum_{1 \le j \le k \le K} d(r_j, r_k)$$
  

donde $r_k$ representa el centro del cluster $k$:
  $$r_k = \frac{1}{n_k} \sum_{x_i \in C_k} x_i$$

:::
::: {.column}
* Distancia Within-Cluster:
  $$wc(C) = \sum_{k=1}^K \sum_{x_i \in C_k} d(x_i, r_k)$$

:::
::: 

::: {.columns}
::: {.column style="font-size: 70%;"}
::: {.callout-tip}
Distancia entre los centros de cada cluster.
:::
:::
::: {.column style="font-size: 70%;"}
::: {.callout-tip}
Distancia entre todos los puntos del cluster y su respectivo centro. 
:::
:::
::: 

## K-Means {.smaller}

K-Means
: > Dado un número de clusters $K$ (determinado por el usuario), cada cluster es asociado a un centro (centroide). Luego, cada punto es asociado al cluster con el centroide más cercano. 

::: {.columns}
::: {.column}
![](img/clase-3/particion.png){.lightbox fig-align="center" width="90%"}
:::
::: {.column}
Normalmente se utiliza la Distancia Euclideana como medida de `similaridad`.

::: {.incremental}
1. Se seleccionan $K$ puntos como centroides iniciales.
2. Repite:
   - Forma K clusters asignando todos los puntos al centroide más cercano. 
   - Recalcula el centroide para cada clase como la media de todos los puntos de dicho cluster.
- Se repite este procedimiento por un **número finito de iteraciones** o hasta que los **centroides no cambien**.  
:::
:::
::: 


## K-Means: Ejemplo {.smaller}

#### Resolvamos el siguiente ejemplo. 

Supongamos que tenemos tipos de manzana, y cada una de ellas tiene 2 atributos (features). Agrupemos estos objetos en 2 grupos de manzanas basados en sus características. 

::: {.columns}
::: {.column}
![](img/clase-3/ej_1.png){.lightbox fig-align="center"} 
:::
::: {.column}
![](img/clase-3/ej_2.png){.lightbox fig-align="center"} 
:::
::: 


## K-Means: Ejemplo {.smaller}

#### 1era Iteración

::: {.columns}
::: {.column}
::: {.fragment fragment-index=1}
* Definamos los centroides. 
$$C_1 = (1,1)$$
$$C_2 = (2,1)$$
:::

::: {.fragment fragment-index=2}
* Calculemos la Matriz de Distancias:
:::

::: {.fragment fragment-index=2}
$$D^1 = \begin{bmatrix}
0 & 1 & 3.61 & 5\\
1 & 0 & 2.83 & 4.24
\end{bmatrix}$$
:::

::: {.fragment fragment-index=3}
* Calculemos la Matriz de Pertenencia $G$: 
:::

::: {.fragment fragment-index=4}
$$G^1 = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 1 & 1
\end{bmatrix}$$
:::
:::
::: {.column .fragment fragment-index=1}
![](img/clase-3/kmeans_1.png){.lightbox fig-align="center"}

::: {.fragment fragment-index=5}

Los nuevos centroides son:
$$C_1 = (1,1)$$
$$C_2 = (\frac{11}{3}, \frac{8}{3})$$
:::

:::

::: 


## K-Means: Ejemplo {.smaller}

#### 2da Iteración

::: {.columns}
::: {.column}
::: {.fragment fragment-index=1}
* Los nuevos centroides son:

$$C_1 = (1,1)$$
$$C_2 = (\frac{11}{3}, \frac{8}{3})$$
:::

::: {.fragment fragment-index=2}
* Calculemos la Matriz de Distancias:
:::

::: {.fragment fragment-index=2}
$$D^1 = \begin{bmatrix}
0 & 1 & 3.61 & 5\\
3.14 & 2.26 & 0.47 & 1.89
\end{bmatrix}$$
:::

::: {.fragment fragment-index=3}
* Calculemos la Matriz de Pertenencia $G$: 
:::

::: {.fragment fragment-index=4}
$$G^1 = \begin{bmatrix}
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1
\end{bmatrix}$$
:::
:::
::: {.column .fragment fragment-index=1}
![](img/clase-3/kmeans_2.png){.lightbox fig-align="center"}

::: {.fragment fragment-index=5}

Los nuevos centroides son:
$$C_1 = (1,1)$$
$$C_2 = (\frac{9}{2}, \frac{7}{2})$$
:::

:::

::: 

## K-Means: Ejemplo {.smaller}

![](img/clase-3/kmeans_3.png){.lightbox fig-align="center"}

::: {.callout-tip}
* Si seguimos iterando notaremos que ya no hay cambios en los clusters. El algoritmo converge. 
* Este es el resultado de usar $K=2$. Utilizar otro valor de $K$ entregará valores distintos. 
* **¿Es este el número de clusters óptimos?**
:::

## K-Means: Número de Clusters Óptimos {.smaller}

::: {.columns}
::: {.column width="70%"}
::: {.r-stack}

![](img/clase-3/optim_1.png){.lightbox fig-align="center" .fragment .fade-in-then-out}

![](img/clase-3/optim_2.png){.lightbox fig-align="center" .fragment}
:::
:::
::: {.column width="30%"}
::: {.callout-caution .fragment}
* Siempre es posible encontrar el número de clusters indicados. 
* Entonces, 

**¿Cómo debería escoger el valor de $K$?**
:::
:::
::: 

## K-Means: Número de Clusters Óptimos {.smaller}

Curva del Codo
: Es una heurísitca en la cual gráfica el valor de una métrica de distancia (e.g. within distance) para distintos valores de $K$. El valor óptimo de $K$ será el codo de la curva, que es el valor donde se estabiliza la métrica. 

::: {.columns}
::: {.column}
![](img/clase-3/elbow.png){.lightbox fig-align="center"} 
:::
::: {.column}
<br>

::: {.callout-important .fragment}
* Este valor del codo muchas veces es subjetivo y distintas apreciaciones pueden llegar a distintos $K$ óptimos. 
:::

::: {.callout-tip .fragment}
Eventualmente otras métricas distintas al `within distance` podrían también ser usadas. 
:::
:::
::: 

## K-Means: Detalles Técnicos

::: {.callout-note appearance="default"}
## Fortalezas

* Algoritmo relativamente eficiente (existen muy buenas implementaciones). $O(k \cdot n \cdot i)$. Donde $k$ es el número de clusters, $n$ el número de puntos, e $i$ el número de iteraciones. 
* Encuentra ***clusters esféricos***. 
:::

::: {.callout-warning appearance="default"}
## Debilidades

* Sensible al punto de inicio. 
* Solo se puede aplicar cuando el promedio es calculable. 
* Se requiere definir K a priori (K es un *hiperparámetro*). 
* Suceptible al ruido. 

:::


# Eso es todo

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-411 Minería de Datos</span> está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::