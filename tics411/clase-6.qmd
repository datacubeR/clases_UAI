---
title: "TICS-411 Miner칤a de Datos"
subtitle: "Clase 6: Evaluaci칩n de Clusters"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo.jpg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
---


# Evaluaci칩n de Clusters

## Evaluaci칩n {.smaller}

> Pensemos en la Evaluaci칩n como una medida de desempe침o el cu치l *"eval칰a"* qu칠 tan bien realizado est치 el clustering.
> El objetivo principal del Clustering debe ser siempre la generaci칩n de `clusters compactos` que est칠n `diferenciados` los unos a los otros.


::: {.columns}
::: {.column width="60%"}
![](img/clase-6/cluster_comparison.png){.lightbox fig-align="center" }
:::
::: {.column width="40%"}
::: {.callout-warning}

쮺u치l es el Clustering que mejor describe el problema.
:::
:::
::: 

## Objetivos de la Evaluaci칩n

![](img/clase-6/clustering_obj.png){.lightbox fig-align="center" }

## Tendencia: Hopkins {.smaller}

Estad칤stico Hopkins
: > Permite evaluar `a priori` si es que efectivamente existen clusters `antes de aplicar` un algoritmo. 

::: {.columns}
::: {.column}
$$H = \frac{\sum_{i = 1}^p w_i}{\sum_{i = 1}^p u_i + \sum_{i = 1}^p w_i}$$

* $w_i$: corresponde a la distancia de un punto *aleatorio* al vecino m치s cercano en los datos originales.
* $u_i$: corresponde a la distancia de un punto *real* del dataset al vecino m치s cercano.
* $p$: N칰mero de puntos generados en el espacio del Dataset.
:::
::: {.column}

```{.python }
from pyclustertend import hopkins

1-hopkins(X, p)
```
::: {.callout-tip}

* X: Dataset al cu치l se le aplica el Estad칤stico.
* p: N칰mero de Puntos para el c치lculo.
:::
::: {.callout-caution}
`pyclustertend` entrega el valor 1-H.
:::
:::
::: 



## Tendencia: Hopkins {.smaller}

![](img/clase-6/hopkins.png){.lightbox fig-align="center" width="70%" }

![](img/clase-6/hopkins_range.png){.lightbox fig-align="center" }



## C치lculo Hopkins: Ejemplo p=2{.smaller}


![](img/clase-6/hopkins_ex.png){.lightbox fig-align="center" width="70%"}

::: {.columns}
::: {.column width="30%"}

##### Puntos obtenidos de los Datos
$$u_1\approx 0$$

$$u_2\approx 0$$

:::
::: {.column width="40%"}

##### Puntos Aleatorios en el Espacio de los Datos
$$w_1\approx 1.8$$

$$w_2\approx 1.12$$

:::

::: {.column width="30%"}

##### C치lculo Hopkins

$$ H = \frac{w_1 + w_2}{u_1 + u_2 + w_1 + w_2}$$
$$ H = \frac{1.8 + 1.12}{0 + 0 + 1.8 + 1.8} = 1$$
:::
::: 

## Visual Assesment of Tendency (VAT) {.smaller}

> Corresponde a una inspecci칩n visual de la distancia entre los puntos (matriz de distancia). Colores m치s oscuros indican menor distancias entre dichos puntos lo que indica mayor cohesi칩n. 

::: {.columns}
::: {.column}
::: {.callout-tip}
Se pueden ver claramente dos bloques.
:::
![](img/clase-6/vat_iris.png){.lightbox fig-align="center" width="50%"}

:::
::: {.column}
::: {.callout-important}
No es posible ver bloques importantes.
:::
![](img/clase-6/vat_random.png){.lightbox fig-align="center" width="50%"}

:::
::: 

```{.python }
from pyclustertend import vat

vat(X)
```

## Correlaci칩n {.smaller}

#### Procedimiento: 
1. Construir una matriz de similaridad entre todos los puntos de la siguiente manera:

$$s(i,j) = \frac{1}{d(i,j) + 1}$$

2. Construir una matriz de similaridad *`"ideal"`* basada en la pertenencia a un Cluster.

    Si $i$ y $j$ pertenecen al mismo cluster entonces $s(i,j)=1$, en otro caso $s(i,j) = 0$

3. Calcular la Correlaci칩n entre la matriz de similaridad y la matriz ideal (obtenidas en los pasos 1 y 2). 

::: {.callout-note}
Una correlaci칩n alta indica que los puntos que est치n en el mismo cluster son cercanos entre ellos.
:::

## Cohesi칩n {.smaller} 

Cohesi칩n
: > Mide cu치n cercanos est치n los objetos dentro de un mismo cluster. Se utiliza la Suma de los Errores al Cuadrado, que es equivalente a la Inercia de K-Means (o Within Cluster). 

$$ SSE_{total} = \sum_{k = 1}^K\sum_{x_i \in C_k} (x_i - \bar{C_k})^2$$

* $C_k$ corresponde al Centroide del Cluster $k$. Dicho centroide puede ser calculado como la media/mediana de todos los puntos del Centroide.
* $K$ corresponde al N칰mero de Clusters.

::: {.callout-caution}

* No me gusta mucho este nombre, porque en realidad es como un `inverso de la Cohesi칩n`.

:::


## Separaci칩n {.smaller}

Separaci칩n
: > Mide cu치n distinto es un cluster de otro. Se usa la suma de las distancias al cuadrado entre los centroides hacia el promedio de todos los puntos. (Between groups sum squares, SSB).

$$ SSB_{total} = \sum_{k = 1}^K |C_k|(\bar{X} - \bar{C_k})^2$$

* $|C_k|$ corresponde al n칰mero de elementos (Cardinalidad) del Cluster $i$.
* $\bar{X}$ corresponde al promedio de todos los puntos.

## Coeficiente de Silhouette (Coeficiente de Silueta)

> El coeficiente de Silhouette es otra medida que combina la cohesi칩n y la separaci칩n. Los valores var칤an entre -1 y 1, donde valores cercanos a 1 representan una mejor agrupaci칩n.

::: {.callout-caution}
Valores cercanos a $-1$ representan que el punto est치 incorrectamente asignado a un cluster.
:::

$$S_i = \frac{b_i - a_i}{max\{a_i, b_i\}}$$

```{.python }
from sklearn.metrics import silhouette_score

silhouette_score(X, labels, sample_size = None, metric="euclidean")
```

## Coeficiente de Silhouette: Ejemplo {.smaller}

![](img/clase-6/silueta.png){.lightbox fig-align="center" width="60%"}

::: {.columns}
::: {.column}

![](img/clase-6/calculo_silueta.png){.lightbox fig-align="center" width="70%"}
:::
::: {.column}

$$C_{silueta} = \frac{1}{n}\sum_{i} s_i$$

* $a_i$: Distancia promedio del punto $i$ a todos los `otros` puntos del mismo cluster. (Cohesi칩n)
* $b_{ij}$: Distancia promedio del punto $i$ a todos los puntos del cluster $j$ donde no pertenezca $i$. (Separaci칩n)
* $b_j$: M칤nimo de $b_{ij}$ tal que el punto i no pertenezca al cluster $j$. (Menor Separaci칩n)

:::
::: 

## Ejercicio Propuesto {.smaller}

::: {.columns}
::: {.column}
![](img/clase-6/ej_silueta_df.png){.lightbox fig-align="center" }
:::
::: {.column}
![](img/clase-6/ej_silueta_plot.png){.lightbox fig-align="center"}
:::
::: 

::: {.callout-tip appearance="default"}
## Ejercicio Propuesto
Calcule el coeficiente de Silueta. Tabla de resultado al final de las Slides.
:::

## Curvas de Silueta {.smaller}

Es com칰n mostrar los resultados del coeficiente de silueta como gr치ficos de este estilo:

::: {.columns}
::: {.column}
![](img/clase-6/sil_1.png){.lightbox fig-align="center" width="60%"}
:::
::: {.column}
![](img/clase-6/sil_2.png){.lightbox fig-align="center" }
:::
::: 


::: {.callout-caution appearance="default"}
## Problemas
* Siluetas negativas.
* Clusters bajo el promedio.
* Mucha variabilidad de Silueta en un s칩lo cluster.
:::

## Curvas de Silueta: Implementaci칩n 

```{.python code-line-numbers="|1-2|4-5|"}
import scikitplot as skplt
import matplotlib.pyplot as plt

skplt.metrics.plot_silhouette(X, labels, metric="euclidean", title="Silhouette Analysis")
plt.show()
```

* L1-2: Importaci칩n de Librer칤as Necesarias. Esta implementaci칩n est치 en la librer칤a Scikit-plot. (Para instalar `pip install scikit-plot`)

* **X**: Dataset usado para el clustering. 
* **labels** : etiquetas obtenidos de alg칰n proceso de Clustering. 
* **metric**: M칠trica a utilizar, por defecto usa *"euclidean"*.
* **title**: Se puede agregar un T칤tulo personalizado a la curva.

# 춰Felicitaciones! 游꿀游꿀游꿀游꿀 Terminamos Clustering

## Resultados Ejercicio Propuesto 

![](img/clase-6/resultados_sil_prop.png){.lightbox fig-align="center"}

Coeficiente de Silhouette = 0.6148

::: {.callout-important}

Comprobar utilizando Scikit-Learn
:::

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-411 Miner칤a de Datos</span> est치 licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::