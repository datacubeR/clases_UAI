---
title: "TICS-411 Minería de Datos"
subtitle: "Clase 12: Naive Bayes"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo.jpg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Naive Bayes: Preliminares {.smaller}

> También conocido como `Clasificador Inexperto de Bayes`, es uno de los clasificadores más conocidos y sencillos. 

::: {.callout-note}
Se hizo particularmente conocido como uno de los primeros algoritmos en funcionar como Clasificador de Spam de manera efectiva.
:::

* Es un modelo netamente probabilístico basado en el `Teorema de Bayes`.
    * Aprende una distribucional de Probabilidad Condicional.
    * Dado un punto $x_i$, el modelo retorna la "probabilidad" de que $x_i$ pertenezca a una clase específica.

## Definiciones {.smaller}

::: {.columns}
::: {.column}
Probabilidad Condicional
: $$P(X|C) = \frac{P(X \cap C)}{P(C)}$$

Teorema de Bayes
: $$P(C|X) = \frac{P(X|C)P(C)}{P(X)}$$

Independencia Condicional
: $$P(X_1, X_2, ..., X_k|C) = \prod_{i=1}^k P(X_i|C)$$
:::
::: {.column}

<br>

::: {.callout-note}
Se lee como la Probabilidad de que Ocurra $X$ dado que tenemos $C$.
:::
<br>

::: {.callout-note}
La `probabilidad a posteriori` (LHS), depende de el ***Likelihood***, la ***probabilidad a priori*** y la ***evidencia*** (RHS). 
:::
<br>

::: {.callout-note}
Si asumimos ***independencia***, entonces la probabilidad conjunta de $k$ eventos, se calcula como la ***productoria de las probabilidades condicionales independientes***.
:::
:::
::: 

## Ejemplo básico

::: {.callout}
Supongamos que:

* Sabemos que la Meningitis produce Tortícolis el 50% de las veces.
* La probabilidad de tener meningitis es: $1/50000$.
* La probabilidad de tener Tortícolis: $1/20$.
:::

Si su paciente tiene tortícolis, ¿Cuál es la probabilidad de que tenga Meningitis?

$$P(M|T) = \frac{P(T|M)P(M)}{P(T)}=\frac{0.5 \cdot 1/50000}{1/20} = 0.0002$$

## Modelo Naive Bayes: Aprendizaje {.smaller}

$$P(y = C_j|X_1, X_2, ..., X_k) = \frac{P(X_1,X_2,..., X_k|y=C_j)P(y=C_j)}{P(X_1, X_2, ..., X_k)}$$

::: {.columns}
::: {.column width="60%"}

* $P(y=C_j|X)$ sería la probabilidad de que la predicción del modelo sea $C_j$ dado que lo alimentamos con las variables $X$.
* Luego $P(y=C_j)$ es la probabilidad *a priori* de que la clase sea $C_j$.
* $P(X|y=C_j)$ es el likelihood (verosimilitud). Corresponde a la distribución de probabilidad de las variables X cuando la clase es $C_j$.
* $P(X)$ es la evidencia, y normalmente es muy complejo de calcular.
:::
::: {.column width="40%"}
::: {.callout-caution}
Por simplicidad reduciremos $X_1, X_2, ..., X_k$ a $X$.
:::

::: {.callout-important}
$P(X)$ tiene como única función la de normalizar la probabilidad para que vaya en un rango entre 0 y 1. 
:::
:::
::: 


## Modelo Naive Bayes: Predicción 


$$\hat{y_i} = \underset{C_j}{argmax} \: P(y=C_j|X) $$

donde, 
$$P(y = C_j|X) \propto \prod_{i=1}^k P(X|y=C_j)P(y=C_j)$$

::: {.callout-tip}
La predicción de `Naive Bayes` corresponde a la clase que entrega ***[un estimado de]*** la `Probabilidad a Posteriori` más grande.
:::

## Ejemplo {.smaller}

::: {.columns}
::: {.column width="50%"}
![](img/clase-12/naive_data.png){.lightbox fig-align="center" width="70%"}
:::
::: {.column width="50%"}

::: {.callout}
¿Cómo clasificamos el siguiente punto?

$$ X = [C=Soleado,T=Media,H=Alta,V=Débil]$$
:::

::: {.callout-note appearance="default" icon="false"}
## Probabilidad de Sí
$$P(y = Sí|X) = P(X|y=Sí)P(y=Sí)$$
:::

::: {.callout-important appearance="default" icon="false"}
## Probabilidad de No

$$ P(y = No|X) = P(X|y=No)P(y=No)$$
:::
:::
::: 




## Ejemplo {.smaller}

::: {.callout-note icon="false"}
$$ P(y = Sí|X) = P(C=Soleado|y=Sí)P(T=Media|y=Sí)P(H=Alta|y=Sí)P(V=Débil|y=Sí)P(y=Sí)$$
:::

::: {.callout-important icon="false"}
$$ P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=Débil|y=No)P(y=No)$$
:::


::: {.columns}
::: {.column width="40%"}
#### Probabilidad Condicional para clase Sí
$$\small P(C = Soleado|y = Sí) = 2/9$$
$$\small P(T = Media|y = Sí) = 4/9$$

$$\small P(H = Alta|y = Sí) = 3/9$$
$$\small P(V=Débil|y = Sí) = 6/9$$
:::
::: {.column width="40%"}
#### Probabilidad Condicional para clase No
$$\small P(C = Soleado|y = No) = 3/5$$
$$\small P(T = Media|y = No) = 2/5$$

$$\small P(H = Alta|y = No) = 4/5$$
$$\small P(V=Débil|y = No) = 2/5$$
:::
::: {.column width="20%"}
##### Probabilidad a priori

$$P(y = Sí) = \frac{9}{14} = 0.642$$
$$P(y = No) = \frac{5}{14} = 0.357$$
:::
::: 

## Predicción

::: {.callout-note icon="false"}
$$\scriptsize P(y = Sí|X) = P(C=Soleado|y=Sí)P(T=Media|y=Sí)P(H=Alta|y=Sí)P(V=Débil|y=Sí)P(y=Sí)$$
$$\small P(y = Sí|X) = \frac{2}{9} \cdot \frac{4}{9} \cdot \frac{3}{9} \cdot \frac{6}{9} \cdot \frac{9}{14} = 0.0141$$
:::


::: {.callout-important icon="false"}
$$\scriptsize P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=Débil|y=No)P(y=No)$$
$$\small P(y = No|X) = \frac{3}{5} \cdot \frac{2}{5} \cdot \frac{4}{5} \cdot \frac{2}{5} \cdot \frac{5}{14} = 0.0274$$
:::

::: {.callout-tip}
$$\hat{y} = argmax \{0.0141, 0.0274\} = No$$
:::

## Smoothing {.smaller}

Supongamos otro dataset más pequeño:


::: {.columns}
::: {.column width="30%"}
![](img/clase-12/laplace_smoothing.png){.lightbox fig-align="center" width="60%"} 
:::
::: {.column width="70%"}

::: {.callout-warning}
Dado que `Naive Bayes` se calcula como una Productoria, al tener probabilidades 0 inmediatamente la ***Probabilidad a Posteriori es 0***.
:::
$$ P(Clima = Soleado|y = Sí) = \frac{0}{6}$$
$$ P(Clima = Soleado|y = No) = \frac{5}{8}$$


::: {style="font-size: 80%;"}
$$P(X_j|C = i) = \frac{N_{yj} + \alpha}{N_y + M\alpha}$$

* **$\alpha$**: Es un Hiperparámetro. Si $\alpha = 1$ se le llama Laplace Smoothing, si $\alpha <1$ entonces se le llama Lidstone Smoothing.
* **M**: Corresponde al número de posibles valores que puede tomar $X_j$
* **$N_{yj}$**: Corresponde a la cantidad de registros que toman el valor de la variable $X_j$ solicitado en la clase $y$.
* **$N_{y}$**: Corresponde a la cantidad de registros totales que tienen la clase $y$.
:::

:::
::: 

## Laplace Smoothing

::: {.columns}
::: {.column}
#### Sin Laplace
![](img/clase-12/sin_laplace.png){.lightbox fig-align="center" width="60%"} 
:::
::: {.column}

#### Con Laplace
![](img/clase-12/con_laplace.png){.lightbox fig-align="center" width="60%"} 
:::
::: 

::: {.callout-important}
En este caso $\alpha = 1$ y $M=3$ ya que Clima puede tomar 3 valores: Soleado, Cubierto y Lluvia.
:::

## Variables Continuas

::: {.columns}
::: {.column width="30%"}
![](img/clase-12/variable_continua.png){.lightbox fig-align="center" width="65%"} 
:::
::: {.column width="70%"}
::: {.callout-tip}
Podemos calcular el Likelihood como una PDF ***(Probability Density Function)***. La más común: Distribución Normal ***(Gaussian Naive Bayes)***. 
:::

$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

![](img/clase-12/gaussian_nb.png){.lightbox fig-align="center" width="35%"} 
:::
::: 

## Variables Continuas: Predicción


::: {.callout-note icon="false"}
$$P(humedad=74|y = Sí) = \frac{1}{\sqrt{2\pi \cdot 10.2^2}}e^{-\frac{(74-79.1)^2}{2\cdot 10.2^2}} = 0.0345 $$
:::

::: {.callout-important icon="false"}
$$P(humedad=74|y = No) = \frac{1}{\sqrt{2\pi \cdot 9.7^2}}e^{-\frac{(74-86.2)^2}{2\cdot 9.7^2}} = 0.01865 $$
:::

::: {.callout-tip}
Luego la predicción es Sí.
:::

## Detalles Técnicos

::: {.callout-tip appearance="default"}
## Fortalezas

* Fácil de Implementar
* A menudo tiene un rendimiento decente a pesar de que las variables pueden no ser independientes.
* Puede aprender de forma incremental.
* Valores faltantes son ignorados en el proceso de Aprendizaje.
* Modelo robusto frente a datos atípicos y/o irrelevantes.

:::
::: {.callout-important appearance="default"}
## Debilidades

* Asumir clases condicionadas produce probabilidades sesgadas.
* Dependencias entre las variables no pueden ser modeladas.
:::

## Implementación en Scikit-Learn

```{.python code-line-numbers="|1|3-5|6|7-9|11-12|"}
from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB(alpha = 1)
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)
y_proba = nb.predict_proba(X_test)
```

```{.python code-line-numbers="|1|3-5|6|7-9|11-12|"}
from sklearn.naive_bayes import GaussianNB

gb = MultinomialNB(alpha = 1)
gb.fit(X_train, y_train)

y_pred = gb.predict(X_test)
y_proba = gb.predict_proba(X_test)
```

# لقد إنتهينا


::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-411 Minería de Datos</span> está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::