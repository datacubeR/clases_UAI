---
title: "TICS-579-Deep Learning"
subtitle: "Clase 4: Model Training"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Entrenamiento de la Red {.smaller}


> A diferencia de un Modelo de Machine Learning, las Redes Neuronales se entrenan de manera progresiva (se espera una mejora en cada Epoch). Si nuestra Arquitectura es apropiada nosotros deber√≠amos esperar que el `Loss` de nuestra ***red siempre disminuya***. ¬øPor qu√©?

:::{.callout-tip .fragment icon=false appearance="default"}
## üí° Dado que el entrenamiento es progresivo, el modelo puede retomar su entrenamiento desde un set de pesos dados.
:::

::: {.callout-warning .fragment icon=false appearance="default"}
## **¬øSiempre buscamos la Red que tenga el mejor Loss de Entrenamiento?** ***¬øCu√°l es la diferencia entre el `Loss` y el `Rendimento del Modelo`?***
:::

::: {.callout-important .fragment icon=false appearance="default"}
## Al igual que en los modelos de Machine Learning debemos evitar a toda costa el Overfitting. ¬øQu√© es el overfitting?
:::

## Entrenamiento de la Red {.smaller}

Bias-Variance Tradeoff (Dilema Sesgo-Varianza)
: > Probablemente el concepto m√°s importante para determinar si un modelo tiene potencial o no. Corresponden a dos tipos de errores que pueden sufrir los modelos de ML. 

::: {.columns .fragment}
::: {.column .callout-note appearance="default" icon=false}
## Bias
Corresponde al sesgo, y tiene que ver con la diferencia entre el valor real y el valor predicho. Bajo sesgo implica una mejor predicci√≥n.
:::
::: {.column .callout-caution appearance="default" icon=false}
## Variance
Corresponde a la varianza y tiene que ver con la dispersi√≥n de los valores predichos. Baja Varianza implica un modelo m√°s estable y menos flexible.
:::

::: {.callout-important .fragment appearance="default" icon=false}
## En general hay que buscar el equilibrio entre ambos tipos de errores:

* Alto Sesgo y baja Varianza: **Underfitting**. 
* Bajo Sesgo y Alta Varianza: **Overfitting**. 

:::
:::

## Model Validation {.smaller}

Validaci√≥n Cruzada
: > Se refiere al proceso de entrenar un modelo en una cierta porci√≥n de los datos, pero validar sus rendimiento y capacidad de ***generalizaci√≥n*** en un set de datos ***no vistos*** por el modelo al momento de entrenar. 

::::{style="font-size: 120%;"}
::: {.callout-warning icon=false appearance="default" }
## ***¬øQu√© es la Generalizaci√≥n?***
:::

::: {.callout-note icon=false appearance="default" }
## Los dos m√©todos m√°s populares que se usan en Machine Learning son **Holdout** y **K-Fold.** M√°s m√©todos se pueden encontrar en los [docs de Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html). 
:::

::: {.callout-important icon=false appearance="default" }
## Debido a los vol√∫menes de datos utilizados, el esquema de validaci√≥n m√°s utilizado es el **Holdout**.
:::
::::

## Model Validation: Holdout {.smaller}

::: {.columns}
::: {.column}

![](img/clase-4/data_split.png){.lightbox fig-align="center"}
:::
::: {.column}

::: {.callout-note appearance="default" icon="false"}
## **Train**
Corresponde a la porci√≥n de utilizado para que el modelo aprenda.
:::
::: {.callout-important appearance="default" icon="false"}
## **Validation**
Corresponde a la porci√≥n de datos no vistos por el modelo durante el entrenamiento. Se utiliza para medir el nivel de generalizaci√≥n del modelo.
:::
::: {.callout-caution appearance="default" icon="false"}
## **Test**
Se utiliza para evaluar reportando una m√©trica de dise√±o del Modelo.
:::

::: {.callout-caution}
A diferencia de un modelo de Machine Learning el proceso de validaci√≥n del modelo se realiza en conjunto con el entrenamiento. Es decir, se entrena y valida el modelo Epoch a Epoch.
:::
:::
::: 

## Model Validation: K-Fold {.smaller}

![](img/clase-4/k-fold.png){.lightbox fig-align="center" width="60%"}

::: {.callout-important}
Corresponde al proceso de Holdout pero repetido $K$ veces. ***No es tan utilizado en Deep Learning debido a los altos costos computacionales***.
:::

# ¬øQu√© es Pytorch?

## Pytorch {.smaller}
> Es una librer√≠a de manipulaci√≥n de Tensores especializada en Deep Learning. Provee principalmente, manipulaci√≥n de tensores (igual que Numpy, pero en GPU), adem√°s de Autograd (calcula derivadas de manera autom√°tica).

Para poder comenzar a utilizarlo se requieren normalmente 3 imports:

```{.python code-line-numbers="|1|2|3|"}
import torch
import torch.nn as nn
import torch.nn.functional as F
```

::: {.callout-important appearance="default" icon=false}
## üëÄ
* `torch` es donde se encuentran la mayor√≠a de funciones b√°sicas para manipular tensores.
* `torch.nn` es donde se encuentran los m√≥dulos necesarios para poder crear redes neuronales (neural networks). Cada m√≥dulo es una clase en Python.
* `torch.nn.functional` es donde se encontrar√°n `utility functions` adem√°s de versiones funcionales de elementos de `torch.nn`. 
:::

::: {.callout-tip appearance="default"}
## üëÄ Una versi√≥n funcional es capaz de replicar la operaci√≥n de un m√≥dulo pero no tiene la capacidad de almacenar los par√°metros aprendidos.
:::


## Pytorch: Modelo {.smaller}

::::{.columns}
:::{.column}
::: {.callout-caution style="font-size: 90%;" icon=false appearance="default"} 
## Una capa en Pytorch
* Son elementos importados desde `torch.nn`.
* Estos m√≥dulos deben ser instanciados para luego ser utilizados.
* Cada capa tiene guarda sus par√°metros como `atributos`: 
  * `.weight.data` y `.bias.data` (para los pesos y bias respectivos).
  * **Ojo**: Pytorch utiliza los par√°metros de manera transpuesta a como lo aprendimos en clases.

:::

```{.python style="font-size: 120%;"}
## Ejemplo de un capa de par√°metros en Pytorch
## Proyecta desde 4 dimensiones a 12 dimensiones
fc1 = nn.Linear(in_features = 4, out_features=12)

## Forward Pass:
## Calcula las activaciones de la capa
fc1(X)

```

:::{.callout-note style="font-size: 90%;" icon=false appearance="default"}
## Output: Activaciones de la capa
:::
```
tensor([[ 0.2620, -0.5313,  0.3907,  ..., -0.0451, -1.2113, -2.9476],
        [-1.2096, -0.2586, -0.1372,  ...,  0.1342, -1.1130, -1.4764],
        [-1.7676, -0.3754, -0.3786,  ...,  0.2964, -1.2234, -1.6176],
        ...,
        [-2.1432, -0.3500, -0.5168,  ...,  0.3048, -1.1075, -1.1932],
        [-2.6030,  0.0033, -0.6494,  ...,  0.8202, -1.6117, -1.0077],
        [ 0.1126, -0.4532,  0.3573,  ...,  0.0660, -1.0688, -2.4856]],
       grad_fn=<AddmmBackward0>)
```
:::
:::{.column}

::: {.callout-tip style="font-size: 90%;" icon=false appearance="default"} 
## Un clase en Pytorch permite crear redes m√°s complejas y poseen 2 m√©todos principales:
  * Todas las clases deben heredar de `nn.Module`.

* `__init__()`:
  * Se inicializan los m√≥dulos con `super().__init__()`.
  * Se definen las capas de la red como atributos de la clase.
    * self.nombre_de_la_capa = nn.Capa(...)
    * self.funcion = nn.Funcion(...)
* `forward()`:
  * Define como se conectan las capas en el Forward Pass.

:::

```{.python style="font-size: 110%;"}
class MLP(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    
    ## Defincici√≥n de capas
    self.fc1 = nn.Linear(in_features, out_features)

  def forward(self,x):
    x = self.fc1(x)
    return x

model = MLP(in_features=4, out_features = 12)
model(X)
```
:::

::::

## Pytorch: Crear modelos m√°s complejos {.smaller}

::::{.columns}

:::{.column width="60%"}

```{.python style="font-size: 120%;"}
class MLP2(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    self.fc1 = nn.Linear(in_features, out_features)
    self.relu = nn.ReLU(inplace = True)
    self.fc2 = nn.Linear(out_features, 1)

  def forward(self,x):
    x = self.fc1(x)
    x = self.relu(x)
    x = self.fc2(x)
    return x

class SuperMLP(nn.Module):
  def __init__(self):
    super().__init__()
    self.mlp1 = MLP(in_features=4, out_features=12)
    self.mlp2 = MLP2(in_features=12, out_features=8)
  def forward(self, x):
    x = self.mlp1(x)
    x = self.mlp2(x)
    return x

super_model = SuperMLP()
logits = super_model(X)
logits.shape
```

:::

:::{.column width="40%"}
::: {.callout-caution style="font-size: 90%;" icon=false appearance="default"} 
## Es posible combinar distintos `nn.Module` en un s√≥lo modelo.
:::

![](img/clase-4/super_mlp.png){.lightbox fig-align="center" width="90%"}

```{.python style="font-size: 90%;"}
super_model
```
```
SuperMLP(
  (mlp1): MLP(
    (fc1): Linear(in_features=4, out_features=12, bias=True)
  )
  (mlp2): MLP2(
    (fc1): Linear(in_features=12, out_features=8, bias=True)
    (relu): ReLU()
    (fc2): Linear(in_features=8, out_features=1, bias=True)
  )
)
```
:::
::::

## Pytorch: Visualizaci√≥n del Modelo {.smaller}

::::{.columns}

:::{.column width="55%"}
![](img/clase-4/super_mlp.png){.lightbox fig-align="center" width="90%"}
:::

:::{.column width="45%"}


```{.python style="font-size: 90%;"}
super_model
```
```
SuperMLP(
  (mlp1): MLP(
    (fc1): Linear(in_features=4, out_features=12, bias=True)
  )
  (mlp2): MLP2(
    (fc1): Linear(in_features=12, out_features=8, bias=True)
    (relu): ReLU()
    (fc2): Linear(in_features=8, out_features=1, bias=True)
  )
)
```

```{.python style="font-size: 90%;"}
from torchinfo import summary
summary(super_model)
```
```
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
SuperMLP                                 --
‚îú‚îÄMLP: 1-1                               --
‚îÇ    ‚îî‚îÄLinear: 2-1                       60
‚îú‚îÄMLP2: 1-2                              --
‚îÇ    ‚îî‚îÄLinear: 2-2                       104
‚îÇ    ‚îî‚îÄReLU: 2-3                         --
‚îÇ    ‚îî‚îÄLinear: 2-4                       9
=================================================================
Total params: 173
Trainable params: 173
Non-trainable params: 0
=================================================================
```
:::
::::

## Pytorch: Optimizador y Loss Function {.smaller}

:::{.callout-important icon=false appearance="default"}
## Loss Function
* El `Loss Function` a utilizar le llamaremos `criterion`.
:::
```{.python style="font-size: 120%;"}
criterion = nn.BCEWithLogitsLoss()
```

:::{.callout-caution icon=false appearance="default"}
## Optimizer
* El Optimizador lo llamaremos `optimizer` y se importa desde torch.optim.
* Adem√°s el optimizador debe ser instanciado junto con los par√°metros del modelo y la tasa de aprendizaje `lr` (learning rate).
:::

```{.python style="font-size: 120%;"}
optimizer = torch.optim.Adam(super_model.parameters(), lr = 3e-4)
```


## Pytorch: Training Loop {.smaller}


::::{.columns}
:::{.column}
```{.python code-line-numbers="|4|5|8|9|11|14|17|" style="font-size: 120%;"}
loss_history = []
for e in range(epochs):
  # Definici√≥n del modo del Modelo
  super_model.train()
  optimizer.zero_grad()

  # Forward Pass
  logits = super_model(X)
  loss = criterion(logits, y)
  ## Calcula los gradientes (Backward Pass)
  loss.backward()

  # Actualizaci√≥n de los Pesos
  optimizer.step()

  # Log del Modelo
  loss_history.append(loss.item())
```
:::
:::{.column}
![](img/clase-4/training_loop.png){.lightbox fig-align="center"} 

:::
::::


:::{.callout-note style="font-size: 90%;" icon=false appearance="default"}
## Elementos clave del Training Loop
* Pytorch requiere fijar el Modo de la Red. Para entrenar se utiliza `model.train()`.
  * Pytorch tiene la costumbre de Acumular Gradientes. Por lo tanto, antes de cada Loop, se deben reiniciar los gradientes a cero utilizando `optimizer.zero_grad()`.
* El Forward Pass lo llamaremos con `logits = super_model(X)`. Esto permite calcular los Logits y las variables intermedias necesarias para el Backward Pass.
* El error/loss de la red lo calculamos con `loss = criterion(logits, y)`.
* El c√°lculo de gradientes lo llamaremos `loss.backward()`. Esto calcula los gradientes y los acumula en cada par√°metro del modelo.
* El optimizador lo llamaremos `optimizer`. Y llamaremos al proceso de actualizar pesos como `optimizer.step()`.
:::

## Pytorch: Validation Loop {.smaller}

::::{.columns}
:::{.column}

```{.python code-line-numbers="2,20-28" style="font-size: 120%;"}
loss_history = []
val_loss_history = []
for e in range(epochs):
  # Definici√≥n del modo del Modelo
  super_model.train()
  optimizer.zero_grad()

  # Forward Pass
  logits = super_model(X)
  loss = criterion(logits, y)
  ## Calcula los gradientes (Backward Pass)
  loss.backward()

  # Actualizaci√≥n de los Pesos
  optimizer.step()

  # Log de Entrenamiento
  loss_history.append(loss.item())

  # Validation Loop
  super_model.eval()

  # Evita c√°lculo de gradientes
  with torch.no_grad():
    val_logits = super_model(X_val)
    val_loss = criterion(val_logits, y_val)
    # Log de Validaci√≥n
    val_loss_history.append(val_loss.item())
```

:::
:::{.column}
![](img/clase-4/validation_loop.png){.lightbox fig-align="center"} 

:::{.callout-note style="font-size: 80%;" icon=false appearance="default"}
## Elementos clave del Validation Loop
* Se debe fijar el modo del modelo a evaluaci√≥n con `model.eval()`. No es necesario calcular gradientes en Validaci√≥n ya que no hay actualizaci√≥n de par√°metros.
  * `with torch.no_grad():` desactiva el c√°lculo de gradientes.
* El Forward Pass lo llamaremos con `val_logits = super_model(X_val)`. Esto permite calcular los Logits para poder calcular el loss de Validaci√≥n.
* El error/loss de la red lo calculamos como `val_loss = criterion(val_logits, y_val)`.
:::

:::
::::

## Pytorch: Model Evaluation {.smaller}

:::{style="font-size: 75%;"}
> Nos referimos a la evaluaci√≥n del modelo como la medici√≥n de la performance esperada por nuestro modelo. La Evaluaci√≥n del Modelo se realiza en torno a una m√©trica definida a priori por el modelador. La m√©trica a utilizar est√° √≠ntimamente ligada al problema a resolver.
:::

::: {.columns}
::: {.column}
#### Clasificaci√≥n

* $Accuracy = \frac{1}{m} \sum_{i = 1}^m 1\{y_i = \hat{y_i}\}$
* $Precision = \frac{TP}{TP + FP}$
* $Recall = \frac{TP}{TP + FN}$
* $F1-Score = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$
:::


::: {.column}
#### Regresi√≥n
* $RMSE = \frac{1}{m} \sum_{i=1}^m (y_i-\hat{y_i})^2$
* $MAE = \frac{1}{m} \sum_{i=1}^m |y_i - \hat{y_i}|$
* $MAPE = 100 \cdot \frac{1}{m} \sum_{i=1}^m \frac{|y_i-\hat{y_i}|}{max(\epsilon,y_i)}$
* $SMAPE = \frac{2}{m} \sum_{i=1}^2 \frac{|y_i - \hat{y_i}  |}{max(|y_i + \hat{y_i}|,\epsilon)}$
:::
::: 


::: {.callout-warning icon=false appearance="default"}
## ü§ì
Las m√©tricas presentadas son las b√°sicas para clasificaci√≥n y regresi√≥n. Existen otras m√°s espec√≠ficas seg√∫n el campo:

* IoU en segmentaci√≥n sem√°ntica,
* MAP@k en recomendaci√≥n,
* BLEU o ROUGE en NLP, etc.
* Una lista extensa puede consultarse en [Torchmetrics](https://lightning.ai/docs/torchmetrics/stable/all-metrics.html)
.
:::

## Pytorch: Training-Validation Loop + Evaluaci√≥n {.smaller}

::::{.columns}
:::{.column}

```{.python code-line-numbers="4-5,15-17,25-27" style="font-size: 120%;"}
loss_history, val_loss_history = [], []
train_metric_history, val_metric_history = [], []
for e in range():
  train_metric = BinaryAccuracy()
  val_metric = BinaryAccuracy()

  ## Training Loop
  super_model.train()
  optimizer.zero_grad()
  logits = super_model(X)
  loss = criterion(logits, y)
  loss.backward()
  optimizer.step()
  loss_history.append(loss.item())
  acc = train_metric(logits, y)
  train_metric_history.append(acc)
  tr_acc = train_metric.compute()

  # Validation Loop
  super_model.eval()
  with torch.no_grad():
    val_logits = super_model(X_val)
    val_loss = criterion(val_logits, y_val)
    val_loss_history.append(val_loss.item())
    acc = train_metric(val_logits, y_val)
    val_metric_history.append(acc)
    val_acc = test_metric.compute()

```

:::
:::{.column}
![](img/clase-4/val_loop_metrics.png){.lightbox fig-align="center"} 

:::{.callout-caution style="font-size: 80%;" icon=false appearance="default"}
## Elementos clave de este loop
* Adicional al c√°lculo de Loss se calcula alguna m√©trica de inter√©s para el problema.
* Se pueden loguear cuantas m√©tricas se deseen.
:::

:::
::::

## Monitoreo de un Modelo: Validation Curve {.smaller}


::: {.columns}
::: {.column width="60%"}
![](img/clase-4/validation_curve.png){.lightbox fig-align="center"}
:::
::: {.column width="40%"}
::: {.callout-important}
Es importante ser capaz de identificar el momento exacto en el cual el momento comienza su overfitting. Para ello se utiliza el **"Checkpointing"**. 
:::

::: {.callout-note appearance="default"}
## Checkpoint

* Corresponde a un snapshot del modelo a un cierto punto. En la **pr√°ctica** se almacenan los par√°metros del **mejor modelo** y del **√∫ltimo Epoch**.
:::

::: {.callout-tip appearance="default"}
## EarlyStopping

* Teoricamente, una vez que la red Neuronal alcanza el punto de Overfitting ya no tiene sentido seguir el entrenamiento. Por lo tanto es posible detener el entrenamiento bajo una cierta condici√≥n.
:::
:::
::: 

## Entrenamiento Eficiente: GPU {.smaller}

La principal ventaja de frameworks como Pytorch es su ejecuci√≥n en GPU, la cual ofrece una enorme capacidad de c√≥mputo por la gran cantidad de n√∫cleos.

:::{.callout-tip appearance="default" style="font-size: 120%;" icon=false}
## ü§ì Las GPUs usan CUDA (una variante compleja de C++), por lo que los errores suelen ser cr√≠pticos. Por ello se recomienda desarrollar en CPU y pasar a GPU solo cuando el c√≥digo ya funcione correctamente y sea necesario entrenar.
:::

```{.python style="font-size: 120%;"}
## Permite autom√°ticamente reconocer si es que existe GPU en el sistema 
## y de existir lo asigna como el dispositivo de entrenamiento.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
:::{.callout-note appearance="default" style="font-size: 120%;" icon=false}
## üñ•Ô∏è El c√≥digo anterior es particularmente √∫til para plataformas como Google Colab donde se permite activar o desactivar el uso de GPU.
:::

```{.python style="font-size: 120%;"}
## Fija el dispositivo de entrenamiento a CPU
device = torch.device("cpu")
```
## Entrenamiento Eficiente: Pytorch Dataset {.smaller}

Pytorch posee varias estrategias para hacer m√°s eficiente el entrenamiento de Redes Neuronales. Dentro de las estrategias m√°s comunes est√° el entrenamiento en GPU en Mini Batches.

:::{.callout-caution icon=false appearance="default"}
### Pytorch Dataset

Corresponde a una clase que hereda de `torch.utils.data.Dataset,` la cual define la forma en que se cargan los datos. Su principal ventaja es que permite realizar una carga perezosa (lazy loading), es decir, los datos se leen √∫nicamente en el momento en que son requeridos.
:::

::::{.columns}
:::{.column}
```{.python}
from torch.utils.data import Dataset

class ExampleDataset(Dataset):
  def __init__(self, x,y):
    ## Convertimos a numpy arrays
    self.X = x.to_numpy()
    self.y = y.values

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return dict(
        ## Tranformarmos cada √≠ndice en Tensor de Floats.
        x=torch.from_numpy(self.X)[idx].float(),
        y=torch.from_numpy(self.y)[idx].float()
        )

train_data = ExampleDataset(X_train, y_train)
val_data = ExampleDataset(X_val, y_val)
```
:::
:::{.column}
:::{.callout-note icon=false appearance="default"}
## `__init__`: Inicializa el dataset solicitando los datos requeridos.
:::
:::{.callout-tip icon=false appearance="default"}
## `__len__`: Define como se calcula la cantidad de muestras del dataset.
:::
:::{.callout-important icon=false appearance="default"}
## `__getitem__`: Define qu√© devuelve el elemento $i$ del dataset. Es decir **train_data[i]** o **val_data[i]**.
:::
:::
::::

## Entrenamiento Eficiente: Pytorch Dataloader {.smaller}

:::{.callout-caution icon=false appearance="default"}
### Pytorch Dataloader

Corresponde a una clase que permite cargar los datos por batch. Adem√°s permite paralelizar la carga de datos utilizando m√∫ltiples workers y controlar aspectos como qu√© hacer con el √∫ltimo batch (si es que no es completo) o si es necesario mezclar los datos (shuffling). La instancia del Dataloader tambi√©n es **lazy.**
:::

```{.python style="font-size: 120%;"}
from torch.utils.data import DataLoader
train_loader=DataLoader(train_data, batch_size=32, num_workers=10, pin_memory=True, drop_last=True, shuffle=True)
val_loader=DataLoader(val_data, batch_size=32, num_workers=10, pin_memory=True, shuffle=False)
```

:::{style="font-size: 70%;"}
* **batch_size**: Tama√±o del mini-batch.
* **num_workers**: N√∫mero de procesos a utilizar para cargar los datos.
* **pin_memory**: Si es True, las GPU destinan un espacio especial en memoria para acelerar la transferencia de datos.
* **drop_last**: Si es True, se descarta el √∫ltimo batch si no es completo.
* **shuffle**: Si es True, se mezclan los datos en cada epoch.
:::


:::{.callout-caution icon=false appearance="default"}
## Para entrenar en GPU es necesario que el modelo y los datos est√©n GPU. 
:::

:::{.callout-tip icon=false appearance="default"}
## Para pasar el modelo a la GPU se utiliza `super_model.to(device)`.
:::

:::{.callout-note icon=false appearance="default"}
## Para pasar los datos a la GPU se utiliza `X.to(device)` o `y.to(device)`.
:::

## Pytorch: Training-Validation Loop + Dataloader {.smaller}
 
::::{.columns}
:::{.column}
```{.python code-line-numbers="|1-2|3|4|5-6|8|9|10|11-17|15|19-21|19|23|24-25|26-30|30|32-34|32|36-37|" style="font-size: 95%;"}
  epoch_loss=dict(train=[], val=[])
  epoch_metric=dict(train=[], val=[])
  super_model.to(device)
  for e in range(epochs):
    train_metric, val_metric = BinaryAccuracy(), BinaryAccuracy()
    batch_loss=dict(train=[], val=[])

    model.train()
    for batch in train_loader:
      X, y = batch["x"].to(device), batch["y"].to(device)
      optimizer.zero_grad()
      logits = super_model(X)
      loss = criterion(logits, y)
      loss.backward()
      acc = train_metric(logits, y)
      optimizer.step()
      batch_loss["train"].append(loss.item())

    tr_acc = train_metric.compute()
    train_epoch_loss = np.mean(batch_loss["train"])
    epoch_metric["train"].append(tr_acc)

    model.eval()
    with torch.no_grad():
      for batch in val_loader:
        X, y = batch["x"].to(device), batch["y"].to(device)
        logits = super_model(X)
        loss = criterion(logits, y)
        batch_loss["val"].append(loss.item())
        acc = val_metric(logits, y)

      val_acc = val_metric.compute()
      val_epoch_loss = np.mean(batch_loss["val"])
      epoch_metric["val"].append(val_acc)

    epoch_loss["train"].append(train_epoch_loss)
    epoch_loss["val"].append(val_epoch_loss)

```
:::
:::{.column}
:::{.callout-note style="font-size: 80%;" icon=false appearance="default"}
## üëÄ Detalles clave de este loop

* Tanto para el Training Loop como para el Validation Loop se itera sobre el `DataLoader` en la GPU.
* Se definen m√©tricas dentro de la Epoch. Las cuales ser√°n utilizadas para calcular por batch y por epoch (utilizando `.compute()`).
* Se definen m√©tricas separadas para entrenamiento y validaci√≥n.
* Normalmente se loguean el Loss y una o m√°s m√©tricas por Epoch para construir las curvas de validaci√≥n.
:::

:::{.callout-warning style="font-size: 80%;" appearance="default"}
## Detach
En ocasiones no es posible convertir un tensor a un valor num√©rico (float, int, etc) si es que el tensor requiere gradientes. En estos casos es necesario "desconectar" el tensor del grafo de c√≥mputo utilizando `.detach()` y luego conviertiendolo a Numpy.
:::

```{.python}
data.detach().numpy()
```

:::
::::

## Categorical Variables: One Hot Encoding {.smaller}

Hasta ahora hemos asumido que las variables de entrada son num√©ricas. Pero en la pr√°ctica es muy com√∫n encontrarse con variables categ√≥ricas. En Deep Learning existen dos t√©cnicas para lidiar con este tipo de variables: One-Hot-Encoding y el uso de Embeddings.

::::{.columns}
:::{.column}
:::{.callout-tip icon=false appearance="default"}
## üìã One-Hot-Encoding
* Permite una representaci√≥n dispersa (sparse) de las variables categ√≥ricas. Consiste en crear una columna por cada categor√≠a, y asignar un 1 o un 0 dependiendo si la instancia pertenece o no a dicha categor√≠a.

* Es una representaci√≥n est√°tica sin par√°metros entrenables asociados.

* Genera tantas columnas/features nuevas como categor√≠as existan en la variable original. Por lo tanto, puede generar problemas de dimensionalidad si existen muchas categor√≠as.
:::
:::
:::{.column}
![](img/clase-4/ohe_example.png){.lightbox fig-align="center" width="90%"}
:::
::::


## Categorical Variables: Embeddings {.smaller}

::::{.columns}
:::{.column}
:::{.callout-important icon=false appearance="default"}
## üóûÔ∏è Embeddings
* Un embedding es una representaci√≥n num√©rica de un objeto (palabra, imagen, nodo, etc.) en un espacio vectorial de menor dimensi√≥n que captura sus caracter√≠sticas y relaciones de manera √∫til para un modelo de machine learning. 

* En lugar de trabajar con las categor√≠as crudas, los **embeddings** convierten esos datos en vectores de n√∫meros reales. De esta forma, objetos similares quedan representados por vectores cercanos.

* Los vectores son aprendidos por el modelo durante el entrenamiento, de tal manera que la representaci√≥n aprendida es la optima para el problema espec√≠fico a resolver. Es decir, agrega par√°metros entrenables al modelo.
:::

:::
:::{.column}
![](img/clase-4/emb_example.png){.lightbox fig-align="center"}
:::

::::

## Aplicaci√≥n en Pytorch {.smaller}


::::{.columns}
:::{.column}

:::{.callout-important icon=false appearance="default"}
## One Hot Encoding
Se puede utilizar directamente en Pytorch utilizando `F.one_hot()`. En general esto se hace fuera del modelo y no es una capa entrenable.
:::


:::{.callout-warning icon=false appearance="default"}
## Embedding
Este caso s√≠ es una capa entrenable y se aplica en Pytorch como una capa m√°s del modelo utilizando `nn.Embedding()`.
:::

```{.python style="font-size: 140%;"}
nn.Embedding(num_embeddings, embedding_dim)
```
* **num_embeddings**: Corresponde al n√∫mero de categ√≥r√≠as.
* **embedding_dim**: El n√∫mero de dimensiones en el cual se quiere representar.



:::
:::{.column}
![](img/clase-4/embedding_net_example.png){.lightbox fig-align="center" width="80%"}

:::{.callout-tip icon=false appearance="default"}
## ü§ì Projection Layer
En algunos casos se agrega una Projection Layer (capa lineal) en las variables num√©ricas a una nueva dimensi√≥n (Esto ya que el Embedding lleva a las variables categ√≥ricas a una dimensi√≥n diferente). Esto permite que el modelo aprenda una representaci√≥n conjunta de las variables num√©ricas y categ√≥ricas. Luego las capas de proyecci√≥n y embedding se concatenan y se pasan a las capas siguientes.
:::
:::
::::



## Embedding: Ejemplo {.smaller}

::::{.columns}
:::{.column}

$$e=Emb(X)$$
$$\phi1 = e \cdot W1$$
$$Z = \phi_1 = + 1_m b^T$$
$$p = \sigma(Z)$$
$$L = \frac{-1}{m}\left[y^T log(p) + (1-y)^T log(1-p)\right]$$

:::{.callout-warning icon=false appearance="default"}
## ü§ì No haremos la derivaci√≥n completa, s√≥lo nos enfocaremos en el Gradiente del Embedding.
:::


:::
:::{.column}
![](img/clase-4/emb_graph.png){.lightbox fig-align="center"}
:::
::::

## Embedding: Ejemplo {.smaller}

$$\frac{\partial L}{\partial Z} = \frac{1}{m} \cdot [p - y]$$
$$\frac{\partial L}{\partial \phi_1} = \frac{1}{m} \cdot [p - y]$$
$$\frac{\partial L}{\partial e} = \frac{\partial L}{\partial \phi_1} \cdot \frac{\partial \phi_1}{\partial e}= \frac{1}{m}[p-y] \cdot W_1^T $$
$$\frac{\partial}{\partial E} = \frac{\partial L}{\partial e} \cdot \frac{\partial e}{\partial E} = \frac{1}{m} [p-y] \cdot W_1^T \cdot \frac{\partial e}{\partial E}$$

:::{.callout-note icon=false appearance="default"}
## ¬øCu√°nto vale $\frac{\partial e}{\partial E}$?

$$\frac{\partial e}{\partial E} = S^T$$

Donde $S$ es es la matriz One-Hot-Encoding de tama√±o $m \times C$ donde C es el n√∫mero de categor√≠as.
:::

## Embedding: Ejemplo Num√©rico {.smaller}


::::{.columns}
:::{.column width="30%"}
![](img/clase-4/data_emb.png){.lightbox fig-align="center"}

:::

:::{.column width="30%"}

![](img/clase-4/emb_matrix.png){.lightbox fig-align="center"}

$$
e = \begin{bmatrix}
0.1000 & 0.2000 & 0.3000 \\
0.4000 & 0.0000 & -0.1000 \\
0.2000 & -0.2000 & 0.1000 \\
0.4000 & 0.0000 & -0.1000
\end{bmatrix}
$$

:::
:::{.column width="40%"}

![](img/clase-4/one_hot_matrix){.lightbox fig-align="center"}

:::
::::

## Embedding: Ejemplo Num√©rico {.smaller}

::::{.columns}
:::{.column width="30%"}

$$
\phi_1 = \begin{bmatrix}
0.0300 \\
0.1900 \\
0.1600 \\
0.1900
\end{bmatrix}
$$

$$
Z = \begin{bmatrix}
0.1300 \\
0.2900 \\
0.2600 \\
0.2900
\end{bmatrix}
$$

$$
p = \begin{bmatrix}
0.5325 \\
0.5720 \\
0.5646 \\
0.5720
\end{bmatrix}
$$

:::
:::{.column .fragment width="70%"}

$$
\begin{align}
\frac{\partial L}{\partial E} &= \frac{1}{m} S^T \cdot [p - y] \cdot W_1^T\\
&= \frac{1}{4} \cdot 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix} \cdot
\left[\begin{bmatrix}
0.5325 \\
0.5720 \\
0.5646 \\
0.5720
\end{bmatrix} -
\begin{bmatrix}
1.0 \\
0.0 \\
1.0 \\
0.0
\end{bmatrix}
\right] \cdot \begin{bmatrix}
0.5 & -0.25 & 0.1
\end{bmatrix}\\
&=\begin{bmatrix}
-0.0584 & 0.0292 & -0.0117 \\
0.0000 & -0.0000 & 0.0000 \\
0.0000 & -0.0000 & 0.0000 \\
0.1430 & -0.0715 & 0.0286 \\
-0.0544 & 0.0272 & -0.0109
\end{bmatrix}
\end{align}
$$
:::
::::


# üéä Estamos listos para entrenar Redes Neuronales üéä

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
