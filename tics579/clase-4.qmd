---
title: "TICS-579-Deep Learning"
subtitle: "Clase 4: Model Training"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Entrenamiento de la Red {.smaller}


> A diferencia de un Modelo de Machine Learning, las Redes Neuronales se entrenan de manera progresiva (se espera una mejora en cada Epoch). Si nuestra Arquitectura es apropiada nosotros deber√≠amos esperar que el `Loss` de nuestra ***red siempre disminuya***. ¬øPor qu√©?

:::{.callout-tip .fragment icon=false appearance="default"}
## üí° Dado que el entrenamiento es progresivo, el modelo puede retomar su entrenamiento desde un set de pesos dados.
:::

::: {.callout-warning .fragment icon=false appearance="default"}
## **¬øSiempre buscamos la Red que tenga el mejor Loss de Entrenamiento?** ***¬øCu√°l es la diferencia entre el `Loss` y el `Rendimento del Modelo`?***
:::

::: {.callout-important .fragment icon=false appearance="default"}
## Al igual que en los modelos de Machine Learning debemos evitar a toda costa el Overfitting. ¬øQu√© es el overfitting?
:::

## Entrenamiento de la Red {.smaller}

Bias-Variance Tradeoff (Dilema Sesgo-Varianza)
: > Probablemente el concepto m√°s importante para determinar si un modelo tiene potencial o no. Corresponden a dos tipos de errores que pueden sufrir los modelos de ML. 

::: {.columns .fragment}
::: {.column .callout-note appearance="default" icon=false}
## Bias
Corresponde al sesgo, y tiene que ver con la diferencia entre el valor real y el valor predicho. Bajo sesgo implica una mejor predicci√≥n.
:::
::: {.column .callout-caution appearance="default" icon=false}
## Variance
Corresponde a la varianza y tiene que ver con la dispersi√≥n de los valores predichos. Baja Varianza implica un modelo m√°s estable y menos flexible.
:::

::: {.callout-important .fragment appearance="default" icon=false}
## En general hay que buscar el equilibrio entre ambos tipos de errores:

* Alto Sesgo y baja Varianza: **Underfitting**. 
* Bajo Sesgo y Alta Varianza: **Overfitting**. 

:::
:::

## Model Validation {.smaller}

Validaci√≥n Cruzada
: > Se refiere al proceso de entrenar un modelo en una cierta porci√≥n de los datos, pero validar sus rendimiento y capacidad de ***generalizaci√≥n*** en un set de datos ***no vistos*** por el modelo al momento de entrenar. 

::::{style="font-size: 120%;"}
::: {.callout-warning icon=false appearance="default" }
## ***¬øQu√© es la Generalizaci√≥n?***
:::

::: {.callout-note icon=false appearance="default" }
## Los dos m√©todos m√°s populares que se usan en Machine Learning son **Holdout** y **K-Fold.** M√°s m√©todos se pueden encontrar en los [docs de Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html). 
:::

::: {.callout-important icon=false appearance="default" }
## Debido a los vol√∫menes de datos utilizados, el esquema de validaci√≥n m√°s utilizado es el **Holdout**.
:::
::::

## Model Validation: Holdout {.smaller}

::: {.columns}
::: {.column}

![](img/clase-4/data_split.png){.lightbox fig-align="center"}
:::
::: {.column}

::: {.callout-note appearance="default" icon="false"}
## **Train**
Corresponde a la porci√≥n de utilizado para que el modelo aprenda.
:::
::: {.callout-important appearance="default" icon="false"}
## **Validation**
Corresponde a la porci√≥n de datos no vistos por el modelo durante el entrenamiento. Se utiliza para medir el nivel de generalizaci√≥n del modelo.
:::
::: {.callout-caution appearance="default" icon="false"}
## **Test**
Se utiliza para evaluar reportando una m√©trica de dise√±o del Modelo.
:::

::: {.callout-caution}
A diferencia de un modelo de Machine Learning el proceso de validaci√≥n del modelo se realiza en conjunto con el entrenamiento. Es decir, se entrena y valida el modelo Epoch a Epoch.
:::
:::
::: 

## Model Validation: K-Fold {.smaller}

![](img/clase-4/k-fold.png){.lightbox fig-align="center" width="60%"}

::: {.callout-important}
Corresponde al proceso de Holdout pero repetido $K$ veces.
:::

# ¬øQu√© es Pytorch?

## Pytorch {.smaller}
> Es una librer√≠a de manipulaci√≥n de Tensores especializada en Deep Learning. Provee principalmente, manipulaci√≥n de tensores (igual que Numpy, pero en GPU), adem√°s de Autograd (calcula derivadas de manera autom√°tica).

Para poder comenzar a utilizarlo se requieren normalmente 3 imports:

```{.python code-line-numbers="|1|2|3|"}
import torch
import torch.nn as nn
import torch.nn.functional as F
```

::: {.callout-important appearance="default" icon=false}
## üëÄ
* `torch` es donde se encuentran la mayor√≠a de funciones b√°sicas para manipular tensores.
* `torch.nn` es donde se encuentran los m√≥dulos necesarios para poder crear redes neuronales (neural networks). Cada m√≥dulo es una clase en Python.
* `torch.nn.functional` es donde se encontrar√°n `utility functions` adem√°s de versiones funcionales de elementos de `torch.nn`. 
:::

::: {.callout-tip appearance="default"}
## üëÄ Una versi√≥n funcional es capaz de replicar la operaci√≥n de un m√≥dulo pero no tiene la capacidad de almacenar los par√°metros aprendidos.
:::

## Pytorch en GPU {.smaller}

La principal ventaja de frameworks como Pytorch es su ejecuci√≥n en GPU, la cual ofrece una enorme capacidad de c√≥mputo por la gran cantidad de n√∫cleos.

:::{.callout-tip appearance="default" style="font-size: 120%;" icon=false}
## ü§ì Las GPUs usan CUDA (una variante compleja de C++), por lo que los errores suelen ser cr√≠pticos. Por ello se recomienda desarrollar en CPU y pasar a GPU solo cuando el c√≥digo ya funcione correctamente y sea necesario entrenar.
:::

```{.python style="font-size: 120%;"}
## Permite autom√°ticamente reconocer si es que existe GPU en el sistema 
## y de existir lo asigna como el dispositivo de entrenamiento.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
:::{.callout-note appearance="default" style="font-size: 120%;" icon=false}
## üñ•Ô∏è El c√≥digo anterior es particularmente √∫til para plataformas como Google Colab donde se permite activar o desactivar el uso de GPU.
:::

```{.python style="font-size: 120%;"}
## Fija el dispositivo de entrenamiento a CPU
device = torch.device("cpu")
```

## Convenciones: Modelo {.smaller}

::::{.columns}
:::{.column}
::: {.callout-caution style="font-size: 90%;" icon=false appearance="default"} 
## Una capa en Pytorch
* Son elementos importados desde `torch.nn`.
* Estos m√≥dulos deben ser instanciados para luego ser utilizados.
* Cada capa tiene guarda sus par√°metros como `atributos`: 
  * `.weight.data` y `.bias.data` (para los pesos y bias respectivos).
  * **Ojo**: Pytorch utiliza los par√°metros de manera transpuesta a como lo aprendimos en clases.

:::

```{.python style="font-size: 120%;"}
## Ejemplo de un capa de par√°metros en Pytorch
## Proyecta desde 4 dimensiones a 12 dimensiones
fc1 = nn.Linear(in_features = 4, out_features=12)

## Forward Pass:
## Calcula las activaciones de la capa
fc1(X)

```

:::{.callout-note style="font-size: 90%;" icon=false appearance="default"}
## Output: Activaciones de la capa
:::
```
tensor([[ 0.2620, -0.5313,  0.3907,  ..., -0.0451, -1.2113, -2.9476],
        [-1.2096, -0.2586, -0.1372,  ...,  0.1342, -1.1130, -1.4764],
        [-1.7676, -0.3754, -0.3786,  ...,  0.2964, -1.2234, -1.6176],
        ...,
        [-2.1432, -0.3500, -0.5168,  ...,  0.3048, -1.1075, -1.1932],
        [-2.6030,  0.0033, -0.6494,  ...,  0.8202, -1.6117, -1.0077],
        [ 0.1126, -0.4532,  0.3573,  ...,  0.0660, -1.0688, -2.4856]],
       grad_fn=<AddmmBackward0>)
```
:::
:::{.column}

::: {.callout-tip style="font-size: 90%;" icon=false appearance="default"} 
## Un clase en Pytorch permite crear redes m√°s complejas y poseen 2 m√©todos principales:
  * Todas las clases deben heredar de `nn.Module`.

* `__init__()`:
  * Se inicializan los m√≥dulos con `super().__init__()`.
  * Se definen las capas de la red como atributos de la clase.
    * self.nombre_de_la_capa = nn.Capa(...)
    * self.funcion = nn.Funcion(...)
* `forward()`:
  * Define como se conectan las capas en el Forward Pass.

:::

```{.python style="font-size: 110%;"}
class MLP(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    
    ## Defincici√≥n de capas
    self.fc1 = nn.Linear(in_features, out_features)

  def forward(self,x):
    x = self.fc1(x)
    return x

model = MLP(in_features=4, out_features = 12)
model(X)
```
:::

::::

## Crear modelos m√°s complejos {.smaller}

::::{.columns}

:::{.column width="60%"}

```{.python style="font-size: 120%;"}
class MLP2(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    self.fc1 = nn.Linear(in_features, out_features)
    self.relu = nn.ReLU(inplace = True)
    self.fc2 = nn.Linear(out_features, 1)

  def forward(self,x):
    x = self.fc1(x)
    x = self.relu(x)
    x = self.fc2(x)
    return x

class SuperMLP(nn.Module):
  def __init__(self):
    super().__init__()
    self.mlp1 = MLP(in_features=4, out_features=12)
    self.mlp2 = MLP2(in_features=12, out_features=8)
  def forward(self, x):
    x = self.mlp1(x)
    x = self.mlp2(x)
    return x

super_model = SuperMLP()
logits = super_model(X)
logits.shape
```

:::

:::{.column width="40%"}
::: {.callout-caution style="font-size: 90%;" icon=false appearance="default"} 
## Es posible combinar distintos `nn.Module` en un s√≥lo modelo.
:::

![](img/clase-4/super_mlp.png){.lightbox fig-align="center" width="90%"}

```{.python style="font-size: 90%;"}
super_model
```
```
SuperMLP(
  (mlp1): MLP(
    (fc1): Linear(in_features=4, out_features=12, bias=True)
  )
  (mlp2): MLP2(
    (fc1): Linear(in_features=12, out_features=8, bias=True)
    (relu): ReLU()
    (fc2): Linear(in_features=8, out_features=1, bias=True)
  )
)
```
:::
::::

## Visualizaci√≥n del Modelo {.smaller}

::::{.columns}

:::{.column width="55%"}
![](img/clase-4/super_mlp.png){.lightbox fig-align="center" width="90%"}
:::

:::{.column width="45%"}


```{.python style="font-size: 90%;"}
super_model
```
```
SuperMLP(
  (mlp1): MLP(
    (fc1): Linear(in_features=4, out_features=12, bias=True)
  )
  (mlp2): MLP2(
    (fc1): Linear(in_features=12, out_features=8, bias=True)
    (relu): ReLU()
    (fc2): Linear(in_features=8, out_features=1, bias=True)
  )
)
```

```{.python style="font-size: 90%;"}
from torchinfo import summary
summary(super_model)
```
```
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
SuperMLP                                 --
‚îú‚îÄMLP: 1-1                               --
‚îÇ    ‚îî‚îÄLinear: 2-1                       60
‚îú‚îÄMLP2: 1-2                              --
‚îÇ    ‚îî‚îÄLinear: 2-2                       104
‚îÇ    ‚îî‚îÄReLU: 2-3                         --
‚îÇ    ‚îî‚îÄLinear: 2-4                       9
=================================================================
Total params: 173
Trainable params: 173
Non-trainable params: 0
=================================================================
```
:::
::::

## Convenciones: Optimizador y Loss Function {.smaller}

:::{.callout-important icon=false appearance="default"}
## Loss Function
* El `Loss Function` a utilizar le llamaremos `criterion`.
:::
```{.python style="font-size: 120%;"}
criterion = nn.BCEWithLogitsLoss()
```

:::{.callout-caution icon=false appearance="default"}
## Optimizer
* El Optimizador lo llamaremos `optimizer` y se importa desde torch.optim.
* Adem√°s el optimizador debe ser instanciado junto con los par√°metros del modelo y la tasa de aprendizaje `lr` (learning rate).
:::

```{.python style="font-size: 120%;"}
optimizer = torch.optim.Adam(super_model.parameters(), lr = 3e-4)
```


## Training Loop {.smaller}


::::{.columns}
:::{.column}
```{.python code-line-numbers="|4|5|8|9|11|14|17|" style="font-size: 120%;"}
loss_history = []
for e in range(epochs):
  # Definici√≥n del modo del Modelo
  super_model.train()
  optimizer.zero_grad()

  # Forward Pass
  logits = super_model(X)
  loss = criterion(logits, y)
  ## Calcula los gradientes (Backward Pass)
  loss.backward()

  # Actualizaci√≥n de los Pesos
  optimizer.step()

  # Log del Modelo
  loss_history.append(loss.item())
```
:::
:::{.column}
![](img/clase-4/training_loop.png){.lightbox fig-align="center"} 

:::
::::


:::{.callout-note style="font-size: 90%;" icon=false appearance="default"}
## Elementos clave del Training Loop
* Pytorch requiere fijar el Modo de la Red. Para entrenar se utiliza `model.train()`.
  * Pytorch tiene la costumbre de Acumular Gradientes. Por lo tanto, antes de cada Loop, se deben reiniciar los gradientes a cero utilizando `optimizer.zero_grad()`.
* El Forward Pass lo llamaremos con `logits = super_model(X)`. Esto permite calcular los Logits y las variables intermedias necesarias para el Backward Pass.
* El error/loss de la red lo calculamos con `loss = criterion(logits, y)`.
* El c√°lculo de gradientes lo llamaremos `loss.backward()`. Esto calcula los gradientes y los acumula en cada par√°metro del modelo.
* El optimizador lo llamaremos `optimizer`. Y llamaremos al proceso de actualizar pesos como `optimizer.step()`.
:::

## Validation Loop {.smaller}

::::{.columns}
:::{.column}

```{.python code-line-numbers="2,20-28" style="font-size: 120%;"}
loss_history = []
val_loss_history = []
for e in range(epochs):
  # Definici√≥n del modo del Modelo
  super_model.train()
  optimizer.zero_grad()

  # Forward Pass
  logits = super_model(X)
  loss = criterion(logits, y)
  ## Calcula los gradientes (Backward Pass)
  loss.backward()

  # Actualizaci√≥n de los Pesos
  optimizer.step()

  # Log de Entrenamiento
  loss_history.append(loss.item())

  # Validation Loop
  super_model.eval()

  # Evita c√°lculo de gradientes
  with torch.no_grad():
    val_logits = super_model(X_val)
    val_loss = criterion(val_logits, y_val)
    # Log de Validaci√≥n
    val_loss_history.append(val_loss.item())
```

:::
:::{.column}
![](img/clase-4/validation_loop.png){.lightbox fig-align="center"} 

:::{.callout-note style="font-size: 80%;" icon=false appearance="default"}
## Elementos clave del Validation Loop
* Se debe fijar el modo del modelo a evaluaci√≥n con `model.eval()`. No es necesario calcular gradientes en Validaci√≥n ya que no hay actualizaci√≥n de par√°metros.
  * `with torch.no_grad():` desactiva el c√°lculo de gradientes.
* El Forward Pass lo llamaremos con `val_logits = super_model(X_val)`. Esto permite calcular los Logits para poder calcular el loss de Validaci√≥n.
* El error/loss de la red lo calculamos como `val_loss = criterion(val_logits, y_val)`.
:::

:::
::::

## Model Evaluation {.smaller}

:::{style="font-size: 75%;"}
> Nos referimos a la evaluaci√≥n del modelo como la medici√≥n de la performance esperada por nuestro modelo. La Evaluaci√≥n del Modelo se realiza en torno a una m√©trica definida a priori por el modelador. La m√©trica a utilizar est√° √≠ntimamente ligada al problema a resolver.
:::

::: {.columns}
::: {.column}
#### Clasificaci√≥n

* $Accuracy = \frac{1}{m} \sum_{i = 1}^m 1\{y_i = \hat{y_i}\}$
* $Precision = \frac{TP}{TP + FP}$
* $Recall = \frac{TP}{TP + FN}$
* $F1-Score = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$
:::


::: {.column}
#### Regresi√≥n
* $RMSE = \frac{1}{m} \sum_{i=1}^m (y_i-\hat{y_i})^2$
* $MAE = \frac{1}{m} \sum_{i=1}^m |y_i - \hat{y_i}|$
* $MAPE = 100 \cdot \frac{1}{m} \sum_{i=1}^m \frac{|y_i-\hat{y_i}|}{max(\epsilon,y_i)}$
* $SMAPE = \frac{2}{m} \sum_{i=1}^2 \frac{|y_i - \hat{y_i}  |}{max(|y_i + \hat{y_i}|,\epsilon)}$
:::
::: 


::: {.callout-warning icon=false appearance="default"}
## ü§ì
Las m√©tricas presentadas son las b√°sicas para clasificaci√≥n y regresi√≥n. Existen otras m√°s espec√≠ficas seg√∫n el campo:

* IoU en segmentaci√≥n sem√°ntica,
* MAP@k en recomendaci√≥n,
* BLEU o ROUGE en NLP, etc.
* Una lista extensa puede consultarse en [Torchmetrics](https://lightning.ai/docs/torchmetrics/stable/all-metrics.html)
.
:::

## Training-Validation Loop + Evaluaci√≥n {.smaller}

::::{.columns}
:::{.column}

```{.python code-line-numbers="4-5,15-17,25-27" style="font-size: 120%;"}
loss_history, val_loss_history = [], []
train_metric_history, val_metric_history = [], []
for e in range():
  train_metric = BinaryAccuracy()
  val_metric = BinaryAccuracy()

  ## Training Loop
  super_model.train()
  optimizer.zero_grad()
  logits = super_model(X)
  loss = criterion(logits, y)
  loss.backward()
  optimizer.step()
  loss_history.append(loss.item())
  acc = train_metric(logits, y)
  train_metric_history.append(acc)
  tr_acc = train_metric.compute()

  # Validation Loop
  super_model.eval()
  with torch.no_grad():
    val_logits = super_model(X_val)
    val_loss = criterion(val_logits, y_val)
    val_loss_history.append(val_loss.item())
    acc = train_metric(val_logits, y_val)
    val_metric_history.append(acc)
    val_acc = test_metric.compute()

```

:::
:::{.column}
![](img/clase-4/val_loop_metrics.png){.lightbox fig-align="center"} 

:::{.callout-caution style="font-size: 80%;" icon=false appearance="default"}
## Elementos clave de este loop
* Adicional al c√°lculo de Loss se calcula alguna m√©trica de inter√©s para el problema.
* Se pueden loguear cuantas m√©tricas se deseen.
:::

:::
::::

## Monitoreo de un Modelo: Validation Curve {.smaller}


::: {.columns}
::: {.column width="60%"}
![](img/clase-4/validation_curve.png){.lightbox fig-align="center"}
:::
::: {.column width="40%"}
::: {.callout-important}
Es importante ser capaz de identificar el momento exacto en el cual el momento comienza su overfitting. Para ello se utiliza el **"Checkpointing"**. 
:::

::: {.callout-note appearance="default"}
## Checkpoint

* Corresponde a un snapshot del modelo a un cierto punto. En la **pr√°ctica** se almacenan los par√°metros del **mejor modelo** y del **√∫ltimo Epoch**.
:::

::: {.callout-tip appearance="default"}
## EarlyStopping

* Teoricamente, una vez que la red Neuronal alcanza el punto de Overfitting ya no tiene sentido seguir el entrenamiento. Por lo tanto es posible detener el entrenamiento bajo una cierta condici√≥n.
:::
:::
::: 

## Entrenamiento Eficiente: Pytorch Dataset {.smaller}

Pytorch posee varias estrategias para hacer m√°s eficiente el entrenamiento de Redes Neuronales. Dentro de las estrategias m√°s comunes est√° el entrenamiento en GPU en Mini Batches.

:::{.callout-caution icon=false appearance="default"}
### Pytorch Dataset

Corresponde a una clase que hereda de torch.utils.data.Dataset, la cual define la forma en que se cargan los datos. Su principal ventaja es que permite realizar una carga perezosa (lazy loading), es decir, los datos se leen √∫nicamente en el momento en que son requeridos.
:::

::::{.columns}
:::{.column}
```{.python}
from torch.utils.data import Dataset

class ExampleDataset(Dataset):
  def __init__(self, x,y):
    ## Convertimos a numpy arrays
    self.X = x.to_numpy()
    self.y = y.values

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return dict(
        ## Tranformarmos cada √≠ndice en Tensor de Floats.
        x=torch.from_numpy(self.X)[idx].float(),
        y=torch.from_numpy(self.y)[idx].float()
        )

train_data = ExampleDataset(X_train, y_train)
val_data = ExampleDataset(X_val, y_val)
```
:::
:::{.column}
:::{.callout-note icon=false appearance="default"}
## `__init__`: Inicializa el dataset solicitando los datos requeridos.
:::
:::{.callout-tip icon=false appearance="default"}
## `__len__`: Define como se calcula la cantidad de muestras del dataset.
:::
:::{.callout-important icon=false appearance="default"}
## `__getitem__`: Define qu√© devuelve el elemento $i$ del dataset. Es decir **train_data[i]** o **val_data[i]**.
:::
:::
::::

## Entrenamiento Eficiente: Pytorch Dataloader {.smaller}

:::{.callout-caution icon=false appearance="default"}
### Pytorch Dataloader

Corresponde a una clase que permite cargar los datos por batch. Adem√°s permite paralelizar la carga de datos utilizando m√∫ltiples workers y controlar aspectos como qu√© hacer con el √∫ltimo batch (si es que no es completo) o si es necesario mezclar los datos (shuffling). La instancia del Dataloader tambi√©n es **lazy.**
:::

```{.python style="font-size: 120%;"}
train_loader=DataLoader(train_data, batch_size=32, num_workers=10, pin_memory=True, drop_last=True, shuffle=True)
val_loader=DataLoader(val_data, batch_size=32, num_workers=10, pin_memory=True, shuffle=False)
```

:::{style="font-size: 70%;"}
* **batch_size**: Tama√±o del mini-batch.
* **num_workers**: N√∫mero de procesos a utilizar para cargar los datos.
* **pin_memory**: Si es True, las GPU destinan un espacio especial en memoria para acelerar la transferencia de datos.
* **drop_last**: Si es True, se descarta el √∫ltimo batch si no es completo.
* **shuffle**: Si es True, se mezclan los datos en cada epoch.
:::


:::{.callout-caution icon=false appearance="default"}
## Para entrenar en GPU es necesario que el modelo y los datos est√©n GPU. 
:::

:::{.callout-tip icon=false appearance="default"}
## Para pasar el modelo a la GPU se utiliza `super_model.to(device)`.
:::

:::{.callout-note icon=false appearance="default"}
## Para pasar los datos a la GPU se utiliza `X.to(device)` o `y.to(device)`.
:::

## Training-Validation Loop + Dataloader {.smaller}
 
::::{.columns}
:::{.column}
```{.python code-line-numbers="|1-2|3|4|5-6|8|9|10|11-17|15|19-21|19|23|24-25|26-30|30|32-34|32|36-37|" style="font-size: 95%;"}
  epoch_loss=dict(train=[], val=[])
  epoch_metric=dict(train=[], val=[])
  super_model.to(device)
  for e in range(epochs):
    train_metric, val_metric = BinaryAccuracy(), BinaryAccuracy()
    batch_loss=dict(train=[], val=[])

    model.train()
    for batch in train_loader:
      X, y = batch["x"].to(device), batch["y"].to(device)
      optimizer.zero_grad()
      logits = super_model(X)
      loss = criterion(logits, y)
      loss.backward()
      acc = train_metric(logits, y)
      optimizer.step()
      batch_loss["train"].append(loss.item())

    tr_acc = train_metric.compute()
    train_epoch_loss = np.mean(batch_loss["train"])
    epoch_metric["train"].append(tr_acc)

    model.eval()
    with torch.no_grad():
      for batch in val_loader:
        X, y = batch["x"].to(device), batch["y"].to(device)
        logits = super_model(X)
        loss = criterion(logits, y)
        batch_loss["val"].append(loss.item())
        acc = val_metric(logits, y)

      val_acc = val_metric.compute()
      val_epoch_loss = np.mean(batch_loss["val"])
      epoch_metric["val"].append(val_acc)

    epoch_loss["train"].append(train_epoch_loss)
    epoch_loss["val"].append(val_epoch_loss)

```
:::
:::{.column}
:::{.callout-note style="font-size: 80%;" icon=false appearance="default"}
## üëÄ Detalles clave de este loop

* Tanto para el Training Loop como para el Validation Loop se itera sobre el `DataLoader` en la GPU.
* Se definen m√©tricas dentro de la Epoch. Las cuales ser√°n utilizadas para calcular por batch y por epoch (utilizando `.compute()`).
* Se definen m√©tricas separadas para entrenamiento y validaci√≥n.
* Normalmente se loguean el Loss y una o m√°s m√©tricas por Epoch para construir las curvas de validaci√≥n.
:::

:::{.callout-warning style="font-size: 80%;" appearance="default"}
## Detach
En ocasiones no es posible convertir un tensor a un valor num√©rico (float, int, etc) si es que el tensor requiere gradientes. En estos casos es necesario "desconectar" el tensor del grafo de c√≥mputo utilizando `.detach()` y luego conviertiendolo a Numpy.
:::

```{.python}
data.detach().numpy()
```

:::
::::

# üéä Estamos listos para entrenar Redes Neuronales üéä

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
