---
title: "TICS-579-Deep Learning"
subtitle: "Clase P1: lgebra Tensorial"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

# Introducci贸n

## Historia {.smaller}

La verdad podr铆amos estudiar historia e importancia de porqu茅 el Deep Learning es importante, pero la verdad...

::: {.callout-important}
NO TENEMOS TIEMPO PARA ESO.
:::

:::: {.columns}
::: {.column width="25%"}

#### Alexnet (2012)
![](img/clase-1/alexnet.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### Transformers (2017)
![](img/clase-1/transformer.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### GPT (2019)
![](img/clase-1/gpt.png){.lightbox}
:::

::: {.column width="25%" .fragment}
#### LLMs (2023) (ChatGPT/Llama)
![](img/clase-1/llms.jpg){.lightbox}
:::
::::


## 驴Por qu茅 estudiar Deep Learning? {.smaller}

![Im谩gen tomada de la Clase de Zico Colter](img/clase-1/google_trends.png){.lightbox width="70%" fig-align="center"}

## 驴Por qu茅 estudiar Deep Learning? {.smaller}

::::{style="font-size: 130%;"}
::: {.callout-tip appearance="default"}
## Facilidad y Autograd
* Frameworks como Tensorflow, Pytorch o Jax permiten realizar esto de manera mucho m谩s sencilla.
    * Frameworks permiten calcular gradientes de manera autom谩tica.
    * Antigua mente trabajar en Torch, Caffe o Theano pod铆a tomar cerca de 50K l铆neas de c贸digo.
:::

::: {.callout-note appearance="default"}
## C贸mputo
* Proliferaci贸n de las GPUs, TPUs, HPUs, IPUs, como sistemas masivos de C贸mputos. 
    * [How many computers to identify a cat? 16,000](https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html)
:::

::: {.callout-important appearance="default"}
## Estado del Arte
* Modelos de Deep Learning pueden generar sistemas que entiendan im谩genes, textos, audios, videos, grafos, etc.
:::
::::

# Prerrequisitos

## Tensores {.smaller}

> Corresponde a una generalizaci贸n de los vectores y matrices que permite representar datos de m煤ltiples dimensiones.


:::: {.columns style="font-size: 120%;"}
::: {.column width="30%"}
#### Escalares (Orden 0)
$$-1, 11.27, \pi$$
:::
::: {.column width="30%"}
#### Vectores Filas (Orden 1)
$$\begin{bmatrix}1.0 & -0.27 & -1.22\end{bmatrix}$$

#### Vectores Columnas (Orden 1)
$$\begin{bmatrix}
1 \\
-0.27\\
-1.22
\end{bmatrix}$$
:::

::: {.column width="25%"}
#### Matrices (Orden 2)
$$\begin{bmatrix}
1.0 & -0.27 & 3\\
3.15 & 2.02 & 1.2\\
-1.22& 0.55 & 3.97 \\
\end{bmatrix}$$
:::
::::

::: {.callout-note appearance="default" style="font-size: 130%;"}
## Normalmente los tensores utilizan un mismo tipo de dato: Integers o Float es lo m谩s com煤n.
:::

## Tensores {.smaller}

:::: {.columns}
::: {.column width="40%"}
#### Tensores (Orden 3+)
$$\begin{bmatrix}
    \begin{bmatrix}
        0.2 & 0.1 & -0.25 \\
        0.1 & -1.0 & 0.22\\
    \end{bmatrix} \\
    \begin{bmatrix}
        0.24 & 0.1 & -0.25 \\
        0.05 & -0.69 & 0.98
    \end{bmatrix} \\
    \begin{bmatrix}
        0.66& -1.0 & 0.22\\
        -0.07 & -0.59 & 0.99
    \end{bmatrix} \\
    \begin{bmatrix}
        0.16& 1.0 & 3.22\\
        9.17 & 7.19 & 9.99
    \end{bmatrix}
\end{bmatrix}
$$
:::
::: {.column width="60%"}
::: {.callout-tip appearance="default"}
## Nomenclatura

* $\alpha$, $\beta$, $\gamma$: Min煤sculas griegas denotan a Escalares.
* x, y, z: Min煤sculas latinas denotan a Vectores.
* X, Y, Z: May煤sculas latinas denotan a Matrices o Tensores.
:::
::: {.callout-important appearance="default"}
## Shape/Tama帽o: Tama帽o del tensor, tiene tantas dimensiones como su orden.
* **Escalar**: No tiene dimensiones.
* **Vector**: Tama帽o es equivalente al n煤mero de elementos del vector. (3,)
  * A veces se usa la versi贸n (1,3) para vectores filas y (3,1) para vectores columnas.
* **Matrices**: Tama帽o es equivalente al n煤mero de filas y columnas. Ejemplo: (3,3)
* **Tensores**: Tama帽o es equivalente al n煤mero de matrices que lo componen y el n煤mero de filas y columnas de cada una de ellas. Ejemplo: (4,2,3)
:::

::: {.callout-warning appearance="default" icon=false}
## Importante
* La primera dimensi贸n del **shape** se conoce como **Batch Size** el cual denota la cantidad de elementos de orden inferior. 

* (3, ) tenemos 3 escalares.
* (3,2) tenemos 3 vectores filas de 2 elementos cada uno.
* (4,2,3) tenemos 4 matrices de (2,3) cada una.
:::
:::
::::

## Vectores: Suma {.smaller}


> Corresponde a un arreglo unidimensional de n煤meros reales. Se puede representar como fila o columna. Por convenci贸n denotaremos $\bar{x}$ como vector columna y $\bar{x}^T$ como vector fila.

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Suma
* Permite sumar dos vectores de igual tama帽o dimensi贸n por dimensi贸n.

Ej: $[7,2]^T + [2,3]^T = [9,5]^T$

:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Conmutatividad: $\bar{x}+\bar{y} = \bar{y}+\bar{x}$
* Asociatividad: $(\bar{x}+\bar{y})+\bar{z} = \bar{x}+(\bar{y}+\bar{z})$
* Elemento Neutro: $\bar{x} + \bar{0} = \bar{x}$
:::

::::


![](img/clase-1/vector_sum.png){.lightbox fig-align="center"}


## Vectores: Ponderaci贸n {.smaller}


> Corresponde a un arreglo unidimensional de n煤meros reales. Se puede representar como fila o columna. Por convenci贸n denotaremos $\bar{x}$ como vector columna y $\bar{x}^T$ como vector fila.

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Ponderaci贸n
* Permite multiplicar/ponderar cada dimensi贸n del vector por un escalar.

  Ej: $2 \cdot [3,2]^T = [6,4]^T$
:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Distributividad Escalar: $a(\bar{x}+\bar{y}) = a\bar{x} + a\bar{y}$
* Distributividad Vectorial: $(a+b)\bar{x} = a\bar{x} + b\bar{x}$
* Elemento Neutro: $1\cdot \bar{x} = \bar{x}$
* Compatibilidad: $a(b\bar{x}) = (ab)\bar{x}$
:::

::::


![](img/clase-1/vector_scaling.png){.lightbox fig-align="center"}

## Vectores: Norma {.smaller}

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Norma (Euclideana)
Para un vector $\bar{x}=[x_1, ..., x_n] \in \mathbb{R}^n$ se define la norma como: 

$$||\bar{x}|| = \sqrt{\sum_{i=1}^n x_i^2}$$


:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Desigualdad Triangular: $||\bar{x}+\bar{y}|| \leq ||\bar{x}|| + ||\bar{y}||$
* $||\alpha \bar{x}||= |\alpha| \cdot ||\bar{x}||$
* $||\bar{x}|| = 0 \Longleftrightarrow \bar{0}$
:::

::::

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-caution icon=false width="50%" appearance="default"}
### Aplicaci贸n
 La norma permite calcular la distancia entre dos vectores.

$$d_{x,y} = ||\bar{x} - \bar{y}||$$

**Tambi茅n servir铆a para puntos. 驴Por qu茅?**


:::
::::{.column}
![](img/clase-1/norm_distance.png){.lightbox fig-align="center" width="80%"}
:::

::::


## Vectores: Producto Interno (Producto Punto) {.smaller}

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Inner Product or Dot Product
El producto interno entre dos vectores en $\mathbb{R}^n$ se define como:

$\bar{x} = [x_1, ..., x_n]$ e $\bar{y} = [y_1, ..., y_n]$

$$ \bar{x} \cdot \bar{y} = ||\bar{x}|| ||\bar{y}|| Cos\theta =  \sum_{i=1}^n x_i y_i = x_1 y_1 + ... + x_n y_n$$

A veces el producto interno se denota como $\bar{x}^T \bar{y}$ o $\langle \bar{x}, \bar{y} \rangle$.
:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Conmutatividad: $\bar{x} \cdot \bar{y} = \bar{y} \cdot \bar{x}$
* Linealidad: $(\alpha \bar{x})\cdot \bar{y} = \alpha(\bar{x}\cdot \bar{y})$
* Distributividad: $\bar{x} \cdot (\bar{y} + \bar{z}) = (\bar{x} \cdot \bar{y}) + (\bar{x} \cdot \bar{z})$
* $||\bar{x}||^2 = \bar{x} \cdot \bar{x}$
:::

::::

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-caution icon=false width="50%" appearance="default"}
### Aplicaciones
* Ortogonalidad: Dos vectores son ortogonales si su producto interno es cero.
* Similaridad: Se puede usar el Cosine Similarity para calcular qu茅 tan parecidos son dos vectores.


:::
::::{.column .callout-important icon=false width="50%" appearance="default"}
## Cosine Similarity

$$sim(\bar{a}, \bar{b}) = \frac{\bar{a} \cdot \bar{b}}{||\bar{a}|| \cdot ||\bar{b}||}$$

* 1 implica misma direcci贸n (id茅nticos)
* -1 implica direcciones opuestas (opuestos).
* 0 implica totalmente distintos (ortogonales).
:::
::::

## Vectores: Otras Propiedades

::: {.callout-tip appearance="default"}
#### Combinaci贸n Lineal
* Se denomina una combinaci贸n lineal de vectores a la suma ponderada de estos.

Ej: $\bar{w} = \alpha \cdot \bar{x} + \beta \cdot \bar{y} + \gamma \cdot \bar{z}$

$\bar{w}$ es una combinaci贸n lineal de los vectores $\bar{x}, \bar{y}, \bar{z}$.
:::

::: {.callout-note appearance="default"}
#### Independencia Lineal
* Un conjunto de vectores es linealmente independiente si:

$\alpha_1 \cdot \bar{x}_1 + \alpha_2 \cdot \bar{x}_2 + ... + \alpha_n \cdot \bar{x}_n = 0$ implica que $\alpha_i = 0$ para todo $i$.
:::

::: {.callout-important appearance="default"}
##  Ojito
Hay otras propiedades sumamente importantes de vectores, por lo que coloquen atenci贸n al curso de Algebra Lineal.
:::

# Matrices

## Matrices: Definici贸n {.smaller}

::::{.columns style="font-size: 120%;"} 
> Corresponde a un arreglo bidimensional de n煤meros reales. Se dice que una matriz es de $m\times n$ o que es $\mathbb{R}^{m \times n}$ cuando tiene $m$ filas y $n$ columnas.

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,n} \\
A_{2,1} & \dots & A_{2,n} \\
\vdots & \ddots & \vdots \\
A_{m, 1} & \dots & A_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$

::: {.callout-important appearance="default"}
## 
* Normalmente se utiliza $m$ para denotar el n煤mero de registros y $n$ como el n煤mero de features de un dataset tabular (o tambi茅n conocido como Dataframe).
* Si $m=n$ nos referimos a una matriz cuadrada.
:::
::::

## Matrices: Notaci贸n {.smaller}

:::: { style="font-size:110%;"}
Si $A$ es una matriz entonces: 

* $A_{i,j}$ corresponde al elemento en la fila $i$ y columna $j$. Es decir, un escalar.
* $A_{i,:}$ corresponde a la fila $i$ completa. Es decir, un vector fila.
* $A_{:,j}$ corresponde a la columna $j$ completa. Es decir un vector columna.

:::: {.columns}
::: {.column width="50%"}
$$A = \begin{bmatrix}
0.2 & 1 & -5.2 & 3.1 & -1.3 \\
-0.5 & 10 & 0 & 3.1 & 3 \\
2 & 25 & -5.2 & 0 & 0 \\
100 & 3.4 & 4.1 & 0 & 42
\end{bmatrix}$$
:::
::: {.column .callout-important width="50%" appearance="default"}
### Importante: Recordar que los 铆ndices en Python son 0-based.

* $A_{2,4} = 3.1$
* $A_{:,3} = \begin{bmatrix}-5.2 & 0 & -5.2 & 4.1\end{bmatrix}^T$
* $A_{1,:} = \begin{bmatrix} 0.2, 1, -5.2, 3.1, -1.3\end{bmatrix}$


:::
::::
::::


## Matrices: Suma {.smaller}

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Suma
* Permite sumar dos matrices elemento a elemento.

  Ej: Sea $A$ y $B$ dos matrices: 

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,n} \\
A_{2,1} & \dots & A_{2,n} \\
\vdots & \ddots & \vdots \\
A_{m, 1} & \dots & A_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$

$$B = \begin{bmatrix}
B_{1,1} & \dots & B_{1,n} \\
B_{2,1} & \dots & B_{2,n} \\
\vdots & \ddots & \vdots \\
B_{m, 1} & \dots & B_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$

:::

:::: {.column}

::::{.callout-warning width="50%" appearance="default" icon=false}
## Resultado
$$A + B = \begin{bmatrix}
A_{1,1} + B_{1,1} & \dots & A_{1,n} + B_{1,n} \\
A_{2,1} + A_{2,1} & \dots & A_{2,n} + B_{2,n} \\
\vdots & \ddots & \vdots \\
A_{m, 1} + B_{m,1} & \dots & A_{m,n} + B_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$
:::

::::{.callout-tip width="50%" appearance="default"}
### Propiedades
* Asociatividad: $(A + B) + C = A + (B + C)$
* Conmutatividad: $A + B = B + A$
* Elemento Neutro: $A + 0 = A$
* Elemento Inverso: $A + (-A) = 0$
:::


::::
::::


## Matrices: Ponderaci贸n {.smaller}

::::{.columns style="font-size: 120%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Ponderaci贸n
* Permite multiplicar/ponderar cada elemento de la matriz por un escalar.

  Ej: Sea $A$ una matriz: 

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,n} \\
A_{2,1} & \dots & A_{2,n} \\
\vdots & \ddots & \vdots \\
A_{m, 1} & \dots & A_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$

y $\gamma$ un escalar.
:::

:::: {.column}

::::{.callout-warning width="50%" appearance="default" icon=false}
## Resultado
$$\gamma \cdot A = \begin{bmatrix}
\gamma \cdot A_{1,1} & \dots & \gamma \cdot A_{1,n} \\
\gamma \cdot A_{2,1} & \dots & \gamma \cdot A_{2,n} \\
\vdots & \ddots & \vdots \\
\gamma \cdot A_{m, 1} & \dots & \gamma \cdot A_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$
:::

::::{.callout-tip width="50%" appearance="default"}
### Propiedades
* Distibutividad Escalar: $\gamma(A + B) = \gamma A + \gamma B$
* Distibutividad Matricial: $(\gamma + \delta) A = \gamma A + \delta A$
* Compatibilidad: $(\gamma \delta) A = \gamma (\delta A) = \delta (\gamma A)$
:::


::::
::::

## Transpuesta y Reshape {.smaller}

::::{.columns style="font-size: 120%;"} 
:::{.column .callout-tip width="50%" appearance="default"}
## Transpuesta

Sea: 

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,n} \\
A_{2,1} & \dots & A_{2,n} \\
\vdots & \ddots & \vdots \\
A_{m, 1} & \dots & A_{m,n} \\
\end{bmatrix} \in \mathbb{R}^{m \times n}$$

Entonces, $A^T$ se define como: 


$$A^T = \begin{bmatrix}
A_{1,1} & \dots & A_{1,m} \\
\vdots & \ddots & \vdots \\
A_{n,1} & \dots & A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

Es decir, intercambiamos filas por las columnas y viceversa.
:::

:::{.column .callout-note width="50%" appearance="default"}
### Reshape

$$B = \begin{bmatrix}
1 & 3 & 5 \\
1 & 7 & 9 \\
4 & 6 & 7 \\
3 & 3 & 5 \\
\end{bmatrix} \in \mathbb{R}^{4 \times 3}$$

Podemos hacer un reshape a (6,2)

$$B_{reshaped} = \begin{bmatrix}
1 & 3 \\
5 & 1 \\
7 & 9 \\
4 & 6 \\
7 & 3 \\
3 & 5
\end{bmatrix} \in \mathbb{R}^{6 \times 2}$$
:::
::::

## Producto Matriz-Vector (Por la derecha) {.smaller}

A diferencia de todas las otras operaciones, el producto entre una matriz y un vector no es conmutativo. 

#### Post-multiplicaci贸n (Multiplicaci贸n por la derecha)


:::: {.columns}
:::{.callout-note .column width="40%" appearance="default" style="font-size: 110%;"}
## Sea

$$\bar{y} = A \cdot \bar{x}$$


$$A = \begin{bmatrix}
2 & 3 & 0 \\
1 & 0 & 7
\end{bmatrix}$$

$$\bar{x} = \begin{bmatrix}
4 \\
2 \\
1
\end{bmatrix}$$

:::

:::{.callout-important .column width="70%" appearance="default" style="font-size: 110%;"}
## Atenci贸n
La post-multiplicaci贸n se puede ver como la combinaci贸n lineal de las columnas de una matriz por cada elemento del vector.
$$
\begin{align}
\bar{y} = A \cdot \bar{x} &= \begin{bmatrix}
2 \cdot 4 + 3 \cdot 2 + 0 \cdot 1 \\
1 \cdot 4 + 0 \cdot 2 + 7 \cdot 1
\end{bmatrix} \\
&= 4 \cdot \begin{bmatrix}2 \\ 1\end{bmatrix} + 2 \cdot \begin{bmatrix}3 \\ 0\end{bmatrix} + 1 \cdot \begin{bmatrix}0 \\ 7\end{bmatrix} \\
&= \begin{bmatrix}14 \\ 11\end{bmatrix}
\end{align}$$
:::
::::

:::{.callout-caution appearance="default" icon=false style="font-size: 120%;"}
### 
* La multiplicaci贸n s贸lo es v谩lida si la dimensi贸n de las columnas de la matriz es igual a la dimensi贸n del vector. El resultado siempre es un vector columna.

* **La multiplicaci贸n de una fila por una columna es equivalente al Producto Interno.** Es decir, $\bar{y}_{i,:} = A_{i,:} \cdot \bar{x}$
:::


## Producto Matriz-Vector (Por la izquierda) {.smaller}

A diferencia de todas las otras operaciones, el producto entre una matriz y un vector no es conmutativo. 

#### Pre-multiplicaci贸n (Multiplicaci贸n por la izquierda)

:::: {.columns}
:::{.callout-note .column width="40%" appearance="default" style="font-size: 110%;"}
## Sea

$$\bar{y}^T = \bar{x}^T \cdot A$$


$$A = \begin{bmatrix}
2 & 3 & 0 \\
1 & 0 & 7
\end{bmatrix}$$

$$\bar{x} = \begin{bmatrix}
2 & 1
\end{bmatrix}$$

:::

:::{.callout-important .column width="70%" appearance="default" style="font-size: 110%;"}
## Atenci贸n
La pre-multiplicaci贸n se puede ver como la combinaci贸n lineal de las filas de una matriz por cada elemento del vector.
$$
\begin{align}
\bar{y}^T = \bar{x}^T \cdot A &=
\begin{bmatrix}
(2 \cdot 2 + 1 \cdot 1)  & (2 \cdot 3 + 1 \cdot 0) & (2 \cdot 0 + 1 \cdot 7) \\
\end{bmatrix} \\
&= 2 \cdot \begin{bmatrix} 2 & 3 & 0\end{bmatrix} + 1 \cdot \begin{bmatrix} 1 & 0 & 7\end{bmatrix} \\
&= \begin{bmatrix}5 & 6 & 7\end{bmatrix}
\end{align}
$$
:::
::::

:::{.callout-caution appearance="default" icon=false style="font-size: 130%;"}
### 
* **La multiplicaci贸n s贸lo es v谩lida si la dimensi贸n de las filas de la matriz es igual a la dimensi贸n del vector. El resultado siempre es un vector fila**
:::
## Producto Matriz-Matriz {.smaller}

Corresponde a una operaci贸n que permite multiplicar 2 matrices si las columnas de la primera son iguales a las filas de la segunda. Una matriz de $n \times p$ multiplicada con una de $p \times m$ nos dar谩 una matriz de $n \times m$. La manera de multiplicar es tomar cada fila de la primera y multiplicarla por cada columna de la segunda.

:::{.callout-important appearance="default" icon=false style="font-size: 115%;"}
## Ojito!!

* La multiplicaci贸n matricial es equivalente a $m$ post-multiplicaciones Matriz-Vector, stackeadas hacia el lado.
* Tambi茅n se puede ver como $n$ pre-multiplicaciones Matriz-Vector, stackeadas hacia abajo.

$$
\begin{align}
AB &= \begin{bmatrix}
A_{1,1} & \dots & A_{1,p} \\
\vdots & \ddots & \vdots \\
A_{n, 1} & \dots & A_{n,p} \\
\end{bmatrix}
\begin{bmatrix}
B_{1,1} & \dots & B_{1,m} \\
\vdots & \ddots & \vdots \\
B_{p, 1} & \dots & B_{p,m} \\
\end{bmatrix} \\
&= \begin{bmatrix}
| & &  | \\
A \cdot B_{:,1}& \dots &  A \cdot B_{:,m} \\
| & &  | \\
\end{bmatrix}\\
&= \begin{bmatrix}
- & A_{1,:} \cdot B & - \\
& \vdots &  \\
- & A_{n,:} \cdot B & - \\
\end{bmatrix}\\
\end{align}$$
:::

## Producto Matriz-Matriz: Propiedades tiles {.smaller}


:::: {.columns}
::: {.column .callout-caution appearance="default" style="font-size: 110%;"}
## Supongamos el siguiente caso:
$$A = \begin{bmatrix}
4 & 3 & 2 \\
2 & 2 & 4 \\
4 & 4 & 4
\end{bmatrix}
B = \begin{bmatrix}
1 & 2 & 1 \\
2 & 3 & 4 \\
4 & 3 & 1 \\
\end{bmatrix}
$$

$$
AB = \begin{bmatrix}
18 & 18 & 23 \\
22 & 22 & 14 \\
28 & 24 & 32 \\
\end{bmatrix} 
$$
:::



::: {.column .callout-important appearance="default" style="font-size: 110%;"}
## Permutaci贸n de Columnas en Post-multiplicaci贸n

Si permuto columnas de
$$B^* = \begin{bmatrix}
1 & 1 & 2 \\
2 & 4 & 3 \\
4 & 1 & 3 \\
\end{bmatrix}
AB^*= \begin{bmatrix}
18 & 23 & 18\\
22 & 14 & 22\\
28 & 32 & 24\\
\end{bmatrix} 
$$
:::
::::

::: {.callout-important appearance="default" style="font-size: 130%;"}
## Recordar que la multiplicaci贸n no es conmutativa. $AB \neq BA$.
:::

## Otros Productos {.smaller}


:::: {.columns}
:::{.callout-warning .column width="30%" icon=False appearance="default" style="font-size: 120%;"}
## Hadamard Product

Corresponde a otra operaci贸n que permite multiplicar 2 matrices si y s贸lo si tienen el mismo tama帽o. La multiplicaci贸n se realiza elemento a elemento.


$$A = \begin{bmatrix}
2 & 3 & 0 \\
1 & 0 & 7
\end{bmatrix}$$

$$B = \begin{bmatrix}
2 & 5 & 1 \\
2& 3 & 7
\end{bmatrix}$$

$$A \odot B = \begin{bmatrix} 4 & 15 & 0 \\ 2 & 0 & 49\end{bmatrix}$$
:::

:::{.callout-caution .column width="70%" icon=False appearance="default" style="font-size: 120%;"}
## Outer Product (Producto Externo)

Corresponde a otra operaci贸n que permite multiplicar 2 vectores. El resultado es una matriz de tama帽o $d1 \times d2$ donde $d1$ es la dimensi贸n del primer vector y $d2$ es la dimensi贸n del segundo vector.

$$\bar{x} = \begin{bmatrix}
2 \\
-1 \\
3
\end{bmatrix} \, \bar{y} = \begin{bmatrix}
4 \\
1 \\
5 \\
-2
\end{bmatrix}$$

$$ 
\begin{align} 
\bar{x} \otimes \bar{y} = \bar{x} \cdot \bar{y}^T &= \begin{bmatrix}
2 \cdot 4 & 2 \cdot 1 & 2 \cdot 5 & 2 \cdot -2 \\
-1 \cdot 4 & -1 \cdot 1 & -1 \cdot 5 & -1 \cdot -2 \\
3 \cdot 4 & 3 \cdot 1 & 3 \cdot 5 & 3 \cdot -2
\end{bmatrix} \\
&= \begin{bmatrix}
8 & 2 & 10 & -4 \\
-4 & -1 & -5 & 2 \\
12 & 3 & 15 & -6
\end{bmatrix}
\end{align}
$$

:::
::::


## Batch Product {.smaller}

Este tipo de operaci贸n es bastante poco com煤n en otras 谩reas, pero extremadamente com煤n en Deep Learning. 


::: {.callout-tip appearance="default" }
## Ejemplo
**驴Qu茅 pasa si queremos calcular la multiplicaci贸n de un tensor de dimensiones (2, 3, 2) y otra de (2, 2, 4)?**

* El resultado es un tensor de dimensiones (2, 3, 4). Podemos interpretarlo como que se har谩n 2 multiplicaciones a matrices de (3,2) y (2,4) respectivamente (las cuales son compatibles).

:::


:::: {.columns}
::: {.column}
::: {style="font-size: 80%;"}
$$A = \begin{bmatrix}
\begin{bmatrix}
2 & 2 \\
2 & 3 \\
1 & 2 \\
\end{bmatrix} \\
\begin{bmatrix}
1 & 2 \\
4 & 1 \\
1 & 4 \\
\end{bmatrix} \\
\end{bmatrix}
B = \begin{bmatrix}
\begin{bmatrix}
4 & 3 & 4 & 4 \\
3 & 3 & 3 & 2 \\
\end{bmatrix} \\
\begin{bmatrix}
4 & 2 & 4 & 4 \\
4 & 1 & 1 & 4 \\
\end{bmatrix}
\end{bmatrix}
$$


$$
AB = \begin{bmatrix}
\begin{bmatrix}
14 & 12 & 14 & 12 \\
17 & 15 & 17 & 14 \\
10 & 9 & 10 & 8 \\
\end{bmatrix} \\
\begin{bmatrix}
12 & 4 & 6 & 12 \\
20 & 9 & 17 & 20 \\
20 & 6 & 8 & 20 \\
\end{bmatrix} \\
\end{bmatrix}
$$ 
:::
:::
::: {.column .callout-important appearance="default" .fragment}
## Importante
* Es importante notar que para que esta multiplicaci贸n sea v谩lida. Las dimensiones de las **matrices internas** deben ser compatibles. 
* El **Batch Size** tiene que ser id茅ntico. 

:::
::::

# 隆隆Eso es todo!!

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est谩 licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
