---
title: "TICS-579-Deep Learning"
subtitle: "Clase 2: Introducci칩n a las Redes Neuronales"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---


# 쯇or qu칠 Machine Learning y Deep Learning no son lo mismo?

## Machine Learning vs Deep Learning  {.smaller}

> Es sabido que la mejor manera de mejorar el performance de un algoritmo en part칤cular no es con ajuste de Hiperpar치metros sino crear features que sean representativas del problema.


![](img/clase-2/ml-dl.jpeg){.lightbox fig-align="center"}


::: {.callout-caution appearance="default" icon=false}
## 游뱁 쮺칩mo se mejora un modelo?
* Normalmente el `Machine Learning` busca generar features de manera manual.
* `Deep Learning` busca que el Algoritmo genere esas features de ***manera autom치tica***.

:::

## 쮺칩mo se crean features? {.smaller}

:::{style="font-size: 80%;"}
> Normalmente un approach es agregar features externas/ex칩genas, pero tambi칠n es completamente v치lido crear features nuevas por medio de las existentes.
:::

::::{.columns}
:::{.column}
:::{.callout-warning appearance="default" icon=false}
## Ejemplo
Si $X_1$ es el Largo de un objeto y $X_2$ es el ancho, entonces una feature nueva podr칤a ser $X_3 = X_1 \cdot X_2$, que es el 치rea del objeto.
:::

:::{.callout-note appearance="default" icon=false}
###  쮺칩mo podr칤amos crear nuevas features pero de manera autom치tica?

* Una opci칩n es hacer una combinaci칩n lineal de features existentes.
  * `Esto lo podemos realizar con operaciones matriciales`.

Por ejemplo un set de nuevas features $\phi(X)$ podr칤a ser:

$\phi(X) = X W$ donde $W \in \mathbb{R}^{n \times d}$, donde $d$ es el n칰mero de nuevas features resultantes luego del proceso de creaci칩n.

* ***Esto implica que si $X$ tiene $n$ features, entonces $\phi(X)$ tendr치 $d$ features que son combinaciones de las anteriores***.
:::
:::

:::{.column style="font-size: 75%;"}
![](img/clase-2/new_features.png){.lightbox fig-align="center" width="50%"}

Es decir:

$$f_j^{(i)} = (\bar{x}^{(i)})^T W_{:,j} = \sum_{k=1}^n x_k^{(i)} W_{k,j}$$

Donde $j=1,...,d$ y $i=1,...,m$.
:::
::::


## Regresi칩n Log칤stica + Features {.smaller}

:::{.callout-note appearance="default" icon=false style="font-size: 120%;"}
## 游땸 쯏 si combinamos ambas ideas?
Perfectamente podemos crear una Regresi칩n log칤stica con nuestras nuevas features y combinar todo en un s칩lo modelo:

$$h_\theta(X) = p = \sigma(\phi(X) \theta) = \sigma(X W \theta)$$
:::


::::{.columns}
:::{.column width="60%"}
![](img/clase-2/2-layer_log_reg.png){.lightbox fig-align="center" width="80%"}
:::
:::{.column width="40%"}
:::{.callout-warning appearance="default" icon=false}
## 游뱁 Nuestra Notaci칩n
* Cada nodo se le conoce como Neurona o `Activaci칩n`.
* Cada conjunto de v칠rtices, se les conoce como `Capa` o `Capa de Par치metros`.
  * ***Ojo***: Las capas ***no son*** los conjuntos de Nodos.
:::

:::{.callout-caution appearance="default"}
## 游 Ojo

* Existen convenciones donde cada conjunto de nodos es una capa.
  * A la primera capa de nodos se le llama Capa de `Entrada/Input Layer`.
  * A la 칰ltima capa de nodos se le llama Capa de `Salida/Output Layer`.
  * A las capas intermedias se les llama `Hidden Layers`.
:::

:::
::::

## 쯏 si hacemos nuestro modelo m치s profundo? {.smaller}

Podemos hacer nuestro modelo m치s profundo, agregando m치s capas de features. Por ejemplo:

::::{.columns}
:::{.column}

$$h_\theta(X) = \sigma(X W_1 W_2 \theta)$$

Donde $X \in \mathbb{R}^{m \times n}$, $W_1 \in \mathbb{R}^{n \times d}$, $W_2 \in \mathbb{R}^{d \times k}$ y $\theta \in \mathbb{R}^{k \times 1}$.

:::{.callout-important appearance="default" icon=false}
## 游뱂 El problema

Lamentablemente agregar capas no soluciona el problema de la linealidad. Agregar muchas capas lineales siguen siendo una transformaci칩n lineal.

$$ h_\theta(X) = \sigma(X W_1 W_2 \theta) = \sigma(X \tilde{\theta})$$

Donde $\tilde{\theta} \in \mathbb{R}^{n \times 1}$ es s칩lo otra matriz de par치metros.

:::
:::{.callout-caution appearance="default"}
## Atenci칩n: En este caso $\sigma(\cdot)$ tiene como 칰nico prop칩sito acotar la salida entre 0 y 1.
:::
:::

:::{.column}

![](img/clase-2/3-layer_log_reg.png){.lightbox fig-align="center" width="90%"}

:::
::::

## El problema de una Hip칩tesis Lineal {.smaller}


::::{.columns}
:::{.column .fragment}
:::{.callout-tip appearance="default" icon=false}
## Esto Funciona
:::

![](img/clase-2/linear_model.png){.lightbox fig-align="center" width="90%"}

:::
:::{.column .fragment}

:::{.callout-caution appearance="default" icon=false}
## Esto no Funciona
:::
![](img/clase-2/non_linear.png){.lightbox fig-align="center" width="100%"}
:::
::::

:::{.callout-warning appearance="default" icon=false .fragment}
## 游 No podemos salir del Origen
Por definici칩n de una transformaci칩n lineal, si $X=0$ entonces $h_\theta(X)=0$. Eso lamentablemente limita las posibilidades de un modelo de poder generar una buena separaci칩n entre clases.
:::


## 쮼ntonces c칩mo solucionamos este problema? {.smaller}

:::{.callout-note appearance="default" syle="font-size: 130%;"}
## Haremos una transformaci칩n Affine. Es decir, agregaremos un `bias` a nuestra transformaci칩n lineal.

$$\phi_{L+1}(X) = \phi_L(X) W_{L+1} + b_{L+1}^T$$

* Donde:
  * Donde $\phi(X)_{L+1}$ corresponde a las activaciones de la capa $L+1$. $L=0,...,L_{net}-1$.
    * Notar que $\phi_0(X) = X$.
    * Asimismo, $\phi_{L_{net}}(X)$ corresponde a las activaciones de la capa de salida, es decir, las predicciones del modelo.
  * $W_{L+1}$ es la matrix de `pesos/par치metros` que lleva de $n_L$ a $n_{L+1}$ dimensiones.
  * $b_{L+1}^T\in \mathbb{R}^{1 \times n_{L+1}}$ es el vector de `bias` de la capa $L$ (Esto no es un error).

:::

:::{.callout-warning appearance="default" icon=false}
## `춰Pero esto es una operaci칩n inv치lida!` 游뗷 No. Gracias al Broadcasting, esto es equivalente a hacer $1_m \bar{b}^T$, donde $1_m$ es un vector columna de unos de tama침o $m x 1$ Esto implica que cada componente de $\bar{b} se suma igual a todas las muestras.
:::

:::{.callout-tip appearance="default" syle="font-size: 130%;"}
Vamos a utilizar funciones no lineales. **Cualquiera sirve** tal que:

$$\phi_{L+1}(X) = \sigma_{L+1}(\phi_L(X) W_{L+1} + b_{L+1}^T)$$

* Donde $\sigma_{L+1}$ es una funci칩n no lineal que aplica a cada elemento de la matriz $X W_{L+1} + b_{L+1}^T$.
* $\sigma_{L+1}$ puede ser cualquier funci칩n no lineal, pero normalmente utilizamos funciones diferenciables.
:::

## Formalmente: Una Red Neuronal Profunda {.smaller}

Vamos a definir una Red Neuronal Profunda como: 

$$h_\theta(X) = \sigma_{L+1}(\phi_L(X) W_{L+1} + b_{L+1}^T)$$

con $L=0,...,L_{net}-1$.

:::{.callout-caution appearance="default" style="font-size: 120%;"}
## Importante
* $L_{net}$ es el n칰mero de capas de par치metros ocultas de la red neuronal.
  * Notar que $\phi_0(X) = X$.
  * Existen $L_{net} + 1$ capas de activaci칩n. Una que es la capa de entrada $\phi_0(X)$ y $L_{net}$ capas ***"calculadas"***.
:::

## Entrenemos una Red Neuronal a mano {.smaller}


::::{.columns}
:::{.column width="50%"}
$W_1 \in \mathbb{R}^{n \times d1}$

$\bar{b_1}^T \in \mathbb{R}^{1 \times d1}$

$W_2 \in \mathbb{R}^{d1 \times d2}$

$\bar{b_2}^T \in \mathbb{R}^{1 \times d2}$

$$u=X W_1$$
$$\phi_1 = u + 1_m\bar{b_1}^T$$
$$Z_1 = \sigma(\phi_1)$$
$$v=Z_1 W_2$$
$$\phi_2 = v + 1_m\bar{b_2}^T$$
$$p = \sigma(\phi_2)$$
$$L = -\frac{1}{m}\left[y^T log(p) + (1-y)^T log(1-p)\right]$$

:::
:::{.column width="50%"}
![](img/clase-2/2-layer-comp-graph.png){.lightbox} 


:::
::::

## Entrenemos una Red Neuronal a mano {.smaller}

Si $L=-[y^T log(p) + (1-y)^T log(1-p)]$ entonces:

::::{.columns}
:::{.column width="70%"}

$$\frac{\partial L}{\partial p} = -\frac{1}{m} \left[\frac{y^T}{p} - \frac{1 -y)^T}{(1-p)}\right] = \frac{1}{m} \left[\frac{p-y}{p(1-p)}\right]$$

$$\frac{\partial L}{\partial \phi_2} = \frac{\partial L}{\partial p} \cdot \sigma'(\phi_2)=\frac{1}{m} \left[\frac{p-y}{p(1-p)}\right] p (1-p) = \frac{1}{m} \left[p-y\right]$$

$$\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial \phi_2} \cdot 1_m = \frac{1}{m} 1_m^T \left[p-y\right]$$

$$\frac{\partial L}{\partial v} = \frac{\partial L}{\partial \phi_2} \frac{\partial \phi_2}{\partial v} = \frac{\partial L}{\partial \phi_2}$$

$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial v} \frac{\partial v}{\partial W_2} = \frac{\partial L}{\partial v} \cdot Z_1= \frac{1}{m} Z_1^T \left[p-y\right]$$

:::
:::{.column width="30%" .fragment}

:::{.callout-tip appearance="default" icon=false}
## Update Rule para $b_2$
$$b_2 = b_2 - \alpha \frac{1}{m} 1_m^T \left[p-y\right]$$
:::

:::{.callout-note appearance="default" icon=false}
## Update Rule para $W_2$
$$W_2 = W_2 - \alpha \frac{1}{m} Z_1^T \left[p-y\right]$$
:::

:::
::::


## Entrenemos una Red Neuronal a mano {.smaller}

$$\frac{\partial L}{\partial Z_1} =\frac{\partial L}{\partial \phi_2} \frac{\partial \phi_2}{\partial Z_1}=  \frac{\partial L}{\partial \phi_2} \cdot W_2^T = \frac{1}{m}[p-y] W_2^T $$

$$\frac{\partial L}{\partial \phi_1} = \frac{\partial L}{\partial Z_1} \frac{\partial Z_1}{\partial \phi_1}= \frac{\partial L}{\partial Z_1} \cdot \sigma'(\phi_1) = \frac{1}{m}[p-y] W_2^T \odot [Z_1  \odot (1-Z_1)]$$

$$\frac{\partial L}{\partial b_1} =\frac{\partial L}{\partial \phi_1} \frac{\partial \phi_1}{\partial \phi_1}{\partial b_1}=  \frac{\partial L}{\partial \phi_1} \cdot 1_m = \frac{1_m^T}{m} [p-y] W_2^T \odot [Z_1  \odot (1-Z_1)]$$

$$\frac{\partial L}{\partial u} = \frac{\partial L}{\partial \phi_1} \frac{\partial \phi_1}{\partial u} = \frac{\partial L}{\partial \phi_1}$$

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial u} \frac{\partial u}{\partial W_1} = \frac{\partial L}{\partial u} \cdot X = \frac{1}{m} X^T [p-y] W_2^T \odot [Z_1  \odot (1-Z_1)]$$


::::{.columns}
:::{.callout-tip appearance="default" icon=false .column}
## Update Rule para $b_1$
$$b_1 = b_1 - \alpha \frac{1_m^T}{m} [p-y] W_2^T \odot [Z_1  \odot (1-Z_1)]$$
:::

:::{.callout-note appearance="default" icon=false .column}
## Update Rule para $W_1$
$$W_1 = W_1 - \alpha \frac{1}{m} X^T [p-y] W_2^T \odot [Z_1  \odot (1-Z_1)]$$
:::
::::

## Entrenamiento de una Red Neuronal {.smaller}

:::{.callout-warning appearance="default" icon=false style="font-size: 120%;"}
## 俱뫮잺 Forward Pass
* Corresponde al proceso de calcular las activaciones de cada capa para una matriz de dise침o $X$.
* El forward pass permitir치 calcular las variables intermedias que son necesarias para el c치lculo del `Backward Pass` las cuales van variando a medida que los par치metros del modelo se actualizan.
:::

:::{.callout-tip appearance="default" icon=false style="font-size: 120%;"}
## 拘勇 Backward Pass
* Corresponde al proceso de calcular las derivadas de cada una de las variables del modelo con respecto a la funci칩n de p칠rdida.
* Notar que muchos calculos del `Backward Pass` dependen de los resultados del `Forward Pass`.

:::

:::{.callout-important appearance="default" icon=false style="font-size: 120%;"}
## 游 Backpropagation
La combinaci칩n del `Forward Pass` y el `Backward Pass` se conoce como `Backpropagation` o `Retropropagaci칩n`.
:::

# 游땸 Terminamos!!

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est치 licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
