---
title: "TICS-579-Deep Learning"
subtitle: "Clase 6: Redes Convolucionales"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Limitaciones de las FFN {.smaller}

::::{.columns}
:::{.column}
![](img/clase-6/MNIST.png){.lightbox fig-align="center" width="70%"}
:::
:::{.column}
![](img/clase-6/MNIST_net.png){.lightbox fig-align="center" width="70%"}
:::
::::

::: {.columns}
::: {.column}
#### N√∫mero de Parametros: 
* $W_1 = 784 \cdot 256 + 256 = 200960$
* $W_2 = 256 \cdot 128 + 128 = 32896$
* $W_3 = 128 \cdot 10 + 10 = 1290$
* Total = 235,146.
:::
::: {.column}
![](img/clase-7/MNIST_params_MLP.png){.lightbox fig-align="center"}

::: {.callout-tip .fragment}
¬øY si tengo una im√°gen de $512 \times 512$? **67,143,306** de par√°metros.
:::
:::
::: 

## Limitaciones de las FFN {.smaller}

::: {.columns}
::: {.column}
![](img/clase-7/translation_invariance.png){.lightbox fig-align="center"}
:::
::: {.column .fragment}
Translation Invariance
: Se refiere a la capacidad de poder detectar un patr√≥n/objeto en diferentes posiciones de la im√°gen.

:::{.callout-caution appearance="default" icon="false"}
## Spoiler: Las FFN no tienen esta propiedad.
:::
:::
::: 

::: {.columns .fragment}
::: {.column}
![](img/clase-7/translation_invariance.gif){.lightbox fig-align="center"}
:::
::: {.column}
::: {.callout-important}
Las FFN no son capaces de ver patrones globales sino que se enfocan en el valor preciso de una feature. Si el objeto cambia de posici√≥n las features cambian a valores completamente distintos haciendo que la red tenga mayor tendencia al error.
:::
:::
::: 

## Im√°genes {.smaller}

Imagen
: Definiremos una im√°gen como un Tensor de Orden 3, en el cual se representa H, W y C (Altura, Ancho y Canales). 


::: {.columns}
::: {.column width="40%"}
![](img/clase-7/channels.png){.lightbox fig-align="center"}
:::
::: {.column width="30%"}
![](img/clase-7/rgb.jpeg){.lightbox fig-align="center" width="50%"}
:::
::: {.column width="30%"}
![](img/clase-7/medical.png){.lightbox fig-align="center"}
![](img/clase-7/satellite.jpeg){.lightbox fig-align="center"}
:::
::: 

::: {.callout-important}
La convenci√≥n m√°s com√∫n es utilizar im√°genes de 24-bits 3 canales de $2^8$ valores (es decir 8-bits) que representan la intensidad de los colores Rojo (R), Verde (G) y Azul (B). Las dimensiones de $H$ y $W$ definen la resoluci√≥n en p√≠xeles de la im√°gen.
:::

## Im√°genes {.smaller}

> Para importar im√°genes en Pytorch existen distintas librer√≠as como PIL u OpenCV. Ambas usan la convenci√≥n de $(H,W,C)$, la diferencia est√° en el orden de los canales. PIL utiliza la convenci√≥n RGB, mientras que OpenCV utiliza BGR por lo que se necesitan algunas transformaciones adicionales.

::: {.callout-important}
Lamentablemente la convenci√≥n que escogi√≥ Pytorch es que una im√°gen tiene (C, H, W), es decir, primero canales, y luego alto y ancho. Normalmente las librer√≠as no usan esta convenci√≥n por lo que una permutaci√≥n o transposici√≥n de dimensiones va a ser necesario casi la mayor√≠a de las veces.
:::

::: {.columns}
::: {.column width="25%"}
```{.python}
image = torch.tensor(...)
```
![](img/clase-7/foto.png){.lightbox fig-align="center"}

:::
::: {.column width="25%"}
```{.python}
image[0,:,:]
imagen[0] # alternativa
```
![](img/clase-7/foto_red.png){.lightbox fig-align="center"}
:::
::: {.column width="25%"}
```{.python}
image[1,:,:]
imagen[1] # alternativa
```
![](img/clase-7/foto_green.png){.lightbox fig-align="center"}
:::
::: {.column width="25%"}
```{.python}
image[2,:,:]
imagen[2] # alternativa
```
![](img/clase-7/foto_blue.png){.lightbox fig-align="center"}
:::
::: 

## Im√°genes 


::: {.columns}
::: {.column}
::: {.callout-important}
Cuando tenemos un batch de im√°genes entonces tendremos un Tensor de Orden 4 dimensiones (B,C,H,W), donde $B$ representa el Batch Size, $C$ el n√∫mero de Canales, $H$ el alto y $W$ el ancho.
:::

::: {.callout-tip}
Luego un Tensor de Dimensiones (32,3,224,512) implica que tenemos 32 im√°genes RGB de dimensiones $224\times512$.
:::
:::
::: {.column}
![](img/clase-7/image_batch.jpg){.lightbox fig-align="center"}
:::
::: 

## Redes Convolucionales: Definici√≥n e Inspiraci√≥n {.smaller}

Redes Convolucionales (CNN)
: Son un tipo de red neuronal cuyos par√°metros entrenables son filtros (tambi√©n llamados Kernels) que aprenden a detectar patrones en los datos de entrada. Aplicando estos Kernels se generan feature maps, que representan la presenecia y localizaci√≥n de ciertos patrones.

::: {.callout-note}
Existe el mito de que las Redes Convolucionales se inspiraron en el funcionamiento del Cortex Visual humano. **No s√© si es tan as√≠**.

El mito dice que las CNNs fueron dise√±adas para imitar el cortex visual humano. Esto viene de los trabajos de Hubel y Wiesel (d√©cada de 1960), que estudiaron c√≥mo las neuronas en la corteza visual de gatos respond√≠an a est√≠mulos:

* Descubrieron neuronas simples que respond√≠an a l√≠neas en cierta orientaci√≥n y posici√≥n.
* Descubrieron neuronas complejas que respond√≠an a patrones similares, pero en distintas posiciones (invarianza local).
:::

::: {.callout-caution}
***¬øPor qu√© necesitamos Redes Convolucionales?*** Evitar la sobreparametrizaci√≥n. ¬øPor qu√© esto es un problema?
:::

## Redes Convolucionales: Definici√≥n e Inspiraci√≥n {.smaller}

:::{.callout-warning appearance="default" icon="false"}
## üîî Importancia
No es exagerado afirmar que las CNNs han sido la arquitectura m√°s influyente en Deep Learning, ya que han impulsado avances importantes en tareas de visi√≥n por computador, como clasificaci√≥n de im√°genes, detecci√≥n de objetos y segmentaci√≥n sem√°ntica. Adem√°s, contribuyeron a que el Deep Learning ganara popularidad en la industria tecnol√≥gica y superara la √©poca conocida como el AI Winter.
:::

::: {.columns}
::: {.column}
### üóìÔ∏è Algunos hitos importantes:
* **1990**: Yann LeCun et al. propone uno de los primeros intentos de CNN, el cual va agregando features m√°s simples en features m√°s complejas progresivamente.
* **1998**: Yann LeCun, propone LeNet-5 con 2 redes convolucionales y 3 FFN.
* **2012**: Krizhevsky, Sutskever y Hinton proponen AlexNet (5 capas convolucionales y 3 FFN), el cual obtiene **SOTA performance** en ImageNet.
:::
::: {.column}
![](img/clase-7/alexnet-paper.png){.lightbox fig-align="center"}
:::
::: 

## Entendiendo el Kernel {.smaller}

::: {.columns}
::: {.column}

##### Gaussian Blur
![](img/clase-7/gaussian_blur.png){.lightbox fig-align="center" width="70%"}


##### L√≠neas Horizontales
![](img/clase-7/horizontal_lines.png){.lightbox fig-align="center" width="70%"}

##### Bordes
![](img/clase-7/bordes.png){.lightbox fig-align="center" width="70%"}

:::
::: {.column}
::: {.callout-note appearance="default"}
## Kernel

El Kernel va a ser el set de par√°metros que la red convolucional va a aprender. En palabras sencillas, la misma red aprende cu√°les son los aspectos m√°s relevantes de la imagen que le permitir√°n entender c√≥mo clasificar o detectar elementos en ella.

:::

::: {.callout-important}
El Kernel se aplica a todos los canales a la vez, lo cu√°l inicialmente lo hace ver como una operaci√≥n bastante costosa computacionalmente.
:::

::: {.callout-tip}
El Kernel introduce el primer hiperpar√°metro de las CNN que es el **Kernel Size**. En general son cuadrados, y de dimensi√≥n impar.
:::
:::
::: 

## Entendiendo la Convoluci√≥n {.smaller}

Convoluci√≥n
: Corresponde a una operaci√≥n para extraer **features maps** en la cual un filtro o kernel se va desplazando en cada secci√≥n de los datos (secuencia, imagen o video). 


::: {.callout-caution appearance="default" icon="false"}
## Ojo
Esto es nuevamente un t√©rmino marketero, porque no es una Convolucional real, sino una operaci√≥n llamada **Cross Correlation**.
:::

Feature Map
: Corresponde a la salida de una convoluci√≥n (equivalente a la Activaci√≥n) y es un nuevo tensor que captura ciertas caracter√≠sticas del dato (secuencia, imagen o video). Cuando se trata de im√°genes, captura features como bordes, cambios de textura, color, formas, o elementos m√°s peque√±os.


::: {.columns}
::: {.column width="65%"}
![](img/clase-7/convolution.gif){.lightbox fig-align="center" width="80%"}
:::
::: {.column width="35%"}
<br>

::: {.callout-warning}
Es importante notar que los features maps son de una **tama√±o menor a la entrada** debido a la operaci√≥n de Convoluci√≥n.
:::
::: {.callout-important}
Se obtendr√°n tantos feature maps como filtros se apliquen.
:::
:::
::: 



## Hiperpar√°metros de la Convoluci√≥n {.smaller}

::: {.columns}
::: {.column width="30%"}

![](img/clase-7/stride.gif){.lightbox fig-align="center"}

::: {.callout-note appearance="default" icon="false"}
## Stride

Se refiere a la cantidad de pasos que el kernel avanza sobre la imagen de entrada. Un ***stride*** mayor genera feature maps m√°s peque√±os y con menos detalles, mientras que un stride menor conserva m√°s informaci√≥n, aunque aumenta el n√∫mero de operaciones requeridas.
:::
:::
::: {.column width="40%"}
![](img/clase-7/convolution_padding.gif){.lightbox fig-align="center" width="130%"}

::: {.callout-tip appearance="default" icon="false"}
## Padding

Se trata de un relleno que facilita el desplazamiento del kernel, evitando que la convoluci√≥n reduzca demasiado la dimensionalidad y permitiendo que se tenga en cuenta la informaci√≥n de los bordes de la imagen. Cuando no se aplica padding se denomina **"valid"**, y cuando se agregan p√≠xeles suficientes para mantener la misma dimensi√≥n se llama ***"same"***.



:::
:::
::: {.column width="30%"}
![](img/clase-7/dilation.gif){.lightbox fig-align="center"}

::: {.callout-important appearance="default" icon="false"}
## Dilation

Se refiere a los espacios (gaps) que se introducen entre los elementos del kernel al aplicarlo. Usar dilation permite aumentar el campo receptivo de la convoluci√≥n, capturando m√°s contexto sin necesidad de incrementar el tama√±o del kernel. Un valor de 1 indica que no se aplica ***dilation.***
:::
:::
::: 

## Convoluci√≥n en Pytorch 

::: {style="font-size: 110%;"}
```{.python}
nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0,dilation=1)
```
:::

::: {style="font-size: 80%;"}
::: {.callout-tip appearance="default" icon="false"}
## Input
Este tipo de redes no requiere que se le den las dimensiones de las entradas, pero s√≠ espera recibir tensores de dimensi√≥n $(N,C_{in}, H_{in},W_{in})$.
:::

::: {.callout-important appearance="default" icon="false"}
## Output
La Red convolucional devuelve un Tensor de Dimensiones $(N,C_{out}, H_{out}, W_{out})$. Donde:

$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - dilation[0]\cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor$$
:::

:::

::: {.callout-warning}
Es importante tener noci√≥n del tama√±o de la imagen para poder escoger un *kernel_size* que recorra la imagen completa y que no deje partes sin convolucionar.
:::

## Partes de una CNN: Pooling {.smaller}

Pooling
: El Pooling es una operaci√≥n de agregaci√≥n que permite ir disminuyendo el tama√±o de las entradas. De esta manera la red puede comenzar a especializarse en aspectos cada vez m√°s finos. 

::: {.callout-note}
El Pooling tambi√©n se aplica de manera m√≥vil como una convoluci√≥n. Pero a diferencia de esta normalmente no genera traslape.
:::
::: {.callout-tip}
Ac√° se introduce otro hiperpar√°metro que es el Pooling Size. En general es cuadrado pero de dimensi√≥n par.
:::

![](img/clase-7/pooling.png){.lightbox fig-align="center" width="35%"} 


## Pooling in Pytorch {.smaller}

::: {style="font-size: 150%;"}
```{.python}
nn.AvgPool2d(kernel_size, stride=None,padding=0)
nn.MaxPool2d(kernel_size, stride=None,padding=0, dilation=1)
```
:::

::: {.callout-important appearance="default" }
## Ojo
`stride=None` implica `stride = kernel_size`.
:::

::: {.callout-warning}
Importante mencionar que Average Pooling no permite Dilation.
:::

$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - dilation[0]\cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor$$

## Arquitectura de una CNN {.smaller}

::: {.columns}
::: {.column width="60%"}
![](img/clase-7/CNN-arch.png){.lightbox fig-align="center" width="60%"} 
:::
::: {.column width="40%"}
::: {.callout-note appearance="default" icon="false"}
## Feature Extractor - Encoder - Backbone
Corresponde al bloque que contiene CNNs que se encargar√° de extraer features. 
:::
::: {.callout-warning appearance="default" icon="false"}
## Flatten
Corresponde a una Operaci√≥n Intermedia que dejar√° todos los p√≠xeles de la imagen como un vector fila que puede ser tomado por la FFN. En estricto rigor prepara las features generadas por la CNN para ser usadas en la parte final de la red.
:::

::: {.callout-tip appearance="default" icon="false"}
## Prediction Head - Head - MLP
Corresponde a una FFN que tomar√° las features aprendidas por la CNN y generar√° una predicci√≥n.
:::
:::
::: 

## MNIST con CNN {.smaller}

![](img/clase-7/CNN_MNIST_arch.png){.lightbox fig-align="center" width="75%"} 

::: {.columns}
::: {.column}
![](img/clase-7/CNN_params.png){.lightbox fig-align="center"} 
:::
::: {.column}
::: {.callout-warning}
* El n√∫mero de Par√°metros para una Red Convolucional con muchas m√°s capas baj√≥ considerablemente, de 67M a 373K de Par√°metros para una imagen de $512 \times 512$.
:::
:::
::: 

## Variante en 1d {.smaller}

::: {.columns}
::: {.column width="60%"}
Conv1d
: Corresponde a la variante de una dimensi√≥n, en la cual la entrada corresponden a secuencias de elementos como podr√≠an ser series de tiempo, audio o hasta cadenas de texto.

::: {.callout-note icon="false"}

En este caso la implementaci√≥n en Pytorch es similar a la 2D s√≥lo que esperando tensores de dimensiones $(N,C_{in}, L_{in})$, donde $C_{in}$ corresponde al n√∫mero de canales, que en el caso de series de tiempo equivale a features, y $L_{in}$ corresponde al largo de la secuencia.
:::

::: {.callout-important icon="false"}
La salida de la Conv1d tendr√° dimensiones $(N,C_{out},L_{out})$ con:

$$L_{out} = \left\lfloor \frac{L_{in} + 2 \cdot padding - dilation \cdot (kernel\_size - 1) - 1}{stride} + 1 \right\rfloor$$

:::
:::
::: {.column width="40%"}

![](img/clase-7/time_series.png){.lightbox fig-align="center" width="80%"} 

![](img/clase-7/audio.png){.lightbox fig-align="center" width="80%"} 
:::
::: 


## Variante en 3d {.smaller}

::: {.columns}
::: {.column width="60%"}
Conv3d
: Corresponde a la variante de tres dimensiones, en la cual la entrada corresponde a secuencias de im√°genes, es decir, videos. 

::: {.callout-note icon="false"}
Este caso tambi√©n es similar s√≥lo que se esperan tensores de dimensiones $(N, C_{in}, D_{in}, H_{in}, W_{in})$ donde $C_in$ corresponde al n√∫mero de canales, $D$ en el caso de un video corresponde al n√∫mero de frames de tama√±o $H_{in} \times W_{in}$.
:::

::: {.callout-important icon="false"}
La salida de la Conv1d tendr√° dimensiones $(N,C_{out},D_{out},H_{out},W_{out})$ con:

$$D_{out} = \left\lfloor \frac{D_{in} + 2 \cdot padding[0] - dilation[0] \cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor$$
$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[2] - dilation[2]\cdot (kernel\_size[2] - 1) - 1}{stride[2]} + 1 \right\rfloor$$

:::
:::
::: {.column width="40%"}


![](img/clase-6/frame-rates.jpg){.lightbox fig-align="center"} 
:::
::: 


## Ejemplos de Arquitecturas

::: {.columns}
::: {.column width="30%"}
![](img/clase-7/resnet.png){.lightbox fig-align="center" width="70%"} 
:::
::: {.column width="40%"}
![](img/clase-7/lenet.png){.lightbox fig-align="center"} 
![](img/clase-7/VGG16.png){.lightbox fig-align="center"}  
:::
::: {.column width="30%"}
![](img/clase-7/EfficientNet.png){.lightbox fig-align="center" width="70%"} 
:::
::: 


# Class Dismissed

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
