---
title: "TICS-579-Deep Learning"
subtitle: "Clase 6: Training Tips & Tricks"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Potenciales problemas al Entrenar un Modelo {.smaller}

> El entrenamiento de un modelo tiene demasiadas variables que pueden influir en el √©xito del modelo. Algunos aspectos relevantes a los que hay que poner √©nfasis al momento de entrenar:

* Lidiar con Variables Categ√≥ricas
* Escala de las Variables
* Determinar convergencia
* Combatir el Overfitting
* Determinar Generalizaci√≥n
* Optimizar Recursos Computacionales/Hardware.
* Prevenir problemas de Vanishing Gradient y Exploding Gradients.

::: {.callout-note}
Muchas de las t√©cnicas que veremos ac√° permiten abordar mejoras en nuestros modelos para uno o m√°s aspectos de los mencionados anteriormente.
:::

## Categorical Variables: One Hot Encoding {.smaller}

Hasta ahora hemos asumido que las variables de entrada son num√©ricas. Pero en la pr√°ctica es muy com√∫n encontrarse con variables categ√≥ricas. En Deep Learning existen dos t√©cnicas para lidiar con este tipo de variables: One-Hot-Encoding y el uso de Embeddings.

::::{.columns}
:::{.column}
:::{.callout-tip icon=false appearance="default"}
## üìã One-Hot-Encoding
* Permite una representaci√≥n dispersa (sparse) de las variables categ√≥ricas. Consiste en crear una columna por cada categor√≠a, y asignar un 1 o un 0 dependiendo si la instancia pertenece o no a dicha categor√≠a.

* Es una representaci√≥n est√°tica sin par√°metros entrenables asociados.

* Genera tantas columnas/features nuevas como categor√≠as existan en la variable original. Por lo tanto, puede generar problemas de dimensionalidad si existen muchas categor√≠as.
:::
:::
:::{.column}
![](img/clase-5/ohe_example.png){.lightbox fig-align="center" width="90%"}
:::
::::


## Categorical Variables: Embeddings {.smaller}

::::{.columns}
:::{.column}
:::{.callout-important icon=false appearance="default"}
## üóûÔ∏è Embeddings
* Un embedding es una representaci√≥n num√©rica de un objeto (palabra, imagen, nodo, etc.) en un espacio vectorial de menor dimensi√≥n que captura sus caracter√≠sticas y relaciones de manera √∫til para un modelo de machine learning. 

* En lugar de trabajar con las categor√≠as crudas, los **embeddings** convierten esos datos en vectores de n√∫meros reales. De esta forma, objetos similares quedan representados por vectores cercanos.

* Los vectores son aprendidos por el modelo durante el entrenamiento, de tal manera que la representaci√≥n aprendida es la optima para el problema espec√≠fico a resolver. Es decir, agrega par√°metros entrenables al modelo.
:::

:::
:::{.column}
![](img/clase-5/emb_example.png){.lightbox fig-align="center"}
:::

::::

En Pytorch esto se puede realizar mediante: `nn.Embedding()`. 

::: {style="font-size: 150%;"}

```{.python }
nn.Embedding(num_embeddings, embedding_dim)
```
:::

* **num_embeddings**: Corresponde al n√∫mero de categ√≥r√≠as.
* **embedding_dim**: El n√∫mero de dimensiones en el cual se quiere representar.

::: {.callout-important}
* Este proceso tiene par√°metros entrenables asociados.
:::



## Normalizaci√≥n

> En general el t√©rmino Normalizaci√≥n est√° muy trillado y en la pr√°ctica se utiliza para referirse a muchos temas distintos. Algunas definiciones conocidas:

::: {.columns}

::: {.column}
### Normalizaci√≥n

$$x_{i\_norm} = \frac{x_i-x_{min}}{x_{max} - x_{min}}$$
Esta operaci√≥n se puede hacer mediante [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) de Scikit-Learn.
:::


::: {.column}
### Estandarizaci√≥n

$$ x_{i\_est} = \frac{x_i - E[x]}{\sqrt(Var[x])}$$

Esta operaci√≥n se puede hacer mediante [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) de Scikit-Learn.

:::
::: 


## Normalizaci√≥n (Batch Norm) {.smaller}

[Paper 2015: Batch Normalization](https://arxiv.org/pdf/1502.03167)

::: {.callout-caution appearance="default" style="font-size: 90%;"}
## ¬øPor qu√©?
* Acelera el entrenamiento
* Disminuye la importancia de los Par√°metros iniciales.
* Regulariza el modelo (un poquito)
* Resuelve el problema de [Internal Covariate Shift](https://machinelearning.wtf/terms/internal-covariate-shift/).
:::

#### Ejemplo: Supongamos que dado la altura y la edad queremos predecir si ser√° deportista de alto rendimiento.

::: {.columns}
::: {.column}
![](img/clase-6/normalize.png){.lightbox fig-align="center"}
:::
::: {.column}
![](img/clase-6/nn_example.png){.lightbox fig-align="center"}

:::
::: 

## Normalizaci√≥n (Batch Norm) {.smaller}

::: {.columns style="font-size: 90%;"}
::: {.column}
![](img/clase-6/normalize.png){.lightbox fig-align="center" width="80%"}

:::
::: {.column}
* Cambios en Altura son mucho m√°s peque√±os que en Edad debido al rango.
* Toma m√°s tiempo optimizar (requiere par√°metros m√°s peque√±os)
* Si el learning rate es alto puede diverger.
* Si el learning rate es bajo implica que demora mucho m√°s en converger.

:::
::: 

::: {.columns style="font-size: 110%;" .fragment}
::: {.column}
![](img/clase-6/post_normalize.png){.lightbox fig-align="center" width="55%"} 
:::
::: {.column}
::: {.callout-tip appearance="default"}
## Pros

* Sin importar el punto inicial, el m√≠nimo se encuentra **casi** a la misma distancia.  
* Es posible utilizar un learning rate m√°s grande sin miedo a diverger.
:::
::: {.callout-important appearance="default"}
## Cons
* M√°s c√°lculos y par√°metros involucrados

:::
:::
::: 

## Normalizaci√≥n (Batch Norm) {.smaller}

![](img/clase-6/batch_norm_nn.png){.lightbox fig-align="center" width="58%"} 

::: {.columns}
::: {.column}
![](img/clase-6/Z1.png){.lightbox fig-align="center" width="60%"}

:::
::: {.column}

![](img/clase-6/Z2.png){.lightbox fig-align="center" width="100%"}

:::
::: 

## Normalizaci√≥n (Batch Norm) {.smaller}

::: {.columns}
::: {.column}
::: {.callout-tip appearance="default"}
## C√°lculo de Estad√≠sticos

$$ \mu_B = \frac{1}{B} \sum_{i=1}^B z^{(i)} = \frac{1}{3}(4 + 7 + 5) = 5.33$$
$$ \sigma_B^2 = \frac{1}{B} \sum_{i=1}^B (z^{(i)} - \mu_B)^2 = 1.555$$

:::
::: {.callout-warning appearance="default"}
## Normalizaci√≥n
$$\widehat{z^{(i)}} = \frac{z^{(i)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
:::
::: {.callout-note appearance="default" .fragment fragment-index=1}
## Scale and Shift: $\gamma$ y $\beta$ son par√°metros.
$$BN_{\gamma,\beta}(z_i)= \gamma \widehat{z_i} + \beta $$
:::
:::
::: {.column}

![](img/clase-6/batch_norm_nn.png){.lightbox fig-align="center" width="70%"} 

#### Z2 norm
![](img/clase-6/Z2_norm.png){.lightbox fig-align="center" width="70%"}

::: {.callout-note appearance="default" .fragment fragment-index=1}
Donde $\gamma$ y $\beta$ son par√°metros aprendidos durante el entrenamiento.
:::

:::
::: 

## Normalizaci√≥n (Batch Norm): Test Time {.smaller}

::: {.callout-caution appearance="default"}
## Problema

La predicci√≥n de una instancia $i$ espec√≠fica, ahora depende de otros elementos dentro del Batch. ***¬øC√≥mo funciona entonces el modelo en Test Time?***
:::

::: {.callout-tip}
Se estiman valores de $\mu_B$ y $\sigma_B$ para usar en inferencia basados en los valores obtenidos en entrenamiento.
:::

::: {.columns}
::: {.column}
::: {.callout-tip appearance="default"}
## Estimaci√≥n de Estad√≠sticos

* $\mu_B^{inf} = E[\mu_B^{j}]$, $j = 1,...,B$
* $\sigma_B^{inf} = \frac{m}{m-1}E[\mu_B^{j}]$, $j = 1,...,B$
:::
:::
::: {.column}
::: {.callout-warning appearance="default"}
## Normalizaci√≥n
$$\widehat{z^{(i)}} = \frac{z^{(i)} - \mu_B^{inf}}{\sqrt{(\sigma_B^{inf})^2 + \epsilon}}$$
:::
:::
::: 

::: {.callout-note appearance="default"}
## Scale and Shift: $\gamma$ y $\beta$ son par√°metros.
$$BN_{\gamma,\beta}(z_i)= \gamma \widehat{z_i} + \beta $$
:::
::: {.callout-important}
Los par√°metros $\gamma$ y $\beta$ son los aprendidos durante el proceso de entrenamiento.
:::

## Normalizaci√≥n (Batch Norm): Consejos {.smaller}

* Andrew Ng propone utilizar BatchNorm justo antes de la funci√≥n de Activacion.
* El paper original tambi√©n propone su uso justo antes de la activaci√≥n.
* Francoise Chollet, creador de Keras dice que los autores del paper en realidad lo utilizaron despu√©s de la funci√≥n de activaci√≥n.
* Adicionalmente existen benchmarks que muestran mejoras usando BatchNorm despu√©s de las funciones de activaci√≥n.

::: {.callout-note}
Entonces, la posici√≥n del BatchNorm termina siendo parte de la Arquitectura, y se debe comprobar donde tiene un mejor efecto.
:::

::: {.callout-warning}
Batchnorm tiene efectos distintos al momento de entrenar o de evaluar/predecir en un modelo. Por lo tanto, de usar Batchnorm es imperativo utilizar los modos `model.train()` y `model.eval()` de manera apropiada.
:::

## Normalizaci√≥n: Layer Norm {.smaller}

[Paper 2016: Layer Normalization](https://arxiv.org/abs/1607.06450)

::: {.callout-important appearance="default"}
## Batch Norm tiene algunos problemas:
* Muy dif√≠cil de calcular en datos secuenciales (lo veremos m√°s adelante).
* Inestable cuando el Batch Size es muy peque√±o.
* Dif√≠cil de Paralelizar.
:::

::: {.callout-tip appearance="default"}
## Beneficios de Layer Norm
* Puede trabajar con secuencias.
* No tiene problemas para trabajar con cualquier tipo de Batch Size.
* Se puede paralelizar, lo cu√°l es √∫til en redes como las RNN.
:::

::: {.callout-note}
* En este caso se realiza la normalizaci√≥n por **capa** o por **Data Point** (instancia). 
* Adem√°s son el elementos cruciales en las Arquitecturas de Transformers.
:::


## Normalizaci√≥n: Layer Norm {.smaller}

::: {.columns}
::: {.column}
![](img/clase-6/Z1.png){.lightbox fig-align="center"}

![](img/clase-6/Z2_ly.png){.lightbox fig-align="center"}
:::
::: {.column .fragment}
$$ \mu_{norm} = \frac{1}{n_i} \sum_{j=1}^{n_i} z_j = \frac{1}{4}(4 + 9 + 6 + 7) = 6.5$$
$$ \sigma_{norm}^2 = \frac{1}{n_i} \sum_{j=1}^{n_i} (z_j - \mu_B)^2 = 3.25$$

::: {.callout-warning appearance="default"}
## Normalizaci√≥n
$$\widehat{z_j} = \frac{z_j - \mu_{norm}}{\sqrt{\sigma_{norm}^2 + \epsilon}}$$
:::
![](img/clase-6/Z2_post_ly.png){.lightbox fig-align="center"}
:::
::: 

## Regularizaci√≥n L2 aka Weight Decay {.smaller}

[Paper 1991: Weight Decay](https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf)

::: {style="font-size: 80%;"}
> En general el gran problema de las Redes Neuronales es el ***Overfitting***. Esto porque las redes neuronales normalmente se denominan como **Overparametrized Models**. ***¬øQu√© significa esto?***

Weight Decay
: > Corresponde a una penalizaci√≥n que se da a los modelos para limitar su complejidad y asegurar que pueda generalizar correctamente en datos no vistos. 

$$ \underset{W_{i:L}}{minimize} \frac{1}{m} \sum_{i=1}^m l(h_\theta(x^{(i)}),y^{(i)}) + \frac{\lambda}{2} \sum_{i=1}^L ||W_i||_f^2$$

Eso implica una transformaci√≥n a nuestro ***Update Rule***:

$$W_i := W_i - \alpha \nabla \frac{1}{m} \sum_{i=1}^m l(h_\theta(x^{(i)}),y^{(i)}) - \alpha \lambda W_i = (1-\alpha\lambda)W_i - \alpha \nabla l(h_\theta(x^{(i)}),y^{(i)})$$
:::


::: {.columns}
::: {.column}
::: {.callout-tip}
Se puede ver que los pesos (weights) se ***contraen*** (decaen) antes de actualizarse en la direcci√≥n del gradiente.
:::
:::
::: {.column}
::: {.callout-important}
Por alguna raz√≥n Pytorch decidi√≥ implementarlo como una propiedad de los ***Optimizers*** cuando en realidad debi√≥ ser de la Loss Function.
:::
:::
::: 

## Dropout {.smaller}

[Paper 2014: Dropout](https://paperswithcode.com/paper/dropout-a-simple-way-to-prevent-neural)

::: {style="font-size: 80%;"}
> A diferencia de la estrategia anterior, este tipo de regularizaci√≥n se aplica a las activaciones de la red (resultados de la Transformaci√≥n Affine, previo a la transformaci√≥n no lineal). 

Definiremos el Dropout como:

$$Z_{i+1} = \sigma(W_i^T Z_i + b_i)$$
$$\widehat{Z_{i+1}} = D(Z_{i+1})$$

donde $D$ implica la aplicaci√≥n de Dropout a la capa $i+1$. El elemento $j$ de la capa $\widehat{Z_i}$ se calcula como:

$$(\widehat{Z_{i+1}})_j = \begin{cases}
\frac{(Z_{i+1})_j}{1-p}  & \text{with prob 1-p} \\
0, & \text{with prob p}
\end{cases}$$

$p$ se conoce como el `Dropout Rate`.

::: {.callout-important}
El factor $\frac{1}{1-p}$ se aplica para mantener la varianza estable luego de haber eliminado activaciones con probabilidad $p$.
:::
:::

::: {.callout-warning}
Dropout se aplica normalmente al momento de entrenar el modelo. Por lo tanto, de usar Dropout es imperativo cambiar al modo `model.eval()` al momento de predecir.
:::


## Weights Initialization {.smaller}

[Paper 2010: Xavier Initialization](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
[Paper 2015: Kaiming Initialization](https://arxiv.org/abs/1502.01852)

Hemos hablado que los m√©todos basados en SGD normalmente utilizan valores aleatorios para partir su entrenamiento, lo cual deja un poco al azar el √©xito de un proceso de entrenamiento.

Existen diversos estudios de c√≥mo `inicializar` los par√°metros para una convergencia √≥ptima. Algunas de las inicializaciones son:

::: {.callout-note appearance="default"}
## Activaciones Triviales
* Constante
* S√≥lo unos
* S√≥lo Zeros
:::

## Weights Initialization {.smaller}

::: {.columns}
::: {.column}
#### Xavier o Glorot Uniforme

Se inicia con valores provenientes de una distribuci√≥n uniforme: $\mathcal{U}(-a,a)$

$$ a = gain \cdot \sqrt{\frac{6}{fan_{in} + fan_{out}}}$$
:::
::: {.column}

#### Xavier o Glorot Normal

Se inicia con valores provenientes de una distribuci√≥n uniforme: $\mathcal{N}(0,std^2)$

$$ std = gain \cdot \sqrt{\frac{2}{fan_{in} + fan_{out}}}$$
:::
::: 

::: {.callout-note}
* $fan_{in}$ corresponde al n√∫mero de conexiones que entran a una neurona. Mientras que $fan_{out}$ corresponde al n√∫mero de neuronas que salen de dicha neurona.
* $fan\_mode$ corresponde a la elecci√≥n de $fan_{in}$ o $fan_{out}$.
:::

::: {.columns}
::: {.column}
#### Kaiming (aka He) Uniforme

Se inicia con valores provenientes de una distribuci√≥n uniforme: $\mathcal{U}(-bound,bound)$

$$ bound = gain \cdot \sqrt{\frac{3}{fan\_mode}}$$
:::
::: {.column}

#### Kaiming (aka He) Normal

Se inicia con valores provenientes de una distribuci√≥n uniforme: $\mathcal{N}(0,std^2)$

$$std =\sqrt{\frac{gain}{fan\_mode}}$$
:::
::: 

## Training Control {.smaller}

El entrenamiento de una red neuronal puede tomar mucho tiempo. Es por eso que algunas buenas pr√°cticas ser√≠an:

* Disponer de resultados preliminares aunque el entrenamiento no haya terminado. 
* Guardar los pesos del mejor modelo obtenido en el proceso de entrenamiento.
* Evitar entrenar pasado el punto de Overfitting. 
  * Aunque hay nuevas ideas de lo que se llama el [grokking](https://www.linkedin.com/feed/update/urn:li:activity:7214966566696718336?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7214966566696718336%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29).


::: {.columns}
::: {.column}
::: {.callout-tip appearance="default"}
#### Early Stopping
* Se refiere al proceso de detener el entrenamiento luego de `patience` epochs sin mejorar el validation loss u otro criterio.
:::
:::
::: {.column}
::: {.callout-note appearance="default"}
#### Checkpointing
* Corresponde al proceso de guardar los par√°metros obtenidos en un epoch en espec√≠fico. Normalmente se guarda la mejor epoch y la √∫ltima, pero se puede generar alg√∫n criterio.
:::
:::
::: 

## Categorical Variables {.smaller}

> Es importante mencionar que normalmente no se utilizan redes neuronales para poder entrenar datos tabulares. Pero de hacerlo, es muy probable que nos encontremos con variables categ√≥ricas. Para ello existen dos t√©cnicas que son las m√°s comunes en redes neuronales.

One Hot Encoder
: Corresponde a la representaci√≥n mediante dummy variables. Normalmente se considera una representaci√≥n `Sparse` de los datos. 

::: {.callout-important}
En Pytorch se puede implementar como `F.one_hot()`, pero mi recomendaci√≥n es utilizar las herramientas de Scikit-Learn para evitar `Data Leakage`.
:::

![](img/clase-6/one_hot.png){.lightbox fig-align="center"}


# That's all Folks

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
