---
title: "TICS-579-Deep Learning"
subtitle: "Clase 6: Redes Convolucionales"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Limitaciones de las FFN {.smaller}

Sin duda las Redes Feed Forward son una herramienta poderosa para resolver problemas de clasificaci√≥n y regresi√≥n. Sin embargo, presentan ciertas limitaciones cuando se aplican a datos con estructuras espaciales o temporales, como im√°genes o secuencias de texto.

:::{.callout-note appearance="default" icon="false"}
## ‚ö†Ô∏è P√©rdida de estructura espacial o secuencial

Cada registro es considerado de manera independiente, sin tener en cuenta la relaci√≥n espacial o secuencial entre los datos.
:::

:::{.callout-warning appearance="default" icon="false"}
## ‚ò¢Ô∏è Gran cantidad de par√°metros
:::

::::{.columns}
:::{.column width="30%"}
![](img/clase-6/MNIST.png){.lightbox fig-align="center"} 
:::
:::{.column width="30%"}
![](img/clase-6/MNIST_net.png){.lightbox fig-align="center"}
:::

:::{.column width="30%"}

#### N√∫mero de Parametros 

#### (Imagen: 28x28=784 p√≠xeles):
* $W_1 = 784 \cdot 256 + 256 = 200960$
* $W_2 = 256 \cdot 128 + 128 = 32896$
* $W_3 = 128 \cdot 10 + 10 = 1290$
* Total = 235,146.


::: {.callout-tip .fragment}
¬øY si tengo una im√°gen de $512 \times 512$? **67,143,306** de par√°metros.
:::
:::
::::

## Limitaciones de las FFN {.smaller}

:::{.callout-caution appearance="default" icon="false"}
## üöß Ineficiencia en el aprendizaje de patrones locales

Translation Invariance
: Se refiere a la capacidad de poder detectar un patr√≥n/objeto en diferentes posiciones de la im√°gen.

***Problema***: Un perrito centrado, desplazado a la izquierda o a la derecha deber√≠a seguir siendo reconocido como un perrito. Para una FFN, las features que describen los perritos desplazados son completamente distintos. 

:::

![](img/clase-6/translation_invariance.png){.lightbox fig-align="center"}


## Limitaciones de las FFN {.smaller}

:::{.callout-important appearance="default" icon="false"}
## ‚õî Escalabilidad Limitada

Su alto n√∫mero de par√°metros sumado a la incapacidad de capturar patrones espaciales o temporales hace que las FFN no escalen bien a datos complejos como im√°genes de alta resoluci√≥n o secuencias largas haciendo que su rendimiento disminuya considerablemente y no sean aplicables por s√≠ solas a casos reales.
:::


::::{.columns}
:::{.column}
#### Imagenes actuales cada vez m√°s grandes
![](img/clase-6/image_diff.jpg){.lightbox fig-align="center"}
:::
:::{.column}
#### Textos actuales cada vez m√°s largos
![](img/clase-6/text_diff.jpg){.lightbox fig-align="center"}
:::
::::

## Im√°genes {.smaller}


::::{.columns}
:::{.column}
Imagen
: Definiremos una im√°gen como un Tensor de Orden 3. Normalmente cada dimensi√≥n representa H, W y C (Altura, Ancho y Canales). 

:::{.callout-note appearance="default" icon="false"}
## Convenci√≥n en Pytorch
Pytorch utiliza la convenci√≥n (C, H, W) para representar im√°genes, donde C es el n√∫mero de canales, H es la altura y W es el ancho de la imagen. Es decir, un Tensor de Dimensiones (3, 512, 512)
:::

:::

:::{.column}
![](img/clase-6/channels.png){.lightbox fig-align="center" width="70%"}
:::
::::




::: {.callout-important}
La convenci√≥n m√°s com√∫n es utilizar im√°genes de 24-bits, es decir 3 canales de $2^8$ valores (8-bits por canal). Es por eso que el valor de los p√≠xeles va de 0 a 255 y representan la intensidad del color del canal que representan.
:::

## Im√°genes {.smaller}

:::{style="font-size: 80%;"}
> Librer√≠as como `PIL` u `OpenCV` permiten importar im√°genes en Python. Ambas usan la convenci√≥n de $(H,W,C)$, la diferencia est√° en el orden de los canales. `PIL` utiliza la convenci√≥n RGB, mientras que `OpenCV` utiliza BGR por lo que se necesitan algunas transformaciones adicionales.
:::

::::{.columns}
:::{.column}
![](img/clase-6/Kira_channels.png){.lightbox fig-align="center" width="80%"}
:::
:::{.column}
#### Ejemplo para importar  im√°genes con PIL y Pytorch

```{.python}
from PIL import Image
import numpy as np
import torch
path = "path/to/imagen.png"
img = Image.open(path)

# Convierte a Tensor y cambia a (C,H,W)
torch_image= torch.from_numpy(np.array(img)).permute(2,0,1)  
torch_image.shape
```
```
(3,1200,1200)
```

```{.python}
import matplotlib.pyplot as plt

## Im√°gen en canal Rojo
plt.imshow(torch_image[0].numpy(), cmap="Reds")
plt.axis("off")
plt.show()
## Im√°gen en canal Verde
plt.imshow(torch_image[1].numpy(),cmap="Greens")
plt.axis("off")
plt.show()
## Im√°gen en canal Azul
plt.imshow(torch_image[2].numpy(), cmap="Blues")
plt.axis("off")
plt.show()
```
:::
::::

## Batch de Im√°genes {.smaller}

::::{.columns}

:::{.column width="60%"}

:::{.callout-note appearance="default" icon="false"}
## Conjunto/Set/Batch de Im√°genes
Se define como un Tensor de Orden 4. En Pytorch esto se representa como N, C, H, W (N√∫mero de Im√°genes, Canales, Altura y Ancho). 
:::
![](img/clase-6/image_batch.png){.lightbox fig-align="center" width="60%"}

::: {.callout-tip}
Luego un Tensor de Dimensiones (32,3,224,512) implica que tenemos 32 im√°genes RGB de dimensiones $224\times512$.
:::

:::
:::{.column width="40%"}

```{.python}
## Simulaci√≥n de 2 im√°genes RGB de 5x5 p√≠xeles
torch.randint(0,256, (2,3,5,5))
```
:::{style="font-size: 90%;"}
```
tensor([[[[248, 240, 146,  73, 228],
          [ 79, 125, 191, 203, 133],
          [202,  12, 237, 109,  62],
          [133, 227, 148,  78, 229],
          [121, 247, 202,  51,   3]],

         [[253,  28,  20, 144, 255],
          [115, 132, 114,  45, 164],
          [ 57, 238, 117, 250,  41],
          [ 58,  73,  29, 253, 240],
          [246,  84,  93,   2, 145]],

         [[ 83,   4, 144, 126, 202],
          [ 98, 235,  55,  83, 104],
          [ 21, 185,  27, 102, 117],
          [255, 133,  23,  83, 150],
          [ 49, 152,  81, 233,  98]]],
-----------------------------------------------
        [[[216,  92, 251, 214, 178],
          [252,  48,  88,  82,  79],
          [168, 208, 223,   9, 169],
          [145, 148, 254, 128, 156],
          [238, 175, 233, 136, 118]],

         [[112,  68, 143,  93, 150],
          [ 32, 103,  97,  93, 223],
          [205,  56,  90,  24, 108],
          [ 13, 135,  98,  20,  93],
          [ 20,  91,  37,  81,  10]],

         [[109, 145,  90, 243,  63],
          [103, 134, 130,  11,  72],
          [132, 163, 153,  26, 255],
          [ 45, 228,  26, 169, 212],
          [ 34, 211, 229,  82, 201]]]])
```
:::
:::
::::


## Redes Convolucionales: Definici√≥n e Inspiraci√≥n {.smaller}

Redes Convolucionales (CNN)
: Son un tipo de red neuronal cuyos par√°metros entrenables son filtros (tambi√©n llamados ***Kernels***) que aprenden a detectar patrones en los datos de entrada. 

El resultado de una Convolucional es un ***feature map***, el cual representa la presencia y localizaci√≥n de ciertos patrones visuales. 

::::{.columns}
:::{.column}
::: {.callout-note}
Existe el mito de que las Redes Convolucionales se inspiraron en el funcionamiento del Cortex Visual humano. **No s√© si es tan as√≠**.

El mito dice que las CNNs fueron dise√±adas para imitar el cortex visual humano. Esto viene de los trabajos de Hubel y Wiesel (d√©cada de 1960), que estudiaron c√≥mo las neuronas en la corteza visual de gatos respond√≠an a est√≠mulos:

* Descubrieron neuronas simples que respond√≠an a l√≠neas en cierta orientaci√≥n y posici√≥n.
* Descubrieron neuronas complejas que respond√≠an a patrones similares, pero en distintas posiciones (invarianza local).
:::

:::
:::{.column}
![](img/clase-6/visual_cat.png){.lightbox fig-align="center" width="90%"}
:::
::::


## Redes Convolucionales: Definici√≥n e Inspiraci√≥n {.smaller}

::: {.callout-caution}
***¬øPor qu√© necesitamos las Redes Convolucionales?*** Evitar la sobreparametrizaci√≥n. ¬øPor qu√© esto es un problema?
:::

:::{.callout-warning appearance="default" icon="false"}
## üîî Importancia
No es exagerado afirmar que las CNNs han sido la arquitectura m√°s influyente en Deep Learning, ya que han impulsado avances importantes en tareas de visi√≥n por computador, como clasificaci√≥n de im√°genes, detecci√≥n de objetos y segmentaci√≥n sem√°ntica. Adem√°s, contribuyeron a que el Deep Learning ganara popularidad en la industria tecnol√≥gica y superara la √©poca conocida como el ***AI Winter***.
:::

::: {.columns}
::: {.column}
#### üóìÔ∏è Algunos hitos importantes:
* **1990**: Yann LeCun et al. propone uno de los primeros intentos de CNN, el cual va agregando features m√°s simples en features m√°s complejas progresivamente.
* **1998**: Yann LeCun, propone LeNet-5 con 2 redes convolucionales y 3 FFN.
* **2012**: Krizhevsky, Sutskever y Hinton proponen AlexNet (5 capas convolucionales y 3 FFN), el cual obtiene **SOTA performance** en ImageNet.
:::
::: {.column}
![](img/clase-6/alexnet-paper.png){.lightbox fig-align="center"}
:::
::: 

## Convolutional Neural Network (CNNs){.smaller}

:::{.callout-important appearance="default" icon="false"}
## Architectura General
Una Convolutional Neural Network (CNN) est√° formada por m√∫ltiples capas que colaboran para identificar y extraer caracter√≠sticas significativas de las im√°genes, con el fin de clasificarlas o detectar objetos dentro de ellas.
:::

::: {.columns}
::: {.column width="60%"}
![](img/clase-7/CNN-arch.png){.lightbox fig-align="center" width="60%"} 
:::
::: {.column width="40%"}
::: {.callout-note appearance="default" icon="false"}
## Feature Extractor - Encoder - Backbone
Corresponde al bloque en el que se detectan caracter√≠sticas o patrones relevantes de la imagen. En este bloque es donde se aplican normalmente las operaciones de ***Convoluci√≥n*** y ***Pooling***.
:::

::: {.callout-warning appearance="default" icon="false"}
## Flatten
Corresponde a una operaci√≥n intermedia que aplana los feature maps generadas para ser utilizados como features de entrada para ser utilizados por la parte final de la red.
:::

::: {.callout-tip appearance="default" icon="false"}
## Prediction Head - Head - MLP
Corresponde a una FFN que tomar√° las features aprendidas por la CNN y generar√° una predicci√≥n.
:::
:::
::: 

## La Convoluci√≥n {.smaller}

:::{style="font-size: 80%;"}
Convoluci√≥n
: Corresponde a una operaci√≥n que permite extraer ***feature maps***, donde un filtro o kernel se desplaza sobre distintas secciones de los datos, ya sea una secuencia, una imagen o un video, para capturar sus patrones m√°s relevantes.
:::


::: {.callout-caution appearance="default" icon="false"}
## ‚òùÔ∏èAtenci√≥n
Esto es nuevamente un t√©rmino marketero, porque no es una Convolucional real, sino una operaci√≥n llamada **Cross Correlation**.
:::

![](img/clase-6/convolution.gif){.lightbox fig-align="center" width="80%"}


:::{style="font-size: 80%;"}
Feature Map
: Corresponde a la salida de una convoluci√≥n (equivalente a la Activaci√≥n) y es un nuevo tensor que captura ciertas caracter√≠sticas del dato (secuencia, imagen o video). Cuando se trata de im√°genes, captura features como bordes, cambios de textura, color, formas, o elementos m√°s peque√±os.
:::

::: {.callout-warning}
Es importante notar que los features maps son de una **tama√±o menor a la entrada** debido a la operaci√≥n de Convoluci√≥n.
:::
::: {.callout-important}
Se obtendr√°n tantos feature maps como filtros se apliquen. Esto es otro hiperpar√°metro de la Red Convolucional que se conoce como los canales de salida o ***out_channels***.
:::
## El filtro o Kernel {.smaller}

::: {.columns}
::: {.column}

##### Gaussian Blur
![](img/clase-6/gaussian_blur.png){.lightbox fig-align="center" width="65%"}


##### L√≠neas Horizontales
![](img/clase-6/horizontal_lines.png){.lightbox fig-align="center" width="65%"}

##### Bordes
![](img/clase-6/bordes.png){.lightbox fig-align="center" width="65%"}

:::
::: {.column}
::: {.callout-note appearance="default"}
## Kernel
Corresponde a una matriz peque√±a que permite detectar patrones espec√≠ficos en la imagen al aplicarse de manera m√≥vil sobre ella. Estos Kernel sol√≠as estudiarse y dise√±arse manualmente para tareas espec√≠ficas como detecci√≥n de bordes, desenfoque, realce de contraste, entre otros.

En una red convolucional, el kernel es un conjunto de pesos que se ajustan durante el proceso de entrenamiento para identificar caracter√≠sticas relevantes en las im√°genes. Es decir, la CNN aprende qu√© Kernels son m√°s importantes para la tarea que se est√° resolviendo.
:::

::: {.callout-important}
El Kernel se aplica a todos los canales a la vez, lo cu√°l inicialmente lo hace ver como una operaci√≥n bastante costosa computacionalmente.
:::

::: {.callout-tip}
El Kernel introduce el primer hiperpar√°metro de las CNN que es el **Kernel Size**. En general son cuadrados, y de dimensi√≥n impar.
:::
:::
::: 


## Feature Maps {.smaller}

:::{style="font-size: 80%;"}
Feature Map
: Corresponde a la salida de una convoluci√≥n (equivalente a la Activaci√≥n) y es un nuevo tensor que captura ciertas caracter√≠sticas del dato (secuencia, imagen o video). Cuando se trata de im√°genes, captura features como bordes, cambios de textura, color, formas, o elementos m√°s peque√±os.
:::


::::{.columns}
:::{.column width="60%"}
![](img/clase-6/feature_maps.png){.lightbox fig-align="center"}
:::
:::{.column width="40%"}
:::{.callout-tip appearance="default" icon="false"}
### B√°sicamente los feature maps son im√°genes que resaltan ciertos patrones aprendidos por los kernels.
:::

:::{.callout-warning appearance="default" icon="false"}
### A medida que avanzamos en las capas convolucionales, los feature maps tienden a capturar patrones m√°s complejos y abstractos.
:::
:::{.callout-important appearance="default" icon="false"}
### Cada feature map es de tama√±o m√°s peque√±o que la imagen original, pero contiene informaci√≥n m√°s relevante para la tarea de clasificaci√≥n o detecci√≥n.
:::

:::{.callout-caution appearance="default" icon="false"}
## Se obtendr√°n tantos feature maps como filtros se apliquen. Esto es otro hiperpar√°metro de la Red Convolucional que se conoce como los canales de salida o ***out_channels***.
:::
:::
::::


## Hiperpar√°metros de la Convoluci√≥n {.smaller}

::: {.columns}
::: {.column width="30%"}

![](img/clase-6/stride.gif){.lightbox fig-align="center"}

::: {.callout-note appearance="default" icon="false"}
## Stride
Hace referencia al n√∫mero de posiciones que el kernel se desplaza sobre la imagen de entrada en cada paso.
Un ***stride*** m√°s grande produce feature maps m√°s peque√±os y con menos detalle, mientras que un stride m√°s peque√±o preserva mayor informaci√≥n, aunque incrementa la cantidad de operaciones necesarias.
:::
:::
::: {.column width="40%"}
![](img/clase-6/convolution_padding.gif){.lightbox fig-align="center" width="130%"}

::: {.callout-tip appearance="default" icon="false"}
## Padding
Consiste en a√±adir un relleno alrededor de la imagen de entrada para facilitar el desplazamiento del kernel y evitar que la convoluci√≥n reduzca en exceso sus dimensiones.
Este relleno tambi√©n permite conservar la informaci√≥n presente en los bordes de la imagen.
Cuando no se aplica padding, la operaci√≥n se denomina ***"valid"***, mientras que si se agregan los p√≠xeles necesarios para mantener el tama√±o original, se conoce como ***"same"***.
:::
:::
::: {.column width="30%"}
![](img/clase-6/dilation.gif){.lightbox fig-align="center"}

::: {.callout-important appearance="default" icon="false"}
## Dilation

Hace referencia a los espacios o intervalos que se insertan entre los elementos del kernel durante la convoluci√≥n.
El uso de dilation permite ampliar el campo receptivo de la red, capturando un mayor contexto sin aumentar el tama√±o del kernel. Un valor de 1 indica que no se aplica ***dilation***.
:::
:::
::: 


## Convoluci√≥n en Pytorch {.smaller}

:::: {style="font-size: 130%;"}
```{.python}
nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0,dilation=1)
```

::: {style="font-size: 80%;"}
::: {.callout-tip appearance="default" icon="false"}
## Input
Este tipo de redes no requiere que se le den las dimensiones de las entradas, pero s√≠ espera recibir tensores de dimensi√≥n $(N,C_{in}, H_{in},W_{in})$.
:::

::: {.callout-important appearance="default" icon="false"}
## Output
La Red convolucional devuelve un Tensor de Dimensiones $(N,C_{out}, H_{out}, W_{out})$. Donde:

$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - dilation[0]\cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor$$
:::

:::

::: {.callout-warning}
Es importante tener noci√≥n del tama√±o de la imagen para poder escoger un *kernel_size* que recorra la imagen completa y que no deje partes sin convolucionar.
:::
::::

## Partes de una CNN: Pooling {.smaller}

Pooling
: El Pooling es una operaci√≥n de agregaci√≥n que permite ir disminuyendo el tama√±o de las entradas. De esta manera la red puede comenzar a especializarse en aspectos cada vez m√°s finos. 

::::{.columns}
:::{.column}
::: {.callout-note}
El Pooling tambi√©n se aplica de manera m√≥vil como una convoluci√≥n. Pero a diferencia de esta normalmente no genera traslape.
:::
::: {.callout-tip}
Ac√° se introduce otro hiperpar√°metro que es el ***Pooling Size***. En general es cuadrado y de dimensi√≥n par, y utiliza un stride del mismo tama√±o que el ***Pooling Size*** para evitar traslapes.
:::
:::
:::{.column}
![](img/clase-6/pooling_gif.gif){.lightbox fig-align="center"}
:::
::::



## Pooling in Pytorch {.smaller}

:::: {style="font-size: 90%;"}
```{.python}
nn.AvgPool2d(kernel_size, stride=None,padding=0)
nn.MaxPool2d(kernel_size, stride=None,padding=0, dilation=1)
```

::: {.callout-important appearance="default" }
## Ojo
* Pytorch llama tambi√©n `kernel_size` al tama√±o del Pooling.
* `stride=None` implica `stride = kernel_size`.
:::


:::{.callout-tip appearance="default" icon="false"}
#### MaxPool
$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - dilation[0]\cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor$$
:::

:::{.callout-note appearance="default" icon="false"}
#### AvgPool
$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - kernel\_size[0]}{stride[0]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - kernel\_size[1]}{stride[1]} + 1 \right\rfloor$$
:::

::: {.callout-warning}
El Average Pool no permite Dilation.
:::
::::

## AdaptivePooling {.smaller}

> La mayor√≠a de las arquitecturas CNN modernas aplican un procedimiento llamado ***Adaptive Pooling*** antes de la etapa de predicci√≥n (FFN).
Independientemente del tama√±o de la imagen de entrada, el Adaptive Pooling siempre genera una salida de tama√±o fijo, ya que ajusta sus par√°metros para asegurar que la dimensi√≥n de salida sea la deseada.


::::{.columns}
:::{.column}
```{.python}
nn.AdaptiveAvgPool2d(output_size)
nn.AdaptiveMaxPool2d(output_size, return_indices=False)
```
:::{.callout-tip appearance="default" icon="false"}
## `return_indices=True` permite devolver en qu√© posiciones se encontraron los valores m√°ximos, lo cual es √∫til para operaciones de ***unpooling*** para revertir el proceso de pooling.
:::
:::
:::{.column}
![](img/clase-6/pooling.png){.lightbox fig-align="center"}
:::
::::


## MNIST con CNN {.smaller}


::: {.columns}
::: {.column width="60%"}

![](img/clase-6/CNN_MNIST_arch.png){.lightbox fig-align="center"}

:::
::: {.column width="40%"}

![](img/clase-6/CNN_params.png){.lightbox fig-align="center"}

::: {.callout-warning}
* El n√∫mero de Par√°metros para una Red Convolucional con muchas m√°s capas baj√≥ considerablemente, de 67M a 373K de Par√°metros para una imagen de $512 \times 512$.
:::
:::
::: 

## Grafo CNN sencilla {.smaller}


::::{.columns}
:::{.column width="60%"}
![](img/clase-6/conv_graph.png){.lightbox fig-align="center"}
:::
:::{.column width="40%"}
```{.python}
## Una Imagen de 1 Canal de Tama√±o 6x6
X = torch.tensor([
        [[[7., 6., 8., 5., 1., 3.],
          [8., 6., 5., 3., 5., 5.],
          [9., 1., 1., 5., 3., 5.],
          [4., 5., 5., 9., 2., 6.],
          [9., 5., 3., 1., 2., 2.],
          [4., 4., 8., 8., 9., 8.]]]
])
X.shape
```
```
(1,1,6,6)
```

```{.python}
C_out = 2
N, C_in, H, W = X.shape
kH, kW = (3,3)
```

```{.python}
## 2 filtros de un Canal de tama√±o 3x3
given_w = torch.tensor([
        [[[-1.,  0.,  1.],
          [ 0., -1.,  1.],
          [ 0.,  0.,  1.]]],

        [[[-1., -1.,  0.],
          [ 1.,  0.,  0.],
          [ 1.,  1.,  0.]]]])
given_w.shape
```
```
(2,1,3,3)
```

```{.python}
given_bias = torch.tensor([1., 1.])
```



:::
::::


## Grafo CNN sencilla: Convoluci√≥n  {.smaller}

::::{.columns}
:::{.column width="40%"}
![](img/clase-6/convolution.gif){.lightbox fig-align="center"} 
:::
:::{.column width="60%"}
```{.python}
def calculate_out(X, k_size=(3,3), stride=1, dilation=1, padding=0):
  kH, kW = k_size
  N, in_channels, H_in, W_in = X.shape
  out_H = np.floor((H_in +2*padding-dilation*(kH-1)-1)/stride + 1)
  out_W = np.floor((W_in +2*padding-dilation*(kW-1)-1)/stride + 1)
  return int(out_H), int(out_W)

H_out, W_out = calculate_out(X, k_size = (kH,kW))
H_out, W_out
```

```
(4,4)
```

```{.python}
O = torch.zeros((N, C_out, H_out, W_out))
for n in range(N):
    for co in range(C_out):
        for i in range(H_out):
            for j in range(W_out):
                # submatriz de tama√±o kH x kW
                patch = X[n, :, i:i+kH, j:j+kW]
                O[n, co, i, j] = (patch * given_w[co]).sum() + given_bias[co]
O
```
```
tensor([[[[  5.,   5.,  -1.,  -4.],
          [ -4.,  -7.,  -1.,  -6.],
          [ -1., -10.,   6.,  -8.],
          [ -9.,  -8.,  -6.,  -6.]],

         [[ -3.,   5.,   3.,  -6.],
          [ -3.,  -7.,   3., -13.],
          [ -5., -12.,   4.,  -7.],
          [ -9.,  -4.,  -6.,  -5.]]]])
```
:::
::::

:::{.callout-caution}
## Este proceso es ***extremadamente ineficiente*** computacionalmente hablando. Por lo que se utiliza un proceso equivalente llamado ***im2col***.
:::

## im2col {.smaller}

> ***im2col*** es un algoritmo que permite transformar la operaci√≥n de convoluci√≥n en una operaci√≥n de multiplicaci√≥n de matrices, lo cual es computacionalmente m√°s eficiente. En este caso los parches que requieren la convoluci√≥n se aplanan y se organizan en columnas de una nueva matriz.

::::{.columns}
:::{.column width="40%"}
![](img/clase-6/im2col.gif){.lightbox fig-align="center"} 
:::
:::{.column width="60%"}
El procedimiento en Pytorch se realiza de la siguiente manera:
```{.python}
## Cada columna es un patche aplanado de 3x3. 16 patches en total.
X_col = F.unfold(X, kernel_size=(kH, kW))  # (1, 9, 16) (N,kH*kW,n_patches)
print(f"X_col shape: {X_col.shape}")
X_col
```
```
X_col shape: torch.Size([1, 9, 16])
tensor([[[7., 6., 8., 5., 8., 6., 5., 3., 9., 1., 1., 5., 4., 5., 5., 9.],
         [6., 8., 5., 1., 6., 5., 3., 5., 1., 1., 5., 3., 5., 5., 9., 2.],
         [8., 5., 1., 3., 5., 3., 5., 5., 1., 5., 3., 5., 5., 9., 2., 6.],
         [8., 6., 5., 3., 9., 1., 1., 5., 4., 5., 5., 9., 9., 5., 3., 1.],
         [6., 5., 3., 5., 1., 1., 5., 3., 5., 5., 9., 2., 5., 3., 1., 2.],
         [5., 3., 5., 5., 1., 5., 3., 5., 5., 9., 2., 6., 3., 1., 2., 2.],
         [9., 1., 1., 5., 4., 5., 5., 9., 9., 5., 3., 1., 4., 4., 8., 8.],
         [1., 1., 5., 3., 5., 5., 9., 2., 5., 3., 1., 2., 4., 8., 8., 9.],
         [1., 5., 3., 5., 5., 9., 2., 6., 3., 1., 2., 2., 8., 8., 9., 8.]]])
```
:::
::::


## im2col {.smaller}

¬øQu√© pasa si ahora aplanamos los filtros tambi√©n?

::::{.columns}
:::{.column}
```{.python}
## Dejamos cada filtro como una fila
W_row = given_w.reshape(C_out, -1)
print(W_row.shape)
W_row
```

```
(2,9)
tensor([[ 1.,  1.,  0., -1.,  1., -1.,  0., -1., -1.],
        [ 1.,  1.,  0., -1.,  1., -1., -1.,  0., -1.]])
```
:::
:::{.column}
:::{.callout-warning appearance="default" icon="false"}
## üí≠ Luego La convoluci√≥n se puede pensar como una transformaci√≥n lineal aplicada a parches aplanados de la imagen de entrada. Es decir cada parches es transformado linealmente por los filtros aplanados, generando una nueva representaci√≥n de la imagen (un feature map).
:::
:::
::::

:::{.callout-tip style="font-size: 110%;"}
Entonces podemos expresar la convoluci√≥n como una multiplicaci√≥n de matrices muy similar a una FFN:

$$H_{col} = W_{row} \cdot X_{col} + b \cdot 1_{C_{in}*kH*KW}^T $$

Donde $b$ tiene dimensiones $C_{out} \times 1$ y $1^T_{C_{in}*kH*kW}$ tiene dimensiones $1 \times C_{in}*kH*kW$. Aunque es m√°s sencillo pensar el $b$ como un vector que se le aplica **Broadcasting**.
:::

::::{.columns}
:::{.column}
Luego para volver a la forma de la imagen basta con hacer un reshape:

```{.python}
H = H_col.reshape(N, C_out, H_out, W_out)
print(H.shape)
H
```
```
(1, 2, 4, 4)
```
:::
:::{.column}
```
tensor([[[[  5.,   5.,  -1.,  -4.],
          [ -4.,  -7.,  -1.,  -6.],
          [ -1., -10.,   6.,  -8.],
          [ -9.,  -8.,  -6.,  -6.]],

         [[ -3.,   5.,   3.,  -6.],
          [ -3.,  -7.,   3., -13.],
          [ -5., -12.,   4.,  -7.],
          [ -9.,  -4.,  -6.,  -5.]]]])
```
:::
::::


## Pooling {.smaller}

::::{.columns}
:::{.column width="48%"}
#### Nuevamente necesitamos hacer un `im2col` para poder hacer el pooling como una multiplicaci√≥n de matrices. 
#### Eso implica que H quedar√° como:
```{.python}
pool_size = 2
H_pool, W_pool = calculate_out(H, k_size=(2,2),stride=2) 
## (2,2)
h_col = F.unfold(H, kernel_size=pool_size, stride=pool_size)
print(h_col.shape)
h_col
```
```
(1,8,4)
tensor([[[  5.,  -1.,  -1.,   6.],
         [  5.,  -4., -10.,  -8.],
         [ -4.,  -1.,  -9.,  -6.],
         [ -7.,  -6.,  -8.,  -6.],
         [ -3.,   3.,  -5.,   4.],
         [  5.,  -6., -12.,  -7.],
         [ -3.,   3.,  -9.,  -6.],
         [ -7., -13.,  -4.,  -5.]]])
```

:::{.callout-important appearance="default" icon="false"}
## üò±Notar como la operaci√≥n de im2col genera parches para todos los canales a la vez. Por lo tanto es necesario separar por canales para aplicar el pooling.
:::
:::
:::{.column width="52%"}
```{.python}
## Tenemos 2 canales de 4 por cada uno
## al cu√°l debemos aplicar el m√°ximo.
h_col_reshaped = h_col.reshape(N, C_out, pool_size*pool_size, -1)
h_col_reshaped
```
```
(1,2,4,4)
tensor([[[[  5.,  -1.,  -1.,   6.],
          [  5.,  -4., -10.,  -8.],
          [ -4.,  -1.,  -9.,  -6.],
          [ -7.,  -6.,  -8.,  -6.]],

         [[ -3.,   3.,  -5.,   4.],
          [  5.,  -6., -12.,  -7.],
          [ -3.,   3.,  -9.,  -6.],
          [ -7., -13.,  -4.,  -5.]]]])

```
```{.python}
## Calculamos el m√°ximo de cada columna (que es un parche de 2x2)
## Adem√°s guardamos la posici√≥n del m√°ximo
M_flat, pool_indices = h_col_reshaped.max(dim=2)
M_flat
```
```
tensor([[[ 5., -1., -1.,  6.],
         [ 5.,  3., -4.,  4.]]])
```

```{.python}
## Recuperamos la forma del feature map luego del pooling
M = M_flat.reshape(N, C_out, H_pool, W_pool)
print(M.shape)
M
```

```
(1,2,2,2)
tensor([[[[ 5., -1.],
          [-1.,  6.]],

         [[ 5.,  3.],
          [-4.,  4.]]]])
```
:::
::::

## Flatten y FFN {.smaller}

::::{.columns}
:::{.column width="70%"}
```{.python}
f = M.reshape(N, -1)
print(f.shape)
```

```
tensor([[ 5., -1., -1.,  6.,  5.,  3., -4.,  4.]])
```

```{.python}
Z = f @ W_fc + b_fc
Z
```
```
tensor([[18.]])
```

#### Utilizando `nn.Module`
```{.python}
class Conv(nn.Module):
  def __init__(self):
      super().__init__()
      self.conv = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, bias=True)
      self.conv.weight.data = given_w
      self.conv.bias.data = given_bias
      self.max_pool = nn.MaxPool2d(kernel_size=2, return_indices=True)
      self.fc = nn.Linear(8, 1)
      nn.init.ones_(self.fc.weight)
      nn.init.ones_(self.fc.bias)
      self.flatten = nn.Flatten()

  def forward(self, x):
      x = self.conv(x)
      x, self.indices = self.max_pool(x)
      x = self.flatten(x)
      x = self.fc(x)
      return x

model = Conv()
# Forward con PyTorch
logits = model(X)

```
```
tensor([[18.]])
```
:::
:::{.column width="30%"}
```{.python}
# Linear
W_fc = torch.ones(8, 1)
```
```
tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.]])

```
```{.python}
b_fc = torch.ones(1)
```
```
tensor([1.])
```
:::
::::

## ¬øLe interesa calcular los gradientes? 

:::{.callout-note appearance="default" icon="false"}
## Calcular los gradientes de una CNN se simplifica bastante utilizando el enfoque de `im2col`, ya que la convoluci√≥n se ha transformado en una multiplicaci√≥n de matrices. Esto permite aplicar los conceptos que aprendimos en la primera parte del curso.
:::

:::{.callout-tip appearance="default" icon="false"}
## A√∫n as√≠ aparecen conceptos que escapan del conocimiento del C√°lculo que conocemos, como por ejemplo el Gradiente del `im2col` (que Spoiler, es el algoritmo `col2im`).
:::

:::{.callout-important appearance="default" icon="false"}
### √âchele una miradita al notebook de la clase, hay muchas horas de esfuerzo invertidas ah√≠.
:::


## Variante en 1d {.smaller}

::: {.columns}
::: {.column width="60%"}
Conv1d
: Corresponde a la variante de una dimensi√≥n, en la cual la entrada corresponden a secuencias de elementos como podr√≠an ser series de tiempo, audio o hasta cadenas de texto.

::: {.callout-note icon="false"}

En este caso la implementaci√≥n en Pytorch es similar a la 2D s√≥lo que esperando tensores de dimensiones $(N,C_{in}, L_{in})$, donde $C_{in}$ corresponde al n√∫mero de canales, que en el caso de series de tiempo equivale a features, y $L_{in}$ corresponde al largo de la secuencia.
:::

::: {.callout-important icon="false"}
La salida de la Conv1d tendr√° dimensiones $(N,C_{out},L_{out})$ con:

$$L_{out} = \left\lfloor \frac{L_{in} + 2 \cdot padding - dilation \cdot (kernel\_size - 1) - 1}{stride} + 1 \right\rfloor$$

:::
:::
::: {.column width="40%"}

![](img/clase-7/time_series.png){.lightbox fig-align="center" width="80%"} 

![](img/clase-7/audio.png){.lightbox fig-align="center" width="80%"} 
:::
::: 


## Variante en 3d {.smaller}

::: {.columns}
::: {.column width="60%"}
Conv3d
: Corresponde a la variante de tres dimensiones, en la cual la entrada corresponde a secuencias de im√°genes, es decir, videos. 

::: {.callout-note icon="false"}
Este caso tambi√©n es similar s√≥lo que se esperan tensores de dimensiones $(N, C_{in}, D_{in}, H_{in}, W_{in})$ donde $C_in$ corresponde al n√∫mero de canales, $D$ en el caso de un video corresponde al n√∫mero de frames de tama√±o $H_{in} \times W_{in}$.
:::

::: {.callout-important icon="false"}
La salida de la Conv1d tendr√° dimensiones $(N,C_{out},D_{out},H_{out},W_{out})$ con:

$$D_{out} = \left\lfloor \frac{D_{in} + 2 \cdot padding[0] - dilation[0] \cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor$$
$$H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor$$
$$W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[2] - dilation[2]\cdot (kernel\_size[2] - 1) - 1}{stride[2]} + 1 \right\rfloor$$

:::
:::
::: {.column width="40%"}


![](img/clase-6/frame-rates.jpg){.lightbox fig-align="center"} 
:::
::: 

# ü•µ Terminamos

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
