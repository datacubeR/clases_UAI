---
title: "TICS-579-Deep Learning"
subtitle: "Clase 3: Feed Forward Networks"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

# Feed Forward Networks

## Feed Forward Networks {.smaller}

> Son redes neuronales que se caracterizan por tener una arquitectura en la que la informaci√≥n fluye en una sola direcci√≥n, desde las entradas hasta las salidas. En general todas las neuronas de una capa est√°n conectadas a todas las neuronas de la siguiente capa, sin ciclos ni conexiones recurrentes.



Teorema de aproximaci√≥n Universal
: > Una red neuronal feedforward con al menos una capa oculta y un n√∫mero finito de neuronas, usando funciones de activaci√≥n no lineales (como sigmoide, tanh o ReLU), puede aproximar cualquier funci√≥n continua definida en un conjunto compacto (acotado y cerrado) de $\mathbb{R}^n$ a cualquier nivel de precisi√≥n, siempre que se utilicen suficientes neuronas y se ajusten adecuadamente los pesos y sesgos.

::: {.callout-important appearance="default"}
## Importante
* El teorema dice que es posible encontrar aproximar cualquier funci√≥n.
* El teorema no dice ni c√≥mo se hace ni los recursos necesarios para hacerlo (N√∫mero de Neuronas, capas, Hiperpar√°metros, etc.).
:::

::: {.callout-tip}
* B√°sicamente nos est√°n diciendo que tienes la mejor herramienta que existe, pero es tu responsabilidad saber c√≥mo utilizarla y qu√© recursos necesitas para lograrlo.
:::


## Feed Forward Networks (FFN) {.smaller}

::: {.callout-important}
Este tipo de Redes tiene distintos nombres que son usados de manera intercambiable:

* Capas Lineales: Probablemente por su denominaci√≥n en Pytorch.
* Capas/Redes Densas: Probablemente por su denominaci√≥n en Tensorflow.
* Multilayer Perceptron: O tambi√©n conocido como MLP, debido a que es la generalizaci√≥n del Perceptr√≥n, la primera propuesta de Redes Neuronales de Rosenblatt en 1958.
* Projection Layers: Probablemente por su denominaci√≥n en algunos papers. Se usa en el contexto de proyectar de $n$ dimensiones a $d$ dimensiones.
:::

::::{.columns}
:::{.column}
![](img/clase-1/nn_arq.png){.lightbox width="60%" fig-align="center"}
:::
:::{.column}
De ahora en adelante utilizaremos las siguiente notaci√≥n para referirnos a una Red Neuronal Feed Forward:

$$h_\theta(X) = \sigma_s(Z)$$

:::{.callout-warning appearance="default" icon=false}
## Logits
Definiremos $Z=\phi_L(X) W_{L+1} + b_{L+1}^T$ como Logits y corresponden a las activaciones de la √∫ltima capa antes de aplicar la funci√≥n de activaci√≥n de salida $\sigma_s(.)$.
:::
:::

::::


## Hiperpar√°metros de una Red Neuronal {.smaller}

Hiperpar√°metros
: >  Son las configuraciones externas que no se aprenden durante el entrenamiento, sino que se definen antes de entrenar el modelo y controlan su comportamiento y rendimiento.


::: {.callout-caution style="font-size: 120%;" appearance="default" icon=false}
## ü§ì Hiperpar√°metros de una Red Neuronal

* ***Learning Rate*** (Karpathy Constant: 3e-4), valores entre [1e-5, 1e-1] son comunes.
* **N√∫mero de Capas** y sus respectivas dimensiones (Para Pesos/Weights y Sesgos/Biases).
* **Funciones de Activaci√≥n** para cada capa.
* **Funci√≥n de P√©rdida** (Loss Function) a utilizar.
* **Optimizador** a utilizar.
* **Punto de Partida** de los Par√°metros (Inicializaci√≥n de Pesos y Sesgos).
* ¬øCu√°nto tiempo debo entrenar mi modelo? ***¬øC√≥mo sabemos si es que convergi√≥ o no?***
:::

## Output de una Red Neuronal {.smaller}

> En el aprendizaje supervisado se abordan principalmente dos tipos de problemas: clasificaci√≥n y regresi√≥n. Seg√∫n el tipo de problema, la hip√≥tesis debe adoptar una forma distinta en la capa de salida.

:::{.callout-important appearance="default" style="font-size: 120%;" icon=false}
## ‚ö†Ô∏è Dimensi√≥n de Salida
Est√° definida por el n√∫mero de valores a predecir para cada observaci√≥n. Denominaremos $k$ como la dimensi√≥n de salida.

Para una red de dos capas:

$$\phi_0(X) = X$$
$$\phi_1(X) = \sigma_1(W_1 \cdot \phi_0(X) + \bar{b_1}^T)$$
$$\phi_2(X) = \sigma_2(W_2 \cdot \phi_1(X) + \bar{b_2}^T)$$

Donde $W_1 \in \mathbb{R}^{n \times d_1}$ y $W_2 \in \mathbb{R}^{d_1 \times k}$ y $b_1 \in \mathbb{R}^{d_1}$ y $b_2 \in \mathbb{R}^{k}$.
:::

:::{.callout-note appearance="default" style="font-size: 120%;" icon=false}
## ‚úÖ Activaci√≥n de la Salida
Seg√∫n el tipo de problema, la capa de salida puede necesitar una funci√≥n de activaci√≥n particular que ajuste los resultados al formato correcto. En este sentido, $\sigma_2$ estar√° determinada por la naturaleza del problema a resolver.
:::

## Consejos para el Output de una Red {.smaller}
::: {.columns}
::: {.column}
:::{.callout-note appearance="default" icon=false}
## Clasificaci√≥n Binaria
El approach m√°s com√∫n utiliza $k=1$ con una ***Sigmoide*** para calcular la probabilidad de la clase 1. Otros approach utilizan $k=2$ para calcular la probabilidad de ambas clases (Activando con ***Softmax***).
:::

:::{.callout-warning appearance="default" icon=false}
## Clasificaci√≥n Multiclase
Utiliza $k=C$ donde C es el n√∫mero de clases a clasificar. Se usa una funci√≥n ***Softmax*** para transformar el output en una distribuci√≥n de probabilidades.
:::

:::{.callout-important appearance="default" icon=false}
## Clasificaci√≥n Multilabel
Se requiere un $k=C$ donde C es el n√∫mero de clases a clasificar. Se usa una funci√≥n ***Sigmoide*** para transformar cada clase en probabilidades.
:::
:::
::: {.column}

:::{.callout-caution appearance="default" icon=false}
## Regresi√≥n Simple
Se requiere un $k=1$. T√≠picamente no requiere de funciones adicionales aunque a veces se agregan funciones para acotar la salida.
:::

:::{.callout-tip appearance="default" icon=false}
## Regresi√≥n Multiple
Se requiere un $k=V$ con V el n√∫mero de valores a predecir. Se deben tener las mismas consideraciones para acotar la salida.
:::

:::
::: 

::: {.callout .fragment appearance="default" icon=false}
## üëÄ Muy Importante
En la mayor√≠a de las implementaciones en C√≥digo la activaci√≥n de la salida va embebida en la Loss Function. Por lo tanto, no es necesario aplicar una funci√≥n de activaci√≥n expl√≠cita en la capa de salida. Aunque s√≠ deben aplicarse al momento de la ***Predicci√≥n del modelo***.
:::

## Funciones de Activaci√≥n {.smaller}

Activation Functions
: Corresponden a las funciones que agregar√°n caracter√≠sticas no lineales a cada activaci√≥n, impidiendo la composici√≥n de transformaciones Affine.

::: {.callout-tip icon=false appearance="default"}
## ü§ì Convenci√≥n para C√≥digo
En Pytorch, nunca aplicaremos una funci√≥n de activaci√≥n a la capa de salida.
:::

::: {.callout-warning appearance="default"}
## Cuidado
Otros frameworks como Tensorflow, Keras, etc. utilizan una convenci√≥n distinta y aplican funciones de activaci√≥n a la capa de salida.
:::

::: {.callout-note appearance="default" icon=false}
## ¬øPuedo aplicar distintas Funciones de Activaci√≥n a cada Neurona?

Puedo, pero no se hace. Complicar√≠a much√≠simo la implementaci√≥n.
:::

[Activation Functions in Pytorch](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).

## Funciones de Activaci√≥n {.smaller}

### Sigmoide

::::{.columns}
:::{.column width="60%"}
![](img/clase-1/sigmoid.png){.lightbox fig-align="center" width="80%"}
:::

:::{.column width="40%"}

:::{.callout-tip appearance="default" icon=false style="font-size: 120%;"}
## Definici√≥n

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
:::
:::{.callout-warning appearance="default" icon=false}
## Propiedades

* Acota la salida entre 0 y 1.
* Su derivada es $\sigma'(z) = \sigma(z)(1 - \sigma(z))$.
* Su gradiente es general es muy peque√±o, lo que lleva a problemas de ***Vanishing Gradient***.
* Su principal uso es en la capa de salida para problemas de clasificaci√≥n binaria y Clasificaci√≥n Multilabel.
:::
:::
::::

## Funciones de Activaci√≥n {.smaller}

### Softmax

::::{.columns}
:::{.column width="60%"}
![](img/clase-3/softmax.png){.lightbox fig-align="center" width="80%"}
:::

:::{.column width="40%"}

:::{.callout-tip appearance="default" icon=false style="font-size: 120%;"}
## Definici√≥n

$$S_i(z) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}$$
:::
:::{.callout-warning appearance="" icon=false}
## Propiedades

* Transofrma un vector en una distribuci√≥n de probabilidad.
* Su principal uso es en la capa de salida para problemas de clasificaci√≥n multiclase. Es por lejos la funci√≥n de activaci√≥n m√°s utilizada en la salida, pero en casos m√°s avanzados tambi√©n en Mecanismos de Atenci√≥n.
:::
:::
::::


## Funciones de Activaci√≥n {.smaller}

### Tanh

::::{.columns}
:::{.column width="60%"}
![](img/clase-3/tanh.jpg){.lightbox fig-align="center" width="80%"}
:::

:::{.column width="40%"}

:::{.callout-tip appearance="default" icon=false style="font-size: 120%;"}
## Definici√≥n

$$Tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$
:::
:::{.callout-warning appearance="" icon=false}
## Propiedades

* Acota su salida entre -1 y 1.
* Su derivada es $Tanh'(z) = 1 - Tanh^2(z)$.
* Su gradiente normalmente es m√°s grande que el de la Sigmoide, pero a√∫n as√≠ puede llevar a problemas de ***Vanishing Gradient***.
* A pesar de estar un poco en desuso, tiene un rol protag√≥nico en las `Redes Recurrentes` (RNNs).
:::
:::
::::

## Funciones de Activaci√≥n {.smaller}

### ReLU (Rectified Linear Unit)

::::{.columns}
:::{.column width="60%"}
![](img/clase-3/relu.png){.lightbox fig-align="center" width="80%"}
:::

:::{.column width="40%"}

:::{.callout-tip appearance="default" icon=false style="font-size: 120%;"}
## Definici√≥n

$$ReLU(z) = max(0, z)$$
:::

:::{.callout-warning appearance="" icon=false}
## Propiedades

* Acota su salida entre 0 e $\infty$.
* Su derivada es $ReLU'(z) = \begin{cases}
1,  & \text{if $z \ge$ 0} \\
0 & \text{if $z < 0$}
\end{cases}$
* Es la funci√≥n de activaci√≥n m√°s utilizada en la actualidad, principalmente en las capas ocultas de las Redes Neuronales.
* Se hizo extremadamente popular por su simplicidad y efectividad en `Redes Convolucionales` (CNNs).
:::
:::
::::

## Funciones de Activaci√≥n Modernas {.smaller}

::: {.columns}
::: {.column}
### Leaky ReLU
![](img/clase-3/leaky_relu.png){.lightbox width="60%" fig-align="center"}

$$g(z) = max(0.1z, z)$$
:::
::: {.column}
### Parametrized ReLU (PReLU)
![](img/clase-3/prelu.png){.lightbox width="60%" fig-align="center"}

$$g(z) = max(az, z)$$
:::
::: 

## Funciones de Activaci√≥n {.smaller}

::: {.columns}
::: {.column}
### ELU
![](img/clase-3/elu.png){.lightbox width="60%" fig-align="center"}
$g(z) =
\begin{cases}
z,  & \text{if $z \ge$ 0} \\[2ex]
\alpha(e^{z}-1), & \text{if $z < 0$}
\end{cases}$
:::
::: {.column}
### GELU
![](img/clase-3/gelu.png){.lightbox width="60%" fig-align="center"}
$$\begin{align} g(z) &= z \cdot \Phi(z) \\
g(z)&= 0.5 \cdot z \cdot \left(1 + Tanh\left(\sqrt{2/\pi}\right) \cdot \left(z + 0.044715 \cdot z^3\right)\right)\end{align}$$
:::
::: 

## Funciones de Activaci√≥n {.smaller}
::: {.columns}
::: {.column}
### SELU
![](img/clase-3/selu.png){.lightbox width="60%" fig-align="center"}
$$ g(z) = scale \cdot (max(0,z) + min(0,\alpha(e^z - 1)))$$

con $\alpha=1.6732632423543772848170429916717$ y $scale = 1.0507009873554804934193349852946$
:::
::: {.column}
### Swish
![](img/clase-3/swish.png){.lightbox width="60%" fig-align="center"}
$$g(z) = z \cdot sigmoid(z)$$

:::
::: 

## Loss Functions: Clasificaci√≥n {.smaller}

> Son las encargadas de medir el error entre la predicci√≥n del modelo y el valor real. En general, se busca minimizar la Loss Function durante el entrenamiento del modelo.

:::{.callout-note appearance="default" icon=false style="font-size: 120%;"}
## Clasificaci√≥n Binaria: Binary Cross Entropy

$$BCE(Z) = - \frac{1}{m}\left[y^T log(\sigma(Z)) + (1-y)^T log(1-\sigma(Z))\right]$$

Donde $Z$ corresponden a los Logits del Modelo.
:::

::: {.callout-important}
En Pytorch esta funci√≥n se llama `BCEWithLogitsLoss`.
:::

:::{.callout-warning appearance="default" icon=false style="font-size: 120%;"}
## ü§ì Logits
Se refiere a las activaciones finales del modelo antes de aplicar la funci√≥n de activaci√≥n.
:::

::: {.callout-important appearance="default"}
## üëä Clasificaci√≥n Multilabel: BCEWithLogitsLoss
En Pytorch se suele utilizar `BCEWithLogitsLoss` ya que combina una sigmoide a cada activaci√≥n de la salida.
:::

## Loss Functions: Clasificaci√≥n {.smaller}

:::{.callout-note appearance="default" icon=false style="font-size: 120%;"}
## Clasificaci√≥n Multiclase: CrossEntropy
$$CE(Z)= -\frac{1}{m}Tr(Y^T Log(\hat{Y}))$$

Donde $Tr(.)$ es la traza de una matriz e $Y \in \{0,1\}^{m \times k}$ es la codificaci√≥n One-Hot de las etiquetas e $\hat{Y} = Softmax(Z)$, donde $Z$ son los Logits del modelo.
:::

:::{.callout-tip appearance="default" icon=false style="font-size: 120%;"}
## ü§ì Traza ($Tr(.)$)
Corresponde a la suma de los elementos de la diagonal principal de una matriz.
:::

:::{.callout-warning appearance="default" icon=false style="font-size: 120%;"}
## Derivada
$$\frac{\partial CE(X)}{\partial Z} = \frac{1}{m}\left(\hat{Y} - Y\right)$$
:::

::: {.callout-tip}
En Pytorch se suele utilizar `CrossEntropyLoss` ya que combina aplica una funci√≥n Softmax a la capa de salida adem√°s de ser una clase numericamente m√°s estable.
:::

## Ejemplo de Cross Entropy Loss

TODO:

## Loss Functions: Regresi√≥n {.smaller}

:::{.callout-note appearance="default" icon=false style="font-size: 120%;"}
## Regresi√≥n: Mean Squared Error (MSELoss)
$$MSE(Z) = \frac{1}{m}||Z - \bar{y}||^2$$

Donde $||.||$ corresponde a la norma Euclideana e $\bar{y} \in \mathbb{R}^{m \times 1}$.
:::

:::{.callout-warning appearance="default" icon=false style="font-size: 120%;"}
## Derivada
$$\frac{\partial MSE(Z)}{\partial Z} = \frac{2}{m}(Z - \bar{y})$$
:::

## Loss Functions: Regresi√≥n {.smaller}


:::{.callout-note appearance="default" icon=false style="font-size: 120%;"}
## Regresi√≥n: Mean Absolute Error (L1Loss)

$$L1Loss(Z) = \frac{1}{m}|Z - \bar{y}|$$

Donde $||.||$ corresponde a la norma Euclideana y $\bar{y} \in \mathbb{R}^{m \times 1}$.
:::

:::{.callout-warning appearance="default" icon=false style="font-size: 120%;"}
## Derivada
$$\frac{\partial L1Loss(Z)}{\partial Z} = \frac{1}{m}sign(Z-\bar{y})$$

$$\operatorname{sign}(z) =
\begin{cases}
+1 & \text{si  z > 0},\\[2mm]
0 & \text{si z = 0},\\[1mm]
-1 & \text{si z < 0}
\end{cases}$$
:::


## Optimizers: Gradient Descent {.smaller}

:::{style="font-size: 80%;"}
> Gradient Descent corresponde al algoritmo de Optimizaci√≥n m√°s popular, pero no necesariamente el m√°s eficiente. Distintas variantes han ido apareciendo para ir mejorando eventuales deficiencias de la proposici√≥n inicial.
:::

:::{.callout-warning appearance="default" icon=false}
## Epochs
Corresponden a la cantidad de iteraciones que se realizan a la Update Rule para que el modelo se optimize.
:::

:::{.callout-note appearance="default" icon=false style="font-size: 120%;"}
#### Standard Gradient Descent

$$\theta := \theta - \frac{\alpha}{m}\nabla_\theta L$$
:::

::: {.callout-caution appearance="default"} 
### Importante
* En Deep Learning, los conjuntos de datos suelen ser tan grandes que calcular el gradiente sobre todos ellos es inviable por memoria y tiempo de c√≥mputo.
* Adicionalmente no basta con calcular el gradiente una vez, sino que se debe hacer varias veces seg√∫n el n√∫mero de Epochs definido.
* Practicar Standard Gradient Descent en la pr√°ctica es muy poco com√∫n, ya que no es eficiente.
:::

## Minibatch Gradient Descent {.smaller}

::: {.callout-important}

Los `minibatches` permiten estimar el gradiente con un subconjunto de datos, manteniendo la direcci√≥n correcta para actualizar los par√°metros de manera m√°s eficiente. Se realiza en un subconjunto de $B$ datos donde $B << m$.
:::


$$\theta := \theta - \frac{\alpha}{B}\nabla_\theta L$$

:::{.callout-warning appearance="default" icon=false}
## üëÄ Importante
* $X \in \mathbb{R}^{B \times n}$ e $y \in \mathbb{R}^{B \times 1}$ son versiones reducidas de los datos totales. Se deben hacer suficientes `minibatches` para utilizar todos los datos. El minibatch se implementa en Pytorch utilizando el `DataLoader`. Cada actualizaci√≥n de par√°metros ahora se le denomina `step`.
* Cuando todos los `minibatches` han sido utilizados, se dice que se ha completado una `epoch`.
* Es com√∫n utilizar un `minibatch` de tama√±o 32, 64, 128, etc. 
* A veces se deshecha el √∫ltimo `minibatch` (remanente) si no tiene el tama√±o completo para evitar problemas de estabilidad de gradientes.
:::

:::{.callout-tip appearance="default" icon=false}
## Pros
* Permite optimizar utilizando menos recursos computacionales.
* Al actualizar los par√°metros de manera m√°s frecuente, se puede converger m√°s r√°pido.
:::

:::{.callout-important appearance="default" icon=false}
## Contras
* Si $B$ es muy peque√±o, el gradiente puede ser muy ruidoso y no converger.
* El entrenamiento toma m√°s tiempo que el Standard Gradient Descent.
:::

## SGD with Momentum {.smaller}

::::{.columns}
:::{.column style="font-size: 90%;"}
#### Update Rule

$$\theta_{t+1} = \theta_t - \alpha v_{t + 1}$$
$$v_{t+1} = \beta v_{t} + (1-\beta) \nabla_\theta L(\theta_{t+1})$$

donde $0<\beta<1$, pero normalmente $\beta=0.9$.
:::
:::{.column}
![](img/clase-3/momentum_update.png){.lightbox width="45%" fig-align="center"}
:::
::::

::: {.callout-note appearance="default" icon=false}
## ‚òùÔ∏è Intuici√≥n
* Este c√°lculo se denomina un Exponential Moving Average de los Gradientes. Y se puede interpretar como una especie de velocidad del gradiente. Su objetivo es ponderar con un cierto porcentaje el gradiente actual y el gradiente anterior.
* $v_{0} = 0$
:::

$$\begin{align} v_{t+1}&=(1-\beta)\nabla_\theta L(\theta_{t}) + \beta v_t \\
v_{t+1}&=(1-\beta)\nabla_\theta L(\theta_{t}) + \beta \left[(1-\beta) \nabla_\theta L(\theta_{t-1}) + \beta v_{t-1}\right] \\
v_{t+1}&=(1-\beta)\nabla_\theta L(\theta_{t}) + \beta (1-\beta) \nabla_\theta L(\theta_{t-1}) + \beta^2 (1-\beta) \nabla_\theta L(\theta_{t-2})... \\
\end{align}$$

## SGD with Nesterov Momentum {.smaller}

::::{.columns}
:::{.column}
$$\theta_{t+1} = \theta_t - \alpha u_{t + 1}$$
$$v_{t + 1} = \beta v_t + (1-\beta) \nabla_\theta f(\theta_{t+1} + \beta v_t)$$


donde $0<\beta<1$, pero normalmente $\beta=0.9$.
:::
:::{.column}
![](img/clase-3/nesterov_update.png){.lightbox fig-align="center" width="50%"}
:::
::::
::: {.callout-note appearance="default" icon=false}

## ‚òùÔ∏è Intuici√≥n
El m√©todo de Nesterov "mira hacia adelante" en la direcci√≥n del momentum antes de calcular el gradiente, lo que le da una correcci√≥n m√°s precisa y evita en parte el sobrepaso de m√≠nimos. En este caso $\theta_{t+1} + \beta v_t$ es el punto "futuro" para calcular el gradiente.
:::

## Efecto del Momentum en el Update Rule {.smaller}


![](img/clase-3/momentum_effect.png){.lightbox fig-align="center"}

:::{.callout-note appearance="default" icon=false}
## ‚òùÔ∏è Intuici√≥n

* El SGD tiende a ser m√°s oscilante.
* El SGD con Momentum tiende a ser m√°s suave y r√°pido debido a la inercia recibida por el t√©rmino de momentum.
:::

## M√©todos Adaptativos: Adagrad {.smaller}

:::{.callout-note appearance="default" icon=false}
## ‚òùÔ∏è Intuici√≥n
¬øQu√© tal, si el learning rate se va adaptando en el tiempo y deja de ser est√°tica?
:::


$$r_{t+1} = r_t + \nabla_\theta f(\theta_t)^2$$
$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{r_{t+1}}}\nabla_\theta f(\theta_t)$$

:::{.callout-warning appearance="default" icon=false}
## Efecto
* Par√°metros con gradientes grandes $\rightarrow$ tasa de aprendizaje disminuye m√°s r√°pido.
* Par√°metros con gradientes peque√±os $\rightarrow$ tasa de aprendizaje se mantiene m√°s alta.
:::

:::{.callout-tip appearance="default" icon=false}
## Pros
* Util cuando hay par√°metros que se actualizan con distinta frecuencia.
* Acelera la convergencia en direcciones poco exploradas.
:::

:::{.callout-important appearance="default" icon=false}
## Contras
* Como el denominador acumula gradientes al cuadrado, la tasa de aprendizaje puede llegar a volverse muy peque√±a $\rightarrow$ el entrenamiento se ***"frena"*** antes de llegar al √≥ptimo.
:::


## M√©todos Adaptativos: RMSProp {.smaller}

::: {.callout-note appearance="default" icon=false}
## ‚òùÔ∏è Intuici√≥n
* Normalizar por el Exponential Moving Average de los Gradientes al cuadrado para controlar el efecto de reducci√≥n del learning rate.
:::


$$s_{t+1} = \beta r_t + (1-\beta) \nabla_\theta f(\theta_t)^2$$
$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{s_{t+1}}}\nabla_\theta f(\theta_t)$$


:::{.callout-tip appearance="default" icon=false}
## Pros
* Normalizaci√≥n adaptativa: cada par√°metro tiene su propia tasa de aprendizaje ajustada din√°micamente.
* A diferencia de Adagrad, el denominador no crece indefinidamente porque el promedio exponencial ‚Äúolvida‚Äù gradientes antiguos. Esto permite seguir aprendiendo incluso despu√©s de muchos pasos.
:::

:::{.callout-important appearance="default" icon=false}
## Contras
* Depende mucho de la elecci√≥n de su hiperpar√°metro $\beta$
:::

## M√©todos Adaptativos: Adam {.smaller}

::: {.callout-note appearance="default" icon=false}
## ‚òùÔ∏è Intuici√≥n
Se mantiene el Exponential Moving average para: Los gradientes (como utilizando momentum), los gradientes al cuadrado (como RMSprop).

:::


::: {.columns}
::: {.column}
$$v_{t+1} = \beta_1 v_t + (1-\beta_1) \nabla_\theta f(\theta_t)$$
$$s_{t+1} = \beta_2 s_t + (1-\beta_2) \nabla_\theta f(\theta_t)^2$$
$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{s'_{t+1}}} v'_{t+1}$$
:::
::: {.column}
##### Correcciones Iniciales

$$v'_{t+1} = \frac{v_{t+1}}{1-\beta_1^{t+1}}$$
$$s'_{t+1} = \frac{s_{t+1}}{1-\beta_2^{t+1}}$$
:::
::: 


:::{.callout-tip appearance="default" icon=false}
## Pros
* Combina momentum + RMSprop + correcci√≥n $\rightarrow$ r√°pido, estable.
* Es por lejos el optimizador m√°s usado.
:::

:::{.callout-important appearance="default" icon=false}
## Contras
* Sensible a la elecci√≥n de sus hiperpar√°metros $\beta_1$ y $\beta_2$. Pytorch utiliza 0.9 y 0.999 como valores de $\beta_1$ y $\beta_2$ respectivamente.
:::

# üëä Eso es todo amigos

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
