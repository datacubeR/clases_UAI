<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Alfonso Tobar-Arancibia">
  <title>Clases UAI – TICS-579-Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-673c2e7d040da7fd4b9d655d29f657a0.css">
  <link rel="stylesheet" href="../logo.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">TICS-579-Deep Learning</h1>
  <p class="subtitle">Clase 5: Training Tips &amp; Tricks</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Alfonso Tobar-Arancibia 
</div>
        <p class="quarto-title-affiliation">
            <a href="mailto:alfonso.tobar.a@edu.uai.cl" class="email">alfonso.tobar.a@edu.uai.cl</a>
          </p>
    </div>
</div>

</section>
<section id="machine-learning-vs-deep-learning-workflow" class="slide level2 smaller">
<h2>Machine Learning vs Deep Learning Workflow</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-5/ML_vs_DL.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="img/clase-5/ML_vs_DL.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></a></p>
</figure>
</div>
<div class="columns">
<div class="column">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Machine Learning</strong></p>
</div>
<div class="callout-content">
<p>El proceso completo de entrenamiento del modelo se divide en dos etapas: preprocesamiento de datos y entrenamiento del modelo.</p>
</div>
</div>
</div>
</div><div class="column">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Deep Learning</strong></p>
</div>
<div class="callout-content">
<p>El entrenamiento del modelo abarca tres etapas: preprocesamiento de datos, diseño de la arquitectura y entrenamiento propiamente tal como uno solo.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="preprocesamiento" class="slide level2">
<h2>Preprocesamiento</h2>
<p>Algunos de los problemas más comunes para los que se requieren preprocesamientos en Deep Learning son:</p>
<div class="columns">
<div class="column">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Encoding de Variables Categóricas</strong></p>
</div>
<div class="callout-content">
<ul>
<li>One Hot Encoding</li>
<li>Embeddings</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Problemas de Escala:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Estandarización</li>
<li>Normalización</li>
<li>Normalization Layers</li>
</ul>
</div>
</div>
</div>
</div><div class="column">
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Problemas de Convergencia y combate contra el Overfitting:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Regularización L2 (Weight Decay)</li>
<li>Dropout</li>
<li>Weights Initialization</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Problemas de Recursos Computacionales:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Checkpointing</li>
<li>Early Stopping</li>
<li>Gradient Accumulation</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="problemas-de-escala-1" class="slide level2 smaller">
<h2>Problemas de Escala</h2>
<blockquote>
<p>En general el término Normalización está muy trillado y en la práctica se utiliza para referirse a muchos temas distintos. Algunas definiciones conocidas:</p>
</blockquote>
<div class="columns">
<div class="column">
<h3 id="normalización">Normalización</h3>
<p><span class="math display">\[x_{i\_norm} = \frac{x_i-x_{min}}{x_{max} - x_{min}}\]</span> Esta operación se puede hacer mediante <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html">MinMaxScaler</a> de Scikit-Learn.</p>
</div><div class="column">
<h3 id="estandarización">Estandarización</h3>
<p><span class="math display">\[ x_{i\_est} = \frac{x_i - E[x]}{\sqrt(Var[x])}\]</span></p>
<p>Esta operación se puede hacer mediante <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">StandardScaler</a> de Scikit-Learn.</p>
</div></div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>👀 Ojo</strong></p>
</div>
<div class="callout-content">
<p>La implementación de este tipo de técnicas normalmente requiere librerías externas a Pytorch (como Scikit-Learn o Feature Engine). Por lo que no es tan común en prácticas más avanzadas del área.</p>
</div>
</div>
</div>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Alerta de Data Leakage</strong></p>
</div>
<div class="callout-content">
<p>Utilizar estas estrategias externas pueden producir problemas de Data Leakage cuando se hace entrenamiento por Mini-Batches que es lo más común. Para evitar esto, lo más común es utilizar Normalization Layers dentro de la misma Arquitectura de la Red Neuronal.</p>
</div>
</div>
</div>
</section>
<section id="normalization-layers-batch-normalization" class="slide level2 smaller">
<h2>Normalization Layers: Batch Normalization</h2>
<p><a href="https://arxiv.org/pdf/1502.03167">Ioffe &amp; Szegedy, 2015: Batch Normalization</a></p>
<blockquote>
<p>Consiste en la primera implementación de una Normalización para cada Batch.</p>
</blockquote>
<div class="columns">
<div class="column">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Resuelve el problema de <a href="https://machinelearning.wtf/terms/internal-covariate-shift/">Internal Covariate Shift</a>.</li>
<li>Disminuye la importancia de los Parámetros iniciales e inicio del aprendizaje.</li>
<li>Genera un modelo más estable debido a que las activaciones están normalizadas.</li>
</ul>
</div>
</div>
</div>
</div><div class="column">
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Cons</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Genera más parámetros entrenables y, por ende, más cálculos en la red.</li>
<li>Se complica el proceso de inferencia (test time).</li>
</ul>
</div>
</div>
</div>
</div></div>
<section id="internal-covariate-shift" style="font-size: 80%;">
<h4>Internal Covariate Shift</h4>
<ul>
<li>Al entrenar una red neuronal, cada capa depende de las salidas (activaciones) de la capa anterior.</li>
<li>Durante el entrenamiento, esas salidas cambian porque sus pesos se están actualizando, lo que implica que la distribución de entrada que ve cada capa está cambiando constantemente. Esto es lo que se conoce como <code>internal covariate shift</code>.</li>
</ul>
<blockquote>
<p>Esto significa que cada capa debe adaptarse continuamente a nuevas distribuciones de entrada, lo que ralentiza el entrenamiento y dificulta la convergencia.</p>
</blockquote>
</section>
</section>
<section id="normalization-layers-batch-norm-punto-de-partida" class="slide level2 smaller">
<h2>Normalization Layers: Batch Norm, Punto de Partida</h2>
<h4 id="ejemplo-supongamos-que-dado-la-altura-y-la-edad-queremos-predecir-si-será-deportista-de-alto-rendimiento.">Ejemplo: Supongamos que dado la altura y la edad queremos predecir si será deportista de alto rendimiento.</h4>
<div class="columns" style="font-size: 90%;">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/normalize.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="img/clase-6/normalize.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Observaciones</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Rango de Edad es mucho mayor que el de Altura.</li>
<li>Cambios en Altura deben ser mucho más pequeños que en Edad debido al rango.</li>
<li>Si el learning rate es alto puede diverger si nos movemos en la dirección de altura.</li>
<li>Si el learning rate es bajo, podría demorar mucho más en converger si nos movemos en la dirección de edad.</li>
</ul>
</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/post_normalize.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="img/clase-6/post_normalize.png" class="quarto-figure quarto-figure-center" style="width:55.0%"></a></p>
</figure>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Observaciones</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Sin importar el punto inicial, el mínimo se encuentra <strong>casi</strong> a la misma distancia.<br>
</li>
<li>Es posible utilizar un learning rate más grande sin miedo a diverger.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="batch-norm-implementación-forward-pass" class="slide level2 smaller">
<h2>Batch Norm: Implementación (Forward Pass)</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p>Supongamos el siguiente Batch de tamaño <span class="math inline">\(B=15\)</span> con 3 features:</p>
<p><span class="math display">\[
X = \begin{bmatrix}
16 &amp; 14 &amp; 12 \\
15 &amp; 3  &amp; 15 \\
16 &amp; 7  &amp; 6  \\
14 &amp; 3  &amp; 16 \\
4  &amp; 4  &amp; 18 \\
8  &amp; 11 &amp; 9  \\
18 &amp; 18 &amp; 14 \\
5  &amp; 17 &amp; 11 \\
15 &amp; 9  &amp; 11 \\
10 &amp; 7  &amp; 7
\end{bmatrix}
\]</span></p>
</div><div class="column" style="width:70%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>1. Calcular estadísticos de cada Batch: Promedio y Varianza.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\mu_B = \begin{bmatrix}
12.1 &amp; 9.3 &amp; 11.9
\end{bmatrix}
\]</span> <span class="math display">\[
\sigma^2_B=\begin{bmatrix}
22.29 &amp; 27.81 &amp; 13.69
\end{bmatrix}
\]</span></p>
<ul>
<li><span class="math inline">\(\sigma^2_B\)</span> corresponde a la varianza sesgada (dividida por N).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>2. Calcular normalización de cada Batch.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[x_{norm} = \frac{X - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>3. Calcular Activación</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[h = \gamma \odot x_{norm} + \beta\]</span></p>
<p>Por defecto, Pytorch considera que <span class="math inline">\(\gamma = \begin{bmatrix}
1 &amp; ... &amp; 1
\end{bmatrix}\)</span> y <span class="math inline">\(\beta = \begin{bmatrix}
0 &amp; ... &amp; 0
\end{bmatrix}\)</span>.</p>
<p>Luego <span class="math inline">\(\gamma\)</span> y <span class="math inline">\(\beta\)</span> son parámetros entrenables por el modelo. Tanto <span class="math inline">\(\gamma\)</span> como <span class="math inline">\(\beta\)</span> tienen la misma dimensión que el número de features (en este caso 3).</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="batch-norm-implementación-forward-pass-1" class="slide level2 smaller">
<h2>Batch Norm: Implementación (Forward Pass)</h2>
<div class="columns">
<div class="column">
<h4 id="cálculo-manual">Cálculo Manual</h4>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a>eps <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb1-2"><a></a>batch_mean <span class="op">=</span> X.mean(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-3"><a></a>batch_var_train <span class="op">=</span> X.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-4"><a></a>x_norm <span class="op">=</span> (X<span class="op">-</span>batch_mean)<span class="op">/</span>torch.sqrt(batch_var_train <span class="op">+</span> eps)</span>
<span id="cb1-5"><a></a>w <span class="op">=</span> torch.tensor([<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb1-6"><a></a>b <span class="op">=</span> torch.tensor([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb1-7"><a></a>w<span class="op">*</span>x_norm <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass Train obtenido Manualmente
============================================================
tensor([[ 0.8261,  0.8912,  0.0270],
        [ 0.6142, -1.1946,  0.8378],
        [ 0.8261, -0.4361, -1.5946],
        [ 0.4024, -1.1946,  1.1081],
        [-1.7157, -1.0050,  1.6486],
        [-0.8684,  0.3224, -0.7838],
        [ 1.2497,  1.6498,  0.5676],
        [-1.5038,  1.4601, -0.2432],
        [ 0.6142, -0.0569, -0.2432],
        [-0.4448, -0.4361, -1.3243]])
</code></pre>
</div><div class="column">
<h4 id="cálculo-en-pytorch">Cálculo en Pytorch</h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>bn <span class="op">=</span> nn.BatchNorm1d(<span class="dv">3</span>)</span>
<span id="cb3-2"><a></a><span class="co">## Importantísimo ya que BatchNorm tiene distinto </span></span>
<span id="cb3-3"><a></a><span class="co">## funcionamiento en Modo Train y Eval</span></span>
<span id="cb3-4"><a></a>bn.train()</span>
<span id="cb3-5"><a></a>output_pytorch<span class="op">=</span> bn(X)</span>
<span id="cb3-6"><a></a>output_pytorch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass en Modo Train utilizando Pytorch
============================================================
tensor([[ 0.8261,  0.8912,  0.0270],
        [ 0.6142, -1.1946,  0.8378],
        [ 0.8261, -0.4361, -1.5946],
        [ 0.4024, -1.1946,  1.1081],
        [-1.7157, -1.0050,  1.6486],
        [-0.8684,  0.3224, -0.7838],
        [ 1.2497,  1.6498,  0.5676],
        [-1.5038,  1.4601, -0.2432],
        [ 0.6142, -0.0569, -0.2432],
        [-0.4448, -0.4361, -1.3243]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</code></pre>
</div></div>
</section>
<section id="batch-norm-implementación-test-time" class="slide level2 smaller">
<h2>Batch Norm: Implementación (Test Time)</h2>
<h4 id="problema">Problema</h4>
<p>La predicción de una instancia <span class="math inline">\(i\)</span> específica, ahora depende de otros elementos dentro del Batch.</p>
<p><strong><em>¿Cómo funciona entonces el modelo en Test Time?</em></strong></p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>1. Calcular running mean y running var.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\begin{aligned}
\mu_{running} &amp;= (1 - \alpha) \mu_{running} + \mu_B \\
s^2_{running} &amp;= (1 - \alpha) \cdot s^2_{running} + s^2_B
\end{aligned}
\]</span></p>
<p><strong>Ojo</strong>: En este caso se usa la Varianza insesgada (dividida por N-1). <span class="math inline">\(\alpha\)</span> es un hiperparámetro que mide la contribución del Batch actual a los estadísticos globales. Por defecto en Pytorch <span class="math inline">\(\alpha = 0.1\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>2. Calcular normalización de cada Batch.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[x_{norm}^{test} = \frac{X - \mu_{running}}{\sqrt{s_{running}^2 + \epsilon}}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>3. Calcular Activación</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[h = \gamma \cdot x_{norm}^{test} + \beta\]</span></p>
<p>En este caso, <span class="math inline">\(\gamma\)</span> y <span class="math inline">\(\beta\)</span> son parámetros aprendidos durante el entrenamiento.</p>
</div>
</div>
</div>
</section>
<section id="batch-norm-implementación-test-time-1" class="slide level2 smaller">
<h2>Batch Norm: Implementación (Test Time)</h2>
<div class="columns">
<div class="column">
<h4 id="cálculo-manual-1">Cálculo Manual</h4>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>batch_var_eval <span class="op">=</span> X.var(dim<span class="op">=</span><span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">True</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-2"><a></a></span>
<span id="cb5-3"><a></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb5-4"><a></a>rm <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>alpha)<span class="op">*</span>torch.tensor([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>]) <span class="op">+</span> alpha<span class="op">*</span>batch_mean</span>
<span id="cb5-5"><a></a>rv <span class="op">=</span> (<span class="dv">1</span><span class="op">-</span>alpha)<span class="op">*</span>torch.tensor([<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]) <span class="op">+</span> alpha<span class="op">*</span>batch_var_eval</span>
<span id="cb5-6"><a></a></span>
<span id="cb5-7"><a></a>x_normalized_eval <span class="op">=</span> (X <span class="op">-</span> rm)<span class="op">/</span>torch.sqrt(rv <span class="op">+</span> eps)</span>
<span id="cb5-8"><a></a>bn.weight.data<span class="op">*</span>x_normalized_eval<span class="op">+</span>bn.bias.data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Test Time: 
============================================================
Media: 
tensor([[1.2100, 0.9300, 1.1900]])
Varianza: 
tensor([[3.3767, 3.9900, 2.4211]])
Forward Pass en Modo Evaluación obtenido de manera manual...
============================================================
tensor([[ 8.0487,  6.5432,  6.9473],
        [ 7.5045,  1.0363,  8.8753],
        [ 8.0487,  3.0388,  3.0913],
        [ 6.9603,  1.0363,  9.5180],
        [ 1.5183,  1.5369, 10.8034],
        [ 3.6951,  5.0413,  5.0193],
        [ 9.1370,  8.5457,  8.2327],
        [ 2.0625,  8.0451,  6.3046],
        [ 7.5045,  4.0400,  6.3046],
        [ 4.7835,  3.0388,  3.7339]])</code></pre>
</div><div class="column">
<h4 id="cálculo-en-pytorch-1">Cálculo en Pytorch</h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>bn.<span class="bu">eval</span>()</span>
<span id="cb7-2"><a></a>bn(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass en Modo Evaluación usando Pytorch...
============================================================
tensor([[ 8.0487,  6.5432,  6.9473],
        [ 7.5045,  1.0363,  8.8753],
        [ 8.0487,  3.0388,  3.0913],
        [ 6.9603,  1.0363,  9.5180],
        [ 1.5183,  1.5369, 10.8034],
        [ 3.6951,  5.0413,  5.0193],
        [ 9.1370,  8.5457,  8.2327],
        [ 2.0625,  8.0451,  6.3046],
        [ 7.5045,  4.0400,  6.3046],
        [ 4.7835,  3.0388,  3.7339]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</code></pre>
</div></div>
</section>
<section id="batch-norm-consejos" class="slide level2 smaller">
<h2>Batch Norm: Consejos</h2>
<ul>
<li>Andrew Ng propone utilizar BatchNorm justo antes de la función de Activacion.</li>
<li>El paper original también propone su uso justo antes de la activación.</li>
<li>Francoise Chollet, creador de Keras dice que los autores del paper en realidad lo utilizaron después de la función de activación.</li>
<li>Adicionalmente existen benchmarks que muestran mejoras usando BatchNorm después de las funciones de activación.</li>
</ul>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Entonces, la posición del BatchNorm termina siendo parte de la Arquitectura, y se debe comprobar donde tiene un mejor efecto.</p>
</div>
</div>
</div>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Batchnorm tiene efectos distintos al momento de entrenar o de evaluar/predecir en un modelo. Por lo tanto, de usar Batchnorm es imperativo utilizar los modos <code>model.train()</code> y <code>model.eval()</code>.</p>
</div>
</div>
</div>
</section>
<section id="normalización-layer-norm" class="slide level2 smaller">
<h2>Normalización: Layer Norm</h2>
<p><a href="https://arxiv.org/abs/1607.06450">Ley, Ryan, Hinton, 2016: Layer Normalization</a></p>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Batch Norm tiene algunos problemas:</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Muy difícil de calcular en datos secuenciales (lo veremos más adelante).</li>
<li>Inestable cuando el Batch Size es muy pequeño.</li>
<li>Difícil de Paralelizar.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Beneficios de Layer Norm</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Puede trabajar con secuencias (Esencial para Transformers).</li>
<li>No tiene problemas para trabajar con cualquier tipo de Batch Size.</li>
<li>Se puede paralelizar, lo cuál es útil en redes como las RNN.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>En este caso se realiza la normalización por instancia y no por Batch.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="normalización-layer-norm-1" class="slide level2 smaller">
<h2>Normalización: Layer Norm</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>1. Calcular estadísticos de cada Instancia: Promedio y Varianza.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\mu = \begin{bmatrix}
14.0000 &amp; 11.0000 &amp; 9.6667 &amp; 11.0000 &amp; 8.6667 &amp; 9.3333 &amp; 16.6667 &amp; 11.0000 &amp; 11.6667 &amp; 8.0000
\end{bmatrix}^T
\]</span></p>
<p><span class="math display">\[
\sigma^2 = \begin{bmatrix}
2.6667 &amp; 32.0000 &amp; 20.2222 &amp; 32.6667 &amp; 43.5556 &amp; 1.5556 &amp; 3.5556 &amp; 24.0000 &amp; 6.2222 &amp; 2.0000
\end{bmatrix}^T
\]</span></p>
<ul>
<li><span class="math inline">\(\sigma^2_B\)</span> corresponde a la varianza sesgada (dividida por N).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>2. Calcular normalización de cada Instancia.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[x_{norm} = \frac{X - \mu}{\sqrt{\sigma^2 + \epsilon}}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>3. Calcular Activación</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[h = \gamma \odot x_{norm} + \beta\]</span></p>
<p>Por defecto, Pytorch considera que <span class="math inline">\(\gamma = \begin{bmatrix}
1 &amp; ... &amp; 1
\end{bmatrix}\)</span> y <span class="math inline">\(\beta = \begin{bmatrix}
0 &amp; ... &amp; 0
\end{bmatrix}\)</span>.</p>
<p>Luego <span class="math inline">\(\gamma\)</span> y <span class="math inline">\(\beta\)</span> son parámetros entrenables por el modelo. Tanto <span class="math inline">\(\gamma\)</span> como <span class="math inline">\(\beta\)</span> tienen la misma dimensión que el número de features (en este caso 3).</p>
</div>
</div>
</div>
</section>
<section id="layer-norm-implementación-forward-pass" class="slide level2 smaller">
<h2>Layer Norm: Implementación (Forward Pass)</h2>
<div class="columns">
<div class="column">
<h4 id="cálculo-manual-2">Cálculo Manual</h4>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a>eps <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb9-2"><a></a>sample_mean <span class="op">=</span> X.mean(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-3"><a></a>sample_var <span class="op">=</span> X.var(dim<span class="op">=</span><span class="dv">1</span>, unbiased<span class="op">=</span><span class="va">False</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-4"><a></a>x_normalized <span class="op">=</span> (X <span class="op">-</span> sample_mean) <span class="op">/</span> torch.sqrt(sample_var <span class="op">+</span> eps)</span>
<span id="cb9-5"><a></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb9-6"><a></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb9-7"><a></a>ln.weight.data<span class="op">*</span>x_normalized <span class="op">+</span> ln.bias.data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass Obtenido de manera Manual
============================================================
tensor([[ 1.2247,  0.0000, -1.2247],
        [ 0.7071, -1.4142,  0.7071],
        [ 1.4084, -0.5930, -0.8154],
        [ 0.5249, -1.3997,  0.8748],
        [-0.7071, -0.7071,  1.4142],
        [-1.0690,  1.3363, -0.2673],
        [ 0.7071,  0.7071, -1.4142],
        [-1.2247,  1.2247,  0.0000],
        [ 1.3363, -1.0690, -0.2673],
        [ 1.4142, -0.7071, -0.7071]])</code></pre>
</div><div class="column">
<h4 id="cálculo-en-pytorch-2">Cálculo en Pytorch</h4>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>ln <span class="op">=</span> nn.LayerNorm(<span class="dv">3</span>)</span>
<span id="cb11-2"><a></a>output_pytorch <span class="op">=</span> ln(X)</span>
<span id="cb11-3"><a></a>output_pytorch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass obtenido utilizando Pytorch
============================================================
tensor([[ 1.2247,  0.0000, -1.2247],
        [ 0.7071, -1.4142,  0.7071],
        [ 1.4084, -0.5930, -0.8154],
        [ 0.5249, -1.3997,  0.8748],
        [-0.7071, -0.7071,  1.4142],
        [-1.0690,  1.3363, -0.2673],
        [ 0.7071,  0.7071, -1.4142],
        [-1.2247,  1.2247,  0.0000],
        [ 1.3363, -1.0690, -0.2673],
        [ 1.4142, -0.7071, -0.7071]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</code></pre>
</div></div>
</section>
<section id="normalization-rmsnorm" class="slide level2 smaller">
<h2>Normalization: RMSNorm</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>1. Calcular Root Mean Square de cada Instancia.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[RMS(X)= \sqrt{\frac{1}{d} \sum_{i=1}^{d} X^2 + \epsilon}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>2. Calcular normalización de cada Instancia.</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[x_{norm} = \frac{X}{RMS(X)}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>3. Calcular Activación</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[h = \gamma \odot x_{norm}\]</span></p>
<p>Por defecto, Pytorch considera que <span class="math inline">\(\gamma = \begin{bmatrix}
1 &amp; ... &amp; 1
\end{bmatrix}\)</span>.</p>
<p>Luego <span class="math inline">\(\gamma\)</span> son parámetros entrenables por el modelo. <span class="math inline">\(\gamma\)</span> tienen la misma dimensión que el número de features (en este caso 3).</p>
</div>
</div>
</div>
</section>
<section id="rmsnorm-implementación" class="slide level2 smaller">
<h2>RMSNorm: Implementación</h2>
<div class="columns">
<div class="column">
<h4 id="cálculo-manual-3">Cálculo Manual</h4>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>rms <span class="op">=</span> torch.sqrt((X<span class="op">**</span><span class="dv">2</span>).mean(dim<span class="op">=</span><span class="dv">1</span>, keepdims <span class="op">=</span> <span class="va">True</span>))</span>
<span id="cb13-2"><a></a>rms_layer.weight.data<span class="op">*</span>X<span class="op">/</span>rms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass Obtenido de manera Manual
============================================================
tensor([[1.1352, 0.9933, 0.8514],
        [1.2127, 0.2425, 1.2127],
        [1.5007, 0.6566, 0.5628],
        [1.1294, 0.2420, 1.2907],
        [0.3672, 0.3672, 1.6524],
        [0.8496, 1.1682, 0.9558],
        [1.0732, 1.0732, 0.8347],
        [0.4152, 1.4118, 0.9135],
        [1.2573, 0.7544, 0.9220],
        [1.2309, 0.8616, 0.8616]])</code></pre>
</div><div class="column">
<h4 id="cálculo-en-pytorch-3">Cálculo en Pytorch</h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a>rms_layer <span class="op">=</span> nn.RMSNorm(<span class="dv">3</span>)</span>
<span id="cb15-2"><a></a>rms_layer(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>============================================================
Forward Pass Obtenido utilizando Pytorch
============================================================
tensor([[1.1352, 0.9933, 0.8514],
        [1.2127, 0.2425, 1.2127],
        [1.5007, 0.6566, 0.5628],
        [1.1294, 0.2420, 1.2907],
        [0.3672, 0.3672, 1.6524],
        [0.8496, 1.1682, 0.9558],
        [1.0732, 1.0732, 0.8347],
        [0.4152, 1.4118, 0.9135],
        [1.2573, 0.7544, 0.9220],
        [1.2309, 0.8616, 0.8616]], grad_fn=&lt;MulBackward0&gt;)</code></pre>
</div></div>
</section>
<section id="regularización-l2-aka-weight-decay" class="slide level2 smaller">
<h2>Regularización L2 aka Weight Decay</h2>
<p><a href="https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf">Paper 1991: Weight Decay</a></p>
<blockquote>
<p>En general, el principal problema de las redes neuronales es el overfitting, ya que estas redes suelen ser consideradas modelos sobredimensionados (overparameterized models). ¿Qué implica esto exactamente?</p>
</blockquote>
<dl>
<dt>Weight Decay</dt>
<dd>
<blockquote>
<p>Corresponde a una penalización que se da a los modelos para limitar su complejidad y asegurar que pueda generalizar correctamente en datos no vistos.</p>
</blockquote>
</dd>
</dl>
<p><span class="math display">\[ \underset{W_{i:L}}{minimize} \frac{1}{m} L + \frac{\lambda}{2} \sum_{i=1}^L ||W_i||_f^2\]</span></p>
</section>
<section id="regularización-l2-aka-weight-decay-1" class="slide level2 smaller">
<h2>Regularización L2 aka Weight Decay</h2>
<p>Eso implica una transformación a nuestro <strong><em>Update Rule</em></strong>:</p>
<p><span class="math display">\[W_i := W_i - \alpha \frac{1}{m} \nabla L - \alpha \lambda W_i = (1-\alpha\lambda)W_i - \alpha \nabla L\]</span></p>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Se puede ver que los pesos (weights) se <strong><em>contraen</em></strong> (decaen) antes de actualizarse en la dirección del gradiente. Lo que genera parámetros más pequeños y, por ende, un modelo más simple.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Por alguna razón Pytorch decidió implementarlo como una propiedad de los <strong><em>Optimizers</em></strong> cuando en realidad debió ser de la Loss Function.</p>
</div>
</div>
</div>
<div class="sourceCode" id="cb17" style="font-size: 150%"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>, weight_decay<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>En este caso 0.3 corresponde al valor de <span class="math inline">\(\lambda\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="dropout" class="slide level2 smaller">
<h2>Dropout</h2>
<p><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">Paper 2014: Dropout</a></p>
<p>Definiremos el Dropout como:</p>
<p><span class="math display">\[D(h)= \begin{cases}
h  &amp; \text{con prob 1-p} \\
0, &amp; \text{con prob p}
\end{cases}\]</span></p>
<p>donde <span class="math inline">\(D\)</span> implica la aplicación de Dropout a la activación <span class="math inline">\(h\)</span>. <span class="math inline">\(p\)</span> se conoce como el <code>Dropout Rate</code>.</p>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>El factor <span class="math inline">\(\frac{1}{1-p}\)</span> se aplica para mantener la varianza estable luego de haber eliminado activaciones con probabilidad <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Dropout se aplica normalmente al momento de entrenar el modelo. Por lo tanto, de usar Dropout es imperativo cambiar al modo <code>model.eval()</code> al momento de predecir.</p>
</div>
</div>
</div>
</section>
<section id="weights-initialization" class="slide level2 smaller">
<h2>Weights Initialization</h2>
<p><a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Paper 2010: Xavier Initialization</a></p>
<p><a href="https://arxiv.org/abs/1502.01852">Paper 2015: Kaiming Initialization</a></p>
<p>Hasta ahora, hemos inicializado los pesos de las redes neuronales de forma aleatoria. Sin embargo, diversos estudios han explorado estrategias de inicialización de los parámetros para lograr una convergencia más eficiente. Entre las técnicas más comunes se encuentran:</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Activaciones Triviales</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Constante</li>
<li>Sólo unos</li>
<li>Sólo Zeros</li>
<li>Con Distribución Uniforme</li>
<li>Con Distribución Normal</li>
</ul>
</div>
</div>
</div>
</section>
<section id="weights-initialization-1" class="slide level2 smaller">
<h2>Weights Initialization</h2>
<div class="columns">
<div class="column">
<h4 id="xavier-o-glorot-uniforme">Xavier o Glorot Uniforme</h4>
<p>Se inicia con valores provenientes de una distribución uniforme: <span class="math inline">\(\mathcal{U}(-a,a)\)</span></p>
<p><span class="math display">\[ a = gain \cdot \sqrt{\frac{6}{fan_{in} + fan_{out}}}\]</span></p>
</div><div class="column">
<h4 id="xavier-o-glorot-normal">Xavier o Glorot Normal</h4>
<p>Se inicia con valores provenientes de una distribución Normal: <span class="math inline">\(\mathcal{N}(0,std^2)\)</span></p>
<p><span class="math display">\[ std = gain \cdot \sqrt{\frac{2}{fan_{in} + fan_{out}}}\]</span></p>
</div></div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(fan_{in}\)</span> corresponde al número de conexiones que entran a una neurona. Mientras que <span class="math inline">\(fan_{out}\)</span> corresponde al número de neuronas que salen de dicha neurona.</li>
<li><span class="math inline">\(fan\_mode\)</span> corresponde a la elección de <span class="math inline">\(fan_{in}\)</span> o <span class="math inline">\(fan_{out}\)</span>.</li>
</ul>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<h4 id="kaiming-aka-he-uniforme">Kaiming (aka He) Uniforme</h4>
<p>Se inicia con valores provenientes de una distribución Normal: <span class="math inline">\(\mathcal{U}(-bound,bound)\)</span></p>
<p><span class="math display">\[ bound = gain \cdot \sqrt{\frac{3}{fan\_mode}}\]</span></p>
</div><div class="column">
<h4 id="kaiming-aka-he-normal">Kaiming (aka He) Normal</h4>
<p>Se inicia con valores provenientes de una distribución uniforme: <span class="math inline">\(\mathcal{N}(0,std^2)\)</span></p>
<p><span class="math display">\[std =\sqrt{\frac{gain}{fan\_mode}}\]</span></p>
</div></div>
</section>
<section id="checkpointing" class="slide level2 smaller">
<h2>Checkpointing</h2>
<p>Entrenar una red neuronal puede requerir una gran cantidad de tiempo y recursos computacionales. Por ello, es una buena práctica <strong>guardar los pesos del modelo en distintos momentos del entrenamiento</strong>, con el fin de no perder el progreso alcanzado ante posibles imprevistos (como un corte de energía o un fallo del sistema).</p>
<p>Esto ofrece varias ventajas, entre ellas:</p>
<ul>
<li>Poder disponer de resultados intermedios incluso si el entrenamiento aún no finaliza.</li>
<li>Almacenar los pesos correspondientes al <strong>mejor modelo</strong> obtenido durante el proceso de entrenamiento.</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Checkpoints</strong></p>
</div>
<div class="callout-content">
<p>Pytorch permite el uso de Checkpointing de manera sencilla mediante la función <code>torch.save()</code> y <code>torch.load()</code>.</p>
<p>Algunas estrategias comunes de Checkpointing son:</p>
<ul>
<li>Guardar el modelo cada cierto número de epochs.</li>
<li>Guardar el modelo cuando se alcanza un nuevo mejor desempeño en el conjunto de validación.</li>
<li>Guardar tanto el modelo como el optimizador para poder reanudar el entrenamiento desde el último checkpoint. En especial cuando existen restricciones de tiempo en el uso de recursos computacionales.</li>
<li>Guardar el modelo antes de alcanzar el Overfitting.</li>
<li>Guardar el modelo final al concluir el entrenamiento.</li>
<li>Guardar modelos intermedios para analizar si se va por buen camino.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="early-stopping" class="slide level2 smaller">
<h2>Early Stopping</h2>
<div style="font-size: 80%;">
<p>El Early Stopping consiste en monitorear el rendimiento sobre un conjunto de validación durante el entrenamiento y detener el proceso cuando el rendimiento comienza a empeorar. De esta manera, se evita invertir tiempo y recursos computacionales en seguir entrenando un modelo que ya no mejora su capacidad de generalización.</p>
</div>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Grokking</strong></p>
</div>
<div class="callout-content">
<p>Un nuevo concepto que anda dando vuelta en el último tiempo es el <a href="https://www.linkedin.com/feed/update/urn:li:activity:7214966566696718336?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7214966566696718336%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29">grokking</a>. El cuál es una mejora del performance del modelo pasado el punto de overfitting. Por lo que el Early Stopping podría impedir que se alcance este punto.</p>
</div>
</div>
</div>
<p>En general este proceso detiene el entrenamiento luego de <code>patience</code> epochs sin mejorar el validation loss u otro criterio. Una lógica simple para implementarlo es agregar lo siguiente al training loop:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a><span class="co"># ---- Early Stopping ----</span></span>
<span id="cb18-2"><a></a>    <span class="cf">if</span> val_loss <span class="op">&lt;</span> best_val_loss:</span>
<span id="cb18-3"><a></a>        best_val_loss <span class="op">=</span> val_loss</span>
<span id="cb18-4"><a></a>        best_model_state <span class="op">=</span> model.state_dict()</span>
<span id="cb18-5"><a></a>        counter <span class="op">=</span> <span class="dv">0</span>  <span class="co"># resetea paciencia</span></span>
<span id="cb18-6"><a></a>    <span class="cf">else</span>:</span>
<span id="cb18-7"><a></a>        counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-8"><a></a>        <span class="cf">if</span> counter <span class="op">&gt;=</span> patience:</span>
<span id="cb18-9"><a></a>            <span class="bu">print</span>(<span class="ss">f"Early stopping en epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-10"><a></a>            <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="gradient-accumulation" class="slide level2 smaller">
<h2>Gradient Accumulation</h2>
<blockquote>
<p>Otra estrategia para enfrentar limitaciones de memoria es el Gradient Accumulation. Esta técnica permite simular batch sizes más grandes al acumular los gradientes durante varios pasos. De esta forma, se obtienen resultados equivalentes a los de un entrenamiento con batches grandes, pero utilizando menos recursos computacionales (a costa de requerir más iteraciones o epochs).</p>
</blockquote>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a><span class="co">## Training with Accumulation</span></span>
<span id="cb19-2"><a></a>epochs<span class="op">=</span><span class="dv">80</span></span>
<span id="cb19-3"><a></a>accumulation_steps<span class="op">=</span><span class="dv">4</span></span>
<span id="cb19-4"><a></a>model.zero_grad()                                   <span class="co"># Resetea Gradientes Iniciales</span></span>
<span id="cb19-5"><a></a><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb19-6"><a></a>  logits <span class="op">=</span> model(X)                     </span>
<span id="cb19-7"><a></a>  loss <span class="op">=</span> criterion(logits, y.unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb19-8"><a></a>  <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>e<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-9"><a></a>  loss <span class="op">=</span> loss <span class="op">/</span> accumulation_steps                <span class="co"># Normaliza Loss</span></span>
<span id="cb19-10"><a></a>  loss.backward()                                 <span class="co"># Backward pass </span></span>
<span id="cb19-11"><a></a>  <span class="co"># (Recordar que Pytorch Acumula Gradientes hasta que se use .zero_grad())</span></span>
<span id="cb19-12"><a></a>  <span class="cf">if</span> (e<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:             </span>
<span id="cb19-13"><a></a>      optimizer.step()                            <span class="co"># Se actualizan pesos sólo cada ciertos steps</span></span>
<span id="cb19-14"><a></a>      model.zero_grad()                           <span class="co"># Y ahora se resetea</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="thats-all-folks" class="title-slide slide level1 center">
<h1>That’s all Folks</h1>
<div class="footer">
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
</p><p><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0</a></p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">
<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a>
<p></p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../logo-uai-blanco.jpeg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': true,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1366,

        height: 768,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/datacubeR\.github\.io\/clases_UAI\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>