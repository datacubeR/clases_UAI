<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Alfonso Tobar-Arancibia">
  <title>Clases UAI – TICS-579-Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-673c2e7d040da7fd4b9d655d29f657a0.css">
  <link rel="stylesheet" href="../logo.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">TICS-579-Deep Learning</h1>
  <p class="subtitle">Clase 3: Feed Forward Networks</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Alfonso Tobar-Arancibia 
</div>
        <p class="quarto-title-affiliation">
            <a href="mailto:alfonso.tobar.a@edu.uai.cl" class="email">alfonso.tobar.a@edu.uai.cl</a>
          </p>
    </div>
</div>

</section>
<section>
<section id="feed-forward-networks" class="title-slide slide level1 center">
<h1>Feed Forward Networks</h1>

</section>
<section id="feed-forward-networks-1" class="slide level2 smaller">
<h2>Feed Forward Networks</h2>
<blockquote>
<p>Son redes neuronales que se caracterizan por tener una arquitectura en la que la información fluye en una sola dirección, desde las entradas hasta las salidas. En general todas las neuronas de una capa están conectadas a todas las neuronas de la siguiente capa, sin ciclos ni conexiones recurrentes.</p>
</blockquote>
<dl>
<dt>Teorema de aproximación Universal</dt>
<dd>
<blockquote>
<p>Una red neuronal feedforward con al menos una capa oculta y un número finito de neuronas, usando funciones de activación no lineales (como sigmoide, tanh o ReLU), puede aproximar cualquier función continua definida en un conjunto compacto (acotado y cerrado) de <span class="math inline">\(\mathbb{R}^n\)</span> a cualquier nivel de precisión, siempre que se utilicen suficientes neuronas y se ajusten adecuadamente los pesos y sesgos.</p>
</blockquote>
</dd>
</dl>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Importante</strong></p>
</div>
<div class="callout-content">
<ul>
<li>El teorema dice que es posible encontrar aproximar cualquier función.</li>
<li>El teorema no dice ni cómo se hace ni los recursos necesarios para hacerlo (Número de Neuronas, capas, Hiperparámetros, etc.).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Básicamente nos están diciendo que tienes la mejor herramienta que existe, pero es tu responsabilidad saber cómo utilizarla y qué recursos necesitas para lograrlo.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="feed-forward-networks-ffn" class="slide level2 smaller">
<h2>Feed Forward Networks (FFN)</h2>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Este tipo de Redes tiene distintos nombres que son usados de manera intercambiable:</p>
<ul>
<li>Capas Lineales: Probablemente por su denominación en Pytorch.</li>
<li>Capas/Redes Densas: Probablemente por su denominación en Tensorflow.</li>
<li>Multilayer Perceptron: O también conocido como MLP, debido a que es la generalización del Perceptrón, la primera propuesta de Redes Neuronales de Rosenblatt en 1958.</li>
<li>Projection Layers: Probablemente por su denominación en algunos papers. Se usa en el contexto de proyectar de <span class="math inline">\(n\)</span> dimensiones a <span class="math inline">\(d\)</span> dimensiones.</li>
</ul>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-1/nn_arq.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="img/clase-1/nn_arq.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a></p>
</figure>
</div>
</div><div class="column">
<p>De ahora en adelante utilizaremos las siguiente notación para referirnos a una Red Neuronal Feed Forward:</p>
<p><span class="math display">\[h_\theta(X) = \sigma_s(Z)\]</span></p>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Logits</strong></p>
</div>
<div class="callout-content">
<p>Definiremos <span class="math inline">\(Z=\phi_L(X) W_{L+1} + b_{L+1}^T\)</span> como Logits y corresponden a las activaciones de la última capa antes de aplicar la función de activación de salida <span class="math inline">\(\sigma_s(.)\)</span>.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="hiperparámetros-de-una-red-neuronal" class="slide level2 smaller">
<h2>Hiperparámetros de una Red Neuronal</h2>
<dl>
<dt>Hiperparámetros</dt>
<dd>
<blockquote>
<p>Son las configuraciones externas que no se aprenden durante el entrenamiento, sino que se definen antes de entrenar el modelo y controlan su comportamiento y rendimiento.</p>
</blockquote>
</dd>
</dl>
<div style="font-size: 120%;">
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>🤓 Hiperparámetros de una Red Neuronal</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong><em>Learning Rate</em></strong> (Karpathy Constant: 3e-4), valores entre [1e-5, 1e-1] son comunes.</li>
<li><strong>Número de Capas</strong> y sus respectivas dimensiones (Para Pesos/Weights y Sesgos/Biases).</li>
<li><strong>Funciones de Activación</strong> para cada capa.</li>
<li><strong>Función de Pérdida</strong> (Loss Function) a utilizar.</li>
<li><strong>Optimizador</strong> a utilizar.</li>
<li><strong>Punto de Partida</strong> de los Parámetros (Inicialización de Pesos y Sesgos).</li>
<li>¿Cuánto tiempo debo entrenar mi modelo? <strong><em>¿Cómo sabemos si es que convergió o no?</em></strong></li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="output-de-una-red-neuronal" class="slide level2 smaller">
<h2>Output de una Red Neuronal</h2>
<blockquote>
<p>En el aprendizaje supervisado se abordan principalmente dos tipos de problemas: clasificación y regresión. Según el tipo de problema, la hipótesis debe adoptar una forma distinta en la capa de salida.</p>
</blockquote>
<div style="font-size: 120%;">
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>⚠️ Dimensión de Salida</strong></p>
</div>
<div class="callout-content">
<p>Está definida por el número de valores a predecir para cada observación. Denominaremos <span class="math inline">\(k\)</span> como la dimensión de salida.</p>
<p>Para una red de dos capas:</p>
<p><span class="math display">\[\phi_0(X) = X\]</span> <span class="math display">\[\phi_1(X) = \sigma_1(W_1 \cdot \phi_0(X) + \bar{b_1}^T)\]</span> <span class="math display">\[\phi_2(X) = \sigma_2(W_2 \cdot \phi_1(X) + \bar{b_2}^T)\]</span></p>
<p>Donde <span class="math inline">\(W_1 \in \mathbb{R}^{n \times d_1}\)</span> y <span class="math inline">\(W_2 \in \mathbb{R}^{d_1 \times k}\)</span> y <span class="math inline">\(b_1 \in \mathbb{R}^{d_1}\)</span> y <span class="math inline">\(b_2 \in \mathbb{R}^{k}\)</span>.</p>
</div>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>✅ Activación de la Salida</strong></p>
</div>
<div class="callout-content">
<p>Según el tipo de problema, la capa de salida puede necesitar una función de activación particular que ajuste los resultados al formato correcto. En este sentido, <span class="math inline">\(\sigma_2\)</span> estará determinada por la naturaleza del problema a resolver.</p>
</div>
</div>
</div>
</div>
</section>
<section id="consejos-para-el-output-de-una-red" class="slide level2 smaller">
<h2>Consejos para el Output de una Red</h2>
<div class="columns">
<div class="column">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Clasificación Binaria</strong></p>
</div>
<div class="callout-content">
<p>El approach más común utiliza <span class="math inline">\(k=1\)</span> con una <strong><em>Sigmoide</em></strong> para calcular la probabilidad de la clase 1. Otros approach utilizan <span class="math inline">\(k=2\)</span> para calcular la probabilidad de ambas clases (Activando con <strong><em>Softmax</em></strong>).</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Clasificación Multiclase</strong></p>
</div>
<div class="callout-content">
<p>Utiliza <span class="math inline">\(k=C\)</span> donde C es el número de clases a clasificar. Se usa una función <strong><em>Softmax</em></strong> para transformar el output en una distribución de probabilidades.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Clasificación Multilabel</strong></p>
</div>
<div class="callout-content">
<p>Se requiere un <span class="math inline">\(k=C\)</span> donde C es el número de clases a clasificar. Se usa una función <strong><em>Sigmoide</em></strong> para transformar cada clase en probabilidades.</p>
</div>
</div>
</div>
</div><div class="column">
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Regresión Simple</strong></p>
</div>
<div class="callout-content">
<p>Se requiere un <span class="math inline">\(k=1\)</span>. Típicamente no requiere de funciones adicionales aunque a veces se agregan funciones para acotar la salida.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Regresión Multiple</strong></p>
</div>
<div class="callout-content">
<p>Se requiere un <span class="math inline">\(k=V\)</span> con V el número de valores a predecir. Se deben tener las mismas consideraciones para acotar la salida.</p>
</div>
</div>
</div>
</div></div>
<div class="fragment">
<div class="callout callout-none no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>👀 Muy Importante</strong></p>
</div>
<div class="callout-content">
<p>En la mayoría de las implementaciones en Código la activación de la salida va embebida en la Loss Function. Por lo tanto, no es necesario aplicar una función de activación explícita en la capa de salida. Aunque sí deben aplicarse al momento de la <strong><em>Predicción del modelo</em></strong>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="funciones-de-activación" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<dl>
<dt>Activation Functions</dt>
<dd>
Corresponden a las funciones que agregarán características no lineales a cada activación, impidiendo la composición de transformaciones Affine.
</dd>
</dl>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>🤓 Convención para Código</strong></p>
</div>
<div class="callout-content">
<p>En Pytorch, nunca aplicaremos una función de activación a la capa de salida.</p>
</div>
</div>
</div>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Cuidado</strong></p>
</div>
<div class="callout-content">
<p>Otros frameworks como Tensorflow, Keras, etc. utilizan una convención distinta y aplican funciones de activación a la capa de salida.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>¿Puedo aplicar distintas Funciones de Activación a cada Neurona?</strong></p>
</div>
<div class="callout-content">
<p>Puedo, pero no se hace. Complicaría muchísimo la implementación.</p>
</div>
</div>
</div>
<p><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">Activation Functions in Pytorch</a>.</p>
</section>
<section id="funciones-de-activación-1" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<h3 id="sigmoide">Sigmoide</h3>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-1/sigmoid.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="img/clase-1/sigmoid.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div style="font-size: 120%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definición</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Propiedades</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Acota la salida entre 0 y 1.</li>
<li>Su derivada es <span class="math inline">\(\sigma'(z) = \sigma(z)(1 - \sigma(z))\)</span>.</li>
<li>Su gradiente es general es muy pequeño, lo que lleva a problemas de <strong><em>Vanishing Gradient</em></strong>.</li>
<li>Su principal uso es en la capa de salida para problemas de clasificación binaria y Clasificación Multilabel.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="funciones-de-activación-2" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<h3 id="softmax">Softmax</h3>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/softmax.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="img/clase-3/softmax.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div style="font-size: 120%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definición</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[S_i(z) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Propiedades</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Transofrma un vector en una distribución de probabilidad.</li>
<li>Su principal uso es en la capa de salida para problemas de clasificación multiclase. Es por lejos la función de activación más utilizada en la salida, pero en casos más avanzados también en Mecanismos de Atención.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="funciones-de-activación-3" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<h3 id="tanh">Tanh</h3>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/tanh.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="img/clase-3/tanh.jpg" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div style="font-size: 120%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definición</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[Tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Propiedades</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Acota su salida entre -1 y 1.</li>
<li>Su derivada es <span class="math inline">\(Tanh'(z) = 1 - Tanh^2(z)\)</span>.</li>
<li>Su gradiente normalmente es más grande que el de la Sigmoide, pero aún así puede llevar a problemas de <strong><em>Vanishing Gradient</em></strong>.</li>
<li>A pesar de estar un poco en desuso, tiene un rol protagónico en las <code>Redes Recurrentes</code> (RNNs).</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="funciones-de-activación-4" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<h3 id="relu-rectified-linear-unit">ReLU (Rectified Linear Unit)</h3>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/relu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="img/clase-3/relu.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div style="font-size: 120%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definición</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[ReLU(z) = max(0, z)\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Propiedades</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Acota su salida entre 0 e <span class="math inline">\(\infty\)</span>.</li>
<li>Su derivada es <span class="math inline">\(ReLU'(z) = \begin{cases}
1,  &amp; \text{if $z \ge$ 0} \\
0 &amp; \text{if $z &lt; 0$}
\end{cases}\)</span></li>
<li>Es la función de activación más utilizada en la actualidad, principalmente en las capas ocultas de las Redes Neuronales.</li>
<li>Se hizo extremadamente popular por su simplicidad y efectividad en <code>Redes Convolucionales</code> (CNNs).</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="funciones-de-activación-modernas" class="slide level2 smaller">
<h2>Funciones de Activación Modernas</h2>
<div class="columns">
<div class="column">
<h3 id="leaky-relu">Leaky ReLU</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/leaky_relu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="img/clase-3/leaky_relu.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a></p>
</figure>
</div>
<p><span class="math display">\[g(z) = max(0.1z, z)\]</span></p>
</div><div class="column">
<h3 id="parametrized-relu-prelu">Parametrized ReLU (PReLU)</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/prelu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="img/clase-3/prelu.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a></p>
</figure>
</div>
<p><span class="math display">\[g(z) = max(az, z)\]</span></p>
</div></div>
</section>
<section id="funciones-de-activación-5" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<div class="columns">
<div class="column">
<h3 id="elu">ELU</h3>
<p><a href="img/clase-3/elu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="img/clase-3/elu.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a> <span class="math inline">\(g(z) =
\begin{cases}
z,  &amp; \text{if $z \ge$ 0} \\[2ex]
\alpha(e^{z}-1), &amp; \text{if $z &lt; 0$}
\end{cases}\)</span></p>
</div><div class="column">
<h3 id="gelu">GELU</h3>
<p><a href="img/clase-3/gelu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="img/clase-3/gelu.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a> <span class="math display">\[\begin{align} g(z) &amp;= z \cdot \Phi(z) \\
g(z)&amp;= 0.5 \cdot z \cdot \left(1 + Tanh\left(\sqrt{2/\pi}\right) \cdot \left(z + 0.044715 \cdot z^3\right)\right)\end{align}\]</span></p>
</div></div>
</section>
<section id="funciones-de-activación-6" class="slide level2 smaller">
<h2>Funciones de Activación</h2>
<div class="columns">
<div class="column">
<h3 id="selu">SELU</h3>
<p><a href="img/clase-3/selu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="img/clase-3/selu.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a> <span class="math display">\[ g(z) = scale \cdot (max(0,z) + min(0,\alpha(e^z - 1)))\]</span></p>
<p>con <span class="math inline">\(\alpha=1.6732632423543772848170429916717\)</span> y <span class="math inline">\(scale = 1.0507009873554804934193349852946\)</span></p>
</div><div class="column">
<h3 id="swish">Swish</h3>
<p><a href="img/clase-3/swish.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img data-src="img/clase-3/swish.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a> <span class="math display">\[g(z) = z \cdot sigmoid(z)\]</span></p>
</div></div>
</section>
<section id="loss-functions-clasificación" class="slide level2 smaller">
<h2>Loss Functions: Clasificación</h2>
<blockquote>
<p>Son las encargadas de medir el error entre la predicción del modelo y el valor real. En general, se busca minimizar la Loss Function durante el entrenamiento del modelo.</p>
</blockquote>
<div style="font-size: 120%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Clasificación Binaria: Binary Cross Entropy</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[BCE(Z) = - \frac{1}{m}\left[y^T log(\sigma(Z)) + (1-y)^T log(1-\sigma(Z))\right]\]</span></p>
<p>Donde <span class="math inline">\(Z\)</span> corresponden a los Logits del Modelo.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>En Pytorch esta función se llama <code>BCEWithLogitsLoss</code>.</p>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>🤓 Logits</strong></p>
</div>
<div class="callout-content">
<p>Se refiere a las activaciones finales del modelo antes de aplicar la función de activación.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>👊 Clasificación Multilabel: BCEWithLogitsLoss</strong></p>
</div>
<div class="callout-content">
<p>En Pytorch se suele utilizar <code>BCEWithLogitsLoss</code> ya que combina una sigmoide a cada activación de la salida.</p>
</div>
</div>
</div>
</section>
<section id="loss-functions-clasificación-1" class="slide level2 smaller">
<h2>Loss Functions: Clasificación</h2>
<div style="font-size: 120%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Clasificación Multiclase: CrossEntropy</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[CE(Z)= -\frac{1}{m}Tr(Y^T Log(\hat{Y}))\]</span></p>
<p>Donde <span class="math inline">\(Tr(.)\)</span> es la traza de una matriz e <span class="math inline">\(Y \in \{0,1\}^{m \times k}\)</span> es la codificación One-Hot de las etiquetas e <span class="math inline">\(\hat{Y} = Softmax(Z)\)</span>, donde <span class="math inline">\(Z\)</span> son los Logits del modelo.</p>
</div>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>🤓 Traza (<span class="math inline">\(Tr(.)\)</span>)</strong></p>
</div>
<div class="callout-content">
<p>Corresponde a la suma de los elementos de la diagonal principal de una matriz.</p>
</div>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Derivada</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\frac{\partial CE(X)}{\partial Z} = \frac{1}{m}\left(\hat{Y} - Y\right)\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>En Pytorch se suele utilizar <code>CrossEntropyLoss</code> ya que combina aplica una función Softmax a la capa de salida además de ser una clase numericamente más estable.</p>
</div>
</div>
</div>
</section>
<section id="ejemplo-de-cross-entropy-loss" class="slide level2">
<h2>Ejemplo de Cross Entropy Loss</h2>
<p>TODO:</p>
</section>
<section id="loss-functions-regresión" class="slide level2 smaller">
<h2>Loss Functions: Regresión</h2>
<div style="font-size: 120%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Regresión: Mean Squared Error (MSELoss)</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[MSE(Z) = \frac{1}{m}||Z - \bar{y}||^2\]</span></p>
<p>Donde <span class="math inline">\(||.||\)</span> corresponde a la norma Euclideana e <span class="math inline">\(\bar{y} \in \mathbb{R}^{m \times 1}\)</span>.</p>
</div>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Derivada</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\frac{\partial MSE(Z)}{\partial Z} = \frac{2}{m}(Z - \bar{y})\]</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="loss-functions-regresión-1" class="slide level2 smaller">
<h2>Loss Functions: Regresión</h2>
<div style="font-size: 120%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Regresión: Mean Absolute Error (L1Loss)</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[L1Loss(Z) = \frac{1}{m}|Z - \bar{y}|\]</span></p>
<p>Donde <span class="math inline">\(||.||\)</span> corresponde a la norma Euclideana y <span class="math inline">\(\bar{y} \in \mathbb{R}^{m \times 1}\)</span>.</p>
</div>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Derivada</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\frac{\partial L1Loss(Z)}{\partial Z} = \frac{1}{m}sign(Z-\bar{y})\]</span></p>
<p><span class="math display">\[\operatorname{sign}(z) =
\begin{cases}
+1 &amp; \text{si  z &gt; 0},\\[2mm]
0 &amp; \text{si z = 0},\\[1mm]
-1 &amp; \text{si z &lt; 0}
\end{cases}\]</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="optimizers-gradient-descent" class="slide level2 smaller">
<h2>Optimizers: Gradient Descent</h2>
<div style="font-size: 80%;">
<blockquote>
<p>Gradient Descent corresponde al algoritmo de Optimización más popular, pero no necesariamente el más eficiente. Distintas variantes han ido apareciendo para ir mejorando eventuales deficiencias de la proposición inicial.</p>
</blockquote>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Epochs</strong></p>
</div>
<div class="callout-content">
<p>Corresponden a la cantidad de iteraciones que se realizan a la Update Rule para que el modelo se optimize.</p>
</div>
</div>
</div>
<div style="font-size: 120%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Standard Gradient Descent</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\theta := \theta - \frac{\alpha}{m}\nabla_\theta L\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Importante</strong></p>
</div>
<div class="callout-content">
<ul>
<li>En Deep Learning, los conjuntos de datos suelen ser tan grandes que calcular el gradiente sobre todos ellos es inviable por memoria y tiempo de cómputo.</li>
<li>Adicionalmente no basta con calcular el gradiente una vez, sino que se debe hacer varias veces según el número de Epochs definido.</li>
<li>Practicar Standard Gradient Descent en la práctica es muy poco común, ya que no es eficiente.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="minibatch-gradient-descent" class="slide level2 smaller">
<h2>Minibatch Gradient Descent</h2>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Los <code>minibatches</code> permiten estimar el gradiente con un subconjunto de datos, manteniendo la dirección correcta para actualizar los parámetros de manera más eficiente. Se realiza en un subconjunto de <span class="math inline">\(B\)</span> datos donde <span class="math inline">\(B &lt;&lt; m\)</span>.</p>
</div>
</div>
</div>
<p><span class="math display">\[\theta := \theta - \frac{\alpha}{B}\nabla_\theta L\]</span></p>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>👀 Importante</strong></p>
</div>
<div class="callout-content">
<ul>
<li><span class="math inline">\(X \in \mathbb{R}^{B \times n}\)</span> e <span class="math inline">\(y \in \mathbb{R}^{B \times 1}\)</span> son versiones reducidas de los datos totales. Se deben hacer suficientes <code>minibatches</code> para utilizar todos los datos. El minibatch se implementa en Pytorch utilizando el <code>DataLoader</code>. Cada actualización de parámetros ahora se le denomina <code>step</code>.</li>
<li>Cuando todos los <code>minibatches</code> han sido utilizados, se dice que se ha completado una <code>epoch</code>.</li>
<li>Es común utilizar un <code>minibatch</code> de tamaño 32, 64, 128, etc.</li>
<li>A veces se deshecha el último <code>minibatch</code> (remanente) si no tiene el tamaño completo para evitar problemas de estabilidad de gradientes.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Permite optimizar utilizando menos recursos computacionales.</li>
<li>Al actualizar los parámetros de manera más frecuente, se puede converger más rápido.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Contras</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Si <span class="math inline">\(B\)</span> es muy pequeño, el gradiente puede ser muy ruidoso y no converger.</li>
<li>El entrenamiento toma más tiempo que el Standard Gradient Descent.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="sgd-with-momentum" class="slide level2 smaller">
<h2>SGD with Momentum</h2>
<div class="columns">
<div class="column" style="font-size: 90%;">
<h4 id="update-rule">Update Rule</h4>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \alpha v_{t + 1}\]</span> <span class="math display">\[v_{t+1} = \beta v_{t} + (1-\beta) \nabla_\theta L(\theta_{t+1})\]</span></p>
<p>donde <span class="math inline">\(0&lt;\beta&lt;1\)</span>, pero normalmente <span class="math inline">\(\beta=0.9\)</span>.</p>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/momentum_update.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img data-src="img/clase-3/momentum_update.png" class="quarto-figure quarto-figure-center" style="width:45.0%"></a></p>
</figure>
</div>
</div></div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️ Intuición</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Este cálculo se denomina un Exponential Moving Average de los Gradientes. Y se puede interpretar como una especie de velocidad del gradiente. Su objetivo es ponderar con un cierto porcentaje el gradiente actual y el gradiente anterior.</li>
<li><span class="math inline">\(v_{0} = 0\)</span></li>
</ul>
</div>
</div>
</div>
<p><span class="math display">\[\begin{align} v_{t+1}&amp;=(1-\beta)\nabla_\theta L(\theta_{t}) + \beta v_t \\
v_{t+1}&amp;=(1-\beta)\nabla_\theta L(\theta_{t}) + \beta \left[(1-\beta) \nabla_\theta L(\theta_{t-1}) + \beta v_{t-1}\right] \\
v_{t+1}&amp;=(1-\beta)\nabla_\theta L(\theta_{t}) + \beta (1-\beta) \nabla_\theta L(\theta_{t-1}) + \beta^2 (1-\beta) \nabla_\theta L(\theta_{t-2})... \\
\end{align}\]</span></p>
</section>
<section id="sgd-with-nesterov-momentum" class="slide level2 smaller">
<h2>SGD with Nesterov Momentum</h2>
<div class="columns">
<div class="column">
<p><span class="math display">\[\theta_{t+1} = \theta_t - \alpha u_{t + 1}\]</span> <span class="math display">\[v_{t + 1} = \beta v_t + (1-\beta) \nabla_\theta f(\theta_{t+1} + \beta v_t)\]</span></p>
<p>donde <span class="math inline">\(0&lt;\beta&lt;1\)</span>, pero normalmente <span class="math inline">\(\beta=0.9\)</span>.</p>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/nesterov_update.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img data-src="img/clase-3/nesterov_update.png" class="quarto-figure quarto-figure-center" style="width:50.0%"></a></p>
</figure>
</div>
</div></div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️ Intuición</strong></p>
</div>
<div class="callout-content">
<p>El método de Nesterov “mira hacia adelante” en la dirección del momentum antes de calcular el gradiente, lo que le da una corrección más precisa y evita en parte el sobrepaso de mínimos. En este caso <span class="math inline">\(\theta_{t+1} + \beta v_t\)</span> es el punto “futuro” para calcular el gradiente.</p>
</div>
</div>
</div>
</section>
<section id="efecto-del-momentum-en-el-update-rule" class="slide level2 smaller">
<h2>Efecto del Momentum en el Update Rule</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-3/momentum_effect.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img data-src="img/clase-3/momentum_effect.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️ Intuición</strong></p>
</div>
<div class="callout-content">
<ul>
<li>El SGD tiende a ser más oscilante.</li>
<li>El SGD con Momentum tiende a ser más suave y rápido debido a la inercia recibida por el término de momentum.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="métodos-adaptativos-adagrad" class="slide level2 smaller">
<h2>Métodos Adaptativos: Adagrad</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️ Intuición</strong></p>
</div>
<div class="callout-content">
<p>¿Qué tal, si el learning rate se va adaptando en el tiempo y deja de ser estática?</p>
</div>
</div>
</div>
<p><span class="math display">\[r_{t+1} = r_t + \nabla_\theta f(\theta_t)^2\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{r_{t+1}}}\nabla_\theta f(\theta_t)\]</span></p>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Efecto</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Parámetros con gradientes grandes <span class="math inline">\(\rightarrow\)</span> tasa de aprendizaje disminuye más rápido.</li>
<li>Parámetros con gradientes pequeños <span class="math inline">\(\rightarrow\)</span> tasa de aprendizaje se mantiene más alta.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Util cuando hay parámetros que se actualizan con distinta frecuencia.</li>
<li>Acelera la convergencia en direcciones poco exploradas.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Contras</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Como el denominador acumula gradientes al cuadrado, la tasa de aprendizaje puede llegar a volverse muy pequeña <span class="math inline">\(\rightarrow\)</span> el entrenamiento se <strong><em>“frena”</em></strong> antes de llegar al óptimo.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="métodos-adaptativos-rmsprop" class="slide level2 smaller">
<h2>Métodos Adaptativos: RMSProp</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️ Intuición</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Normalizar por el Exponential Moving Average de los Gradientes al cuadrado para controlar el efecto de reducción del learning rate.</li>
</ul>
</div>
</div>
</div>
<p><span class="math display">\[s_{t+1} = \beta r_t + (1-\beta) \nabla_\theta f(\theta_t)^2\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{s_{t+1}}}\nabla_\theta f(\theta_t)\]</span></p>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Normalización adaptativa: cada parámetro tiene su propia tasa de aprendizaje ajustada dinámicamente.</li>
<li>A diferencia de Adagrad, el denominador no crece indefinidamente porque el promedio exponencial “olvida” gradientes antiguos. Esto permite seguir aprendiendo incluso después de muchos pasos.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Contras</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Depende mucho de la elección de su hiperparámetro <span class="math inline">\(\beta\)</span></li>
</ul>
</div>
</div>
</div>
</section>
<section id="métodos-adaptativos-adam" class="slide level2 smaller">
<h2>Métodos Adaptativos: Adam</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️ Intuición</strong></p>
</div>
<div class="callout-content">
<p>Se mantiene el Exponential Moving average para: Los gradientes (como utilizando momentum), los gradientes al cuadrado (como RMSprop).</p>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<p><span class="math display">\[v_{t+1} = \beta_1 v_t + (1-\beta_1) \nabla_\theta f(\theta_t)\]</span> <span class="math display">\[s_{t+1} = \beta_2 s_t + (1-\beta_2) \nabla_\theta f(\theta_t)^2\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{s'_{t+1}}} v'_{t+1}\]</span></p>
</div><div class="column">
<h5 id="correcciones-iniciales">Correcciones Iniciales</h5>
<p><span class="math display">\[v'_{t+1} = \frac{v_{t+1}}{1-\beta_1^{t+1}}\]</span> <span class="math display">\[s'_{t+1} = \frac{s_{t+1}}{1-\beta_2^{t+1}}\]</span></p>
</div></div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Combina momentum + RMSprop + corrección <span class="math inline">\(\rightarrow\)</span> rápido, estable.</li>
<li>Es por lejos el optimizador más usado.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Contras</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Sensible a la elección de sus hiperparámetros <span class="math inline">\(\beta_1\)</span> y <span class="math inline">\(\beta_2\)</span>. Pytorch utiliza 0.9 y 0.999 como valores de <span class="math inline">\(\beta_1\)</span> y <span class="math inline">\(\beta_2\)</span> respectivamente.</li>
</ul>
</div>
</div>
</div>
</section></section>
<section id="eso-es-todo-amigos" class="title-slide slide level1 center">
<h1>👊 Eso es todo amigos</h1>
<div class="footer">
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
</p><p><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0</a></p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">
<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a>
<p></p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../logo-uai-blanco.jpeg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': true,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1366,

        height: 768,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/datacubeR\.github\.io\/clases_UAI\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>