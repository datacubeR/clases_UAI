{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "SEED = 10\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[37,  5, 32, 67, 32,  5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 100, (1, 6))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8799, -0.8493,  3.3999,  1.4201],\n",
       "         [-0.1888,  0.1051,  0.4773, -3.1130],\n",
       "         [ 1.2626,  1.2161, -2.1373, -4.4780],\n",
       "         [-1.1958,  3.4485, -0.8264, -0.4976],\n",
       "         [ 1.2626,  1.2161, -2.1373, -4.4780],\n",
       "         [-0.1888,  0.1051,  0.4773, -3.1130]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "embedding_encoder = InputEmbeddings(d_model=4, vocab_size=100)\n",
    "output = embedding_encoder(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ## (L,d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        ## (L, 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float)\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # (N, L, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        ## Register Buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## x = x + self.pe[:, : x.shape[1], :].requires_grad_(False)\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0888,  0.1674,  3.7777,  0.0000],\n",
       "         [ 0.7252,  0.7172,  0.5414, -2.3479],\n",
       "         [ 0.0000,  0.8888, -0.0000, -3.8646],\n",
       "         [-1.1719,  2.7317, -0.8849,  0.5577],\n",
       "         [ 0.5620,  0.6249, -2.3304, -3.8653],\n",
       "         [-1.2753,  0.4320,  0.5858, -2.3492]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_encoder = PositionalEncoding(d_model=4, seq_len=6, dropout=0.1)\n",
    "output_pe = pe_encoder(output)\n",
    "output_pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0834, -0.5228,  0.0203, -0.0140],\n",
       "         [ 0.0389, -0.4723, -0.1963, -0.0514],\n",
       "         [ 0.1011, -0.5052, -0.5318, -0.1051],\n",
       "         [-0.0349, -0.4584, -0.4303, -0.1006],\n",
       "         [ 0.1187, -0.5545, -0.5845, -0.1033],\n",
       "         [ 0.0778, -0.5032, -0.2475, -0.0361]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        ## Tensores empaquetados\n",
    "        self.W_q = nn.Linear(d_model, self.d_k * h)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k * h)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k * h)\n",
    "\n",
    "        self.w_o = nn.Linear(self.d_k * h, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_dot_prod(Q, K, V, mask=None, dropout=None):\n",
    "        d_k = Q.shape[-1]\n",
    "\n",
    "        # (N, h, L, L)\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        ## (N, h, L, d_v)\n",
    "        return attention_scores @ V, attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        ## (N, L, d_k*h)\n",
    "        Q = self.W_q(q)\n",
    "        K = self.W_k(k)\n",
    "        ## (N, L, d_v*h)\n",
    "        V = self.W_v(v)\n",
    "\n",
    "        # (N, L, h, d_k) --> (N,h,L,d_k)\n",
    "        Q = Q.view(q.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # (N, L, h, d_k) --> (N,h,L,d_k)\n",
    "        K = K.view(k.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # (N, L, h, d_k) --> (N,h,L,d_k)\n",
    "        V = V.view(v.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_scores = self.scale_dot_prod(\n",
    "            Q, K, V, mask, self.dropout\n",
    "        )\n",
    "        x = x.transpose(1, 2).reshape(q.shape[0], -1, self.h * self.d_k)\n",
    "        return self.w_o(x)\n",
    "\n",
    "\n",
    "mh_attention = MultiHeadAttentionBlock(d_model=4, h=2, dropout=0.1)\n",
    "output_mh = mh_attention(output, output, output)\n",
    "output_mh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add&Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9388, -0.2235,  1.6751, -0.5128],\n",
       "         [ 0.5153,  0.6049,  0.6106, -1.7308],\n",
       "         [ 0.5091,  0.6443,  0.5766, -1.7301],\n",
       "         [-1.0167,  1.4599, -0.8285,  0.3853],\n",
       "         [ 1.0495,  0.7769, -0.3532, -1.4732],\n",
       "         [-0.1209,  0.5691,  1.1103, -1.5585]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        ## Multiplicative\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        ## Additive\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        return (x - mean) / torch.sqrt(\n",
    "            var + self.eps\n",
    "        ) * self.alpha + self.bias\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        output = sublayer(x)\n",
    "        output = self.dropout(output)\n",
    "        return self.norm(x + output)\n",
    "\n",
    "\n",
    "residual_mh = ResidualConnection(dropout=0.1)\n",
    "mh_attention = MultiHeadAttentionBlock(d_model=4, h=2, dropout=0.1)\n",
    "output = residual_mh(output_pe, lambda x: mh_attention(x, x, x))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2459,  0.4804,  1.3635, -0.5981],\n",
       "         [ 0.1829,  0.9943,  0.4803, -1.6575],\n",
       "         [ 0.1813,  1.0141,  0.4568, -1.6523],\n",
       "         [-1.3174,  1.4358, -0.3721,  0.2538],\n",
       "         [ 0.8782,  0.8883, -0.2200, -1.5464],\n",
       "         [-0.4689,  1.0912,  0.7833, -1.4056]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "residual_ffn = ResidualConnection(dropout=0.1)\n",
    "ffn = FeedForward(d_model=4, d_ff=8)\n",
    "output = residual_ffn(output, ffn)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4794, -0.3560,  0.9296,  0.9059],\n",
       "         [ 0.9185,  0.6873,  0.0322, -1.6380],\n",
       "         [ 0.9489,  0.8417, -0.2676, -1.5230],\n",
       "         [-0.6131,  1.1944, -1.2978,  0.7165],\n",
       "         [ 1.2573,  0.6877, -0.8072, -1.1378],\n",
       "         [-0.3268,  1.6067, -0.1436, -1.1363]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, h, dropout):\n",
    "        super().__init__()\n",
    "        self.mh_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.residuals = nn.ModuleDict(\n",
    "            dict(\n",
    "                mh=ResidualConnection(dropout),\n",
    "                ffn=ResidualConnection(dropout),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residuals[\"mh\"](x, lambda x: self.mh_attention(x, x, x))\n",
    "        x = self.residuals[\"ffn\"](x, self.ffn)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder = EncoderBlock(d_model=4, d_ff=8, h=2, dropout=0.1)\n",
    "encoder(output_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1471, -0.7726,  1.3095,  0.6102],\n",
       "         [ 0.8043, -1.5599,  0.9410, -0.1854],\n",
       "         [ 0.7972, -1.3047,  1.1294, -0.6219],\n",
       "         [-1.2670,  0.3383, -0.4971,  1.4258],\n",
       "         [ 1.5233,  0.1592, -1.1868, -0.4956],\n",
       "         [-0.3347, -1.1785,  1.5795, -0.0662]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, N, d_model, d_ff, h, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.h = h\n",
    "        self.dropout = dropout\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(self.d_model, self.d_ff, self.h, self.dropout)\n",
    "                for _ in range(N)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "transformer_encoder = TransformerEncoder(\n",
    "    N=6, d_model=4, d_ff=8, h=2, dropout=0.1\n",
    ")\n",
    "encoder_output = transformer_encoder(output_pe)\n",
    "encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4902, -0.1060, -0.2673,  0.9572],\n",
       "         [ 0.0000, -1.3015,  3.5659, -0.9811],\n",
       "         [ 1.9236, -4.7471,  1.0052, -0.0000],\n",
       "         [-1.0813,  1.5492, -0.6965,  2.6818],\n",
       "         [ 0.9349,  2.8193,  0.6835,  5.1822],\n",
       "         [-0.1522, -3.9696,  1.0385, -1.7455]],\n",
       "\n",
       "        [[-0.0000, -1.7481, -0.6793,  2.1605],\n",
       "         [-1.4601, -2.3530, -0.7971,  0.4837],\n",
       "         [-1.2516,  0.9073, -2.5760, -1.4898],\n",
       "         [ 3.9898, -2.8120, -0.0000, -0.0000],\n",
       "         [-2.0904,  3.8685,  0.0390,  1.1420],\n",
       "         [-2.1899, -2.4715,  0.2579,  1.6673]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_decoder = torch.randint(0, 200, (2, 6))\n",
    "embedding_decoder = InputEmbeddings(d_model=4, vocab_size=200)\n",
    "output_decoder = embedding_decoder(x_decoder)\n",
    "\n",
    "pe_decoder = PositionalEncoding(d_model=4, seq_len=6, dropout=0.1)\n",
    "output_pe_decoder = pe_decoder(output_decoder)\n",
    "output_pe_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2385,  1.3848, -0.2166, -1.4066],\n",
       "         [-0.2838,  0.8749,  0.9249, -1.5160],\n",
       "         [ 1.2895, -0.1632,  0.3523, -1.4786],\n",
       "         [-0.9994,  1.6398, -0.5515, -0.0889],\n",
       "         [-0.1921,  1.5629, -0.1464, -1.2244],\n",
       "         [ 1.0752, -0.8200,  0.9065, -1.1617]],\n",
       "\n",
       "        [[ 0.1356,  0.0096, -1.4816,  1.3365],\n",
       "         [-1.2769,  0.2926,  1.4407, -0.4565],\n",
       "         [-0.6868,  1.6583, -0.0942, -0.8773],\n",
       "         [ 0.7595,  1.1732, -0.6672, -1.2655],\n",
       "         [-0.8131,  1.6748, -0.1457, -0.7160],\n",
       "         [-1.2560,  0.6754,  1.2384, -0.6578]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, h, dropout):\n",
    "        super().__init__()\n",
    "        self.causal_mh_attention = MultiHeadAttentionBlock(\n",
    "            d_model, h, dropout\n",
    "        )\n",
    "        self.cross_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.residuals = nn.ModuleDict(\n",
    "            dict(\n",
    "                causal=ResidualConnection(dropout),\n",
    "                cross=ResidualConnection(dropout),\n",
    "                ffn=ResidualConnection(dropout),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.residuals[\"causal\"](\n",
    "            x, lambda x: self.causal_mh_attention(x, x, x, tgt_mask)\n",
    "        )\n",
    "        x = self.residuals[\"cross\"](\n",
    "            x,\n",
    "            lambda x: self.cross_attention(\n",
    "                x, encoder_output, encoder_output, src_mask\n",
    "            ),\n",
    "        )\n",
    "        x = self.residuals[\"ffn\"](x, self.ffn)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, N, d_model, d_ff, h, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = N\n",
    "        self.decoders = nn.ModuleList(\n",
    "            [DecoderBlock(d_model, d_ff, h, dropout) for _ in range(N)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "transformer_decoder = TransformerDecoder(\n",
    "    N=6, d_model=4, d_ff=8, h=2, dropout=0.1\n",
    ")\n",
    "decoder_output = transformer_decoder(output_pe_decoder, encoder_output)\n",
    "decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 200])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_proj(x)\n",
    "        ## En general esto no se hace ya que el Loss puede incluir el Softmax\n",
    "        ## (batch, vocab_size)\n",
    "        return x.softmax(dim=-1)\n",
    "\n",
    "\n",
    "proj_layer = ProjectionLayer(d_model=4, vocab_size=200)\n",
    "logits = proj_layer(decoder_output)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 21, 188,  33, 180,  21,  27],\n",
       "        [  9,  80, 188,  33, 188,  80]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Secuencias Predichas\n",
    "torch.argmax(logits, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
