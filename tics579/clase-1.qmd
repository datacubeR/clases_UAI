---
title: "TICS-579-Deep Learning"
subtitle: "Clase 1: Preliminares"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

# Introducci√≥n al Curso

## ¬øPor qu√© estudiar Deep Learning? {.smaller}

> Principalmente porque es el Estado del Arte en las aplicaciones m√°s Impresionantes en la Inteligencia Artificial.

:::: {.columns}
::: {.column width="25%" .fragment}

#### Alexnet (2012)
![](img/clase-1/alexnet.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### AlphaGo (2016)
![](img/clase-1/alphago.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### Transformers (2017)
![](img/clase-1/transformer.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### GPT (2019)
![](img/clase-1/gpt.png){.lightbox}
:::
::::


## ¬øPor qu√© estudiar Deep Learning? {.smaller}

> Principalmente porque es el Estado del Arte en las aplicaciones m√°s Impresionantes en la Inteligencia Artificial.

:::: {.columns}
::: {.column width="25%" .fragment}

#### GPT-3 (2021)
![](img/clase-1/gpt3.jpeg){.lightbox}
:::
::: {.column width="25%" .fragment}
#### AlphaFold (2021)
![](img/clase-1/alphafold.jpg){.lightbox}
:::
::: {.column width="25%" .fragment}
#### Stable Diffussion/Dalle (2022)
![](img/clase-1/stable-diffusion-3.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### LLMs (2023) (ChatGPT/Llama)
![](img/clase-1/llms.jpg){.lightbox}
:::
::::

## ¬øPor qu√© estudiar Deep Learning? {.smaller}

![Im√°gen tomada de la Clase de Zico Colter](img/clase-1/google_trends.png){.lightbox width="70%" fig-align="center"}

## ¬øPor qu√© estudiar Deep Learning? {.smaller}

::: {.callout-tip appearance="default"}
## Facilidad y Autograd
* Frameworks como Tensorflow, Pytorch o Jax permiten realizar esto de manera mucho m√°s sencilla.
    * Frameworks permiten calcular gradientes de manera autom√°tica.
    * Antigua mente trabajar en Torch, Caffe o Theano pod√≠a tomar cerca de 50K l√≠neas de c√≥digo.
:::

::: {.callout-note appearance="default"}
## C√≥mputo
* Proliferaci√≥n de las GPUs, TPUs, HPUs, IPUs, como sistemas masivos de C√≥mputos. 
    * [How many computers to identify a cat? 16,000](https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html)
:::

::: {.callout-important appearance="default"}
## Estado del Arte
* Modelos de Deep Learning pueden generar sistemas que entiendan im√°genes, textos, audios, videos, grafos, etc.
:::

## El nacimiento de las Redes Neuronales {.smaller}

> Las redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad m√°s b√°sica es una Neurona. 


![](img/clase-1/neuron_1.png){.lightbox fig-align="center"}

## El nacimiento de las Redes Neuronales {.smaller}

> Las redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad m√°s b√°sica es una Neurona. 


::: {.columns}
::: {.column width="60%"}
![](img/clase-1/neuron_2.png){.lightbox fig-align="center" width="70%"}


::: {.callout-note} 
Este tipo de nomenclatura est√° sumamente pasada de moda.
:::
:::
::: {.column width="40%" .fragment}



* Este c√°lculo se puede representar como: 

$$ y = \phi(w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_5 \cdot x_5)$$
$$ y = \phi(w^T \cdot x)$$

donde $w = [w_1, w_2, w_3, w_4, w_5]$ y $x = [x_1, x_2, x_3, x_4, x_5]$.

::: {.callout-warning .fragment .incremental}
* ¬øQu√© pasa si $\phi(.)$ vale la funci√≥n ***identidad***?
* Tenemos una **Regresi√≥n Lineal**.
:::


::: {.callout-warning .fragment .incremental}
* ¬øQu√© pasa si $\phi(.)$ vale la funci√≥n ***sigmoide***?
* Tenemos una **Regresi√≥n Log√≠stica**.
:::
:::
::: 

## Arquitectura de una Red {.smaller}


::: {.columns}
::: {.column}
![](img/clase-1/nn_arq.png){.lightbox fig-align="center"} 
:::
::: {.column}
### Estructura m√°s com√∫n 
###### *(Probablemente tampoco seguiremos esta nomenclatura)*

* Nodos o Neuronas
* Edges o Conexiones
* Capas

::: {.callout-caution .fragment style="font-size:150%;"}
***¬øCu√°ntas capas tiene esta red?***
:::

::: {.callout-tip .fragment style="font-size:150%;"}
***Depende***
:::
:::
::: 

* Normalmente todas las neuronas de una capa anterior se conectan con las de una capa posterior (Hay excepciones). 
* Dependiendo de la forma en la que se conecten, cada **Arquitectura** recibe un nombre.

# Intuici√≥n y conceptos iniciales

## Los Ingredientes de un Algoritmo de Aprendizaje {.smaller}

Hip√≥tesis
: > Una funci√≥n que describe como mapear inputs (features) con outputs (labels) por medio de par√°metros.  

Loss Function
: > Una funci√≥n que especifica cuanta informaci√≥n se pierde. Mayor p√©rdida implica m√°s error de estimaci√≥n.

M√©todo de Optimizaci√≥n
: > Es el responsable de combinar la `hip√≥tesis` y la `loss function`. Corresponde a un procedimiento para determinar los par√°metros de la hip√≥tesis, minimizando la suma de las p√©rdidas en un set de entrenamiento. 

## Ejemplo: Softmax Regression 

Softmax Regression
: > Corresponde la versi√≥n multiclase de una Regresi√≥n Log√≠stica. Tambi√©n se le llama una `Shallow Network`.


::: {.columns}
::: {.column width="50%" style="font-size: 80%;"}
::: {.callout-tip}
#### Consideremos un problema de clasificaci√≥n multiclase de $k$ clases tal que:

* Datos de Entrenamiento: $x^{(i)}, y^{(i)} \in {1,...,k}$ para $i=1,...,m$.
    * $n$: Es el n√∫mero de Features.
    * $m$: Es el n√∫mero de puntos en el training set. 
    * $k$: Es el n√∫mero de clases del problema.
:::

::: {.callout-important}
Vamos a tener en total $n \times k$ par√°metros o pesos que actualizar.
:::
:::
::: {.column width="50%"}
![](img/clase-1/softmax_reg.png){.lightbox fig-align="center"} 
:::
::: 



## Softmax Regression: Hip√≥tesis

::: {style="font-size: 80%;"}
Vamos a definir una funci√≥n que mapea valores de $x \in \mathbb{R}$ a vectores de $k$ dimensiones. 
:::
$$ h: \mathbb{R}^n \rightarrow \mathbb{R}^k$$
$$ x \rightarrow h_\theta(x) = \theta^T x$$

::: {style="font-size: 80%;"}
donde $\theta \in \mathbb{R}^{n \times k}$ y $x \in \mathbb{R}^{n\times 1}$
:::

::: {.callout-warning}
En este caso usamos una `hip√≥tesis lineal`, ya que se usa una multiplicaci√≥n matricial (o producto punto) para relacionar $\theta$ y $x$. 
:::

::: {.callout-note}
En este caso el output de $h_i(x)$ devolver√° la probabilidad de pertenecer a una cierta clase $i$.   
:::

::: {.callout-important .fragment}
***¬øCu√°l es el tama√±o/dimensi√≥n de $h_\theta(x)$?***
:::

## Notaci√≥n Matricial {.smaller}

> Una manera m√°s conveniente de escribir estas operaciones es utilizar ***(Matrix Batch Form)***. 

::: {.columns}
::: {.column}
##### Design Matrix

$$X \in \mathbb{R}^{m \times n} = \begin{bmatrix}
&-x^{(1)T}-\\
& \vdots & \\
&-x^{(m)T}- &\\
\end{bmatrix}$$
:::
::: {.column}
##### Labels Vector
$$y \in {1,...,k} = \begin{bmatrix}
&-y^{(1)}-\\
& \vdots & \\
&-y^{(m)}- &\\
\end{bmatrix}$$
:::
::: 

La hip√≥tesis tambi√©n se puede reescribir de manera matricial como: 

::: {.columns}
::: {.column}
$$h_\theta(X) = \begin{bmatrix}
&-h_\theta(x^{(1)})^T-\\
& \vdots & \\
&-h_\theta(x^{(m)})^T-\\
\end{bmatrix}$$
:::
::: {.column}
$$h_\theta(X)= \begin{bmatrix}
&-x^{(1)T} \theta-\\
& \vdots & \\
&-x^{(m)T} \theta-\\
\end{bmatrix} = X  \theta$$
:::
::: 

::: {.callout-important .fragment}
Normalmente este tipo de operaciones son las que utilizaremos para hacer nuestro c√≥digo.
:::

## Loss Function: Softmax/Cross-Entropy Loss {.smaller}


::: {.callout-warning style="font-size: 130%;"}
La salida de nuestra `Shallow Network` retornar√° valores reales.
:::
::: {.callout-tip style="font-size: 130%;"}
Para poder tener una mejor interpretaci√≥n del significado de cada una aplicaremos la funci√≥n `Softmax` lo cual permitir√° *normalizar* los resultados y llevar√° los resultados a una ***"distribuci√≥n de probabilidad"*** (valores positivos que sumen 1).
:::

::: {.columns}
::: {.column width="60%"}


![](img/clase-1/softmax_example.png){.lightbox fig-align="center"} 
:::
::: {.column width="40%"}


Formalmente definiremos la funci√≥n Softmax como: 

$$s_i = p(label = i) = \frac{exp(h_i(x))}{\sum_{j=1}^k exp(h_j(x))}$$


$$s = \begin{bmatrix}
&s_1&\\
& \vdots & \\
&s_k&\\
\end{bmatrix}$$
:::
::: 

## Loss Function: Softmax/Cross-Entropy Loss {.smaller}

Para medir el error/p√©rdida de informaci√≥n utilizaremos el `Negative Log Loss` o `Cross Entropy Loss`.

$$l_{ce}(h(x), y) = -log\left(p(label = y)\right)$$

::: {.callout-tip style="font-size: 120%;" .fragment}
Para garantizar el √©xito de nuestro modelo, b√°sicamente queremos maximizar la probabilidad de encontrar la etiqueta correcta, es decir, que $p(label = y)$ sea lo m√°s alto posible.
:::
::: {.callout-caution style="font-size: 120%;" .fragment}
Normalmente en los problemas de optimizaci√≥n no se suele maximizar sino minimizar. Minimizar el valor negativo es equivalente a maximizar. Esto ser√≠a equivalente a minimizar el error del modelo. 
:::
::: {.callout-warning style="font-size: 120%;" .fragment}
Finalmente por razones de estabilidad num√©rica, minimizamos el logaritmo de la probabilidad que es una t√©cnica bien conocida en Estad√≠stica.
:::

:::{.fragment}
$$\begin{align}
l_{ce}(h(x), y) = -log\left(p(label = y)\right) &= -log \left(\frac{exp(h_{(i = y)}(x))}{\sum_{j=1}^k exp(h_j(x))}\right) \\
&= - h_{(i=y)}(x) + log\left(\sum_{j = 1}^k exp(h_j(x))\right)\end{align}$$
:::

## M√©todo de Optimizaci√≥n {.smaller}

> El √∫ltimo ingrediente de un algoritmo de aprendizaje es el m√©todo de optimizaci√≥n. Es necesario minimizar la p√©rdida promedio asociada a todos los puntos de un cierto set de entrenamiento. Para ello definimos esto formalmente como:

$$\underset{\theta}{minimize} = \frac{1}{m} \sum_{i=1}^m l_{ce}(h_\theta(x^{(i)}), y^{(i)})$$


::: {.callout-note}
***¬øC√≥mo encontramos los par√°metros $\theta$ que minimizan la p√©rdida de informaci√≥n/error de estimaci√≥n?***
:::

Gradient Descent
: > Es un m√©todo num√©rico que permite minimizar funciones movi√©ndose en direcci√≥n contraria al Gradiente. Es computacionalmente muy eficiente y f√°cil de implementar en c√≥digo.

## Gradient Descent {.smaller}

::: {.columns}
::: {.column width="60%"}
Se define el gradiente como la matriz que contiene las derivadas parciales de una funci√≥n $f$. Se denota como:

$$\nabla_\theta f(\theta) \in \mathbb{R}^{n \times k} =  \begin{bmatrix}
\frac{\partial f(\theta)}{\partial \theta_{11}} & \cdots & \frac{\partial f(\theta)}{\partial \theta_{1k}} \\
\cdots & \ddots & \cdots \\
\frac{\partial f(\theta)}{\partial \theta_{n1}} & \cdots & \frac{\partial f(\theta)}{\partial \theta_{nk}}
\end{bmatrix}$$

::: {.callout-tip}
$\theta_{ij}$ corresponde al par√°metro que une el nodo/feature $i$ con el nodo/predicci√≥n $j$.
:::
:::
::: {.column width="40%"}
![](img/clase-1/gradient.png){.lightbox fig-align="center" } 
:::
::: 


::: {.callout-tip style="font-size: 130%;"}
El gradiente apunta a la direcci√≥n de m√°ximo crecimiento de la funci√≥n $f$. 
:::


## Gradient Descent: Regla de Actualizaci√≥n {.smaller}
Para minimizar la funci√≥n, la idea es descender iterativamente por el trayecto **en contra** del gradiente. La regla de actualizaci√≥n se define como:

$$\theta := \theta - \alpha \nabla_\theta f(\theta) = \theta - \frac{\alpha}{m}\nabla_\theta l_{ce}(X\theta,y)$$

con $\theta \in \mathbb{R}^{n \times k}$ y $\alpha > 0$ corresponde al *step size* o `learning rate`.


![](img/clase-1/lr_effect.png){.lightbox fig-align="center" width="60%"} 

::: {.callout-tip}
En nuestro caso $f$ corresponder√° a nuestro $l_{ce}$ calculado anteriormente. El problema es, ¬øcu√°nto vale el gradiente del `Cross Entropy Loss`?
:::

## Calculando el Gradiente a mano {.smaller}


::: {style="font-size: 130%;"}
Simplifiquemos el problema a calcular para un s√≥lo vector $x$.

$$\theta := \theta - \alpha \nabla_\theta l_{ce}(\theta^Tx,y) $$
:::

::: {.callout-warning style="font-size: 120%;"}
¬øCu√°nto vale el Gradiente?

* No es tan sencillo, ya que derivamos respecto a $\theta$ que es una matriz. 
* Pero derivamos a $\theta^T x$ que es un vector.
* Para ello, lo correcto es utilizar Calculo Diferencial Matricial, Jacobianos y Productos de Kroenecker (que probablemente no han visto en ning√∫n curso).
  * **SPOILER**: Yo tampoco lo he visto en ning√∫n curso.
:::

::: {.columns .fragment}
::: {.column width="70%"}
::: {.callout-tip style="font-size: 120%;"}
* Usaremos un truco (sumamente hacky üò±) que jam√°s deben revelar y que avergonzar√≠a a cualquier profesor de C√°lculo.
    * Pretenderemos que todos los valores son escalares y corregiremos las dimensiones al final.

:::
:::
::: {.column width="30%"}
![](img/clase-1/fuenzi.jpeg){.lightbox fig-align="center" width="40%"} 

:::
::: 

## Calculando el Gradiente a mano {.smaller}

> Simplifiquemos el problema pensando que calcularemos el Gradiente para un s√≥lo vector $x$.

>  Es decir, $x \in \mathbb{R}^{n\times1}$.

Adem√°s sabemos que $\nabla_\theta l_{ce}(\theta^Tx, y)$ debe tener dimensiones $n \times k$.

::: {.callout-important style="font-size: 150%;" .fragment fragment-index=1}
***¬øPor qu√©?***
:::

::: {.columns}
::: {.column .fragment fragment-index=2}
$$\nabla_\theta l_{ce}(\theta^T x,y) = \frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} \cdot \frac{\partial \theta^Tx}{\partial \theta}$$
:::
::: {.column .fragment fragment-index=3}
$$\frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} = \frac{\partial l_{ce}(h_\theta(x), y)}{\partial h_\theta(x)} = \begin{bmatrix}
\frac{\partial l_{ce}(h,y)}{\partial h_1} \\
\vdots\\
\frac{\partial l_{ce}(h,y)}{\partial h_k} \\
\end{bmatrix}$$
:::
::: 

::: {.callout-tip style="font-size: 130%;" .fragment fragment-index=4}
Luego el gradiente de $l_{ce}$ respecto a $h$ tiene dimensiones $k \times 1$.
:::

## Calculando el Gradiente a mano {.smaller}

$$\begin{align}
\frac{\partial l_{ce}(h,y)}{\partial h_i} &= \frac{\partial }{\partial h_i}\left(-h_{(i = y)} + log \sum_{j = 1}^k exp(h_j)\right) \\
&= -\frac{\partial h_{(i = y)}}{\partial h_i}+ \frac{1}{\sum_{j = 1}^k exp(h_j)} \cdot \frac{\partial}{\partial h_i}\left(\sum_{j=1}^k exp(h_j)\right) \\
&= -\frac{\partial h_{(i = y)}}{\partial h_i}+ \frac{exp(h_i)}{\sum_{j = 1}^k exp(h_j)} \\
&= - 1\{i=y\} + s_i = s_i - 1\{i=y\}
\end{align}
$$

::: {.callout-tip .fragment}
$$1\{i = y\} = \begin{cases}
1,  & \text{i = y} \\
0, & \text{otherwise}
\end{cases}
$$
:::

::: {.fragment}
Finalmente en forma vectorial quedar√≠a como:

::: {.columns}
::: {.column}
$$\frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} = s - e_y$$
:::
::: {.column}
::: {.callout-tip}
Donde $z$, es el vector de Softmax y $e_y$ es un vector con un 1 en la posici√≥n $y$ y 0 en el resto.
:::
:::
::: 
:::

## Calculando el Gradiente a mano {.smaller}

::: {.columns}
::: {.column }
$$\nabla_\theta l_{ce}(\theta^T x,y) = \frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} \cdot \frac{\partial \theta^Tx}{\partial \theta}$$
$$\nabla_\theta l_{ce}(\theta^T x,y) = (s-e_y)\cdot x $$
:::

::: {.column .fragment}
::: {.callout-caution appearance="default"}
## Ojo con las dimensiones
* $s-e_y \in \mathbb{R}^{k \times 1}$
* $x \in \mathbb{R}^{n \times 1}$
:::

::: 
:::

:::{.fragment}
Luego: 

$$\nabla_\theta l_{ce}(\theta^T x,y) = x (s-e_y)^T$$
:::

::: {.callout-caution style="font-size: 150%;" .fragment}
***¬øCu√°l es el tama√±o de $\nabla_\theta l_{ce}(\theta^T x,y)$?***
:::

::: {.callout-note style="font-size: 150%;" .fragment}
$n \times k$
:::

::: {.callout-warning style="font-size: 150%;" .fragment}
***¬øPor qu√©?***
:::



## Calculando el Gradiente Matrix Batch Form {.smaller}

#### Esto ser√≠a equivalente a tomar en consideraci√≥n todos los puntos del Training Set

::: {.columns}
::: {.column}
$$\begin{align}\nabla_\theta l_{ce}(X\theta,y) &= \frac{\partial l_{ce}(X\theta,y)}{\partial X\theta} \cdot \frac{\partial X\theta}{\partial \theta}\\
&= (S - I_y) \cdot X \\
&= X^T \cdot (S - I_y)
\end{align}$$

::: {.callout-tip}
* $S$ corresponde al Softmax de $X\theta$ aplicado por filas.
* $I_y$ corresponde al One Hot Encoder de las etiquetas. Filas con 1 en la etiqueta correcta y 0 en el resto.
:::
:::
::: {.column}
::: {.callout-caution appearance="default" .fragment}
## Ojo con las dimensiones

* $S - I_y \in \mathbb{R}^{m \times k}$
* $X \in \mathbb{R}^{m \times n}$
:::

::: {.callout-warning .fragment}
***¬øCu√°l es el tama√±o de $\nabla_\theta l_{ce}(X\theta,y)$?***
:::
:::
::: 



:::{.fragment}
Finalmente la `Regla de Actualizaci√≥n` de par√°metros usando Gradient Descent queda como:

$$\theta := \theta - \frac{\alpha}{m} X^T (S - I_y)$$
:::


## Conclusiones {.smaller}


::: {.columns}
::: {.column}

::: {.callout-tip}
* Acabamos de entrenar una Shallow Network, sin definir ning√∫n concepto Fancy que es propio del √°rea.
* No hemos hablado ni de:
  * `Forward Pass`
  * `Epochs`
  * `Backpropagation`
  * `Adam`
  * `Activation Functions`
  * etc.

::: 
::: {.callout-note .fragment fragment-index=1}
* Aplicando esta simple regla se puede obtener cerca de un 8% de error clasificando d√≠gitos en MNIST.
* Se puede programar en pocas l√≠neas en Python.

![](img/clase-1/mnist.png){.lightbox fig-align="center" width="30%"} 
:::
:::

::: {.column .fragment fragment-index=2}
#### Pero, ¬øqu√© pasa con arquitecturas m√°s complejas?

![](img/clase-1/nn_arq_full.png){.lightbox fig-align="center" width="60%"} 
:::
::: 


# ¬°¬°Eso es todo!!

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::

# Anexos

## Multiplicaci√≥n Matricial {.smaller}

::: {.columns}
::: {.column}
![](img/clase-1/mat_mat_mul.png){.lightbox fig-align="center"} 
:::
::: {.column}
* Donde $B_{*,i}$ corresponde a la columna $i$ de B.
* Donde $A_{i,*}$ corresponde a la fila $i$ de A.
:::
::: 