---
title: "TICS-579-Deep Learning"
subtitle: "Clase 1: Tensores"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs: 
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

# Introducci贸n al Curso

## Historia {.smaller}

La verdad podr铆amos estudiar historia e importancia de porqu茅 el Deep Learning es importante, pero la verdad...

::: {.callout-important}
NO TENEMOS TIEMPO PARA ESO.
:::

:::: {.columns}
::: {.column width="25%"}

#### Alexnet (2012)
![](img/clase-1/alexnet.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### Transformers (2017)
![](img/clase-1/transformer.png){.lightbox}
:::
::: {.column width="25%" .fragment}
#### GPT (2019)
![](img/clase-1/gpt.png){.lightbox}
:::

::: {.column width="25%" .fragment}
#### LLMs (2023) (ChatGPT/Llama)
![](img/clase-1/llms.jpg){.lightbox}
:::
::::


## 驴Por qu茅 estudiar Deep Learning? {.smaller}

![Im谩gen tomada de la Clase de Zico Colter](img/clase-1/google_trends.png){.lightbox width="70%" fig-align="center"}

## 驴Por qu茅 estudiar Deep Learning? {.smaller}

::: {.callout-tip appearance="default"}
## Facilidad y Autograd
* Frameworks como Tensorflow, Pytorch o Jax permiten realizar esto de manera mucho m谩s sencilla.
    * Frameworks permiten calcular gradientes de manera autom谩tica.
    * Antigua mente trabajar en Torch, Caffe o Theano pod铆a tomar cerca de 50K l铆neas de c贸digo.
:::

::: {.callout-note appearance="default"}
## C贸mputo
* Proliferaci贸n de las GPUs, TPUs, HPUs, IPUs, como sistemas masivos de C贸mputos. 
    * [How many computers to identify a cat? 16,000](https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html)
:::

::: {.callout-important appearance="default"}
## Estado del Arte
* Modelos de Deep Learning pueden generar sistemas que entiendan im谩genes, textos, audios, videos, grafos, etc.
:::

# Prerrequisitos del Curso

## Tensores {.smaller}

> Corresponde a una generalizaci贸n de los vectores y matrices que permite representar datos de m煤ltiples dimensiones.


:::: {.columns style="font-size: 120%;"}
::: {.column width="30%"}
#### Escalares (Orden 0)
$$-1, 11.27, \pi$$
:::
::: {.column width="30%"}
#### Vectores Filas (Orden 1)
$$[1.0,-0.27, -1.22]$$

#### Vectores Columnas (Orden 1)
$$\begin{bmatrix}
1 \\
-0.27\\
-1.22
\end{bmatrix}$$
:::

::: {.column width="25%"}
#### Matrices (Orden 2)
$$\begin{bmatrix}
1.0 & -0.27 & 3\\
3.15 & 2.02 & 1.2\\
-1.22& 0.55 & 3.97 \\
\end{bmatrix}$$
:::
::::

::: {.callout-note} 
Normalmente los tensores utilizan un mismo tipo de dato: Integers o Float es lo m谩s com煤n.
:::

## Tensores {.smaller}

:::: {.columns}
::: {.column width="40%"}
#### Tensores (Orden 3+)
$$\begin{bmatrix}
    \begin{bmatrix}
        0.2 & 0.1 & -0.25 \\
        0.1 & -1.0 & 0.22\\
    \end{bmatrix} \\
    \begin{bmatrix}
        0.24 & 0.1 & -0.25 \\
        0.05 & -0.69 & 0.98
    \end{bmatrix} \\
    \begin{bmatrix}
        0.66& -1.0 & 0.22\\
        -0.07 & -0.59 & 0.99
    \end{bmatrix} \\
    \begin{bmatrix}
        0.16& 1.0 & 3.22\\
        9.17 & 7.19 & 9.99
    \end{bmatrix}
\end{bmatrix}
$$
:::
::: {.column width="60%"}
::: {.callout-tip appearance="default"}
## Nomenclatura

* $\alpha$, $\beta$, $\gamma$: Min煤sculas griegas denotan a Escalares.
* x, y, z: Min煤sculas latinas denotan a Vectores.
* X, Y, Z: May煤sculas latinas denotan a Matrices o Tensores.
:::
::: {.callout-important appearance="default"}
## Shape/Tama帽o: Tama帽o del tensor, tiene tantas dimensiones como su orden.
* **Escalar**: No tiene dimensiones.
* **Vector**: Tama帽o es equivalente al n煤mero de elementos del vector. (3,)
  * A veces se usa la versi贸n (1,3) para vectores filas y (3,1) para vectores columnas.
* **Matrices**: Tama帽o es equivalente al n煤mero de filas y columnas. Ejemplo: (3,3)
* **Tensores**: Tama帽o es equivalente al n煤mero de matrices que lo componen y el n煤mero de filas y columnas de cada una de ellas. Ejemplo: (4,2,3)
:::

::: {.callout-warning appearance="default" icon=false}
## Importante
* La primera dimensi贸n del shape se conoce como **Batch Size** el cual denota la cantidad de elementos de orden inferior. 

* (3, ) tenemos 3 escalares.
* (3,2) tenemos 3 vectores filas de 2 elementos cada uno.
* (4,2,3) tenemos 4 matrices de (2,3) cada una.
:::
:::
::::

## Vectores: Suma {.smaller}


> Corresponde a un arreglo unidimensional de n煤meros reales. Se puede representar como fila o columna. Por convenci贸n denotaremos $\bar{x}$ como vector columna y $\bar{x}^T$ como vector fila.

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Suma
* Permite sumar dos vectores de igual tama帽o dimensi贸n por dimensi贸n.

Ej: $[7,2]^T + [2,3]^T = [9,5]^T$

:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Conmutatividad: $\bar{x}+\bar{y} = \bar{y}+\bar{x}$
* Asociatividad: $(\bar{x}+\bar{y})+\bar{z} = \bar{x}+(\bar{y}+\bar{z})$
* Elemento Neutro: $\bar{x} + \bar{0} = \bar{x}$
:::

::::


![](img/clase-1/vector_sum.png){.lightbox fig-align="center"}


## Vectores: Ponderaci贸n {.smaller}


> Corresponde a un arreglo unidimensional de n煤meros reales. Se puede representar como fila o columna. Por convenci贸n denotaremos $\bar{x}$ como vector columna y $\bar{x}^T$ como vector fila.

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Ponderaci贸n
* Permite multiplicar/ponderar cada dimensi贸n del vector por un escalar.

  Ej: $2 \cdot [3,2]^T = [6,4]^T$
:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Distributividad Escalar: $a(\bar{x}+\bar{y}) = a\bar{x} + a\bar{y}$
* Distributividad Vectorial: $(a+b)\bar{x} = a\bar{x} + b\bar{x}$
* Elemento Neutro: $1\cdot \bar{x} = \bar{x}$
* Compatibilidad: $a(b\bar{x}) = (ab)\bar{x}$
:::

::::


![](img/clase-1/vector_scaling.png){.lightbox fig-align="center"}

## Vectores: Norma {.smaller}

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Norma (Euclideana)
Para un vector $\bar{x}=[x_1, ..., x_n] \in \mathbb{R}^n$ se define la norma como: 

$$||\bar{x}|| = \sqrt{\sum_{i=1}^n x_i^2}$$


:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Desigualdad Triangular: $||\bar{x}+\bar{y}|| \leq ||\bar{x}|| + ||\bar{y}||$
* $||\alpha \bar{x}= |\alpha| \cdot ||\bar{x}||$
* $||\bar{x}|| = 0 \Longleftrightarrow \bar{0}$
:::

::::

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-caution icon=false width="50%" appearance="default"}
### Aplicaci贸n
 La norma permite calcular la distancia entre dos vectores.

$$d_{x,y} = ||\bar{x} - \bar{y}$$

**Tambi茅n servir铆a para puntos. 驴Por qu茅?**


:::
::::{.column}
![](img/clase-1/norm_distance.png){.lightbox fig-align="center" width="80%"}
:::

::::


## Vectores: Producto Interno (Producto Punto) {.smaller}

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Inner Product or Dot Product
El producto interno entre dos vectores en $\mathbb{R}^n$ se define como:

$\bar{x} = [x_1, ..., x_n]$ e $\bar{y} = [y_1, ..., y_n]$

$$ \bar{x} \cdot \bar{y} = \sum_{i=1}^n x_i y_i = x_1 y_1 + ... + x_n y_n$$

A veces el producto interno se denota como $\bar{x}^T \bar{y}$ o $\langle \bar{x}, \bar{y} \rangle$.
:::
::::{.column .callout-tip width="50%" appearance="default"}
### Propiedades
* Conmutatividad: $\bar{x} \cdot \bar{y} = \bar{y} \cdot \bar{x}$
* Linealidad: $(\alpha \bar{x})\cdot \bar{y} = \alpha(\bar{x}\cdot \bar{y})
* Distributividad: $\bar{x} \cdot (\bar{y} + \bar{z}) = (\bar{x} \cdot \bar{y}) + (\bar{x} \cdot \bar{z})$
* $||\bar{x}||^2 = \bar{x} \cdot \bar{x}$
:::

::::

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-caution icon=false width="50%" appearance="default"}
### Aplicaciones
* Ortogonalidad: Dos vectores son ortogonales si su producto interno es cero.
* Similaridad: Se puede usar el Cosine Similarity para calcular qu茅 tan parecidos son dos vectores.


:::
::::{.column .callout-important icon=false width="50%" appearance="default"}
## Cosine Similarity

$$sim(\bar{a}, \bar{b}) = \frac{\bar{a} \cdot \bar{b}}{||\bar{a}|| \cdot ||\bar{b}||}$$

* 1 implica misma direcci贸n (id茅nticos)
* -1 implica direcciones opuestas (opuestos).
* 0 implica totalmente distintos (ortogonales).
:::
::::

## Vectores: Otras Propiedades

::: {.callout-tip appearance="default"}
#### Combinaci贸n Lineal
* Se denomina una combinaci贸n lineal de vectores a la suma ponderada de estos.

Ej: $\bar{w} = \alpha \cdot \bar{x} + \beta \cdot \bar{y} + \gamma \cdot \bar{z}$

$\bar{w}$ es una combinaci贸n lineal de los vectores $\bar{x}, \bar{y} y \bar{z}$.
:::

::: {.callout-note appearance="default"}
#### Independencia Lineal
* Un conjunto de vectores es linealmente independiente si:

$$\alpha_1 \cdot \bar{x}_1 + \alpha_2 \cdot \bar{x}_2 + ... + \alpha_n \cdot \bar{x}_n = 0$ implica que $\alpha_i = 0$ para todo $i$.
:::

::: {.callout-important appearance="default"}
##  Ojito
Hay otras propiedades sumamente importantes de vectores, por lo que coloquen atenci贸n al curso de Algebra Lineal.
:::

# Matrices

## Matrices: Definici贸n {.smaller}

::::{.columns style="font-size: 120%;"} 
> Corresponde a un arreglo bidimensional de n煤meros reales. Se dice que una matriz es de $n\times m$ o que es $\mathbb{R}^{n \times m}$ cuando tiene $n$ filas y $m$ columnas.

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,m} \\
A_{2,1} & \dots & A_{2,m} \\
\vdots & \ddots & \vdots \\
A_{n, 1} & \dots & A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

::: {.callout-important appearance="default"}
## 
* Normalmente se utiliza $m$ para denotar el n煤mero de registros y $n$ como el n煤mero de features de un dataset tabular (o tambi茅n conocido como Dataframe).
* $n=m$ nos referimos a una matriz cuadrada.
:::
::::

## Matrices: Notaci贸n {.smaller}

:::: { style="font-size:110%;"}
Si $A$ es una matriz entonces: 

* $A_{i,j}$ corresponde al elemento en la fila $i$ y columna $j$. Es decir, un escalar.
* $A_{i,:}$ corresponde a la fila $i$ completa. Es decir, un vector fila.
* $A_{:,j}$ corresponde a la columna $j$ completa. Es decir un vector columna.

:::: {.columns}
::: {.column width="50%"}
$$A = \begin{bmatrix}
0.2 & 1 & -5.2 & 3.1 & -1.3 \\
-0.5 & 10 & 0 & 3.1 & 3 \\
2 & 25 & -5.2 & 0 & 0 \\
100 & 3.4 & 4.1 & 0 & 42
\end{bmatrix}$$
:::
::: {.column .callout-important width="50%" appearance="default"}
### Importante: Recordar que los 铆ndices en Python son 0-based.

* $A_{2,4} = 3.1$
* $A_{:,3} = [-5.2, 0, -5.2, 4.1]^T$
* $A_{1,:} = [0.2, 1, -5.2, 3.1, -1.3]$


:::
::::
::::


## Matrices: Suma {.smaller}

::::{.columns style="font-size: 110%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Suma
* Permite sumar dos matrices elemento a elemento.

  Ej: Sea $A$ y $B$ dos matrices: 

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,m} \\
A_{2,1} & \dots & A_{2,m} \\
\vdots & \ddots & \vdots \\
A_{n, 1} & \dots & A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

$$B = \begin{bmatrix}
B_{1,1} & \dots & B_{1,m} \\
B_{2,1} & \dots & B_{2,m} \\
\vdots & \ddots & \vdots \\
B_{n, 1} & \dots & B_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

:::

:::: {.column}

::::{.callout-warning width="50%" appearance="default" icon=false}
## Resultado
$$A + B = \begin{bmatrix}
A_{1,1} + B_{1,1} & \dots & A_{1,m} + B_{1,m} \\
A_{2,1} + A_{2,1} & \dots & A_{2,m} + B_{2,m} \\
\vdots & \ddots & \vdots \\
A_{n, 1} + B_{n,1} & \dots & A_{n,m} + B_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$
:::

::::{.callout-tip width="50%" appearance="default"}
### Propiedades
* Asociatividad: $(A + B) + C = A + (B + C)$
* Conmutatividad: $A + B = B + A$
* Elemento Neutro: $A + 0 = A$
* Elemento Inverso: $A + (-A) = 0$
:::


::::
::::


## Matrices: Ponderaci贸n {.smaller}

::::{.columns style="font-size: 120%;"} 
::::{.column .callout-note width="50%" appearance="default"}
### Operaci贸n Ponderaci贸n
* Permite multiplicar/ponderar cada elemento de la matriz por un escalar.

  Ej: Sea $A$ una matriz: 

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,m} \\
A_{2,1} & \dots & A_{2,m} \\
\vdots & \ddots & \vdots \\
A_{n, 1} & \dots & A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

y $\gamma$ un escalar.
:::

:::: {.column}

::::{.callout-warning width="50%" appearance="default" icon=false}
## Resultado
$$\gamma \cdot A = \begin{bmatrix}
\gamma \cdot A_{1,1} & \dots & \gamma \cdot A_{1,m} \\
\gamma \cdot A_{2,1} & \dots & \gamma \cdot A_{2,m} \\
\vdots & \ddots & \vdots \\
\gamma \cdot A_{n, 1} & \dots & \gamma \cdot A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$
:::

::::{.callout-tip width="50%" appearance="default"}
### Propiedades
* Distibutividad Escalar: $\gamma(A + B) = \gamma A + \gamma B$
* Distibutividad Matricial: $(\gamma + \delta) A = \gamma A + \delta A$
* Compatibilidad: $(\gamma \delta) A = \gamma (\delta A) = \delta (\gamma A)$
:::


::::
::::

## Transpuesta y Reshape {.smaller}

::::{.columns style="font-size: 120%;"} 
:::{.column .callout-tip width="50%" appearance="default"}
## Transpuesta

Sea: 

$$A = \begin{bmatrix}
A_{1,1} & \dots & A_{1,m} \\
A_{2,1} & \dots & A_{2,m} \\
\vdots & \ddots & \vdots \\
A_{n, 1} & \dots & A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

Entonces, $A^T$ se define como: 


$$A^T = \begin{bmatrix}
A_{1,1} & \dots & A_{m,1} \\
\vdots & \ddots & \vdots \\
A_{1,m} & \dots & A_{n,m} \\
\end{bmatrix} \in \mathbb{R}^{n \times m}$$

Es decir, intercambiamos filas por las columnas y viceversa.
:::

:::{.column .callout-note width="50%" appearance="default"}
### Reshape

$$B = \begin{bmatrix}
1 & 3 & 5 \\
1 & 7 & 9 \\
4 & 6 & 7 \\
3 & 3 & 5 \\
\end{bmatrix} \in \mathbb{R}^{4 \times 3}$$

Podemos hacer un reshape a (6,2)

$$B_{reshaped} = \begin{bmatrix}
1 & 3 \\
5 & 1 \\
7 & 9 \\
4 & 6 \\
7 & 3 \\
3 & 5
\end{bmatrix} \in \mathbb{R}^{6 \times 2}$$
:::
::::

## Producto Matriz-Vector (Por la derecha) {.smaller}

A diferencia de todas las otras operaciones, el producto entre una matriz y un vector no es conmutativo. 

#### Post-multiplicaci贸n (Multiplicaci贸n por la derecha)


:::: {.columns}
:::{.callout-note .column width="40%" appearance="default" style="font-size: 110%;"}
## Sea

$$\bar{y} = A \cdot \bar{x}$$


$$A = \begin{bmatrix}
2 & 3 & 0 \\
1 & 0 & 7
\end{bmatrix}$$

$$\bar{x} = \begin{bmatrix}
4 \\
2 \\
1
\end{bmatrix}$$

:::

:::{.callout-important .column width="70%" appearance="default" style="font-size: 110%;"}
## Atenci贸n
La post-multiplicaci贸n se puede ver como la combinaci贸n lineal de las columnas de una matriz por cada elemento del vector.
$$
\begin{align}
\bar{y} = A \cdot \bar{x} &= \begin{bmatrix}
2 \cdot 4 + 3 \cdot 2 + 0 \cdot 1 \\
1 \cdot 4 + 0 \cdot 2 + 7 \cdot 1
\end{bmatrix} \\
&= 4 \cdot \begin{bmatrix}2 \\ 1\end{bmatrix} + 2 \cdot \begin{bmatrix}3 \\ 0\end{bmatrix} + 1 \cdot \begin{bmatrix}0 \\ 7\end{bmatrix} \\
&= \begin{bmatrix}14 \\ 11\end{bmatrix}
\end{align}$$
:::
::::

:::{.callout-caution appearance="default" icon=false style="font-size: 120%;"}
### 
* La multiplicaci贸n s贸lo es v谩lida si la dimensi贸n de las columnas de la matriz es igual a la dimensi贸n del vector. El resultado siempre es un vector columna.

* **La multiplicaci贸n de una fila por una columna es equivalente al Producto Interno.** Es decir, $\bar{y}_{i,:} = A_{i,:} \cdot \bar{x}$
:::


## Producto Matriz-Vector (Por la izquierda) {.smaller}

A diferencia de todas las otras operaciones, el producto entre una matriz y un vector no es conmutativo. 

#### Pre-multiplicaci贸n (Multiplicaci贸n por la izquierda)

:::: {.columns}
:::{.callout-note .column width="40%" appearance="default" style="font-size: 110%;"}
## Sea

$$\bar{y}^T = \bar{x}^T \cdot A$$


$$A = \begin{bmatrix}
2 & 3 & 0 \\
1 & 0 & 7
\end{bmatrix}$$

$$\bar{x} = \begin{bmatrix}
2 & 1
\end{bmatrix}$$

:::

:::{.callout-important .column width="70%" appearance="default" style="font-size: 110%;"}
## Atenci贸n
La pre-multiplicaci贸n se puede ver como la combinaci贸n lineal de las filas de una matriz por cada elemento del vector.
$$
\begin{align}
\bar{y}^T = \bar{x}^T \cdot A &=
\begin{bmatrix}
(2 \cdot 2 + 1 \cdot 1)  & (2 \cdot 3 + 1 \cdot 0) & (2 \cdot 0 + 1 \cdot 7) \\
\end{bmatrix} \\
&= 2 \cdot \begin{bmatrix} 2 & 3 & 0\end{bmatrix} + 1 \cdot \begin{bmatrix} 1 & 0 & 7\end{bmatrix} \\
&= \begin{bmatrix}5 & 6 & 7\end{bmatrix}
\end{align}
$$
:::
::::

:::{.callout-caution appearance="default" icon=false style="font-size: 130%;"}
### 
* **La multiplicaci贸n s贸lo es v谩lida si la dimensi贸n de las filas de la matriz es igual a la dimensi贸n del vector. El resultado siempre es un vector fila**
:::
## Producto Matriz-Matriz {.smaller}

Corresponde a una operaci贸n que permite multiplicar 2 matrices si las columnas de la primera son iguales a las filas de la segunda. Una matriz de $n \times p$ multiplicada con una de $p \times m$ nos dar谩 una matriz de $n \times m$. La manera de multiplicar es tomar cada fila de la primera y multiplicarla por cada columna de la segunda.

:::{.callout-important appearance="default" icon=false style="font-size: 115%;"}
## Ojito!!

* La multiplicaci贸n matricial es equivalente a $m$ post-multiplicaciones Matriz-Vector, stackeadas hacia el lado.
* Tambi茅n se puede ver como $n$ pre-multiplicaciones Matriz-Vector, stackeadas hacia abajo.

$$
\begin{align}
AB &= \begin{bmatrix}
A_{1,1} & \dots & A_{1,p} \\
\vdots & \ddots & \vdots \\
A_{n, 1} & \dots & A_{n,p} \\
\end{bmatrix}
\begin{bmatrix}
B_{1,1} & \dots & B_{1,m} \\
\vdots & \ddots & \vdots \\
B_{p, 1} & \dots & B_{p,m} \\
\end{bmatrix} \\
&= \begin{bmatrix}
| & &  | \\
A \cdot B_{:,1}& \dots &  A \cdot B_{:,m} \\
| & &  | \\
\end{bmatrix}\\
&= \begin{bmatrix}
- & A_{1,:} \cdot B & - \\
& \vdots &  \\
- & A_{n,:} \cdot B & - \\
\end{bmatrix}\\
\end{align}$$
:::

## Otros Productos {.smaller}


:::: {.columns}
:::{.callout-warning .column width="30%" icon=False appearance="default" style="font-size: 120%;"}
## Hadamard Product

Corresponde a otra operaci贸n que permite multiplicar 2 matrices si y s贸lo si tienen el mismo tama帽o. La multiplicaci贸n se realiza elemento a elemento.


$$A = \begin{bmatrix}
2 & 3 & 0 \\
1 & 0 & 7
\end{bmatrix}$$

$$B = \begin{bmatrix}
2 & 5 & 1 \\
2& 3 & 7
\end{bmatrix}$$

$$A \odot B = \begin{bmatrix} 4 & 15 & 0 \\ 2 & 0 & 49\end{bmatrix}$$
:::

:::{.callout-caution .column width="70%" icon=False appearance="default" style="font-size: 120%;"}
## Outer Product (Producto Externo)

Corresponde a otra operaci贸n que permite multiplicar 2 vectores. El resultado es una matriz de tama帽o $d1 \times d2$ donde $d1$ es la dimensi贸n del primer vector y $d2$ es la dimensi贸n del segundo vector.

$$\bar{x} = \begin{bmatrix}
2 \\
-1 \\
3
\end{bmatrix} \, \bar{y} = \begin{bmatrix}
4 \\
1 \\
5 \\
-2
\end{bmatrix}$$

$$ 
\begin{align} 
\bar{x} \otimes \bar{y} = \bar{x} \cdot \bar{y}^T &= \begin{bmatrix}
2 \cdot 4 & 2 \cdot 1 & 2 \cdot 5 & 2 \cdot -2 \\
-1 \cdot 4 & -1 \cdot 1 & -1 \cdot 5 & -1 \cdot -2 \\
3 \cdot 4 & 3 \cdot 1 & 3 \cdot 5 & 3 \cdot -2
\end{bmatrix} \\
&= \begin{bmatrix}
8 & 2 & 10 & -4 \\
-4 & -1 & -5 & 2 \\
12 & 3 & 15 & -6
\end{bmatrix}
\end{align}
$$

:::
::::


## Batch Product {.smaller}

Este tipo de operaci贸n es bastante poco com煤n en otras 谩reas, pero extremadamente com煤n en Deep Learning. 


::: {.callout-tip appearance="default"}
## Ejemplo
驴Qu茅 pasa si queremos calcular la multiplicaci贸n de un tensor de dimensiones (2, 3, 2) y otra de (2, 2, 4)? El resultado es un tensor de dimensiones (2, 3, 4).

Podemos interpretarlo como que se har谩n 2 multiplicaciones a matrices de (3,2) y (2,4) respectivamente (las cuales son compatibles).

TODO: Ejemplo num茅rico Pendiente...
:::




## El nacimiento de las Redes Neuronales {.smaller}

> Las redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad m谩s b谩sica es una Neurona. 


![](img/clase-1/neuron_1.png){.lightbox fig-align="center"}

## El nacimiento de las Redes Neuronales {.smaller}

> Las redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad m谩s b谩sica es una Neurona. 


::: {.columns}
::: {.column width="60%"}
![](img/clase-1/neuron_2.png){.lightbox fig-align="center" width="70%"}


::: {.callout-note} 
Este tipo de nomenclatura est谩 sumamente pasada de moda.
:::
:::
::: {.column width="40%" .fragment}



* Este c谩lculo se puede representar como: 

$$ y = \phi(w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_5 \cdot x_5)$$
$$ y = \phi(w^T \cdot x)$$

donde $w = [w_1, w_2, w_3, w_4, w_5]$ y $x = [x_1, x_2, x_3, x_4, x_5]$.

::: {.callout-warning .fragment .incremental}
* 驴Qu茅 pasa si $\phi(.)$ vale la funci贸n ***identidad***?
* Tenemos una **Regresi贸n Lineal**.
:::


::: {.callout-warning .fragment .incremental}
* 驴Qu茅 pasa si $\phi(.)$ vale la funci贸n ***sigmoide***?
* Tenemos una **Regresi贸n Log铆stica**.
:::
:::
::: 

## Arquitectura de una Red {.smaller}


::: {.columns}
::: {.column}
![](img/clase-1/nn_arq.png){.lightbox fig-align="center"} 
:::
::: {.column}
### Estructura m谩s com煤n 
###### *(Probablemente tampoco seguiremos esta nomenclatura)*

* Nodos o Neuronas
* Edges o Conexiones
* Capas

::: {.callout-caution .fragment style="font-size:150%;"}
***驴Cu谩ntas capas tiene esta red?***
:::

::: {.callout-tip .fragment style="font-size:150%;"}
***Depende***
:::
:::
::: 

* Normalmente todas las neuronas de una capa anterior se conectan con las de una capa posterior (Hay excepciones). 
* Dependiendo de la forma en la que se conecten, cada **Arquitectura** recibe un nombre.

# Intuici贸n y conceptos iniciales

## Los Ingredientes de un Algoritmo de Aprendizaje {.smaller}

Hip贸tesis
: > Una funci贸n que describe como mapear inputs (features) con outputs (labels) por medio de par谩metros.  

Loss Function
: > Una funci贸n que especifica cuanta informaci贸n se pierde. Mayor p茅rdida implica m谩s error de estimaci贸n.

M茅todo de Optimizaci贸n
: > Es el responsable de combinar la `hip贸tesis` y la `loss function`. Corresponde a un procedimiento para determinar los par谩metros de la hip贸tesis, minimizando la suma de las p茅rdidas en un set de entrenamiento. 

## Ejemplo: Softmax Regression 

Softmax Regression
: > Corresponde la versi贸n multiclase de una Regresi贸n Log铆stica. Tambi茅n se le llama una `Shallow Network`.


::: {.columns}
::: {.column width="50%" style="font-size: 80%;"}
::: {.callout-tip}
#### Consideremos un problema de clasificaci贸n multiclase de $k$ clases tal que:

* Datos de Entrenamiento: $x^{(i)}, y^{(i)} \in {1,...,k}$ para $i=1,...,m$.
    * $n$: Es el n煤mero de Features.
    * $m$: Es el n煤mero de puntos en el training set. 
    * $k$: Es el n煤mero de clases del problema.
:::

::: {.callout-important}
Vamos a tener en total $n \times k$ par谩metros o pesos que actualizar.
:::
:::
::: {.column width="50%"}
![](img/clase-1/softmax_reg.png){.lightbox fig-align="center"} 
:::
::: 



## Softmax Regression: Hip贸tesis

::: {style="font-size: 80%;"}
Vamos a definir una funci贸n que mapea valores de $x \in \mathbb{R}$ a vectores de $k$ dimensiones. 
:::
$$ h: \mathbb{R}^n \rightarrow \mathbb{R}^k$$
$$ x \rightarrow h_\theta(x) = \theta^T x$$

::: {style="font-size: 80%;"}
donde $\theta \in \mathbb{R}^{n \times k}$ y $x \in \mathbb{R}^{n\times 1}$
:::

::: {.callout-warning}
En este caso usamos una `hip贸tesis lineal`, ya que se usa una multiplicaci贸n matricial (o producto punto) para relacionar $\theta$ y $x$. 
:::

::: {.callout-note}
En este caso el output de $h_i(x)$ devolver谩 la probabilidad de pertenecer a una cierta clase $i$.   
:::

::: {.callout-important .fragment}
***驴Cu谩l es el tama帽o/dimensi贸n de $h_\theta(x)$?***
:::

## Notaci贸n Matricial {.smaller}

> Una manera m谩s conveniente de escribir estas operaciones es utilizar ***(Matrix Batch Form)***. 

::: {.columns}
::: {.column}
##### Design Matrix

$$X \in \mathbb{R}^{m \times n} = \begin{bmatrix}
&-x^{(1)T}-\\
& \vdots & \\
&-x^{(m)T}- &\\
\end{bmatrix}$$
:::
::: {.column}
##### Labels Vector
$$y \in {1,...,k} = \begin{bmatrix}
&-y^{(1)}-\\
& \vdots & \\
&-y^{(m)}- &\\
\end{bmatrix}$$
:::
::: 

La hip贸tesis tambi茅n se puede reescribir de manera matricial como: 

::: {.columns}
::: {.column}
$$h_\theta(X) = \begin{bmatrix}
&-h_\theta(x^{(1)})^T-\\
& \vdots & \\
&-h_\theta(x^{(m)})^T-\\
\end{bmatrix}$$
:::
::: {.column}
$$h_\theta(X)= \begin{bmatrix}
&-x^{(1)T} \theta-\\
& \vdots & \\
&-x^{(m)T} \theta-\\
\end{bmatrix} = X  \theta$$
:::
::: 

::: {.callout-important .fragment}
Normalmente este tipo de operaciones son las que utilizaremos para hacer nuestro c贸digo.
:::

## Loss Function: Softmax/Cross-Entropy Loss {.smaller}


::: {.callout-warning style="font-size: 130%;"}
La salida de nuestra `Shallow Network` retornar谩 valores reales.
:::
::: {.callout-tip style="font-size: 130%;"}
Para poder tener una mejor interpretaci贸n del significado de cada una aplicaremos la funci贸n `Softmax` lo cual permitir谩 *normalizar* los resultados y llevar谩 los resultados a una ***"distribuci贸n de probabilidad"*** (valores positivos que sumen 1).
:::

::: {.columns}
::: {.column width="60%"}


![](img/clase-1/softmax_example.png){.lightbox fig-align="center"} 
:::
::: {.column width="40%"}


Formalmente definiremos la funci贸n Softmax como: 

$$s_i = p(label = i) = \frac{exp(h_i(x))}{\sum_{j=1}^k exp(h_j(x))}$$


$$s = \begin{bmatrix}
&s_1&\\
& \vdots & \\
&s_k&\\
\end{bmatrix}$$
:::
::: 

## Loss Function: Softmax/Cross-Entropy Loss {.smaller}

Para medir el error/p茅rdida de informaci贸n utilizaremos el `Negative Log Loss` o `Cross Entropy Loss`.

$$l_{ce}(h(x), y) = -log\left(p(label = y)\right)$$

::: {.callout-tip style="font-size: 120%;" .fragment}
Para garantizar el 茅xito de nuestro modelo, b谩sicamente queremos maximizar la probabilidad de encontrar la etiqueta correcta, es decir, que $p(label = y)$ sea lo m谩s alto posible.
:::
::: {.callout-caution style="font-size: 120%;" .fragment}
Normalmente en los problemas de optimizaci贸n no se suele maximizar sino minimizar. Minimizar el valor negativo es equivalente a maximizar. Esto ser铆a equivalente a minimizar el error del modelo. 
:::
::: {.callout-warning style="font-size: 120%;" .fragment}
Finalmente por razones de estabilidad num茅rica, minimizamos el logaritmo de la probabilidad que es una t茅cnica bien conocida en Estad铆stica.
:::

:::{.fragment}
$$\begin{align}
l_{ce}(h(x), y) = -log\left(p(label = y)\right) &= -log \left(\frac{exp(h_{(i = y)}(x))}{\sum_{j=1}^k exp(h_j(x))}\right) \\
&= - h_{(i=y)}(x) + log\left(\sum_{j = 1}^k exp(h_j(x))\right)\end{align}$$
:::

## M茅todo de Optimizaci贸n {.smaller}

> El 煤ltimo ingrediente de un algoritmo de aprendizaje es el m茅todo de optimizaci贸n. Es necesario minimizar la p茅rdida promedio asociada a todos los puntos de un cierto set de entrenamiento. Para ello definimos esto formalmente como:

$$\underset{\theta}{minimize} = \frac{1}{m} \sum_{i=1}^m l_{ce}(h_\theta(x^{(i)}), y^{(i)})$$


::: {.callout-note}
***驴C贸mo encontramos los par谩metros $\theta$ que minimizan la p茅rdida de informaci贸n/error de estimaci贸n?***
:::

Gradient Descent
: > Es un m茅todo num茅rico que permite minimizar funciones movi茅ndose en direcci贸n contraria al Gradiente. Es computacionalmente muy eficiente y f谩cil de implementar en c贸digo.

## Gradient Descent {.smaller}

::: {.columns}
::: {.column width="60%"}
Se define el gradiente como la matriz que contiene las derivadas parciales de una funci贸n $f$. Se denota como:

$$\nabla_\theta f(\theta) \in \mathbb{R}^{n \times k} =  \begin{bmatrix}
\frac{\partial f(\theta)}{\partial \theta_{11}} & \cdots & \frac{\partial f(\theta)}{\partial \theta_{1k}} \\
\cdots & \ddots & \cdots \\
\frac{\partial f(\theta)}{\partial \theta_{n1}} & \cdots & \frac{\partial f(\theta)}{\partial \theta_{nk}}
\end{bmatrix}$$

::: {.callout-tip}
$\theta_{ij}$ corresponde al par谩metro que une el nodo/feature $i$ con el nodo/predicci贸n $j$.
:::
:::
::: {.column width="40%"}
![](img/clase-1/gradient.png){.lightbox fig-align="center" } 
:::
::: 


::: {.callout-tip style="font-size: 130%;"}
El gradiente apunta a la direcci贸n de m谩ximo crecimiento de la funci贸n $f$. 
:::


## Gradient Descent: Regla de Actualizaci贸n {.smaller}
Para minimizar la funci贸n, la idea es descender iterativamente por el trayecto **en contra** del gradiente. La regla de actualizaci贸n se define como:

$$\theta := \theta - \alpha \nabla_\theta f(\theta) = \theta - \frac{\alpha}{m}\nabla_\theta l_{ce}(X\theta,y)$$

con $\theta \in \mathbb{R}^{n \times k}$ y $\alpha > 0$ corresponde al *step size* o `learning rate`.


![](img/clase-1/lr_effect.png){.lightbox fig-align="center" width="60%"} 

::: {.callout-tip}
En nuestro caso $f$ corresponder谩 a nuestro $l_{ce}$ calculado anteriormente. El problema es, 驴cu谩nto vale el gradiente del `Cross Entropy Loss`?
:::

## Calculando el Gradiente a mano {.smaller}


::: {style="font-size: 130%;"}
Simplifiquemos el problema a calcular para un s贸lo vector $x$.

$$\theta := \theta - \alpha \nabla_\theta l_{ce}(\theta^Tx,y) $$
:::

::: {.callout-warning style="font-size: 120%;"}
驴Cu谩nto vale el Gradiente?

* No es tan sencillo, ya que derivamos respecto a $\theta$ que es una matriz. 
* Pero derivamos a $\theta^T x$ que es un vector.
* Para ello, lo correcto es utilizar Calculo Diferencial Matricial, Jacobianos y Productos de Kroenecker (que probablemente no han visto en ning煤n curso).
  * **SPOILER**: Yo tampoco lo he visto en ning煤n curso.
:::

::: {.columns .fragment}
::: {.column width="70%"}
::: {.callout-tip style="font-size: 120%;"}
* Usaremos un truco (sumamente hacky ) que jam谩s deben revelar y que avergonzar铆a a cualquier profesor de C谩lculo.
    * Pretenderemos que todos los valores son escalares y corregiremos las dimensiones al final.

:::
:::
::: {.column width="30%"}
![](img/clase-1/fuenzi.jpeg){.lightbox fig-align="center" width="40%"} 

:::
::: 

## Calculando el Gradiente a mano {.smaller}

> Simplifiquemos el problema pensando que calcularemos el Gradiente para un s贸lo vector $x$.

>  Es decir, $x \in \mathbb{R}^{n\times1}$.

Adem谩s sabemos que $\nabla_\theta l_{ce}(\theta^Tx, y)$ debe tener dimensiones $n \times k$.

::: {.callout-important style="font-size: 150%;" .fragment fragment-index=1}
***驴Por qu茅?***
:::

::: {.columns}
::: {.column .fragment fragment-index=2}
$$\nabla_\theta l_{ce}(\theta^T x,y) = \frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} \cdot \frac{\partial \theta^Tx}{\partial \theta}$$
:::
::: {.column .fragment fragment-index=3}
$$\frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} = \frac{\partial l_{ce}(h_\theta(x), y)}{\partial h_\theta(x)} = \begin{bmatrix}
\frac{\partial l_{ce}(h,y)}{\partial h_1} \\
\vdots\\
\frac{\partial l_{ce}(h,y)}{\partial h_k} \\
\end{bmatrix}$$
:::
::: 

::: {.callout-tip style="font-size: 130%;" .fragment fragment-index=4}
Luego el gradiente de $l_{ce}$ respecto a $h$ tiene dimensiones $k \times 1$.
:::

## Calculando el Gradiente a mano {.smaller}

$$\begin{align}
\frac{\partial l_{ce}(h,y)}{\partial h_i} &= \frac{\partial }{\partial h_i}\left(-h_{(i = y)} + log \sum_{j = 1}^k exp(h_j)\right) \\
&= -\frac{\partial h_{(i = y)}}{\partial h_i}+ \frac{1}{\sum_{j = 1}^k exp(h_j)} \cdot \frac{\partial}{\partial h_i}\left(\sum_{j=1}^k exp(h_j)\right) \\
&= -\frac{\partial h_{(i = y)}}{\partial h_i}+ \frac{exp(h_i)}{\sum_{j = 1}^k exp(h_j)} \\
&= - 1\{i=y\} + s_i = s_i - 1\{i=y\}
\end{align}
$$

::: {.callout-tip .fragment}
$$1\{i = y\} = \begin{cases}
1,  & \text{i = y} \\
0, & \text{otherwise}
\end{cases}
$$
:::

::: {.fragment}
Finalmente en forma vectorial quedar铆a como:

::: {.columns}
::: {.column}
$$\frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} = s - e_y$$
:::
::: {.column}
::: {.callout-tip}
Donde $z$, es el vector de Softmax y $e_y$ es un vector con un 1 en la posici贸n $y$ y 0 en el resto.
:::
:::
::: 
:::

## Calculando el Gradiente a mano {.smaller}

::: {.columns}
::: {.column }
$$\nabla_\theta l_{ce}(\theta^T x,y) = \frac{\partial l_{ce}(\theta^T x,y)}{\partial \theta^T x} \cdot \frac{\partial \theta^Tx}{\partial \theta}$$
$$\nabla_\theta l_{ce}(\theta^T x,y) = (s-e_y)\cdot x $$
:::

::: {.column .fragment}
::: {.callout-caution appearance="default"}
## Ojo con las dimensiones
* $s-e_y \in \mathbb{R}^{k \times 1}$
* $x \in \mathbb{R}^{n \times 1}$
:::

::: 
:::

:::{.fragment}
Luego: 

$$\nabla_\theta l_{ce}(\theta^T x,y) = x (s-e_y)^T$$
:::

::: {.callout-caution style="font-size: 150%;" .fragment}
***驴Cu谩l es el tama帽o de $\nabla_\theta l_{ce}(\theta^T x,y)$?***
:::

::: {.callout-note style="font-size: 150%;" .fragment}
$n \times k$
:::

::: {.callout-warning style="font-size: 150%;" .fragment}
***驴Por qu茅?***
:::



## Calculando el Gradiente Matrix Batch Form {.smaller}

#### Esto ser铆a equivalente a tomar en consideraci贸n todos los puntos del Training Set

::: {.columns}
::: {.column}
$$\begin{align}\nabla_\theta l_{ce}(X\theta,y) &= \frac{\partial l_{ce}(X\theta,y)}{\partial X\theta} \cdot \frac{\partial X\theta}{\partial \theta}\\
&= (S - I_y) \cdot X \\
&= X^T \cdot (S - I_y)
\end{align}$$

::: {.callout-tip}
* $S$ corresponde al Softmax de $X\theta$ aplicado por filas.
* $I_y$ corresponde al One Hot Encoder de las etiquetas. Filas con 1 en la etiqueta correcta y 0 en el resto.
:::
:::
::: {.column}
::: {.callout-caution appearance="default" .fragment}
## Ojo con las dimensiones

* $S - I_y \in \mathbb{R}^{m \times k}$
* $X \in \mathbb{R}^{m \times n}$
:::

::: {.callout-warning .fragment}
***驴Cu谩l es el tama帽o de $\nabla_\theta l_{ce}(X\theta,y)$?***
:::
:::
::: 



:::{.fragment}
Finalmente la `Regla de Actualizaci贸n` de par谩metros usando Gradient Descent queda como:

$$\theta := \theta - \frac{\alpha}{m} X^T (S - I_y)$$
:::


## Conclusiones {.smaller}


::: {.columns}
::: {.column}

::: {.callout-tip}
* Acabamos de entrenar una Shallow Network, sin definir ning煤n concepto Fancy que es propio del 谩rea.
* No hemos hablado ni de:
  * `Forward Pass`
  * `Epochs`
  * `Backpropagation`
  * `Adam`
  * `Activation Functions`
  * etc.

::: 
::: {.callout-note .fragment fragment-index=1}
* Aplicando esta simple regla se puede obtener cerca de un 8% de error clasificando d铆gitos en MNIST.
* Se puede programar en pocas l铆neas en Python.

![](img/clase-1/mnist.png){.lightbox fig-align="center" width="30%"} 
:::
:::

::: {.column .fragment fragment-index=2}
#### Pero, 驴qu茅 pasa con arquitecturas m谩s complejas?

![](img/clase-1/nn_arq_full.png){.lightbox fig-align="center" width="60%"} 
:::
::: 


# 隆隆Eso es todo!!

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est谩 licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::

# Anexos

## Multiplicaci贸n Matricial {.smaller}

::: {.columns}
::: {.column}
![](img/clase-1/mat_mat_mul.png){.lightbox fig-align="center"} 
:::
::: {.column}
* Donde $B_{*,i}$ corresponde a la columna $i$ de B.
* Donde $A_{i,*}$ corresponde a la fila $i$ de A.
:::
::: 
