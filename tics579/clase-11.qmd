---
title: "TICS-579-Deep Learning"
subtitle: "Clase 11: Transformers"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Transformers (Attention is all you need, 2017) {.smaller}

::: {.columns}
::: {.column width="60%"}
Transformers
: Corresponden a la arquitectura más moderna diseñada al día de hoy. Está basado en mecanismos de atención y posee hasta 4 tipos de atención distintos.

::: {.callout-note appearance="default" icon="false"}
## Ventajas

* No tiene problemas de "memoria" para modelar dependencias de largo plazo.
* Permite procesamiento en paralelo.
* Su bajísimo ***inductive bias*** le permite adaptarse a distintos dominios.
* No es necesario utilizar el transformer completo para un problema en específico.
:::

::: {.callout-important appearance="default" icon="false"}
## Desventajas

* Apto sólo para datos secuenciales.
* Las secuencias deben de ser del mismo largo.
* Alta demanda de recursos computacionales GPU y/o TPUs para entrenamiento distribuido.
* Data hungry.
* Limitaciones de secuencias muy largas por restricciones de memoria computacional.
:::
:::
::: {.column width="40%"}
![](img/clase-10/transformer.png){.lightbox fig-align="center"}   
:::
::: 

## Encoder {.smaller}

::: {.columns}
::: {.column width="40%"}
![](img/clase-11/encoder.png){.lightbox fig-align="center" width="70%"}   
:::
::: {.column width="60%"}
::: {.callout-tip appearance="default" icon="false"}
## Objetivo
Codificar y comprimir información en Logits que puedan ser usados para clasificar o para ser utilizados por un Decoder.
:::
::: {.callout-important appearance="default" icon="false"}
## Forward Pass en el Encoder
* El embedding asociado a una secuencia se bifurca en 4 ramas:
  * Residual Connection
  * Query
  * Key
  * Value
* Query, Key y Value ingresan al Multihead Attention.
* La salida del Multihead Attention + el Residual Connection pasan por un LayerNorm.
* Nueva bifurcación. 
  * Una parte entra a un MLP 
  * Otra va como skip connection.
* La salida del MLP + la Residual Connection pasan por un segundo LayerNorm para generar la salida del Encoder.

:::

:::
::: 

## Decoder {.smaller}

::: {.columns}
::: {.column width="40%"}
![](img/clase-11/decoder.png){.lightbox fig-align="center" width="70%"}   
:::
::: {.column width="60%"}
::: {.callout-tip appearance="default" icon="false"}
## Objetivo
Tomar información de entrada y generar una salida fijándose sólo en tokens pasados. 
:::
::: {.callout-important appearance="default" icon="false"}
## Forward Pass en el Decoder
* El embedding asociado a una secuencia se bifurca en 4 ramas:
  * Residual Connection
  * Query
  * Key
  * Value
* Query, Key y Value ingresan al Masked (Causal) Multihead Attention.
* La salida del Masked Multihead Attention + el Residual Connection pasan por LayerNorm.
* Se pasa por un Cross Attention (esto podría ser opcional).
  * Key y Value provienen del Encoder como contexto.
  * La salida del Causal Multihead Attention se utiliza como Query.
* Nueva bifurcación.
  * Una parte entra a un MLP 
  * Otra va como skip connection.
* La salida del MLP + la Residual Connection pasan por LayerNorm para generar el Output.

:::

:::
::: 

## Ejemplo {.smaller}

Supongamos que tenemos la siguiente frase: 

> Me gusta la pizza de Pepperoni

Tokenización
: > Aplicaremos un proceso de Tokenización simple, donde cada palabra es un Token.

::: {.callout-important}
Ya sabemos que esto no tiene por qué ser así. De hecho cada modelo tiene su propio tipo de tokenización, e incluso se pueden entrenar Tokenizaciones nuevas. Más información al respecto pueden encontrarla [acá](https://huggingface.co/learn/nlp-course/es/chapter6/1?fw=pt).
:::
::: {.callout-warning}
La documentación de Tokenizers de HuggingFace la traduje yo, así que si encuentran algo me dicen para corregir.
:::

Luego la secuencia tokenizada de largo $L=6$ será: 

> [105,6587,5475,301,708,358]

::: {.callout-warning}
Recordar que dependiendo del modelo se pueden agregar al inicio o al final tokens especiales. Hablaremos de eso más adelante.
:::

## Embedding {.smaller}

::: {.columns}
::: {.column}
Embedding
: No es más que una Lookup Table. Es una tabla de parámetros entrenables que permitirá transformar índices enteros en vectores de una dimensión determinada. 


```{.python }
nn.Embedding(num_embeddings, embedding_dim)
```

::: {.callout-tip}
Esta clase permite el ingreso de tensores de cualquier tamaño $(*)$ y devuelve tensores de tamaño $(*,H)$. Donde $H$ es el `embedding_dim`.
:::

::: {.callout-note}
La sección 3.1 del paper se refiere al tamaño del embedding como $d_{model}=512$.
:::
::: {.callout-caution}
En la sección 3.4 del paper se menciona que los parámetros de los embeddings son multiplicados por $\sqrt{d_{model}}$.
:::
:::
::: {.column}
![](img/clase-11/transformer_embedding.png){.lightbox fig-align="center" width="60%"}   
:::
::: 

## Embedding {.smaller}

![](img/clase-11/embedding.png){.lightbox fig-align="center"}


::: {.callout-note}
Cada token está representado por un Embedding de $d_{model}$ dimensiones.
:::

## Positional Encoder {.smaller}

::: {.columns}
::: {.column}
::: {.callout-caution}
Un potencial problema que puede tener un transformer es reconocer el orden de las frases. 
:::

::: {.callout-warning}
No es lo mismo decir **"El perro del papá mordió al niño"** que **"El perro del niño mordió al papá"**. Las palabras usadas en ambas frases son exactamente las mismas, pero en un orden distinto implican desenlaces distintos. ***¿Cómo podemos entender el concepto de orden si no tenemos recurrencia?***
:::
::: {.callout-important}
Incluso algunos órdenes no tienen tanto sentido lógico: **"El niño del perro mordió al papá"**.
:::

Positional Encoder
: > Corresponden a una manera en la que se pueden generar un **vector único** que representa el orden en el que aparece cada token.
:::
::: {.column}
![](img/clase-11/transformer_pe.png){.lightbox fig-align="center" width="75%"}   
:::
::: 




## Positional Encoder {.smaller}

::: {.columns}
::: {.column}
$$PE_{(pos,2i)} = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos,2i+1)} = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
:::
::: {.column}
::: {.callout-note icon="false"}
* $pos$ corresponde a la posición del Token en la secuencia, y $2i$ y $2i+1$ corresponden a las posiciones pares e impares respectivamente del embedding dimension de cada token, en este caso llamado $d_{model}$ ($i$ comienza en 0).
:::
::: {.callout-caution}
Una forma más clara de ver esto es que la posición está definida por sinusoidales de periodo $2\pi \cdot 10000^{i/d_{model}/2}$.
:::
:::
::: 

::: {.callout-warning}
El positional encoder debe tener el mismo tamaño que el Embedding para que se puedan sumar.
:::

::: {.callout-tip appearance="default" icon="false"}
## Estabilidad numérica
Por temas de estabilidad el argumento del $sin(\cdot)$ y $cos(\cdot)$ se suele implementar como: 
$$pos \cdot exp\left[-\frac{2i}{d_{model}} log(10000)\right]$$

:::
::: {.callout-important appearance="default" icon="false"}
## Regularización
La sección 5.4 menciona que se aplica Dropout posterior a sumar los Embeddings con el Positional Encoding. Se utilizo un $P_{drop}=0.1$.
:::


## Positional Encoder: Ejemplo {.smaller}

::: {.columns}
::: {.column width="60%"}
![](img/clase-11/pe_example.png){.lightbox fig-align="center" width="90%"}   
:::
::: {.column width="20%"}
![](img/clase-11/pe_1.png){.lightbox fig-align="center"}   
:::
::: {.column width="20%"}
![](img/clase-11/pe_2.png){.lightbox fig-align="center"}   

:::
::: 

::: {.callout-tip appearance="default" icon="false"}
## Implementación Eficiente
El Positional Encoding se implementa como una matriz de tamaño $(L,d_{model})$ en la que cada fila es un embedding de $d_{model}$ dimensiones asociado a cada token.

:::
::: {.callout-important appearance="default" icon="false"}
## Importante
La suma del Embedding con el Positional Encoder codifica la información del token y su posición relativa dentro de la secuencia.
:::

## Encoder: Self-Attention {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/scale_dot_product.png){.lightbox fig-align="center" width="55%"}   

$$Attention(Q,K,V) = Softmax\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) V$$

::: {.callout-note appearance="default" icon="false"}
## Ejemplo
*"La sopa se cocinó en la olla y estaba rica"*. *Rica* podría estar refiriéndose a *olla* o a *sopa*. Sabemos que se refiere a la sopa.
:::
:::
::: {.column}
::: {.callout-warning icon="true"}

El Scaled Dot-Product, más conocido como `Self-Attention`, es el mecanismo clave en las redes neuronales modernas. Permite determinar la atención/relación que existe entre palabras de una misma secuencia.
:::

::: {.callout-note}
* Está compuesto por 3 proyecciones lineales las cuales reciben los nombres de Query (Q), Key (K) y Value (V).
* Estás 3 proyecciones se combinan para poder determinar la atención/relación que cada Token tiene con los otros tokens de una misma secuencia.
* Varios procesos de `Self-Attention` dan pie al `Multihead Attention`.
:::
::: {.callout-important}
* El `Self-Attention` tiene la capacidad de acceder a toda la secuencia, por ende modelar relaciones a larga distancia.
* El `Causal Self-Attention`, una variante que se utiliza en el Decoder sólo puede ver la relación con tokens pasados. 
:::
::: {.callout-tip}
* Su característica más importante es que el `Multihead Attention` es paralelizable y no secuencial como las RNN.
* Tiene capacidad de escalabilidad para secuencias largas. 
:::
:::
::: 

## Encoder: Self-Attention {.smaller}

::: {.callout-caution}
* Supongamos que tenemos la secuencia *"Me gusta la Pizza de Pepperoni"*. 
* Utilizaremos $d_{model} = 512$ y $h=8$.
* El paper utiliza $d_k=d_v=d_{model}/h=64$ para el cálculo de los Attention Weights.
:::

::: {.columns}
::: {.column width="40%"}
$$
\begin{array}{c c} 
X = \left[
\begin{array}{c c c}
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
\end{array}
\right]
\begin{array}{c c c}Me\\gusta\\la\\pizza\\de\\pepperoni \end{array} &
\end{array}
$$

::: {.callout-warning appearance="default" icon="false" .fragment}
## 👀 Ojito

Esto se debe aplicar a cada secuencia. Por lo tanto se debe agregar una dimensión (como unsqueeze(0)) que contabilice el número de secuencias para $Q$, $K$, y $V$.

:::

:::
::: {.column width="60%"}

::: {.callout-important appearance="default" icon="false"}
## Matrices de Proyección

Definiremos 3 matrices de Proyección. Una matriz de proyección permite llevar transportar un vector $X$ a otro espacio (es decir, son entrenables). En este caso crearemos matrices que puedan multiplicarse con $X$. Por lo tanto irán desde $d_{model}$ hasta $d_q=d_k$ y $d_v$ respectivamente.

* $W_q = (d_{model}, d_k)$ 
* $W_k = (d_{model}, d_k)$ 
* $W_v = (d_{model}, d_v)$ 
:::

::: {.callout-note appearance="default" icon="false"}
## Dimensiones de Q,K y V
* $Q = (L, d_k)$
* $K = (L, d_k)$
* $V = (L, d_v)$
* $L$ corresponde a largo de la secuencia (es decir, el número de Tokens)
:::

:::
:::
::: 

## Encoder: Self-Attention {.smaller}

> Siguiendo nuestro ejemplo: $d_k = d_v = 64$

::: {.columns}
::: {.column width="30%"}
::: {.callout-note appearance="default" icon="false"}
## Query (6,64)

$$
\begin{array}{c c} 
Q = \left[
\begin{array}{c c c}
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
\end{array}
\right]
\end{array}
$$

:::
:::
::: {.column width="30%"}
::: {.callout-caution appearance="default" icon="false"}
## Key (6,64)

$$
\begin{array}{c c} 
K = \left[
\begin{array}{c c c}
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
\end{array}
\right]
\end{array}
$$

:::
:::
::: {.column width="30%"}
::: {.callout-tip appearance="default" icon="false"}
## Value (6,64)

$$
\begin{array}{c c} 
V = \left[
\begin{array}{c c c}
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
\end{array}
\right]
\end{array}
$$

:::
:::
::: 

::: {.callout-warning appearance="default" icon="false" .fragment}
## 👀 Ojito

Como esto se aplica a una sola secuencia, la dimensión real de estos tensores debería ser $(1,6,64)$. Ese 1 cambiará si tenemos más secuencias. Pero, ***todas las secuencias deben ser del mismo largo***.

:::

## Encoder: Self-Attention (Scale Dot Product) {.smaller}

::: {.columns}
::: {.column width="40%"}
$$\frac{Q \cdot K^T}{\sqrt{d_k}}$$

::: {.callout-important appearance="default" icon="false"}
## Similaridad
* $Q \cdot K^T$ representa el producto punto entre $Q$ (un token de referencia que está consultando la atención contra otros tokens) y $K$ (otro token que se compara contra la "query").
:::

::: {.callout-tip appearance="default" icon="false"}
## Control de Gradientes
$\sqrt{d_k}$ es un factor que reduce la escala de los valores para el control de los gradientes. Recordar que esta matriz es de parámetros entrenables.
:::

:::
::: {.column width="60%"}
![](img/clase-11/dot_prod.png){.lightbox fig-align="center"}    
:::
::: 

::: {.callout-important appearance="default" icon="false"}
## Attention
Dado que el rango de estos valores van de $-\infty$ a $\infty$, es más común aplicar una softmax para poder garantizar que la suma de las atenciones para cada palabra "query" sume 1.
:::

## Encoder: Self-Attention (Scale Dot Product) {.smaller}

![](img/clase-11/softmax_scale_dot.png){.lightbox fig-align="center" width="50%"}    

::: {.callout-note appearance="default" icon="false"}
## Attention Weights
* Esta matriz indica cuánta atención (en términos porcentuales) entrega cada palabra *"query"* a cada palabra *"key"*.
* Esta matriz permitirá crear embeddings contextualizados, que incluyen la información de la palabra y su contexto de atención.
:::

## Encoder: Self-Attention (Scale Dot Product) {.smaller}

$$Attention(Q,K,V) = Softmax\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) V$$

![](img/clase-11/self_attention_calculation.png){.lightbox fig-align="center" width="70%"}    

::: {.callout-warning appearance="default" icon="false"}
## OJO
* A modo de ejemplo, el elemento en Rojo representa una suma ponderada de la primera dimensión de cada proyección de tokens.
* El resultado de cada dimensión es una combinación lineal de las dimensiones de cada token.
* Cada fila corresponde a un embedding contextualizado que tiene información sobre el token y su contexto combinado.
:::

::: {.callout-important icon="false"}
***¿Y, estamos seguros que las atenciones/relaciones obtenidas por este algoritmos son (las) únicas/más correctas?***
:::


## Encoder: Multihead Attention {.smaller}

![](img/clase-11/multihead_attention.png){.lightbox fig-align="center" width="23%"}    

::: {.callout-tip appearance="default" icon="false"}
## Multihead Attention
Es una extensión del `Self-Attention`. En lugar de calcular sólo ***"una atención"*** sobre el input, genera distintas ***"atenciones"*** en múltiples ***"cabezas"*** independientes. Cada `Attention Head` se encarga de aprender relaciones diferentes, lo que mejora la capacidad del modelo de captar patrones cada vez más complejos.
:::
::: {.callout-important}
Normalmente se calculan entre $h=8$ y $h=12$ attention heads, las cuales se concatenan para luego pasar por una proyección lineal.
:::

## Encoder: Multihead Attention {.smaller}

::: {.callout-caution icon="false"}
Si queremos calcular $h=8$ attention heads. Necesitamos 8 $Q$, 8 $K$ y 8 $V$. Por lo tanto, necesitamos 8 matrices de proyección. ***¿Cómo lo paralelizamos?***
:::

::: {.columns}
::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Implementación en paralelo

Podemos definir en realidad todas las matrices de manera análoga rescribiendo las matrices de proyección para $Q$, $K$ y $V$ como una subdivisión de cada embedding en $h$ cabezas.

*  Por lo tanto si $d_k=d_v=64$ y $h=8$ tendríamos una dimensión total de 512. 
* $dim(Q) = dim(K) = (N,h \cdot d_k) = (6, 512)$
* $dim(V) = (N, h \cdot d_v) = (6, 512)$
* $dim(W_q) = (d_{model}, h, d_k)$
* $dim(W_k) = (d_{model}, h, d_k)$
* $dim(W_v) = (d_{model}, h, d_v)$

:::
:::
::: {.column}
::: {.callout-caution appearance="default" icon="false"}
## Dimensiones de Q, K y V
* $Q = X \cdot W_q = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
* $K = X \cdot W_k = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
* $V = X \cdot W_v = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
:::

:::
::: 

## Encoder: Multihead Attention (Independencia) {.smaller}


::: {.columns}
::: {.column}

::: {.callout-warning appearance="default" icon="false"}
## Independent Heads
Es importante mencionar que cada cabeza debe ser independiente una de otra para que se pueda paralelizar. Para ello basta con transponer las dos primeras dimensiones.

:::
::: {.callout-note appearance="default" icon="false"}
## Query/Key/Value (6, 8, 64) (Previo a Transponer)

$$
\begin{array}{c c} 
Q/K/V = \left[
\begin{array}{c c c}
\overbrace{[1,...,64]}^{Head 1}, \overbrace{[65,...,128]}^{Head 2}, ..., \overbrace{[449,...,512]}^{Head 8}\\
[1,...,64],[65,..., 128],...[449,...,512]\\
[1,...,64],[65,..., 128],...[449,...,512]\\
[1,...,64],[65,..., 128],...[449,...,512]\\
[1,...,64],[65,..., 128],...[449,...,512]\\
[1,...,64],[65,..., 128],...[449,...,512]\\
\end{array}
\right]
\end{array}
$$

:::
::: {.callout-caution icon="false" appearance="default"}
## OJO
Esto permite calcular cada Head en paralelo. Este procedimiento se aplica a cada secuencia. Por lo tanto, un Multihead Attention recibe Tensores de dimensión $(N,L,h \cdot d_i)$ con $i=k,v$.
:::
:::
::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Query/Key/Value (8,6, 64) (Luego de Transponer)

$$
\begin{array}{c c} 
\left[
\begin{array}{c c c}
\text{Head1}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
[1,...,64]\\
\end{bmatrix}
\end{array}
\right. \\
\vdots \\
\vdots \\
\text{Head8}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[449,...,512]\\
[449,...,512]\\
[449,...,512]\\
[449,...,512]\\
[449,...,512]\\
[449,...,512]\\
\end{bmatrix}
\end{array}
\right. \\
\end{array}
\right]
\end{array}
$$
:::
:::
::: 

## Encoder: Multihead Attention (Concatenación) {.smaller}

::: {.columns}
::: {.column width="50%"}
::: {.callout-note appearance="default" icon="false"}
## Self-Attentions (aka Multihead Attention) (6,8,64)

$$
\begin{array}{c c} 
\left[
\begin{array}{c c c}
\text{Head1}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[SA_1,...,SA_{64}]\\
[SA_1,...,SA_{64}]\\
[SA_1,...,SA_{64}]\\
[SA_1,...,SA_{64}]\\
[SA_1,...,SA_{64}]\\
[SA_1,...,SA_{64}]\\
\end{bmatrix}
\end{array}
\right. \\
\vdots \\
\vdots \\
\text{Head8}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[SA_{449},...,SA_{512}]\\
[SA_{449},...,SA_{512}]\\
[SA_{449},...,SA_{512}]\\
[SA_{449},...,SA_{512}]\\
[SA_{449},...,SA_{512}]\\
[SA_{449},...,SA_{512}]\\
\end{bmatrix}
\end{array}
\right. \\
\end{array}
\right]
\end{array}
$$
:::
:::
::: {.column width="50%"}
::: {.callout-important appearance="default" icon="false"}
## Self-Attention Transpuesto (6,8,64)
$$
\begin{array}{c c} 
\left[
\begin{array}{c c c}
\overbrace{[SA_1,...,SA_{64}]}^{Head 1}, \overbrace{[SA_{65},...,SA_{128}}^{Head 2}, ..., \overbrace{[SA_{449},...,SA_{512}]}^{Head 8}\\
[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\
[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\
[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\
[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\
[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\
\end{array}
\right]
\end{array}
$$
:::

::: {.callout-tip appearance="default" icon="false"}
## Self-Attention Concatenado (6,512)
$$
\begin{array}{c c} 
\left[
\begin{array}{c c c}
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
\end{array}
\right]
\end{array}
$$

:::

:::
::: 

## Encoder: Multihead Attention (Output) {.smaller}


::: {.callout-important appearance="default" icon="false"}
## Head Mixing
Los outputs de cada cabeza ahora están uno al lado del otro. Por lo tanto, si aplicamos una capa lineal $W^O \in \mathbb{R}^{d_v \cdot h \times d_{model}}$, estos parámetros entrenables se encargarán de aprender una combinación lineal que mezcla la información aprendida por cada Attention Head de manera óptima.

:::
::: {.callout-tip appearance="default" icon="false"}
## Multihead Attention Output (6,512)
$$
Multihead(Q,K,V) = \begin{array}{c c} 
\left[
\begin{array}{c c c}
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
[SA_1,.....,SA_{512}]\\
\end{array}
\right]
\end{array}
\cdot W^O
$$
:::

::: {.callout-caution}
$W^O$ se encarga de retornar a la dimensión del Input original para poder realizar la Residual Connection (similar al `downsample` de la Resnet).
:::


## Encoder: Add + LayerNorm {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/add_norm.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column}
::: {.callout-note appearance="default" icon="false"}
## Residual Connection (Add&Norm)

Corresponde a una conexión residual. Combina la información de entrada al Multihead y su salida para luego aplicar LayerNorm.

$$Add\&Norm = LayerNorm(X + Multihead(Q,K,V))$$

:::
::: {.callout-tip appearance="default" icon="false"}
## LayerNorm
El LayerNorm calcula el promedio y la varianza por token normalizando las dimensiones del embedding de cada token.

$$X_{norm} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}\cdot \gamma + \beta$$

:::
::: {.callout-important}
* $\gamma$ y $\beta$ son parámetros entrenables.
:::
::: {.callout-important appearance="default" icon="false"}
## Regularización
La sección 5.4 menciona que se aplica Dropout posterior a cada sublayer del Encoder (y el Decoder) con $P_{drop}=0.1$.
:::
:::
::: 

## Encoder: Feed Forward (MLP) {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/ffn.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column}
La sección 3.3 del paper define el bloque Feed Forward de la siguiente manera:

$$FFN(x) = max(0,x \cdot W_1+b_1)W_2 + b_2$$

Donde $W_1 \in \mathbb{R}^{d_{model} \times {d_{ff}}}$ y $W_2 \in \mathbb{R}^{d_{ff} \times {d_{model}}}$.


::: {.callout-note icon="false" appearance="default"} 
## Arquitectura 

* 2 capas Feed Forward con bias. 
* Una RelU como activación intermedia.
* De acuerdo a la sección 5.4, a la salida incluiría un Dropout con $P_{drop}=0.1$.
:::

::: {.callout-important appearance="default" icon="false"}
## Residual Connection (Add&Norm)
Al igual que en el Multihead Attention, se la salida de esta capa se une con una conexión residual y se pasa por un LayerNorm.
:::
:::
::: 

## Encoder: Output Final {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/encoder.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column}
::: {.callout-note appearance="default" icon="false"}
## Encoder Layers
* La combinación de todos los pasos anteriores constituyen un (1) Encoder. En el caso del paper el Transformer está compuesto de $N=6$ Encoder Layers uno después del otro.

:::
::: {.callout-important appearance="default" icon="false"}
## 👀 Ojito
* Sólo antes de la primera Encoder Layer se aplica el Input Embedding y el Positional Encoding.
:::
::: {.callout-tip appearance="default" icon="false"}
## Arquitectura Encoder-Decoder
En el caso de estas arquitecturas, entonces el output del Encoder sirve como Keys y Values para el proceso de Cross Attention.
:::
:::
::: 

## Decoder: Causal Self-Attention {.smaller}


::: {.columns}
::: {.column width=20%}
![](img/clase-11/causal_mh.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column width=80%}

$$Attention(Q,K,V) = Softmax\left(\frac{Q \cdot K^T + Mask}{\sqrt{d_k}}\right) V$$

<br>

::: {.callout-warning}
Corresponde a una variante del `Self-Attention` en el cuál sólo se presta atención a Tokens pasados, esto para preservar las propiedades auto-regresivas.
:::


:::
::: 

![](img/clase-11/causal.png){.lightbox fig-align="center" height="110%"}    



## Decoder: Cross Attention {.smaller}

::: {.columns}
::: {.column width="40%"}
![](img/clase-11/cross.png){.lightbox fig-align="center" height="110%"}    

::: {.callout-caution}
Opcionalmente podría utilizar una Máscara en caso de querer evitar el Look Ahead.
:::

:::
::: {.column width="60%"}
::: {.callout-warning appearance="default" icon="false"}
## Cross Attention

Este mecanismo permite generar relaciones/atenciones entre dos secuencias de datos distintos. En este caso se relaciona una secuencia *"query"* con elementos *"key"* y *"values"* de otra secuencia. Además `limita` la generación del Decoder.
:::


::: {.callout-note appearance="default" icon="false"}
## Dimensiones de Q, K y V
* $Q = X_{decoder} \cdot W_q = (F,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
* $K = X_{encoder} \cdot W_k = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
* $V = X_{encoder} \cdot W_v = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
:::
:::
::: 

$$Attention(Q_{decoder},K_{encoder},V_{encoder}) = Softmax\left(\frac{Q_{decoder} \cdot K_{encoder}^T + Mask}{\sqrt{d_k}}\right) V_{encoder}$$


## Prediction Head {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/pred_head.png){.lightbox fig-align="center" width="70%"}   
:::
::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Arquitectura
Corresponde a una capa Feed Forward que proyecta desde $d_{model}$ hasta $vocab\_size$ seguida de una Softmax.

:::

::: {.callout-tip appearance="default" icon="false"}
## ¿Por qué es necesaria?
Por que la salida del Decoder tiene dimensiones $(N, L, d_{model})$. Es decir, tenemos $N$ secuencias de largo $L$, donde cada token está representado como un embedding de $d_{model}$ dimensiones, lo cuál no es interpretable por humanos.

Esta capa tiene el objetivo de estimar la probabilidad de que ocurra el siguiente token, de este modo predecir de manera autoregresiva.

:::
:::
::: 

# Gracias Totales

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
