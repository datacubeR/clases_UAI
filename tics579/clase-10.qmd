---
title: "TICS-579-Deep Learning"
subtitle: "Clase 10: Transformers"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Transformers (Attention is all you need, 2017) {.smaller}

::: {.columns}
::: {.column width="60%"}
Transformers
: Corresponden a la arquitectura m谩s moderna dise帽ada al d铆a de hoy. Est谩 basado en mecanismos de atenci贸n y posee hasta 4 tipos de atenci贸n distintos.

::: {.callout-note appearance="default" icon="false"}
## Ventajas

* No tiene problemas de "memoria" para modelar dependencias de largo plazo.
* Permite procesamiento en paralelo (No requiere entregar elementos de la secuencia uno a uno).
* Su baj铆simo ***inductive bias*** le permite adaptarse a distintos dominios.
* No es necesario utilizar el transformer completo para un problema en espec铆fico.
:::

::: {.callout-important appearance="default" icon="false"}
## Desventajas

* Apto s贸lo para datos secuenciales.
* Las secuencias deben de ser del mismo largo.
* Alta demanda de recursos computacionales GPU y/o TPUs para entrenamiento distribuido.
* Data hungry.
* ~~Limitaciones de secuencias muy largas por restricciones de memoria computacional.~~
:::
:::
::: {.column width="40%"}
![](img/clase-10/transformer.png){.lightbox fig-align="center"}   
:::
::: 

## Encoder {.smaller}

::: {.columns}
::: {.column width="40%"}
![](img/clase-11/encoder.png){.lightbox fig-align="center" width="70%"}   
:::
::: {.column width="60%"}
::: {.callout-tip appearance="default" icon="false"}
## Objetivo
Codificar y comprimir informaci贸n en Logits que puedan ser usados para clasificar o para ser utilizados por un Decoder.
:::
::: {.callout-important appearance="default" icon="false"}
## Forward Pass en el Encoder
* El embedding asociado a una secuencia se bifurca en 4 ramas:
  * Residual Connection
  * Query
  * Key
  * Value
* Query, Key y Value ingresan al ***Multihead Attention***.
* La salida del Multihead Attention + el Residual Connection pasan por un LayerNorm.
* Nueva bifurcaci贸n. 
  * Una parte entra a un MLP 
  * Otra va como skip connection.
* La salida del MLP + la Residual Connection pasan por un segundo LayerNorm para generar la salida del Encoder.

:::
::: {.callout-caution appearance="default" icon="false"}
## 锔 Con el tiempo han aparecido distintas variantes del Transformer (pre y post normalizaci贸n, distintas variantes de la atenci贸n y del positional Encodeing), pero la idea general se mantiene. 
:::

:::
::: 

## Decoder {.smaller}

::: {.columns}
::: {.column width="40%"}
![](img/clase-11/decoder.png){.lightbox fig-align="center" width="70%"}   
:::
::: {.column width="60%"}
::: {.callout-tip appearance="default" icon="false"}
## Objetivo
Tomar informaci贸n de entrada y generar una salida fij谩ndose s贸lo en tokens pasados. 
:::
::: {.callout-important appearance="default" icon="false"}
## Forward Pass en el Decoder
* El embedding asociado a una secuencia se bifurca en 4 ramas:
  * Residual Connection
  * Query
  * Key
  * Value
* Query, Key y Value ingresan al ***Causal Multihead Attention***.
* La salida del Causal Multihead Attention + el Residual Connection pasan por LayerNorm.
* Se pasa por un ***Cross Attention*** (esto podr铆a ser opcional).
  * Key y Value provienen del Encoder como contexto.
  * La salida del Causal Multihead Attention se utiliza como Query.
* Nueva bifurcaci贸n.
  * Una parte entra a un MLP 
  * Otra va como skip connection.
* La salida del MLP + la Residual Connection pasan por LayerNorm para generar el Output.

:::

:::
::: 

## Ejemplo: Tokenizaci贸n y Embedding {.smaller}

:::: {.columns}
::: {.column}
Supongamos que tenemos la siguiente frase: 

> Me gusta la pizza de Pepperoni

Suponiendo cada palabra como un token, tenemos una secuencia de largo $L=6$. Su tokenizaci贸n podr铆a ser algo como:

> [105,6587,5475,301,708,358]

::: {.callout-important appearance="default" icon="false"}
## Dependiendo del modelo tambi茅n aplica la adici贸n de tokens especiales seg煤n corresponda.
:::

::: {.callout-note}
## Luego cada token es transformado en un Embedding. En el caso del paper original, el embedding tiene $d_{model}=512$ dimensiones.
:::

::: {.callout-caution}
En la secci贸n 3.4 del paper se menciona que los par谩metros de los embeddings son multiplicados por $\sqrt{d_{model}}$.
:::
:::
::: {.column}
![](img/clase-10/embed_example.png){.lightbox fig-align="center"}   
:::
::::

## Positional Encoder {.smaller}

::: {.columns}
::: {.column}
::: {.callout-caution}
Un potencial problema que puede tener un transformer es reconocer el orden de las frases. 
:::

::: {.callout-warning}
No es lo mismo decir **"El perro del pap谩 mordi贸 al ni帽o"** que **"El perro del ni帽o mordi贸 al pap谩"**. Las palabras usadas en ambas frases son exactamente las mismas, pero en un orden distinto implican desenlaces distintos. ***驴C贸mo podemos entender el concepto de orden si no tenemos recurrencia?***
:::
::: {.callout-important}
Incluso algunos 贸rdenes no tienen tanto sentido l贸gico: **"El ni帽o del perro mordi贸 al pap谩"**.
:::

Positional Encoder
: > Corresponden a una manera en la que se pueden generar un **vector 煤nico** que representa el orden en el que aparece cada token.
:::
::: {.column}
![](img/clase-11/transformer_pe.png){.lightbox fig-align="center" width="75%"}   
:::
::: 

## Positional Encoder {.smaller}

::: {.columns}
::: {.column}
$$PE_{(pos,2i)} = sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos,2i+1)} = cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
:::
::: {.column}
::: {.callout-note icon="false"}
* $pos$ corresponde a la posici贸n del Token en la secuencia, y $2i$ y $2i+1$ corresponden a las posiciones de la dimensi贸n del embedding de cada token ($d_{model}$). $i$ comienza en 0.
:::
::: {.callout-caution}
Una forma m谩s clara de ver esto es que la posici贸n est谩 definida por sinusoidales de periodo $2\pi \cdot 10000^{i/d_{model}/2}$.
:::
:::
::: 

::: {.callout-warning}
El positional encoder debe tener el mismo tama帽o que el Embedding para que se puedan sumar.
:::

::: {.callout-tip appearance="default" icon="false"}
## Estabilidad num茅rica
Por temas de estabilidad el argumento del $sin(\cdot)$ y $cos(\cdot)$ se suele implementar como: 
$$pos \cdot exp\left[-\frac{2i}{d_{model}} log(10000)\right]$$

:::
::: {.callout-important appearance="default" icon="false"}
## Regularizaci贸n
La secci贸n 5.4 menciona que se aplica Dropout posterior a sumar los Embeddings con el Positional Encoding. Se utilizo un $P_{drop}=0.1$.
:::


## Positional Encoder: Ejemplo {.smaller}

:::::: {.columns}

Supongamos nuestra secuencia de tokens: [me, gusta, la, pizza, de, pepperoni]

::: {.callout-tip appearance="default" icon="false"}
## Para simplificar el problema consideremos que nuestros embeddings, y por ende nuestro positional encodings, tienen s贸lo 4 dimensiones ($d_{model}=4$).
:::
::: {.column width="30%"}
![](img/clase-11/pe_1.png){.lightbox fig-align="center" width="50%"}   

![](img/clase-11/pe_2.png){.lightbox fig-align="center" width="50%"}   
:::
::: {.column style="font-size: 70%" width="70%"}
$$PE_{pos=0} = [sin(0), cos(0), sin(0), cos(0)] = [0, 1, 0, 1]$$
$$PE_{pos=1} = [sin(\frac{1}{10000^{0/4}}), cos(\frac{1}{10000^{0/4}}), sin(\frac{1}{10000^{2/4}}), cos(\frac{1}{10000^{2/4}})] = [0.8415, 0.5403, 0.0099, 1]$$

$$PE_{pos=2} = [sin(\frac{2}{10000^{0/4}}), cos(\frac{2}{10000^{0/4}}), sin(\frac{2}{10000^{2/4}}), cos(\frac{2}{10000^{2/4}})] = [0.9093, -0.4161, 0.02, 0.9998]$$

$$PE_{pos=3} = [sin(\frac{3}{10000^{0/4}}), cos(\frac{3}{10000^{0/4}}), sin(\frac{3}{10000^{2/4}}), cos(\frac{3}{10000^{2/4}})] = [0.1411, -0.99, 0.03, 1]$$

$$PE_{pos=4} = [sin(\frac{4}{10000^{0/4}}), cos(\frac{4}{10000^{0/4}}), sin(\frac{4}{10000^{2/4}}), cos(\frac{4}{10000^{2/4}})] = [-0.7568, -0.6536, 0.04, 0.9992]$$

$$PE_{pos=5} = [sin(\frac{5}{10000^{0/4}}), cos(\frac{5}{10000^{0/4}}), sin(\frac{5}{10000^{2/4}}), cos(\frac{5}{10000^{2/4}})] = [-0.9589, 0.2837, 0.05, 0.99875]$$
:::
::: 




## Positional Encoder: Ejemplo {.smaller}

Por ejemplo, la palabra Pizza quedar谩 definida como:

$$\begin{align}
Input = Embedding(Pizza) + PE_{pos=3} &= [0.3, -0.2, 0.7, 0.1] + [0.1411, -0.99, 0.03, 1] \\
&= [0.4411, -1.19, 0.73, 1.1]
\end{align}$$

::: {.callout-tip appearance="default" icon="false"}
## Este input ahora almacena la informaci贸n tanto de la palabra como de su posici贸n relativa en la secuencia.
:::

:::{.callout-note appearance="default" icon="false"}
## Todos los Positional Encoding son id茅nticos para cada posici贸n, y se encargan de realzar o atenuar ciertas dimensiones del embedding seg煤n la posici贸n del token en la secuencia. En la implementaci贸n original estos vectores son fijos y no entrenables.
:::

:::{.callout-caution appearance="default" icon="false"}
## Implementaci贸n Eficiente
En otras implementaciones modernas como los modelos GPT, estos son Embeddings y son entrenables. Cada fila es un embedding de $d_{model}$ dimensiones asociado a cada posici贸n.
:::


## Encoder: Self-Attention {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/scale_dot_product.png){.lightbox fig-align="center" width="55%"}   

$$Attention(Q,K,V) = Softmax\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) V$$

::: {.callout-note appearance="default" icon="false"}
## Ejemplo
*"La sopa se cocin贸 en la olla y estaba rica"*. *Rica* podr铆a estar refiri茅ndose a *olla* o a *sopa*. Sabemos que se refiere a la sopa.
:::
:::
::: {.column}
::: {.callout-warning icon="true"}

El Scaled Dot-Product, m谩s conocido como `Self-Attention`, es el mecanismo clave en las redes neuronales modernas. Permite determinar la atenci贸n/relaci贸n que existe entre palabras de una misma secuencia.
:::

::: {.callout-note}
* Est谩 compuesto por 3 proyecciones lineales las cuales reciben los nombres de Query (Q), Key (K) y Value (V).
* Est谩s 3 proyecciones se combinan para poder determinar la atenci贸n/relaci贸n que cada Token tiene con los otros tokens de una misma secuencia.
* Varios procesos de `Self-Attention` dan pie al `Multihead Attention`.
:::
::: {.callout-important}
* El `Self-Attention` tiene la capacidad de acceder a toda la secuencia, por ende modelar relaciones a larga distancia.
* El `Causal Self-Attention`, una variante que se utiliza en el Decoder s贸lo puede ver la relaci贸n con tokens pasados. 
:::
::: {.callout-tip}
* Su caracter铆stica m谩s importante es que el `Multihead Attention` es paralelizable y no secuencial como las RNN.
* Tiene capacidad de escalabilidad para secuencias largas. 
:::
:::
::: 

## Encoder: Self-Attention {.smaller}

::: {.callout-caution}
* Supongamos que tenemos la secuencia *"Me gusta la Pizza de Pepperoni"*. 
* Utilizaremos $d_{model} = 512$ y $h=8$.
* El paper utiliza $d_k=d_v=d_{model}/h=64$ para el c谩lculo de los Attention Weights.
:::

::: {.columns}
::: {.column width="40%"}
$$
\begin{array}{c c} 
X = \left[
\begin{array}{c c c}
[a_1,...,a_{256}]\\
[b_1,...,b_{256}]\\
[c_1,...,c_{256}]\\
[d_1,...,d_{256}]\\
[e_1,...,e_{256}]\\
[f_1,...,f_{256}]\\
\end{array}
\right]
\begin{array}{c c c}Me\\gusta\\la\\pizza\\de\\pepperoni \end{array} &
\end{array}
$$

::: {.callout-warning appearance="default" icon="false" .fragment}
##  Ojito

Esto se debe aplicar a cada secuencia. Por lo tanto se debe agregar una dimensi贸n (como unsqueeze(0)) que contabilice el n煤mero de secuencias para $Q$, $K$, y $V$.

:::

:::
::: {.column width="60%"}

::: {.callout-important appearance="default" icon="false"}
## Matrices de Proyecci贸n

Definiremos 3 matrices de Proyecci贸n. Una matriz de proyecci贸n permite llevar transportar un vector $X$ a otro espacio (es decir, son entrenables). En este caso crearemos matrices que puedan multiplicarse con $X$. Por lo tanto ir谩n desde $d_{model}$ hasta $d_q=d_k$ y $d_v$ respectivamente.

* $W_q = (d_{model}, d_k)$ 
* $W_k = (d_{model}, d_k)$ 
* $W_v = (d_{model}, d_v)$ 
:::

::: {.callout-note appearance="default" icon="false"}
## Dimensiones de Q,K y V
* $Q = (L, d_k)$
* $K = (L, d_k)$
* $V = (L, d_v)$
* $L$ corresponde a largo de la secuencia (es decir, el n煤mero de Tokens)
:::

:::
:::
::: 

## Encoder: Self-Attention {.smaller}

> Siguiendo nuestro ejemplo: $d_k = d_v = 64$

::: {.columns}
::: {.column width="30%"}
::: {.callout-note appearance="default" icon="false"}
## Query (6,64)

$$
\begin{array}{c c} 
Q = \left[
\begin{array}{c c c}
[qa_1,...,qa_{64}]\\
[qb_1,...,qb_{64}]\\
[qc_1,...,qc_{64}]\\
[qd_1,...,qd_{64}]\\
[qe_1,...,qe_{64}]\\
[qf_1,...,qf_{64}]\\
\end{array}
\right]
\end{array}
$$

:::
:::
::: {.column width="30%"}
::: {.callout-caution appearance="default" icon="false"}
## Key (6,64)

$$
\begin{array}{c c} 
K = \left[
\begin{array}{c c c}
[ka_1,...,ka_{64}]\\
[kb_1,...,kb_{64}]\\
[kc_1,...,kc_{64}]\\
[kd_1,...,kd_{64}]\\
[ke_1,...,ke_{64}]\\
[kf_1,...,kf_{64}]\\
\end{array}
\right]
\end{array}
$$

:::
:::
::: {.column width="30%"}
::: {.callout-tip appearance="default" icon="false"}
## Value (6,64)

$$
\begin{array}{c c} 
V = \left[
\begin{array}{c c c}
[va_1,...,va_{64}]\\
[vb_1,...,vb_{64}]\\
[vc_1,...,vc_{64}]\\
[vd_1,...,vd_{64}]\\
[ve_1,...,ve_{64}]\\
[vf_1,...,vf_{64}]\\
\end{array}
\right]
\end{array}
$$

:::
:::
::: 

::: {.callout-warning appearance="default" icon="false" .fragment}
##  Ojito

Como esto se aplica a una sola secuencia, la dimensi贸n real de estos tensores deber铆a ser $(1,6,64)$. Ese 1 cambiar谩 si tenemos m谩s secuencias. Pero, ***todas las secuencias deben ser del mismo largo***.

:::

## Encoder: Self-Attention (Scale Dot Product) {.smaller}

::: {.columns}
::: {.column width="40%"}
$$\frac{Q \cdot K^T}{\sqrt{d_k}}$$

::: {.callout-important appearance="default" icon="false"}
## Similaridad
* $Q \cdot K^T$ representa el producto punto entre $Q$ (un token de referencia que est谩 consultando la atenci贸n contra otros tokens) y $K$ (otro token que se compara contra la "query").
:::

::: {.callout-tip appearance="default" icon="false"}
## Control de Gradientes
$\sqrt{d_k}$ es un factor que reduce la escala de los valores para el control de los gradientes. Recordar que esta matriz es de par谩metros entrenables.
:::

:::
::: {.column width="60%"}
![](img/clase-11/dot_prod.png){.lightbox fig-align="center"}    
:::
::: 

::: {.callout-important appearance="default" icon="false"}
## Attention
Dado que el rango de estos valores van de $-\infty$ a $\infty$, es m谩s com煤n aplicar una softmax para poder garantizar que la suma de las atenciones para cada palabra "query" sume 1.
:::

## Encoder: Self-Attention (Scale Dot Product) {.smaller}

![](img/clase-11/softmax_scale_dot.png){.lightbox fig-align="center" width="50%"}    

::: {.callout-note appearance="default" icon="false"}
## Attention Weights
* Tendremos un tensor de $(N,L,L)$ que representa las atenciones entre cada palabra para cada secuencia.
* Se debe entender c贸mo qu茅 tan relevante es cada palabra *"key"* para cada palabra *"query"*.
* Esta matriz permitir谩 crear embeddings contextualizados, que incluyen la informaci贸n de la palabra y su contexto de atenci贸n.
:::

## Encoder: Self-Attention (Scale Dot Product) {.smaller}

$$Attention(Q,K,V) = Softmax\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) V$$

![](img/clase-11/self_attention_calculation.png){.lightbox fig-align="center" width="70%"}    

::: {.callout-warning appearance="default" icon="false"}
## OJO
* A modo de ejemplo, el elemento en Rojo representa una suma ponderada de la primera dimensi贸n de cada proyecci贸n de tokens.
* El resultado de cada dimensi贸n es una combinaci贸n lineal de las dimensiones de cada token.
* Cada fila corresponde a un embedding contextualizado que tiene informaci贸n sobre el token y su contexto combinado.
:::

::: {.callout-important icon="false"}
***驴Y, estamos seguros que las atenciones/relaciones obtenidas por este algoritmos son (las) 煤nicas/m谩s correctas?***
:::


## Encoder: Multihead Attention {.smaller}

![](img/clase-11/multihead_attention.png){.lightbox fig-align="center" width="23%"}    

::: {.callout-tip appearance="default" icon="false"}
## Multihead Attention
Es una extensi贸n del `Self-Attention`. En lugar de calcular s贸lo ***"una atenci贸n"*** sobre el input, genera distintas ***"atenciones"*** en m煤ltiples ***"cabezas"*** independientes. Cada `Attention Head` se encarga de aprender relaciones diferentes, lo que mejora la capacidad del modelo de captar patrones cada vez m谩s complejos.
:::
::: {.callout-important}
Normalmente se calculan entre $h=8$ y $h=12$ attention heads, las cuales se concatenan para luego pasar por una proyecci贸n lineal.
:::

## Encoder: Multihead Attention {.smaller}

::: {.callout-caution icon="false"}
Si queremos calcular $h=8$ attention heads. Necesitamos 8 $Q$, 8 $K$ y 8 $V$. Por lo tanto, necesitamos 8 matrices de proyecci贸n. ***驴C贸mo lo paralelizamos?***
:::

::: {.columns}
::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Implementaci贸n en paralelo

Podemos definir en realidad todas las matrices de manera an谩loga rescribiendo las matrices de proyecci贸n para $Q$, $K$ y $V$ como una subdivisi贸n de cada embedding en $h$ cabezas.

*  Por lo tanto si $d_k=d_v=64$ y $h=8$ tendr铆amos una dimensi贸n total de 512. 
* $dim(Q) = dim(K) = (L,h \cdot d_k) = (6, 512)$
* $dim(V) = (L, h \cdot d_v) = (6, 512)$
* $dim(W_q) = (d_{model}, h, d_k)$
* $dim(W_k) = (d_{model}, h, d_k)$
* $dim(W_v) = (d_{model}, h, d_v)$

:::
:::
::: {.column}
::: {.callout-caution appearance="default" icon="false"}
## Dimensiones de Q, K y V
* $Q = X \cdot W_q = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,64)$
* $K = X \cdot W_k = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,64)$
* $V = X \cdot W_v = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,64)$
:::

:::
::: 

## Encoder: Multihead Attention (Independencia) {.smaller}


::: {.columns}
::: {.column}

::: {.callout-warning appearance="default" icon="false"}
## Independent Heads
Es importante mencionar que cada cabeza debe ser independiente una de otra para que se pueda paralelizar. Para ello basta con transponer las dos primeras dimensiones.

:::
::: {.callout-note appearance="default" icon="false"}
## Query/Key/Value (6, 8, 64) (Previo a Transponer)

$$
\begin{array}{c c} 
Q/K/V = \left[
\begin{array}{c c c}
\overbrace{[ia_{1},...,ia_{64}]}^{Head 1}, \overbrace{[ia_{65},...,ia_{128}]}^{Head 2}, ..., \overbrace{[ia_{449},...,ia_{512}]}^{Head 8}\\
[ib_{1},...,ib_{64}],[ib_{65},..., ib_{128}],...[ib_{449},...,ib_{512}]\\
[ic_{1},...,ic_{64}],[ic_{65},..., ic_{128}],...[ic_{449},...,ic_{512}]\\
[id_{1},...,id_{64}],[id_{65},..., id_{128}],...[id_{449},...,id_{512}]\\
[ie_{1},...,ie_{64}],[ie_{65},..., ie_{128}],...[ie_{449},...,ie_{512}]\\
[if_{1},...,if_{64}],[if_{65},..., if_{128}],...[if_{449},...,if_{512}]\\
\end{array}
\right]
\end{array}
$$

:::
::: {.callout-caution icon="false" appearance="default"}
## OJO
Esto permite calcular cada Head en paralelo. Este procedimiento se aplica a cada secuencia. Por lo tanto, un Multihead Attention recibe Tensores de dimensi贸n $(N,L,h \cdot d_i)$ con $i=k,v$.
:::
:::
::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Query/Key/Value (8,6, 64) (Luego de Transponer)

$$
Q/K/V = \begin{array}{c c} 
\left[
\begin{array}{c c c}
\text{Head1}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[ia_{1},...,ia_{64}]\\
[ib_{1},...,ib_{64}]\\
[ic_{1},...,ic_{64}]\\
[id_{1},...,id_{64}]\\
[ie_{1},...,ie_{64}]\\
[if_{1},...,if_{64}]\\
\end{bmatrix}
\end{array}
\right. \\
\vdots \\
\vdots \\
\text{Head8}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[ia_{449},...,ia_{512}]\\
[ib_{449},...,ib_{512}]\\
[ic_{449},...,ic_{512}]\\
[id_{449},...,id_{512}]\\
[ie_{449},...,ie_{512}]\\
[if_{449},...,if_{512}]\\
\end{bmatrix}
\end{array}
\right. \\
\end{array}
\right]
\end{array}
$$

:::
::: {.callout-tip}
## Luego de Transponer, entonces Q/K/V tienen dimensiones $(N, h, L, d_i)$. Luego se puede aplicar el Self-Attention en paralelo para cada cabeza y para cada secuencia.
:::
:::
::: 

## Encoder: Multihead Attention (Concatenaci贸n) {.smaller}

::: {.columns}
::: {.column width="40%"}
::: {.callout-note appearance="default" icon="false"}
## Self-Attentions (aka Multihead Attention) (8,6,64)

$$
MSA = \begin{array}{c c} 
\left[
\begin{array}{c c c}
\text{Head1}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[SAa_1,...,SAa_{64}]\\
[SAb_1,...,SAb_{64}]\\
[SAc_1,...,SAc_{64}]\\
[SAd_1,...,SAd_{64}]\\
[SAe_1,...,SAe_{64}]\\
[SAf_1,...,SAf_{64}]\\
\end{bmatrix}
\end{array}
\right. \\
\vdots \\
\vdots \\
\text{Head8}\left\{
\begin{array}{c c c}
\begin{bmatrix}
[SAa_{449},...,SAa_{512}]\\
[SAb_{449},...,SAb_{512}]\\
[SAc_{449},...,SAc_{512}]\\
[SAd_{449},...,SAd_{512}]\\
[SAe_{449},...,SAe_{512}]\\
[SAf_{449},...,SAf_{512}]\\
\end{bmatrix}
\end{array}
\right. \\
\end{array}
\right]
\end{array}
$$
:::
:::
::: {.column width="60%"}
::: {.callout-important appearance="default" icon="false"}
## Self-Attention Transpuesto (6,8,64)
$$
MSA = \begin{array}{c c} 
\left[
\begin{array}{c c c}
\overbrace{[SAa_1,...,SAa_{64}]}^{Head 1}, \overbrace{[SAa_{65},...,SAa_{128}}^{Head 2}, ..., \overbrace{[SAa_{449},...,SAa_{512}]}^{Head 8}\\
[SAb_1,...,SAb_{128}],[SAb_{65},..., SAb_{128}],...[SAb_{449},...,SAb_{512}]\\
[SAc_1,...,SAc_{128}],[SAc_{65},..., SAc_{128}],...[SAc_{449},...,SAc_{512}]\\
[SAd_1,...,SAd_{128}],[SAd_{65},..., SAd_{128}],...[SAd_{449},...,SAd_{512}]\\
[SAe_1,...,SAe_{128}],[SAe_{65},..., SAe_{128}],...[SAe_{449},...,SAe_{512}]\\
[SAf_1,...,SAf_{128}],[SAf_{65},..., SAf_{128}],...[SAf_{449},...,SAf_{512}]\\
\end{array}
\right]
\end{array}
$$

:::

::: {.callout-tip appearance="default" icon="false"}
## Self-Attention Concatenado (6,512)

Hint: La concatenaci贸n en este caso equivale a un reshape de las dos 煤ltimas dimensiones.
$$
MSA = \begin{array}{c c} 
\left[
\begin{array}{c c c}
[SAa_1,.....,SAa_{512}]\\
[SAb_1,.....,SAb_{512}]\\
[SAc_1,.....,SAc_{512}]\\
[SAd_1,.....,SAd_{512}]\\
[SAe_1,.....,SAe_{512}]\\
[SAf_1,.....,SAf_{512}]\\
\end{array}
\right]
\end{array}
$$

:::

:::
::: 

## Encoder: Multihead Attention (Output) {.smaller}


::: {.callout-important appearance="default" icon="false"}
## Head Mixing
Los outputs de cada cabeza ahora est谩n uno al lado del otro. Por lo tanto, si aplicamos una capa lineal $W^O  de dimensi贸n $(d_v \times h, d_{model}$), estos par谩metros entrenables se encargar谩n de aprender una combinaci贸n lineal que mezcle la informaci贸n aprendida por cada Attention Head de manera 贸ptima.

:::
::: {.callout-tip appearance="default" icon="false"}
## Multihead Attention Output (6,512)
$$
Multihead(Q,K,V) = \begin{array}{c c} 
\left[
\begin{array}{c c c}
[SAa_1,.....,SAa_{512}]\\
[SAb_1,.....,SAb_{512}]\\
[SAc_1,.....,SAc_{512}]\\
[SAd_1,.....,SAd_{512}]\\
[SAe_1,.....,SAe_{512}]\\
[SAf_1,.....,SAf_{512}]\\
\end{array}
\right]
\end{array}
\cdot W^O
$$
:::

::: {.callout-tip}
## El Multihead Attention Output tiene dimensiones $(N, L, d_{model})$.
:::


## Encoder: Add + LayerNorm {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/add_norm.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column}
::: {.callout-note appearance="default" icon="false"}
## Residual Connection (Add&Norm)

Corresponde a una conexi贸n residual. Combina la informaci贸n de entrada al Multihead y su salida para luego aplicar LayerNorm.

$$Add\&Norm = LayerNorm(X + Multihead(Q,K,V))$$

:::
::: {.callout-tip appearance="default" icon="false"}
## LayerNorm
El LayerNorm calcula el promedio y la varianza por token normalizando las dimensiones del embedding de cada token.

$$X_{norm} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}\cdot \gamma + \beta$$

:::
::: {.callout-important}
* $\gamma$ y $\beta$ son par谩metros entrenables.
:::
::: {.callout-important appearance="default" icon="false"}
## Regularizaci贸n
La secci贸n 5.4 menciona que se aplica Dropout posterior a cada sublayer del Encoder (y el Decoder) con $P_{drop}=0.1$.
:::
:::
::: 

## Encoder: Feed Forward (MLP) {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/ffn.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column}
La secci贸n 3.3 del paper define el bloque Feed Forward de la siguiente manera:

$$FFN(x) = max(0,x \cdot W_1+b_1)W_2 + b_2$$

Donde $W_1 \in \mathbb{R}^{d_{model} \times {d_{ff}}}$ y $W_2 \in \mathbb{R}^{d_{ff} \times {d_{model}}}$. Implementaciones como GPT consideran $d_{ff}=4 \cdot d_{model}$.


::: {.callout-note icon="false" appearance="default"} 
## Arquitectura 

* 2 capas Feed Forward con bias. 
* Una ReLU como activaci贸n intermedia.
* De acuerdo a la secci贸n 5.4, a la salida incluir铆a un Dropout con $P_{drop}=0.1$.
:::

::: {.callout-important appearance="default" icon="false"}
## Residual Connection (Add&Norm)
Al igual que en el Multihead Attention, se la salida de esta capa se une con una conexi贸n residual y se pasa por un LayerNorm.
:::
:::
::: 

## Encoder: Output Final {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/encoder.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column}
::: {.callout-note appearance="default" icon="false"}
## Encoder Layers
* La combinaci贸n de todos los pasos anteriores constituyen un (1) Encoder. En el caso del paper el Transformer est谩 compuesto de $N=6$ Encoder Layers uno despu茅s del otro.

:::
::: {.callout-important appearance="default" icon="false"}
##  Ojito
* S贸lo antes de la primera Encoder Layer se aplica el Input Embedding y el Positional Encoding.
:::
::: {.callout-note appearance="default" icon="false"}
## Arquitectura Encoder Only
Utilizan el output como input de una prediction Head para tareas de clasificaci贸n o regresi贸n.
:::

::: {.callout-tip appearance="default" icon="false"}
## Arquitectura Encoder-Decoder
En el caso de estas arquitecturas, entonces el output del Encoder sirve como Keys y Values para el proceso de Cross Attention.
:::
:::
::: 

## Decoder: Causal Self-Attention {.smaller}


::: {.columns}
::: {.column width=20%}
![](img/clase-11/causal_mh.png){.lightbox fig-align="center" width="70%"}    
:::
::: {.column width=80%}

$$Attention(Q,K,V) = Softmax\left(\frac{Q \cdot K^T + Mask}{\sqrt{d_k}}\right) V$$

<br>

::: {.callout-warning}
Corresponde a una variante del `Self-Attention` en el cu谩l s贸lo se presta atenci贸n a Tokens pasados, esto para preservar las propiedades auto-regresivas.
:::

::: {.callout-important}
## Implementaciones como GPT utilizan una m谩scara triangular superior de -$\infty$ y 1s en el resto. Luego esta m谩scara se multiplica antes de aplicar la Softmax.
:::

:::
::: 

![](img/clase-11/causal.png){.lightbox fig-align="center" height="110%"}    



## Decoder: Cross Attention {.smaller}

::: {.columns}
::: {.column width="40%"}
![](img/clase-11/cross.png){.lightbox fig-align="center" height="110%"}    

::: {.callout-caution}
Opcionalmente podr铆a utilizar una M谩scara en caso de querer evitar el Look Ahead.
:::

:::
::: {.column width="60%"}
::: {.callout-warning appearance="default" icon="false"}
## Cross Attention

Este mecanismo permite generar relaciones/atenciones entre dos secuencias de datos distintos. En este caso se relaciona una secuencia *"query"* con elementos *"key"* y *"values"* de otra secuencia. Adem谩s `limita` la generaci贸n del Decoder.
:::


::: {.callout-note appearance="default" icon="false"}
## Dimensiones de Q, K y V
* $Q = X_{decoder} \cdot W_q = (F,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
* $K = X_{encoder} \cdot W_k = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
* $V = X_{encoder} \cdot W_v = (6,512) \cdot (512,\overbrace{8,64}^{512}) = (6,8,512)$
:::
:::
::: 

$$Attention(Q_{decoder},K_{encoder},V_{encoder}) = Softmax\left(\frac{Q_{decoder} \cdot K_{encoder}^T + Mask}{\sqrt{d_k}}\right) V_{encoder}$$


## Prediction Head {.smaller}

::: {.columns}
::: {.column}
![](img/clase-11/pred_head.png){.lightbox fig-align="center" width="70%"}   
:::
::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Arquitectura
Corresponde a una capa Feed Forward que proyecta desde $d_{model}$ hasta $vocab\_size$ seguida de una Softmax.

:::

::: {.callout-tip appearance="default" icon="false"}
## 驴Por qu茅 es necesaria?
Por que la salida del Decoder tiene dimensiones $(N, L, d_{model})$. Es decir, tenemos $N$ secuencias de largo $L$, donde cada token est谩 representado como un embedding de $d_{model}$ dimensiones, lo cu谩l no es interpretable por humanos.

Esta capa tiene el objetivo de estimar la probabilidad de que ocurra el siguiente token, de este modo predecir de manera autoregresiva.

Luego la salida del transformer tendr谩 dimensiones $(N, L, vocab\_size)$, pero la secuencia queda ***desplazada***.
:::

::: {.callout-important}
## Implementaciones como GPT agregan weight sharing o weighting tying entre la capa de Embedding y la capa final de proyecci贸n.
:::

:::
::: 

# コ Se acab贸

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est谩 licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
