---
title: "TICS-579-Deep Learning"
subtitle: "Clase 7: Transfer Learning y Data Augmentation"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Arquitecturas Famosas {.smaller}

:::{styyle="font-size: 70%;"}
Crear arquitecturas de CNN desde cero es una tarea compleja y que requiere de mucho conocimiento y experiencia. Afortunadamente, existen diversas arquitecturas famosas que han sido probadas y testeadas en el tiempo, las cu치les pueden ser utilizadas como ***backbones*** para distintas tareas de Visi칩n por Computador.
:::

::: {.columns}
::: {.column width="30%"}
![](img/clase-7/resnet.png){.lightbox fig-align="center" width="70%"} 
:::
::: {.column width="40%"}
![](img/clase-7/lenet.png){.lightbox fig-align="center"} 
![](img/clase-7/VGG16.png){.lightbox fig-align="center"}  
:::
::: {.column width="30%"}
![](img/clase-7/EfficientNet.png){.lightbox fig-align="center" width="70%"} 
:::
::: 


## LeNet-5 (LeCun et al., 1998) {.smaller}

> Probablemente la primera arquitectura famosa en poder realizar tareas importantes de reconocimiento de imagen. Dise침ada especialmente para reconocimiento de d칤gitos, introduce los bloques de convoluci칩n m치s pooling para luego conectarse con FFN.

![](img/clase-7/lenet.png){.lightbox fig-align="center"} 

::: {.callout-caution appearance="default" icon="false"}
## Adaptive Pooling
La mayor칤a de arquitecturas m치s modernas utiliza una capa llamada Adaptive Pooling antes del proceso de Flatten. El Adaptive Pooling es una especie de Pooling inverso, donde uno define el tama침o del output, y autom치ticamente se calcula el Kernel, Stride, Padding, etc. necesario para obtener ese tama침o.

Eso garantiza que cualquier tama침o de imagen puede pasar por la red sin romper las dimensiones necesarias para la transici칩n al MLP.
:::

## AlexNext (Krizhevsky, Sutskever y Hinton, 2012) {.smaller}

> Gan칩 el concurso Imagenet (ILSVRC) en 2012 por un largo margen (algo impensado para ese tiempo). Introdujo los conceptos de ReLU, Dropout y Aceleraci칩n por GPU. Esta arquitectura est치 disponible en `torchvision`.

![](img/clase-7/alexnet.png){.lightbox fig-align="center" width="60%"} 

```{.python}
import torchvision
torchvision.models.alexnet(weights = "IMAGENET1K_V1")
```
::: {.callout-important}
La arquitectura de Torchvision est치 inspirada en una ***versi칩n alternativa*** de Alexnet. Esto probablemente *no ser치* corregido ya que no es una arquitectura que se utilice comunmente en la actualidad.
:::

## VGGNet (Simonyan, Zisserman, 2014) {.smaller}

> Presentaron las primeras redes relativamente profundas con Kernels peque침os de $3 \times 3$. Su propuesta incluye Redes de hasta 19 capas.

::: {.columns}
::: {.column}
![](img/clase-7/VGG16.png){.lightbox fig-align="center"} 
:::
::: {.column}
```{.python}
import torchvision
torchvision.models.vgg16(weights = "IMAGENET1K_V1")
## Versi칩n con Batchnorm
torchvision.models.vgg16_bn(weights = "IMAGENET1K_V1")
```
::: {.callout-tip}
`torchvision` incluye las arquitecturas de 11, 13, 16 y 19 capas, adem치s de variantes que incluyen Batchnorm (que en eltiempo del paper no exist칤an a칰n).
:::
:::
::: 

## GoogleNet/Inception (Szegedy et al., 2014) {.smaller}

::: {.columns}
::: {.column width=60% style="font-size: 80%;"}

> Introduce las *"Pointwise Convolutions"* (Convoluciones de 1x1) que permiten reducir la complejidad de canales (mediante una combinaci칩n lineal) manteniendo las dimensiones de la imagen. Adem치s introduce los Inception Modules, que combinan resultados de Kernels de distinto tama침o. Fue la Arquitectura ganadora de ILSVRC 2014.

```{.python}
import torchvision
torchvision.models.googlenet(weights = "IMAGENET1K_V1")
```

![](img/clase-7/googlenet.png){.lightbox fig-align="center"} 
:::
::: {.column width=40%}

:::{.callout-note appearance="default" icon="false"}
#### 1x1 Convolutions
Las convoluciones de 1칑1 representan una de las principales novedades de este paper. Para cada posici칩n espacial (h, w), la convoluci칩n no combina p칤xeles adyacentes, sino que realiza una combinaci칩n lineal entre los diferentes canales de entrada.
:::

![](img/clase-7/1x1_convs_1.png){.lightbox fig-align="center"} 
![](img/clase-7/1x1_convs_2.png){.lightbox fig-align="center"} 
:::
::: 


## Resnet (He et al., 2015) {.smaller}

:::: {.columns}
::: {.column width="20%"}
![](img/clase-7/resnet.png){.lightbox fig-align="center" width="45%"}  
:::
::: {.column width="80%"}
> Introduce las conexiones residuales, lo cual permite evitar el problema del ***vanishing gradient*** para redes muy profundas. Es la Arquitectura ganadora de ILSVRC 2015.

::: {.callout-tip}
Esta arquitectura se puede encontrar tanto en `torchvision` como `timm`. Recomiendo `timm`, ya que hay muchas m치s variantes, mejor mantenci칩n y procesos de entrenamiento actualizados.
:::

```{.python}
import timm
model = timm.create_model("resnet50", pretrained = True)

## Listar todas las versiones de Resnet disponibles
timm.list_models("resnet*")
```
<br>

#### Conexiones Residuales

![](img/clase-7/residual.png){.lightbox fig-align="center" width="40%"}  

:::
:::: 

## EfficientNet (Tan, Le, 2019) {.smaller}

> Introducen el concepto de ***Compound Scaling*** que permite cambiar la escala de profundidad (n칰mero de capas en la red), ancho (n칰mero de canales en cada capa) y resoluci칩n (dimensiones de la imagen) para poder mejorar la performance. Permite crear resultados al nivel del estado del arte con much칤simos menos par치metros.

![](img/clase-7/efficientnet.png){.lightbox fig-align="center" width="70%"}  

```{.python}
import timm
model = timm.create_model("efficientnet_b0", pretrained = True)

## Listar todas las versiones de Resnet disponibles
timm.list_models("efficientnet*")
```

## Pre-training {.smaller}

Imagenet
: Es un dataset que contiene aproximadamente 14 millones de im치genes anotadas manualmente. Fue empleado en la competencia ImageNet Large Scale Visual Recognition Challenge (ILSVRC) entre los a침os 2010 y 2017, la cual impuls칩 importantes avances en el estado del arte del reconocimiento visual.

Las im치genes presentan resoluciones variadas, que van desde $4288 \times 2848$ hasta $75 \times 56$ p칤xeles. Adem치s, se encuentran normalizadas restando la media por canal $[0.485,0.456,0.406]$ y dividiendo por la desviaci칩n est치ndar correspondiente $[0.229,0.224,0.225]$.

::: {.callout-important appearance="default" icon="false"}
## 游뱂
Las dos variantes m치s conocidas son ***ImageNet-1K,*** que contiene 1.281.167, 50.000 y 100.000 im치genes para los conjuntos de train, validation y test, respectivamente, distribuidas en 1000 categor칤as; y ***ImageNet-21K***, que incluye 14.197.122 im치genes organizadas en 21.841 clases.
:::

::: {.callout-tip}
Debido a la relevancia y complejidad de este conjunto de datos, la mayor칤a de los backbones han sido preentrenados en 칠l. Gracias a esto, las distintas arquitecturas *"aprenden a ver"* a partir del conocimiento adquirido mediante este dataset.
:::

::: {.callout-note}
Debido a que muchas arquitecturas pueden/saben ver en un dataset tan complejo como Imagenet. ***쯉er칤a posible utilizar ese conocimiento en otro dataset?***
:::

::: {.fragment}
### Entering Transfer Learning
:::

## Transfer Learning {.smaller}

::: {.columns}
::: {.column width="60%"}
![](img/clase-7/transfer_learning.png){.lightbox fig-align="center"}  
:::
::: {.column width="40%"}
::: {.callout-note appearance="default" icon="false"}
## Dataset P칰blico/alta complejidad
Normalmente se utilizan datos p칰blicos y de alta complejidad y se utiliza para ***pre-entrenar*** una arquitectura.
:::

::: {.callout-tip appearance="default" icon="false"}
## Pre-entrenamiento
Se entrena una arquitectura para una tarea en espec칤fico con los detalles del dataset a utilizar.

:::
::: {.callout-important appearance="default" icon="false"}
## Fine-Tuning
Se carga la arquitectura pre-entrenada, con los pesos obtenidos en el pre-entrenamiento y se ajusta el prediction head para la nueva tarea y se vuelve a entrenar el modelo.
:::
::: {.callout-caution appearance="default" icon="false"}
## Freezing Layers
Se refiere a congelar los par치metros del **backbone** pre-entrenado, es decir, estos no se actualizan. Este paso es opcional, y en ocasiones puede funcionar de mejor manera que un ***Full-Fine-Tuning***

:::
:::
::: 

## Image Preprocessing y Data Augmentation {.smaller}

> En general el proceso de Preprocesamiento de Im치genes es bastante m치s engorroso que el de datos tabulares. Afortunadamente Pytorch tiene algunos `utilities` que permiten hacer el proceso m치s sencillo:

ImageFolder
: Permite cargar im치genes de un Path en espec칤fico. Dentro de esa carpeta `ImageFolder` considerar치 cada carpeta como una clase y los elementos (im치genes) dentro de dicha clase como instancia de la clase en cuesti칩n.

```{.python}
from torchvision.dataset import ImageFolder

train_data = ImageFolder("path/to/train/images", transform = None)
validation_data = ImageFolder("path/to/validation/images", transform = None)
test_data = ImageFolder("path/to/test/images", transform = None)
```

::: {.callout-tip}
Adem치s `ImageFolder` posee un par치metro llamado transform en el cu치l se pueden ingresar transformaciones a los datos para realizar procesos de Data Augmentation.
:::

::: {.callout-important appearance="default" icon="false"}
## Ojo
`ImageFolder` entrega los datos como una Imagen PIL. Por lo tanto, es necesario aplicar procesamientos que permitan su transformaci칩n en Tensor.
:::

## Data Augmentation {.smaller}

::: {.columns}
::: {.column width=70% style="font-size: 70%;"}

Corresponde a un proceso de generaci칩n de datos sint칠ticos. Este proceso se puede utilizar para:

* Permite la generaci칩n de datos adicionales debido a escasez por costo o disponibilidad de ellos. Ejemplo: Datos m칠dicos.
* Genera variedad de datos, que entrega al modelo un mayor poder de generalizaci칩n en datos no vistos.
* Al introducir mayor variabilidad en los datos entrega una mayor robustez ante el overfitting (Regularizaci칩n).
* Simular condiciones adversas para el modelo en la cu치l se quiera generar robustez.
    * Ej: Se tiene un modelo de reconocimiento de veh칤culos, pero que tiene que funcionar en condiciones de niebla.
:::
::: {.column width="30%"}
![](img/clase-7/augmentations.jpg){.lightbox fig-align="center" width="90%"}  
:::
::: 

::: {.callout-tip appearance="default" icon="false"}
## Albumentations

Existen diversas librer칤as que permiten generar Aumento de Datos. La librer칤as m치s famosas son Albumentations y Kornia. Albumentations, permite transformaciones extremadamente eficientes en CPU, mientras que Kornia hace lo mismo pero en GPU. Debido a las limitaciones de GPU que contamos, utilizaremos Albumentations, de manera tal de balancear procesamiento tanto en CPU como en GPU.
:::

::: {.callout-note}
Normalmente este tipo de transformaciones entrega mejores resultados cuando se generan de manera aleatoria y `on-the-fly`. Es decir, se genera el aumento de datos en la carga de datos durante el entrenamiento.
:::

## Transformaciones B치sicas {.smaller}

```{.python}
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
```
::: {.callout-caution}
Albumentations espera que la imagen venga como Numpy Array. Adem치s es una librer칤a bastante quisquillosa, por lo que toma un rato acostumbrarse. Pero su eficiencia y utilidad hace que valga la pena.
:::

A.Compose()
: Permite generar Pipelines de Transformaci칩n. Es decir, ir치 aplicando transformaciones una a una.

A.ToFloat()
: Transforma los datos en tipo Float. Esto a veces es necesario cuando hay incompatibilidad de data types en ciertos m칩dulos.

ToTensorV2()
: Transforma a Tensor de Pytorch. Existe una versi칩n `ToTensor()` pero est치 deprecada y no deber칤a usarse.

A.Normalize()
: Permite normalizar im치genes seg칰n su proceso de pre-entrenamiento. Normalmente estos provienen de pre-entrenamiento en Imagenet por lo que se debe normalizar con $mean=[0.485,0.456,0.406]$ y $SD=[0.229,0.224,0.225]$.

A.Resize()
: Se utiliza para estandarizar el tama침o de las im치genes. Im치genes m치s grandes permiten mejores resultados pero son computacionalmente m치s costosas. 

## Transformaciones Probabil칤sticas {.smaller}

::: {.callout-tip}
Como su nombre lo indica, la transformaci칩n se aplicar치 con una cierta probabilidad, lo que permitir치 que cada epoch haya mayor variabilidad.
:::

[A.CenterCrop/A.RandomCrop](https://explore.albumentations.ai/transform/RandomCrop)
: Genera un Crop de la imagen o al centro o Random. Esto lograr치 que los elementos de la imagen cambien de posici칩n.

[A.VerticalFlip](https://explore.albumentations.ai/transform/VerticalFlip)
: Genera Flip Vertical.

[A.HorizontalFlip](https://explore.albumentations.ai/transform/HorizontalFlip)
: Genera Flip Horizontal.

[A.Rotate](https://explore.albumentations.ai/transform/Rotate)
: Genera rotaciones aleatorias entre un 치ngulo m칤nimo y m치ximo.


::: {.callout-important}
Existen un sinn칰mero de transformaciones que se pueden aplicar. La lista completa se puede encontrar [ac치](https://explore.albumentations.ai/). Y existen transformaciones que incluso permiten simular niebla, lluvia, nieve, sepia, Zoom, y variados otros efectos.
:::

::: {.callout-caution}
Aplicar estas transformaciones es de extremo cuidado ya que para tareas m치s complejas como Semantic Segmentation, Object Detection, Keypoint Detection, se debe aplicar dichas transformaciones tambi칠n a las etiquetas. 
:::

# Terminamos con las CNN

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est치 licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
