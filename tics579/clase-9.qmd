---
title: "TICS-579-Deep Learning"
subtitle: "Clase 9: Manipulaci√≥n de Datos de Texto"
author: Alfonso Tobar-Arancibia
institute: <alfonso.tobar.a@edu.uai.cl>
format:
  revealjs:
    width: 1366
    height: 768
    theme: simple
    slide-number: true
    controls: true
    controls-layout: edges
    controls-back-arrows: faded
    transition: slide
    transition-speed: fast
    chalkboard: true
    callout-appearance: simple
    logo: ../logo-uai-blanco.jpeg
    css: ../logo.css
    code-copy: true
    highlight-style: arrow
    pdf-separate-fragments: true
---

## Natural Language Processing (NLP) {.smaller}

::: {.callout-warning appearance="default" icon="false"}
## üîî
El Natural Language Processing (NLP) es el √°rea que se dedica al an√°lisis y procesamiento de Texto. Existe una variedad de tareaas que se pueden abordar utilizando t√©cnicas de Deep Learning tales como:
:::


::: {.columns}
::: {.column width="30%"}
![](img/clase-10/sentiment_analysis.png){.lightbox fig-align="center"} 
![](img/clase-10/summarization.jpg){.lightbox fig-align="center"} 
:::
::: {.column width="40%"}
![](img/clase-10/ner.jpg){.lightbox fig-align="center"} 
![](img/clase-10/neural_translation.png){.lightbox fig-align="center"} 
:::
::: {.column width="30%"}
![](img/clase-10/question_answering.jpg){.lightbox fig-align="center"} 
:::
::: 

## Speech Recognition

Dada la naturaleza secuencial del lenguaje, el contexto ayuda a interpretar cu√°l es la manera correcta de interpretar el sonido emitido.

![](img/clase-9/meme_speech.png){.lightbox fig-align="center" } 

::: {.callout-important appearance="default" icon="false"}
## El problema
¬øC√≥mo hacemos que una red neuronal entienda y procese el texto?
:::

## Proceso de Tokenizaci√≥n y Embedding {.smaller}

::: {.columns}
::: {.column}
::: {.callout-tip appearance="default" icon="false"}
## Tokenizaci√≥n

El proceso de Tokenizaci√≥n permite transformar texto en datos num√©ricos. Cada dato num√©rico se mapea con un "trozo de texto". Normalmente los modelos van asociados a la tokenizaci√≥n con la que fueron entrenados. Cambiar la tokenizaci√≥n puede generar gran degradaci√≥n.
:::

::: {.callout-note appearance="default" icon="false"}
## Embedding
Corresponde el proceso en el que los Tokens se transforman en vectores densos en las cuales la distancia entre ellos representa una noci√≥n de similaridad.
:::
::: {.callout-important icon="false"}
* En este caso la frase ***"Frog on a log" *** es separada en Tokens (en este caso cada token es una palabra). 
* Luego cada Token es mapeado a un Token id proveniente de un vocabulario. ***¬øQu√© es un vocabulario?***
* Los embeddings en este caso representan una secuencia de largo 7 con 3 dimensiones.
:::
:::
::: {.column}
![](img/clase-10/tokenization_process.jpg){.lightbox fig-align="center"} 
:::
::: 

## Embeddings {.smaller}

![](img/clase-10/embedding_space.png){.lightbox fig-align="center" width="80%"} 

::: {.callout-tip appearance="default"}
## ¬øPor qu√© es tan importante el uso de Embeddings?
* Primero porque son entrenables. Es decir la red puede aprender cu√°l es la mejor manera de representar palabras.
* Existen embeddings pre-entrenados, es decir, se puede hacer transfer learning de embeddings.
* La red puede aprender relaciones sem√°nticas entre palabras, algo imposible utilizando otras representaciones.

:::

## Un tensor de Texto {.smaller}

::: {.columns}
::: {.column}
#### Tokenizaci√≥n

Suponiendo un Tensor de Entrada que representa 4 frases de 6 Tokens cada una:
```{.python}
X = torch.tensor([[42, 67, 76, 14, 26, 35],
        [20, 24, 50, 13, 78, 14],
        [10, 54, 31, 72, 15, 95],
        [67,  6, 49, 76, 73, 11]])
print(X.shape)
```
```
(4, 6)
```
::: {.callout-tip appearance="default" icon="false"}
## üëÄ Una palabra no necesariamente es un solo Token. [Ver ac√°](https://tiktokenizer.vercel.app/?model=meta-llama%2FMeta-Llama-3-8B)
:::

::: {.callout-important appearance="default" icon="false"}
## üö® La tokenizaci√≥n depende del vocabulario utilizado. La Tokenizaci√≥n va de la mano con el embedding y con el modelo a utilizar.
:::
:::
::: {.column}
#### Embedding
```{.python}
emb = nn.Embedding(100, 3)
output = emb(X)
print("Shape de un Embedding:", output.shape)
output
```
```
tensor([[[-1.4391,  0.5214,  1.0414],
         [-0.8293, -1.0809, -0.7839],
         [ 0.6427,  0.5742,  0.5867],
         [-1.0759,  0.5357,  1.1754],
         [ 0.2191,  0.5526, -0.6788],
         [ 0.5069, -0.4752, -0.4920]],

        [[-0.5644,  1.0563, -1.4692],
         [-0.1976,  1.2683,  1.2243],
         [-0.5855, -0.3900,  0.9812],
         [-0.4459, -1.2024,  0.7078],
         [-0.9109, -0.5291, -0.8051],
         [-1.0759,  0.5357,  1.1754]],

        [[-1.3864, -1.2862, -0.8371],
         [-1.4510, -0.7861, -0.9563],
         [ 0.2332,  4.0356,  1.2795],
         [-1.5754,  2.2508,  1.0012],
         [ 0.5612, -0.4527, -0.7718],
         [ 0.3714, -0.0047,  0.0795]],

        [[-0.8293, -1.0809, -0.7839],
         [ 1.5736, -0.8455,  1.3123],
         [ 1.1179, -1.2956,  0.0503],
         [ 0.6427,  0.5742,  0.5867],
         [ 1.3642,  0.6333,  0.4050],
         [-0.9224,  1.8113,  0.1606]]])
```
:::
::: 

## Tokens Especiales {.smaller}

Cada modelo puede incluir el tipo de Tokenizaci√≥n que m√°s le acomode. Adem√°s cada tokenizaci√≥n suele incluir tokens especiales que ayudan a la red a entender mejor el contexto o a resolver problemas intr√≠nsecos del modelamiento de secuencias. Algunos de estos Tokens especiales son:

* \<PAD\> : Token utilizado para rellenar secuencias m√°s cortas y que todas tengan el mismo largo.
* \<SOS\> o \<BOS\> : Start of Sequence/Beginning of Sequence. Token que indica el inicio de una secuencia. 
* \<EOS\> : End of Sequence. Token que indica el final de una secuencia.
* \<UNK\> : Unknown Token. Token utilizado para representar palabras fuera del vocabulario.
* \<MASK\> : Token utilizado en tareas de Masked Language Modeling (MLM) para indicar palabras que deben ser predichas por el modelo.
* \<SEP\> : Separator Token. Utilizado para separar diferentes partes de una secuencia, como en tareas de preguntas y respuestas.
* \<CLS\> : Classification Token. Utilizado en tareas de clasificaci√≥n para representar toda la secuencia.

::: {.callout-caution appearance="default" icon="false"}
## üö© Ojo
Exiten tantos tokens especiales como tokenizadores y modelos existen. Siempre revisar la documentaci√≥n del modelo a utilizar. Algunos tambi√©n indican roles como por ejemplo en el caso de un asistente, se indica el rol del asistente, del usuario, y prompts de sistemas. 
:::

## Problema de las RNN comunes {.smaller}

::: {.callout-warning}
A pesar de las habilidades de las RNN, estas no son suficientes para distintas tareas de NLP.
:::

![](img/clase-10/rnn.png){.lightbox fig-align="center"} 

Las RNN inicialmente toman cada elemento de una secuencia y generan un output para cada entrada. Esto genera ciertas limitantes en tareas como Machine Translation, donde por ejemplo, el modelo asume que la traducci√≥n es uno a uno, lo cu√°l no es necesariamente cierto.


## Machine Translation: Ejemplo del Ingl√©s {.smaller}

Supongamos que necesitamos hacer la siguiente traducci√≥n:

Ingl√©s 
: > Hi, my name is Alfonso

:::{.fragment}

Espa√±ol
: > Hola, mi nombre es Alfonso

::: {.callout-note}
Este tipo de traducci√≥n es uno a uno. Cada input puede tener asociado una salida de manera directa puede realizarse de manera directa con una RNN.
:::
:::

## Machine Translation: Ejemplo del Ingl√©s {.smaller}

Ingl√©s
: > Would you help me prepare something for tomorrow?

:::{.fragment}
Espa√±ol
: > ¬øMe ayudar√≠as a preparar algo para ma√±ana?

::: {.callout-important appearance="default" icon="false"}
# ‚õî Problemas

* La traducci√≥n no es uno. De hecho en ingl√©s se utilizan 8 palabras y 1 signo de puntuaci√≥n. En espa√±ol se traduce en 7 palabras y 2 signos de puntuaci√≥n.
* *"Would"* no tiene equivalente en espa√±ol. 
* *"a"* no tiene equivalente en el ingl√©s. 
* *"Me"* se traduce como *"me"* en ingl√©s pero en vez de ir al inicio, va continuaci√≥n de *"help"*.
* "*¬ø*" no existe en ingl√©s.

:::

::: {.callout-warning appearance="default" icon="false"}
## üîî
* Otros idiomas como el Alem√°n o el Ruso, tienen fusi√≥n de palabras o declinaciones que hacen la traducci√≥n mucho m√°s dif√≠cil. 
* Es por ello que se requiere una cierta libertad entre los tokens de entradas y los tokens de salida.
:::
:::


## Soluciones: Redes Convolucionales {.smaller}

Una potencial soluci√≥n se puede dar por medio de Redes Convolucionales de 1D. En este caso las redes convolucionales tienen la ventaja de poder mirar tanto al pasado como al futuro de manera m√≥vil.

::: {.columns}
::: {.column}
![](img/clase-10/conv1d.png){.lightbox fig-align="center" width="80%"} 
:::
::: {.column}
::: {.callout-note appearance="default"}
## Ventajas
* Pueden tomar contexto desde el inicio y desde el final.

:::
::: {.callout-important appearance="default"}
## Desventajas
* Su campo receptivo es mucho m√°s acotado y depende del n√∫mero de capas y el largo del Kernel lo cual repercute directamente en el n√∫mero de par√°metros del modelo.
* No tienen estado latente (o memoria) que almacena contexto.
* No es √∫til para modelos de generaci√≥n (ya que ve contexto desde el futuro).
:::
:::
::: 

## Soluciones: Arquitecturas Encoder-Decoder {.smaller}

Encoder
: Corresponde a una arquitectura que permitir√° tomar datos de entrada y codificarlos en una representaci√≥n num√©rica (normalmente como hidden states, embeddings o logits).

Decoder
: Corresponde a una arquitectura que toma una representaci√≥n codificada de datos (normalmente generado por un encoder) y la transforma nuevamente en una salida con un formato comprensible y no solamente una "simple etiqueta".

![](img/clase-10/autoencoder.jpeg){.lightbox fig-align="center"} 

::: {.callout-note}
Este tipo de arquitecturas son quiz√°s las m√°s populares hoy en d√≠a y tienen aplicaciones en distintos dominios.
:::

## Soluciones: Arquitecturas Encoder-Decoder {.smaller}

::: {.callout-note}
Una arquitectura Encoder-Decoder convolucional permite devolver una imagen como salida. Este ejemplo se conoce como Segmentaci√≥n Sem√°ntica.
:::

![](img/clase-10/conv_enc-dec.png){.lightbox fig-align="center"} 


## Soluciones: Arquitecturas Encoder-Decoder {.smaller}

::: {.callout-note}
Una arquitectura recurrente permite devolver una secuencia como salida. La cual puede utilizarse para generaci√≥n o traducci√≥n de texto.
:::

![](img/clase-10/neural_translation.png){.lightbox fig-align="center" width="80%"} 

::: {.callout-note appearance="default" icon="false"}
## Ventajas
* Permite "desligarse" de la predicci√≥n uno a  uno. El Encoder resuelve una tarea many (la secuencia) to one (un hidden state) y el Decoder resuelve una tarea one (hidden state) to many (secuencia de salida).
* La salida de este tipo de modelos depende principalmente del contexto almacenado en el Hidden State/Bottleneck ya que se utiliza como Hidden State inicial del Decoder.
:::

::: {.callout-important appearance="default" icon="false"}
## Desventajas
* Dado los problemas de Vanishing/Exploding Gradients es ingenuo pensar que todo el contexto de una frase vive en el √∫ltimo hidden state.
* A√∫n as√≠, son una buena primera aproximaci√≥n al problema.
:::


## Ejemplo: Traducci√≥n de Texto {.smaller}

Supongamos que queremos traducir la frase **How are you?** al espa√±ol.

::: {.r-stack}
![](img/clase-9/translation_1.png){.lightbox .fragment fragment-index=1} 

![](img/clase-9/translation_2.png){.lightbox .fragment fragment-index=2} 

![](img/clase-9/translation_3.png){.lightbox .fragment fragment-index=3} 

:::

::: {.callout-note .fragment fragment-index=1 appearance="default" icon="false"}
## Para comenzar el Proceso de Traducci√≥n se debe ingresar la Frase a Traducir al Encoder y luego dar inicio a la Traducci√≥n en el Decoder mediante el Token Especial \<SOS\>.
:::

::: {.callout-warning .fragment fragment-index=2 appearance="default" icon="false"}
## \<SOS\> genera como primera predicci√≥n el token **¬ø** el cu√°l ser√° utilizado como segundo input del Decoder. Esto nos entrega como predicci√≥n el token **C√≥mo**. Que a su vez se utiliza como el tercer input del Decoder y nos entrega el Token **est√°s**. 

Este proceso de conoce como ***Predicci√≥n/Generaci√≥n Autoregresiva***.
:::

::: {.callout-important .fragment fragment-index=3 appearance="default" icon="false"}
## Este proceso de repite hasta que nos encontramos con el Token Especial \<EOS\>, el cu√°l nos indica que debemos dejar de generar.
:::

## Trucos de Entrenamiento {.smaller}

El entrenamiento de una RNN puede ser muy ineficiente y "trabado" si es que no se utilizan ciertos trucos.  Algunos de estos trucos son:

#### Teacher Forcing

![](img/clase-9/teacher_forcing.png){.lightbox }  

::: {.callout-tip appearance="default" icon="false"}
## ‚òùÔ∏è Dado que al momento de comenzar el entrenamiento el modelo no tiene por qu√© traducir correctamente, es que al Decoder se le entrega el Token correcto en vez de la predicci√≥n del modelo. Esto permite que el modelo aprenda m√°s r√°pido y de mejor manera.
:::

::: {.callout-important appearance="default" icon="false"}
## üö® Normalmente esto se hace de manera pseudoaleatoria utilizando un `teacher_forcing_ratio`. Ojo con el ***Exposure Bias***
:::

## Trucos de Inferencia: Greedy Search {.smaller}

#### Estrategias para escoger el siguiente token
Al momento de generar texto, la salida de cada step del Decoder tendr√° una distribuci√≥n de probabilidad sobre el vocabulario. Existen distintas estrategias para escoger el siguiente token a utilizar:

#### Greedy Search

![](img/clase-9/greedy_search.png){.lightbox }  

::: {.callout-tip appearance="default" icon="false"}
## üôÇ En este caso Greedy Search escoge el Token con mayor probabilidad en cada step y lo utiliza como el siguiente input del Decoder.

En este caso, la probabilidad m√°xima en el primer step es p4, luego al ingresar p4 como input, la probabilidad m√°xima es p2, y as√≠ sucesivamente.
:::


## Trucos de Inferencia: Beam Search {.smaller}

#### Beam Search

En este caso se pueden escoger m√∫ltiples caminos en vez de solamente el de mayor probabilidad. Por ejemplo, si se escogen 2 caminos (beam width = 2), en el primer step se escogen los tokens con probabilidad p4 y p3. Luego en el segundo step, se generan las siguientes probabilidades para los distintos caminos. Ac√° se escoge el camino con la mayor probabilidad conjunta. 


![](img/clase-9/beam_search.png){.lightbox }  

## Ejemplo: Greedy vs Beam Search {.smaller}

Supongamos queremos traducir la frase **"Good Night"** al espa√±ol.

::: {.columns}
::: {.column}

#### Supongamos que la predicci√≥n para el Token \<SOS\> es la siguiente:
::::{style="font-size: 110%;"}

| Token | Probabilidad |
|-------|--------------|
| good  | 0.45         |
| nice  | 0.15         |
| well  | 0.10         |
| hello | 0.30         |
::::
:::

::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Greedy Search dir√≠a que la predicci√≥n de la primera palabra es **good** (p=0.45)
:::

::: {.callout-tip appearance="default" icon="false"}
## Beam Search con width=2 dir√≠a lo siguiente:

* Beam 1: good (p=0.45)
* Beam 2: hello (p=0.30)
:::
:::
::: 


## Ejemplo: Greedy vs Beam Search {.smaller}

::: {.columns}
::: {.column}
#### Prediciendo para "good"
| Secuencia       | C√°lculo             | Resultado |
|-----------------|----------------------|-----------|
| good night      | 0.45 √ó 0.40          | 0.18      |
| good evening    | 0.45 √ó 0.58          | 0.261     |
| good bye        | 0.45 √ó 0.02          | 0.009     |

<br>

#### Prediciendo para "hello"

| Secuencia        | C√°lculo             | Resultado |
|------------------|----------------------|-----------|
| hello night       | 0.30 √ó 0.10          | 0.03      |
| hello evening     | 0.30 √ó 0.15          | 0.045     |
| hello there       | 0.30 √ó 0.5          | 0.15     |

:::

::: {.column}
::: {.callout-important appearance="default" icon="false"}
## Greedy Search dir√≠a que la predicci√≥n de la segunda palabra es 
:::

::: {.callout-tip appearance="default" icon="false"}
## Beam Search con width=2 dir√≠a lo siguiente:

* Beam 1: good night (p=0.18)
* Beam 2: good evening (p=0.261)

:::
::: {.callout-caution appearance="default" icon="false"}
## En este caso, ambas opciones son traducciones v√°lidas. Adem√°s, puede combinarse con un muestreo probabil√≠stico para introducir mayor diversidad en los resultados.
:::

:::
:::

## Trucos de Inferencia: Top-k Sampling {.smaller}

Supongamos que tenemos la siguiente distribuci√≥n de Probabilidad sobre un vocabulario:

::: {.callout-tip appearance="default" icon="false"}
## Top-k Sampling
Se escoger√° aleatoriamente entre los k tokens con mayor probabilidad. Esto introduce diversidad en la generaci√≥n.
:::

::: {.callout-important appearance="default" icon="false"}
## Temperatura
Corresponde a un par√°metro que permite controlar la "concentraci√≥n" de la distribuci√≥n de probabilidad. A mayor temperatura, m√°s uniforme ser√° la distribuci√≥n. A menor temperatura, m√°s concentrada estar√° la distribuci√≥n en los tokens con mayor probabilidad.
:::

::: {.columns}
::: {.column style="font-size: 70%"}
| token | logit | logit/T=1.5 | prob T=1.5 |logit/T=4 | prob T=4 |
|-------|-------|---------|---------|---------|---------|
| the   | 5.0   |3.33    | 0.53    | 1.25    | 0.32    |
| and   | 4.0   |2.67    | 0.27    | 1    | 0.25    |
| a     | 3.0   |2.00    | 0.14    | 0.75       | 0.20    |
| dog   | 1.0   |0.67    | 0.04    | 0.25    | 0.12    |
| fast  | 0.5   |0.33    | 0.02    | 0.13    | 0.11    |
:::
::: {.column}
::: {.callout-note appearance="default" icon="false"}
Se puede ver que a medida que la temperatura aumenta, la distribuci√≥n de probabilidad se vuelve m√°s uniforme. Es decir la probabilidad de cualquiera de los tokens aparezca aumenta. 
:::

::: {.callout-important appearance="default" icon="false"}
## üö®Otra forma de verlo es que la probabilidad de que no ocurra el m√°s token m√°s probable es mucho mayor.

Por ejemplo con T=1.5 la probabilidad de que no ocurra "the" es 0.47, mientras que con T=4 es 0.68.
:::
:::
::: 


# Terminamos RNNs üòä

::: {.footer}
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
:::
