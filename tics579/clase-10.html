<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Alfonso Tobar-Arancibia">
  <title>Clases UAI – TICS-579-Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-673c2e7d040da7fd4b9d655d29f657a0.css">
  <link rel="stylesheet" href="../logo.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">TICS-579-Deep Learning</h1>
  <p class="subtitle">Clase 10: Mecanismos de Atención</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Alfonso Tobar-Arancibia 
</div>
        <p class="quarto-title-affiliation">
            <a href="mailto:alfonso.tobar.a@edu.uai.cl" class="email">alfonso.tobar.a@edu.uai.cl</a>
          </p>
    </div>
</div>

</section>
<section id="datos-de-texto" class="slide level2 smaller">
<h2>Datos de Texto</h2>
<p>Los datos de texto corresponden a un caso particular de datos secuenciales. En este caso, dependiendo del idioma, las dependencias pueden venir tanto del pasado como del futuro. Consideramos texto libre como una secuencia de Strings, el cuál es inteligible para seres humanos pero no necesariamente para un computador.</p>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Es probablemente el tipo de dato más abundante, aunque también uno de los más sensible al ruido (variabilidad, idioma, tono, formalidad, etc.).</p>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>El problema</strong></p>
</div>
<div class="callout-content">
<p>Los computadores, y por ende los modelos, no pueden entender strings. El computador <strong><em>sólo puede entender datos numéricos</em></strong>.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Para poder manipular texto dentro de un modelo será necesario un pre-procesamiento que permita transformar el texto en datos numéricos que un modelo pueda entender.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>La disciplina encargada de desarrollar modelos asociado a lenguaje/texto es conocida como <strong><em>Procesamiento de Lenguaje Natural</em></strong> (NLP en inglés).</p>
</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/wordcloud.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="img/clase-10/wordcloud.jpeg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="tareas-asociadas-a-nlp" class="slide level2">
<h2>Tareas asociadas a NLP</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><a href="img/clase-10/sentiment_analysis.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="img/clase-10/sentiment_analysis.png" class="quarto-figure quarto-figure-center"></a> <a href="img/clase-10/summarization.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="img/clase-10/summarization.jpg" class="quarto-figure quarto-figure-center"></a></p>
</div><div class="column" style="width:40%;">
<p><a href="img/clase-10/ner.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="img/clase-10/ner.jpg" class="quarto-figure quarto-figure-center"></a> <a href="img/clase-10/neural_translation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="img/clase-10/neural_translation.png" class="quarto-figure quarto-figure-center"></a></p>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/question_answering.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="img/clase-10/question_answering.jpg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="proceso-de-tokenización-y-embedding" class="slide level2 smaller">
<h2>Proceso de Tokenización y Embedding</h2>
<div class="columns">
<div class="column">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tokenización</strong></p>
</div>
<div class="callout-content">
<p>El proceso de Tokenización permite transformar texto en datos numéricos. Cada dato numérico se mapea con un “trozo de texto”. Normalmente los modelos van asociados a la tokenización con la que fueron entrenados. Cambiar la tokenización puede generar gran degradación.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Embedding</strong></p>
</div>
<div class="callout-content">
<p>Corresponde el proceso en el que los Tokens se transforman en vectores densos en las cuales la distancia entre ellos representa una noción de similaridad.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<ul>
<li>En este caso la frase <strong><em>“Frog on a log” </em></strong> es separada en Tokens (en este caso cada token es una palabra).</li>
<li>Luego cada Token es mapeado a un Token id proveniente de un vocabulario. <strong><em>¿Qué es un vocabulario?</em></strong></li>
<li>Los embeddings en este caso representan una secuencia de largo 7 con 3 dimensiones.</li>
</ul>
</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/tokenization_process.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="img/clase-10/tokenization_process.jpg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="embeddings" class="slide level2 smaller">
<h2>Embeddings</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/embedding_space.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="img/clase-10/embedding_space.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>¿Por qué es tan importante el uso de Embeddings?</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Primero porque son entrenables. Es decir la red puede aprender cuál es la mejor manera de representar palabras.</li>
<li>Existen embeddings pre-entrenados, es decir, se puede hacer transfer learning de embeddings.</li>
<li>La red puede aprender relaciones semánticas entre palabras, algo imposible utilizando otras representaciones.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="problema-de-las-rnn" class="slide level2 smaller">
<h2>Problema de las RNN</h2>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>A pesar de las habilidades de las RNN, estas no son suficientes para distintas tareas de NLP.</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/rnn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="img/clase-10/rnn.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<p>Las RNN inicialmente toman cada elemento de una secuencia y generan un output para cada entrada. Esto potencialmente genera ciertas limitantes. Una de ellas es el proceso llamado Machine Translation.</p>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Este es un ejemplo de Modelamiento <strong><em>seq2seq</em></strong> en el que se utiliza una secuencia de entrada pero se espera también una secuencia de salida.</p>
</div>
</div>
</div>
</section>
<section id="machine-translation-ejemplo-del-inglés" class="slide level2 smaller">
<h2>Machine Translation: Ejemplo del Inglés</h2>
<p>Supongamos que necesitamos hacer la siguiente traducción:</p>
<dl>
<dt>Inglés</dt>
<dd>
<blockquote>
<p>Hi, my name is Alfonso</p>
</blockquote>
</dd>
</dl>
<div class="fragment">
<dl>
<dt>Español</dt>
<dd>
<blockquote>
<p>Hola, mi nombre es Alfonso</p>
</blockquote>
</dd>
</dl>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Este tipo de traducción es uno a uno. Cada input puede tener asociado una salida de manera directa puede realizarse de manera directa con una RNN.</p>
</div>
</div>
</div>
</div>
</section>
<section id="machine-translation-ejemplo-del-inglés-1" class="slide level2 smaller">
<h2>Machine Translation: Ejemplo del Inglés</h2>
<dl>
<dt>Inglés</dt>
<dd>
<blockquote>
<p>Would you help me prepare something for tomorrow?</p>
</blockquote>
</dd>
</dl>
<div class="fragment">
<dl>
<dt>Español</dt>
<dd>
<blockquote>
<p>¿Me ayudarías a preparar algo para mañana?</p>
</blockquote>
</dd>
</dl>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Problemas</strong></p>
</div>
<div class="callout-content">
<ul>
<li>La traducción no es uno. De hecho en inglés se utilizan 8 palabras y 1 signo de puntuación. En español se traduce en 7 palabras y 2 signos de puntuación.</li>
<li><em>“Would”</em> no tiene equivalente en español.</li>
<li><em>“a”</em> no tiene equivalente en el inglés.</li>
<li><em>“Me”</em> se traduce como <em>“me”</em> en inglés pero en vez de ir al inicio, va al final de <em>“help”</em>.</li>
<li>“<em>¿</em>” no existe en inglés.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Otros idiomas como el Alemán o el Ruso, tienen fusión de palabras o declinaciones que hacen la traducción mucho más difícil.</li>
<li>Es por ello que se requiere una cierta libertad entre los tokens de entradas y los tokens de salida.</li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id="soluciones-redes-convolucionales" class="slide level2 smaller">
<h2>Soluciones: Redes Convolucionales</h2>
<p>Una potencial solución se puede dar por medio de Redes Convolucionales de 1D. En este caso las redes convolucionales tienen la ventaja de poder mirar tanto al pasado como al futuro de manera móvil.</p>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/conv1d.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="img/clase-10/conv1d.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div><div class="column">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Ventajas</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Pueden tomar contexto desde el inicio y desde el final.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Desventajas</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Su campo receptivo es mucho más acotado y depende del número de capas y el largo del Kernel lo cual repercute directamente en el número de parámetros del modelo.</li>
<li>No tienen estado latente (o memoria) que almacena contexto.</li>
<li>No es útil para modelos de generación (ya que ve contexto desde el futuro).</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="soluciones-arquitecturas-encoder-decoder" class="slide level2 smaller">
<h2>Soluciones: Arquitecturas Encoder-Decoder</h2>
<dl>
<dt>Encoder</dt>
<dd>
Corresponde a una arquitectura que permitirá tomar datos de entrada y codificarlos en una representación numérica (normalmente como hidden states o como embeddings).
</dd>
<dt>Decoder</dt>
<dd>
Corresponde a una arquitectura que toma una representación codificada de datos (normalmente generado por un encoder) y la transforma nuevamente en una salida con un formato comprensible y no solamente una “simple etiqueta”.
</dd>
</dl>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/autoencoder.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img data-src="img/clase-10/autoencoder.jpeg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Este tipo de arquitecturas son quizás las más populares hoy en día y tienen aplicaciones en distintos dominios.</p>
</div>
</div>
</div>
</section>
<section id="soluciones-arquitecturas-encoder-decoder-1" class="slide level2 smaller">
<h2>Soluciones: Arquitecturas Encoder-Decoder</h2>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Una arquitectura Encoder-Decoder convolucional permite devolver una imagen como salida. Este ejemplo se conoce como Segmentación Semántica.</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/conv_enc-dec.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img data-src="img/clase-10/conv_enc-dec.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</section>
<section id="soluciones-arquitecturas-encoder-decoder-2" class="slide level2 smaller">
<h2>Soluciones: Arquitecturas Encoder-Decoder</h2>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Una arquitectura recurrente permite devolver una secuencia como salida. La cual puede utilizarse para generación o traducción de texto.</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/rnn_enc-dec.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img data-src="img/clase-10/rnn_enc-dec.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Ventajas</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Permite “desligarse” de la predicción uno a uno.</li>
<li>La salida de este tipo de modelos depende principalmente del contexto almacenado en el Hidden State/Bottleneck.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Desventajas</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Dado los problemas de Vanishing/Exploding Gradients es ingenuo pensar que todo el contexto de una frase vive en el último hidden state.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="soluciones-arquitecturas-encoder-decoder-3" class="slide level2 smaller">
<h2>Soluciones: Arquitecturas Encoder-Decoder</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/neural_translation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img data-src="img/clase-10/neural_translation.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Ojo</strong></p>
</div>
<div class="callout-content">
<p>El último Hidden State del Encoder se utilizará como Hidden State inicial del Decoder.</p>
</div>
</div>
</div>
</section>
<section id="entrenamiento-de-una-arquitectura-encoder-decoder" class="slide level2 smaller">
<h2>Entrenamiento de una Arquitectura Encoder-Decoder</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/seq2seq_training.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img data-src="img/clase-10/seq2seq_training.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Arquitectura</strong></p>
</div>
<div class="callout-content">
<ul>
<li>RNN Bidireccional</li>
<li>Con un Encoder (colores pastel) y un Decoder independientes (colores más oscuros).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<ul>
<li>Tenemos dos frases una en inglés (input) y una en español (output). Ambas frases son claramente de tamaños distintos.</li>
<li>Tenemos el token especial <code>&lt;eos&gt;</code> (end of sentence) que separa el input del output.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<ul>
<li>Los últimos Hidden States del Encoder son los Hidden States iniciales del Decoder. Estos también se conocen como <strong><em>Context Vectors</em></strong>.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Entrenamiento</strong></p>
</div>
<div class="callout-content">
<p>Al momento de entrenarse, las salidas <span class="math inline">\(y1\)</span> e <span class="math inline">\(y2\)</span> pueden ser distintas al valor esperado y deben ir ajustándose epoch a epoch.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="inferencia-de-una-arquitectura-encoder-decoder" class="slide level2 smaller">
<h2>Inferencia de una Arquitectura Encoder-Decoder</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/prediction_seq2seq.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img data-src="img/clase-10/prediction_seq2seq.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Predicción</strong></p>
</div>
<div class="callout-content">
<ul>
<li>La predicción se va realizando de <strong><em>manera autoregresiva</em></strong>. Es decir, la predicción del primer step corresponde a la entrada del segundo step y así sucesivamente.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<ul>
<li>La primera entrada siempre será el token especial <code>&lt;eos&gt;</code> (otros modelos pueden utilizar otros tokens especiales).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<ul>
<li>El modelo irá prediciendo de manera autoregresiva hasta predecir el token <code>&lt;eos&gt;</code>.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Problema</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Pensar que todo el contexto se puede almacenar en el último hidden state es un poco ingenuo.</li>
<li>El último hidden state tiene más influencia de las palabras más cercanas y menos de las palabras iniciales (debido al problema de vanishing/exploding gradients).</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="mecanismo-de-atención-bahdanau-et-al-2015" class="slide level2 smaller">
<h2>Mecanismo de Atención (Bahdanau et al, (2015))</h2>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/Attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img data-src="img/clase-10/Attention.png" class="quarto-figure quarto-figure-center" style="width:75.0%"></a></p>
</figure>
</div>
</div><div class="column">
<dl>
<dt>Atención</dt>
<dd>
Se refiere a cualquier mecanismo en el que los hidden states se ponderan y combinan para poder utilizarlos como contexto. En otras palabras, el mecanismo busca a qué inputs iniciales debe poner más “atención” para poder generar la predicción.
</dd>
</dl>
<p><span class="math display">\[c_i = \sum_{t=1}^T a_{i,t} \cdot h_t\]</span></p>
<p>Donde <span class="math inline">\(c_i\)</span> corresponde al contexto para la predicción del output <span class="math inline">\(i\)</span> y <span class="math inline">\(a_{i,t}\)</span> (que van entre 0 y 1) corresponden a cuánta atención le presta el output <span class="math inline">\(i\)</span> al token <span class="math inline">\(t\)</span>.</p>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Ojo</strong></p>
</div>
<div class="callout-content">
<p>En el paper original, se interpreta <span class="math inline">\(a_{i,t}\)</span> como cuánto se “alínea” o se parece el estado <span class="math inline">\(h_t\)</span> con <span class="math inline">\(S_{t-1}\)</span>.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="atención-de-bahdanau" class="slide level2 smaller">
<h2>Atención de Bahdanau</h2>
<p><span class="math display">\[a_{i,t} = align(h_t, S_{i-1})\]</span> <a href="img/clase-10/bahdanau.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img data-src="img/clase-10/bahdanau.png" class="quarto-figure quarto-figure-center"></a></p>
<p><span class="math display">\[[a_{i,1},...,a_{i,T}] = Softmax([\tilde{a_{i,1}},...\tilde{a_{i,T}}])\]</span></p>
</section>
<section id="otras-formas-de-atención" class="slide level2">
<h2>Otras formas de Atención</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>1. Proyecciones Lineales</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[k_t = W_k \cdot h_t\]</span> <span class="math display">\[q_{i-1} = W_q \cdot S_{i-1}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>2. Similaridad: Producto Punto es equivalente al Cosine Similarity</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\tilde{a}_{i,t} = k_t^T \cdot q_{i-1}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>3. Normalización:</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[[a_{i,1},...,a_{i,T}] = Softmax([\tilde{a}_{i,1},...\tilde{a}_{i,T}])\]</span></p>
</div>
</div>
</div>
</section>
<section id="transformers-arquitectura" class="slide level2 smaller">
<h2>Transformers: Arquitectura</h2>
<dl>
<dt>Transformer</dt>
<dd>
Corresponde a la Arquitectura más avanzada que tenemos hoy en día. Está basada en distintos mecanismos de atención.
</dd>
</dl>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/transformer.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img data-src="img/clase-10/transformer.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a></p>
</figure>
</div>
</div><div class="column">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Detalles de la Arquitectura</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Corresponde a un Encoder + un Decoder.</li>
<li>Cada uno contiene una capa de Embeddings.</li>
<li>Además posee un Positional Encoding para entender el orden de la secuencia.</li>
<li>El decoder funciona de manera autoregresiva.</li>
<li>Posee 4 tipos de atención.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="transformers-self-y-multihead-attention" class="slide level2">
<h2>Transformers: Self y Multihead Attention</h2>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/self_attention.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img data-src="img/clase-10/self_attention.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Detalles</strong></p>
</div>
<div class="callout-content">
<ul>
<li>El self-attention pone atención (aprendiendo la relación) existente entre datos de una misma secuencia. La gran ventaja es que permite la paralelización de cálculos.</li>
<li>El Multihead Attention corresponde a la concatenación de varios Self-Attention. Esto permite no “sesgarse” con sólo una forma de poner atención, permitiendo aprender relaciones en distintas direcciones.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="transformers-causal-self-attention" class="slide level2">
<h2>Transformers: Causal Self Attention</h2>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/causal_self_attention.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img data-src="img/clase-10/causal_self_attention.jpg" class="quarto-figure quarto-figure-center" style="width:90.0%"></a></p>
</figure>
</div>
</div><div class="column">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Detalles</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Corresponde a un tipo particular de Self Attention, en el cuál sólo se puede poner atención a valores de secuencia previa (no puede ver al futuro). Esto es particularmente necesario para tareas de generación autoregresiva.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="transformers-cross-attention" class="slide level2">
<h2>Transformers: Cross Attention</h2>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-10/cross_attention.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img data-src="img/clase-10/cross_attention.jpg" class="quarto-figure quarto-figure-center" style="width:90.0%"></a></p>
</figure>
</div>
</div><div class="column">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Detalles</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Corresponde a la atención que relaciona información proveniente tanto del Encoder como del Decoder. Muy similar al concepto original de Atención.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="y-estamos-por-hoy" class="title-slide slide level1 center">
<h1>Y… estamos por hoy</h1>
<div class="footer">
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
</p><p><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0</a></p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">
<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a>
<p></p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../logo-uai-blanco.jpeg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': true,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1366,

        height: 768,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/datacubeR\.github\.io\/clases_UAI\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>