{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"TICS-579-Deep Learning\"\n",
        "subtitle: \"Clase 4: Model Training\"\n",
        "author: Alfonso Tobar-Arancibia\n",
        "institute: <alfonso.tobar.a@edu.uai.cl>\n",
        "format:\n",
        "  revealjs:\n",
        "    width: 1366\n",
        "    height: 768\n",
        "    theme: simple\n",
        "    slide-number: true\n",
        "    controls: true\n",
        "    controls-layout: edges\n",
        "    controls-back-arrows: faded\n",
        "    transition: slide\n",
        "    transition-speed: fast\n",
        "    chalkboard: true\n",
        "    callout-appearance: simple\n",
        "    logo: ../logo-uai-blanco.jpeg\n",
        "    css: ../logo.css\n",
        "    code-copy: true\n",
        "    highlight-style: arrow\n",
        "    pdf-separate-fragments: true\n",
        "---\n",
        "\n",
        "```{css, echo=FALSE}\n",
        ".reveal code {\n",
        "  max-height: 100% !important;\n",
        "}\n",
        "```\n",
        "\n",
        "## Entrenamiento de la Red {.smaller}\n",
        "\n",
        "> A diferencia de un Modelo de Machine Learning, las Redes Neuronales se entrenan de manera progresiva (se espera una mejora en cada Epoch). Si nuestra Arquitectura es apropiada nosotros deber√≠amos esperar que el `Loss` de nuestra ***red siempre disminuya***. ¬øPor qu√©?\n",
        "\n",
        ":::{.callout-tip .fragment icon=false appearance=\"default\"}\n",
        "## üí° Dado que el entrenamiento es progresivo, el modelo puede retomar su entrenamiento desde un set de pesos dados.\n",
        ":::\n",
        "\n",
        "::: {.callout-warning .fragment icon=false appearance=\"default\"}\n",
        "## **¬øSiempre buscamos la Red que tenga el mejor Loss de Entrenamiento?** ***¬øCu√°l es la diferencia entre el `Loss` y el `Rendimento del Modelo`?***\n",
        ":::\n",
        "\n",
        "::: {.callout-important .fragment icon=false appearance=\"default\"}\n",
        "## Al igual que en los modelos de Machine Learning debemos evitar a toda costa el Overfitting. ¬øQu√© es el overfitting?\n",
        ":::\n",
        "\n",
        "## Entrenamiento de la Red {.smaller}\n",
        "\n",
        "Bias-Variance Tradeoff (Dilema Sesgo-Varianza)\n",
        ": > Probablemente el concepto m√°s importante para determinar si un modelo tiene potencial o no. Corresponden a dos tipos de errores que pueden sufrir los modelos de ML. \n",
        "\n",
        "::: {.columns .fragment}\n",
        "::: {.column .callout-note appearance=\"default\" icon=false}\n",
        "## Bias\n",
        "Corresponde al sesgo, y tiene que ver con la diferencia entre el valor real y el valor predicho. Bajo sesgo implica una mejor predicci√≥n.\n",
        ":::\n",
        "::: {.column .callout-caution appearance=\"default\" icon=false}\n",
        "## Variance\n",
        "Corresponde a la varianza y tiene que ver con la dispersi√≥n de los valores predichos. Baja Varianza implica un modelo m√°s estable y menos flexible.\n",
        ":::\n",
        "\n",
        "::: {.callout-important .fragment appearance=\"default\" icon=false}\n",
        "## En general hay que buscar el equilibrio entre ambos tipos de errores:\n",
        "\n",
        "* Alto Sesgo y baja Varianza: **Underfitting**. \n",
        "* Bajo Sesgo y Alta Varianza: **Overfitting**. \n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Model Validation {.smaller}\n",
        "\n",
        "Validaci√≥n Cruzada\n",
        ": > Se refiere al proceso de entrenar un modelo en una cierta porci√≥n de los datos, pero validar sus rendimiento y capacidad de ***generalizaci√≥n*** en un set de datos ***no vistos*** por el modelo al momento de entrenar. \n",
        "\n",
        "::::{style=\"font-size: 120%;\"}\n",
        "::: {.callout-warning icon=false appearance=\"default\" }\n",
        "## ***¬øQu√© es la Generalizaci√≥n?***\n",
        ":::\n",
        "\n",
        "::: {.callout-note icon=false appearance=\"default\" }\n",
        "## Los dos m√©todos m√°s populares que se usan en Machine Learning son **Holdout** y **K-Fold.** M√°s m√©todos se pueden encontrar en los [docs de Scikit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html). \n",
        ":::\n",
        "\n",
        "::: {.callout-important icon=false appearance=\"default\" }\n",
        "## Debido a los vol√∫menes de datos utilizados, el esquema de validaci√≥n m√°s utilizado es el **Holdout**.\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Model Validation: Holdout {.smaller}\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "\n",
        "![](img/clase-4/data_split.png){.lightbox fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column}\n",
        "\n",
        "::: {.callout-note appearance=\"default\" icon=\"false\"}\n",
        "## **Train**\n",
        "Corresponde a la porci√≥n de utilizado para que el modelo aprenda.\n",
        ":::\n",
        "::: {.callout-important appearance=\"default\" icon=\"false\"}\n",
        "## **Validation**\n",
        "Corresponde a la porci√≥n de datos no vistos por el modelo durante el entrenamiento. Se utiliza para medir el nivel de generalizaci√≥n del modelo.\n",
        ":::\n",
        "::: {.callout-caution appearance=\"default\" icon=\"false\"}\n",
        "## **Test**\n",
        "Se utiliza para evaluar reportando una m√©trica de dise√±o del Modelo.\n",
        ":::\n",
        "\n",
        "::: {.callout-caution}\n",
        "A diferencia de un modelo de Machine Learning el proceso de validaci√≥n del modelo se realiza en conjunto con el entrenamiento. Es decir, se entrena y valida el modelo Epoch a Epoch.\n",
        ":::\n",
        ":::\n",
        "::: \n",
        "\n",
        "## Model Validation: K-Fold {.smaller}\n",
        "\n",
        "![](img/clase-4/k-fold.png){.lightbox fig-align=\"center\" width=\"60%\"}\n",
        "\n",
        "::: {.callout-important}\n",
        "Corresponde al proceso de Holdout pero repetido $K$ veces.\n",
        ":::\n",
        "\n",
        "# ¬øQu√© es Pytorch?\n",
        "\n",
        "## Pytorch {.smaller}\n",
        "> Es una librer√≠a de manipulaci√≥n de Tensores especializada en Deep Learning. Provee principalmente, manipulaci√≥n de tensores (igual que Numpy, pero en GPU), adem√°s de Autograd (calcula derivadas de manera autom√°tica).\n",
        "\n",
        "Para poder comenzar a utilizarlo se requieren normalmente 3 imports:\n",
        "\n",
        "```{.python code-line-numbers=\"|1|2|3|\"}\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "```\n",
        "\n",
        "::: {.callout-important appearance=\"default\" icon=false}\n",
        "## üëÄ\n",
        "* `torch` es donde se encuentran la mayor√≠a de funciones b√°sicas para manipular tensores.\n",
        "* `torch.nn` es donde se encuentran los m√≥dulos necesarios para poder crear redes neuronales (neural networks). Cada m√≥dulo es una clase en Python.\n",
        "* `torch.nn.functional` es donde se encontrar√°n `utility functions` adem√°s de versiones funcionales de elementos de `torch.nn`. \n",
        ":::\n",
        "\n",
        "::: {.callout-tip appearance=\"default\"}\n",
        "## üëÄ Una versi√≥n funcional es capaz de replicar la operaci√≥n de un m√≥dulo pero no tiene la capacidad de almacenar los par√°metros aprendidos.\n",
        ":::\n",
        "\n",
        "## Pytorch en GPU {.smaller}\n",
        "\n",
        "La principal ventaja de frameworks como Pytorch es su ejecuci√≥n en GPU, la cual ofrece una enorme capacidad de c√≥mputo por la gran cantidad de n√∫cleos.\n",
        "\n",
        ":::{.callout-tip appearance=\"default\" style=\"font-size: 120%;\" icon=false}\n",
        "## ü§ì Las GPUs usan CUDA (una variante compleja de C++), por lo que los errores suelen ser cr√≠pticos. Por ello se recomienda desarrollar en CPU y pasar a GPU solo cuando el c√≥digo ya funcione correctamente y sea necesario entrenar.\n",
        ":::\n",
        "\n",
        "```{.python style=\"font-size: 120%;\"}\n",
        "## Permite autom√°ticamente reconocer si es que existe GPU en el sistema y de existir lo asigna.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "```\n",
        ":::{.callout-note appearance=\"default\" style=\"font-size: 120%;\" icon=false}\n",
        "## üñ•Ô∏è El c√≥digo anterior es particularmente √∫til para plataformas como Google Colab donde se permite activar o desactivar el uso de GPU.\n",
        ":::\n",
        "\n",
        "```{.python style=\"font-size: 120%;\"}\n",
        "## Fija el entrenamiento en CPU.\n",
        "device = torch.device(\"cpu\")\n",
        "```\n",
        "\n",
        "## Convenciones: Modelo {.smaller}\n",
        "\n",
        "::::{.columns}\n",
        ":::{.column}\n",
        "::: {.callout-caution style=\"font-size: 90%;\" icon=false appearance=\"default\"} \n",
        "## Una capa en Pytorch\n",
        "* Son elementos importados desde `torch.nn`.\n",
        "* Estos m√≥dulos deben ser instanciados para luego ser utilizados.\n",
        "* Cada capa tiene guarda sus par√°metros como `atributos`: \n",
        "  * `.weight.data` y `.bias.data` (para los pesos y bias respectivos).\n",
        "  * **Ojo**: Pytorch utiliza los par√°metros de manera transpuesta a como lo aprendimos en clases.\n",
        "\n",
        ":::\n",
        "\n",
        "```{.python style=\"font-size: 120%;\"}\n",
        "## Ejemplo de un capa de par√°metros en Pytorch\n",
        "## Proyecta desde 4 dimensiones a 12 dimensiones\n",
        "fc1 = nn.Linear(in_features = 4, out_features=12)\n",
        "\n",
        "## Forward Pass:\n",
        "## Calcula las activaciones de la capa\n",
        "fc1(X)\n",
        "\n",
        "```\n",
        "\n",
        ":::{.callout-note style=\"font-size: 90%;\" icon=false appearance=\"default\"}\n",
        "## Output: Activaciones de la capa\n",
        ":::\n",
        "```{.python style=\"font-size: 90%;\"}\n",
        "tensor([[ 0.2620, -0.5313,  0.3907,  ..., -0.0451, -1.2113, -2.9476],\n",
        "        [-1.2096, -0.2586, -0.1372,  ...,  0.1342, -1.1130, -1.4764],\n",
        "        [-1.7676, -0.3754, -0.3786,  ...,  0.2964, -1.2234, -1.6176],\n",
        "        ...,\n",
        "        [-2.1432, -0.3500, -0.5168,  ...,  0.3048, -1.1075, -1.1932],\n",
        "        [-2.6030,  0.0033, -0.6494,  ...,  0.8202, -1.6117, -1.0077],\n",
        "        [ 0.1126, -0.4532,  0.3573,  ...,  0.0660, -1.0688, -2.4856]],\n",
        "       grad_fn=<AddmmBackward0>)\n",
        "```\n",
        ":::\n",
        ":::{.column}\n",
        "\n",
        "::: {.callout-tip style=\"font-size: 90%;\" icon=false appearance=\"default\"} \n",
        "## Un clase en Pytorch permite crear redes m√°s complejas y poseen 2 m√©todos principales:\n",
        "  * Todas las clases deben heredar de `nn.Module`.\n",
        "\n",
        "* `__init__()`:\n",
        "  * Se inicializan los m√≥dulos con `super().__init__()`.\n",
        "  * Se definen las capas de la red como atributos de la clase.\n",
        "    * self.nombre_de_la_capa = nn.Capa(...)\n",
        "    * self.funcion = nn.Funcion(...)\n",
        "* `forward()`:\n",
        "  * Define como se conectan las capas en el Forward Pass.\n",
        "\n",
        ":::\n",
        "\n",
        "```{.python style=\"font-size: 110%;\"}\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    ## Defincici√≥n de capas\n",
        "    self.fc1 = nn.Linear(in_features, out_features)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "model = MLP(in_features=4, out_features = 12)\n",
        "model(X)\n",
        "```\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Crear Modelos m√°s complejos {.smaller}\n",
        "\n",
        "\n",
        "\n",
        "::::{.columns}\n",
        ":::{.column}\n",
        "::: {.callout-caution style=\"font-size: 90%;\" icon=false appearance=\"default\"} \n",
        "## Es posible combinar distintos `nn.Module` en un s√≥lo modelo.\n",
        ":::\n",
        "\n",
        "```{.python style=\"font-size: 120%;\"}\n",
        "class MLP2(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, out_features)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.fc2 = nn.Linear(out_features, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "class SuperMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.mlp1 = MLP(in_features=4, out_features=12)\n",
        "    self.mlp2 = MLP2(in_features=12, out_features=8)\n",
        "  def forward(self, x):\n",
        "    x = self.mlp1(x)\n",
        "    x = self.mlp2(x)\n",
        "    return x\n",
        "\n",
        "super_model = SuperMLP()\n",
        "logits = super_model(X)\n",
        "logits.shape\n",
        "\n",
        "```\n",
        "\n",
        ":::\n",
        ":::{.column}\n",
        "\n",
        "::: {.callout-tip style=\"font-size: 90%;\" icon=false appearance=\"default\"} \n",
        "## Un clase en Pytorch permite crear redes m√°s complejas y poseen 2 m√©todos principales:\n",
        "  * Todas las clases deben heredar de `nn.Module`.\n",
        "\n",
        "* `__init__()`:\n",
        "  * Se inicializan los m√≥dulos con `super().__init__()`.\n",
        "  * Se definen las capas de la red como atributos de la clase.\n",
        "    * self.nombre_de_la_capa = nn.Capa(...)\n",
        "    * self.funcion = nn.Funcion(...)\n",
        "* `forward()`:\n",
        "  * Define como se conectan las capas en el Forward Pass.\n",
        "\n",
        ":::\n",
        "\n",
        "```{.python style=\"font-size: 110%;\"}\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    \n",
        "    ## Defincici√≥n de capas\n",
        "    self.fc1 = nn.Linear(in_features, out_features)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "model = MLP(in_features=4, out_features = 12)\n",
        "model(X)\n",
        "```\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Convenciones: Optimizador y Loss Function {.smaller}\n",
        "\n",
        "* En Pytorch utilizaremos la siguiente nomenclatura:\n",
        "  * Un `nn.Module` es una Red o un M√≥dulo de una Red.\n",
        "  * Varios `nn.Module` pueden ser combinados en distintas formas para crear redes m√°s complejas.\n",
        "* Pytorch requiere fijar el Modo de la Red. Para entrenar se utiliza `model.train()`, mientras que para evaluar se utiliza `model.eval()`.\n",
        "* El `Loss Function` a utilizar le llamaremos `criterion`.\n",
        "  * Pero el error de la red lo llamaremos `loss`.\n",
        "* El c√°lculo de gradientes lo llamaremos `loss.backward()`. Esto calcula los gradientes y los acumula en cada par√°metro del modelo.\n",
        "  * Pytorch tiene la costumbre de Acumular Gradientes. Por lo tanto, antes de cada Backward Pass se debe reiniciar los gradientes a cero utilizando `optimizer.zero_grad()`.\n",
        "* El optimizador lo llamaremos `optimizer`. Y llamaremos al proceso de actualizar pesos como `optimizer.step()`.\n",
        "\n",
        "* Supongamos el caso particular en el cual queremos resolver un problema de clasificaci√≥n binaria. ¬øCu√°nto valdr√≠a $k$ y cu√°l ser√≠a la **Loss Function** a utilizar?\n",
        "\n",
        "* Supongamos que queremos transformar una Matriz $X$ de 1000 registros y 10 variables. Adem√°s tenemos un vector $y$ el cu√°l queremos predecir. \n",
        "* Supongamos que queremos llevar a 32 variables, luego a 64 para luego generar nuestra predicci√≥n. \n",
        "* Supongamos adem√°s que queremos usar como funci√≥n de activaci√≥n la funci√≥n ReLU  en ambas capas de transformaci√≥n.\n",
        ":::\n",
        "::: {.callout-caution .fragment}\n",
        "¬øC√≥mo definimos los 3 elementos principales de una red? \n",
        ":::\n",
        "::: {.callout-tip .fragment}\n",
        "(***Hip√≥tesis***, ***Loss Function*** y ***Optimizador***)\n",
        ":::\n",
        "\n",
        "\n",
        "![](img/clase-2/formal_nn.png){.lightbox fig-align=\"center\" width=\"80%\"}  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Training-Validation Loop {.smaller}\n",
        "\n",
        "> Corresponde a la modificaci√≥n del Training Loop con el Objetivo de Entrenar y Validar de manera simult√°nea.\n",
        "\n",
        "![](img/clase-4/forward_train.png){.lightbox fig-align=\"center\" width=\"60%\"}\n",
        "\n",
        "::: {.callout-note}\n",
        "Se realiza un Forward Pass con datos de **Train** y se calcula el Loss asociado. Internamente, Pytorch comienza a acumular Gradientes.\n",
        ":::\n",
        "\n",
        "## Training-Validation Loop {.smaller}\n",
        "\n",
        "![](img/clase-4/backward_train.png){.lightbox fig-align=\"center\"}\n",
        "\n",
        "::: {.callout-note}\n",
        "Se realiza un Backward Pass, se aplican los gradientes y se aplica el Update Rule.\n",
        ":::\n",
        "\n",
        "## Training-Validation Loop {.smaller}\n",
        "\n",
        "![](img/clase-4/forward_val.png){.lightbox fig-align=\"center\"}\n",
        "\n",
        "::: {.callout-note}\n",
        "Se realiza un nuevo Forward Pass, pero esta vez con los datos de **Validaci√≥n**. En este caso Pytorch internamente sigue acumulando gradientes, lo cual no es correcto. Para ello se debe utilizar un `with torch.no_grad()`. Se calcula un Validation Loss.\n",
        ":::\n",
        "\n",
        "## Monitoreo de un Modelo: Validation Curve {.smaller}\n",
        "\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "![](img/clase-4/validation_curve.png){.lightbox fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        "::: {.callout-important}\n",
        "Es importante ser capaz de identificar el momento exacto en el cual el momento comienza su overfitting. Para ello se utiliza el **\"Checkpointing\"**. \n",
        ":::\n",
        "\n",
        "::: {.callout-note appearance=\"default\"}\n",
        "## Checkpoint\n",
        "\n",
        "* Corresponde a un snapshot del modelo a un cierto punto. En la **pr√°ctica** se almacenan los par√°metros del **mejor modelo** y del **√∫ltimo Epoch**.\n",
        ":::\n",
        "\n",
        "::: {.callout-tip appearance=\"default\"}\n",
        "## EarlyStopping\n",
        "\n",
        "* Teoricamente, una vez que la red Neuronal alcanza el punto de Overfitting ya no tiene sentido seguir el entrenamiento. Por lo tanto es posible detener el entrenamiento bajo una cierta condici√≥n.\n",
        ":::\n",
        ":::\n",
        "::: \n",
        "\n",
        "\n",
        "\n",
        "## Model Evaluation {.smaller}\n",
        "\n",
        "> La Evaluaci√≥n del Modelo se har√° en torno a una m√©trica definida a priori por el modelador.\n",
        "\n",
        "::: {.callout-important}\n",
        "La m√©trica a utilizar est√° √≠ntimamente ligada al tipo de modelo.\n",
        ":::\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "#### Clasificaci√≥n\n",
        "\n",
        "* $Accuracy = \\frac{1}{m} \\sum_{i = 1}^m 1\\{y_i = \\hat{y_i}\\}$\n",
        "* $Precision = \\frac{TP}{TP + FP}$\n",
        "* $Recall = \\frac{TP}{TP + FN}$\n",
        "* $F1-Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.column}\n",
        "#### Regresi√≥n\n",
        "* $RMSE = \\frac{1}{m} \\sum_{i=1}^m (y_i-\\hat{y_i})^2$\n",
        "* $MAE = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y_i}|$\n",
        "* $MAPE = 100 \\cdot \\frac{1}{m} \\sum_{i=1}^m \\frac{|y_i-\\hat{y_i}|}{max(\\epsilon,y_i)}$\n",
        "* $SMAPE = \\frac{2}{m} \\sum_{i=1}^2 \\frac{|y_i - \\hat{y_i}  |}{max(|y_i + \\hat{y_i}|,\\epsilon)}$\n",
        ":::\n",
        "::: \n",
        "\n",
        "::: {.callout-warning}\n",
        "* Las m√©tricas ac√° explicadas son m√©tricas b√°sicas de cualquier modelo general de Clasificaci√≥n y Regresi√≥n. Existen muchas otras m√©tricas que son espec√≠ficas para campos espec√≠ficos. IoU por ejemplo es una M√©trica de Segmentaci√≥n Sem√°ntica, Map\\@k es una m√©trica para modelos de Recomendaci√≥n, Bleu o Rouge son m√©tricas para NLP, etc. Para ver millones de m√©tricas pueden ver las [docs de Torchmetrics](https://lightning.ai/docs/torchmetrics/stable/all-metrics.html).\n",
        ":::\n",
        "::: {.callout-tip}\n",
        "Es posible utilizar m√©tricas para ir ***monitoreando*** el progreso del modelo Epoch a Epoch. \n",
        ":::\n",
        "\n",
        "# Continuar√°\n",
        "\n",
        "::: {.footer}\n",
        "<p xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dct=\"http://purl.org/dc/terms/\"><span property=\"dct:title\">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia est√° licenciado bajo <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC-SA 4.0\n",
        "\n",
        "<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\"></a></p>\n",
        ":::"
      ],
      "id": "0352822d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}