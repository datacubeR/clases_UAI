<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.32">

  <meta name="author" content="Alfonso Tobar-Arancibia">
  <title>Clases UAI – TICS-579-Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-673c2e7d040da7fd4b9d655d29f657a0.css">
  <link rel="stylesheet" href="../logo.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">TICS-579-Deep Learning</h1>
  <p class="subtitle">Clase 6: Redes Convolucionales</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Alfonso Tobar-Arancibia 
</div>
        <p class="quarto-title-affiliation">
            <a href="mailto:alfonso.tobar.a@edu.uai.cl" class="email">alfonso.tobar.a@edu.uai.cl</a>
          </p>
    </div>
</div>

</section>
<section id="limitaciones-de-las-ffn" class="slide level2 smaller">
<h2>Limitaciones de las FFN</h2>
<p>Sin duda las Redes Feed Forward son una herramienta poderosa para resolver problemas de clasificación y regresión. Sin embargo, presentan ciertas limitaciones cuando se aplican a datos con estructuras espaciales o temporales, como imágenes o secuencias de texto.</p>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>⚠️ Pérdida de estructura espacial o secuencial</strong></p>
</div>
<div class="callout-content">
<p>Cada registro es considerado de manera independiente, sin tener en cuenta la relación espacial o secuencial entre los datos.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☢️ Gran cantidad de parámetros</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/MNIST.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img data-src="img/clase-6/MNIST.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/MNIST_net.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img data-src="img/clase-6/MNIST_net.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:30%;">
<h4 id="número-de-parametros">Número de Parametros</h4>
<h4 id="imagen-28x28784-píxeles">(Imagen: 28x28=784 píxeles):</h4>
<ul>
<li><span class="math inline">\(W_1 = 784 \cdot 256 + 256 = 200960\)</span></li>
<li><span class="math inline">\(W_2 = 256 \cdot 128 + 128 = 32896\)</span></li>
<li><span class="math inline">\(W_3 = 128 \cdot 10 + 10 = 1290\)</span></li>
<li>Total = 235,146.</li>
</ul>
<div class="fragment">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>¿Y si tengo una imágen de <span class="math inline">\(512 \times 512\)</span>? <strong>67,143,306</strong> de parámetros.</p>
</div>
</div>
</div>
</div>
</div></div>
</section>
<section id="limitaciones-de-las-ffn-1" class="slide level2 smaller">
<h2>Limitaciones de las FFN</h2>
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>🚧 Ineficiencia en el aprendizaje de patrones locales</strong></p>
</div>
<div class="callout-content">
<dl>
<dt>Translation Invariance</dt>
<dd>
Se refiere a la capacidad de poder detectar un patrón/objeto en diferentes posiciones de la imágen.
</dd>
</dl>
<p><strong><em>Problema</em></strong>: Un perrito centrado, desplazado a la izquierda o a la derecha debería seguir siendo reconocido como un perrito. Para una FFN, las features que describen los perritos desplazados son completamente distintos.</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/translation_invariance.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img data-src="img/clase-6/translation_invariance.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</section>
<section id="limitaciones-de-las-ffn-2" class="slide level2 smaller">
<h2>Limitaciones de las FFN</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>⛔ Escalabilidad Limitada</strong></p>
</div>
<div class="callout-content">
<p>Su alto número de parámetros sumado a la incapacidad de capturar patrones espaciales o temporales hace que las FFN no escalen bien a datos complejos como imágenes de alta resolución o secuencias largas haciendo que su rendimiento disminuya considerablemente y no sean aplicables por sí solas a casos reales.</p>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<h4 id="imagenes-actuales-cada-vez-más-grandes">Imagenes actuales cada vez más grandes</h4>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/image_diff.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img data-src="img/clase-6/image_diff.jpg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column">
<h4 id="textos-actuales-cada-vez-más-largos">Textos actuales cada vez más largos</h4>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/text_diff.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img data-src="img/clase-6/text_diff.jpg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="imágenes" class="slide level2 smaller">
<h2>Imágenes</h2>
<div class="columns">
<div class="column">
<dl>
<dt>Imagen</dt>
<dd>
Definiremos una imágen como un Tensor de Orden 3. Normalmente cada dimensión representa H, W y C (Altura, Ancho y Canales).
</dd>
</dl>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Convención en Pytorch</strong></p>
</div>
<div class="callout-content">
<p>Pytorch utiliza la convención (C, H, W) para representar imágenes, donde C es el número de canales, H es la altura y W es el ancho de la imagen. Es decir, un Tensor de Dimensiones (3, 512, 512)</p>
</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/channels.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img data-src="img/clase-6/channels.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></a></p>
</figure>
</div>
</div></div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>La convención más común es utilizar imágenes de 24-bits, es decir 3 canales de <span class="math inline">\(2^8\)</span> valores (8-bits por canal). Es por eso que el valor de los píxeles va de 0 a 255 y representan la intensidad del color del canal que representan.</p>
</div>
</div>
</div>
</section>
<section id="imágenes-1" class="slide level2 smaller">
<h2>Imágenes</h2>
<div style="font-size: 80%;">
<blockquote>
<p>Librerías como <code>PIL</code> u <code>OpenCV</code> permiten importar imágenes en Python. Ambas usan la convención de <span class="math inline">\((H,W,C)\)</span>, la diferencia está en el orden de los canales. <code>PIL</code> utiliza la convención RGB, mientras que <code>OpenCV</code> utiliza BGR por lo que se necesitan algunas transformaciones adicionales.</p>
</blockquote>
</div>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/Kira_channels.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img data-src="img/clase-6/Kira_channels.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div><div class="column">
<h4 id="ejemplo-para-importar-imágenes-con-pil-y-pytorch">Ejemplo para importar imágenes con PIL y Pytorch</h4>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a></a>path <span class="op">=</span> <span class="st">"path/to/imagen.png"</span></span>
<span id="cb1-5"><a></a>img <span class="op">=</span> Image.<span class="bu">open</span>(path)</span>
<span id="cb1-6"><a></a></span>
<span id="cb1-7"><a></a><span class="co"># Convierte a Tensor y cambia a (C,H,W)</span></span>
<span id="cb1-8"><a></a>torch_image<span class="op">=</span> torch.from_numpy(np.array(img)).permute(<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)  </span>
<span id="cb1-9"><a></a>torch_image.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(3,1200,1200)</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a><span class="co">## Imágen en canal Rojo</span></span>
<span id="cb3-4"><a></a>plt.imshow(torch_image[<span class="dv">0</span>].numpy(), cmap<span class="op">=</span><span class="st">"Reds"</span>)</span>
<span id="cb3-5"><a></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb3-6"><a></a>plt.show()</span>
<span id="cb3-7"><a></a><span class="co">## Imágen en canal Verde</span></span>
<span id="cb3-8"><a></a>plt.imshow(torch_image[<span class="dv">1</span>].numpy(),cmap<span class="op">=</span><span class="st">"Greens"</span>)</span>
<span id="cb3-9"><a></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb3-10"><a></a>plt.show()</span>
<span id="cb3-11"><a></a><span class="co">## Imágen en canal Azul</span></span>
<span id="cb3-12"><a></a>plt.imshow(torch_image[<span class="dv">2</span>].numpy(), cmap<span class="op">=</span><span class="st">"Blues"</span>)</span>
<span id="cb3-13"><a></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb3-14"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div>
</section>
<section id="batch-de-imágenes" class="slide level2 smaller">
<h2>Batch de Imágenes</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Conjunto/Set/Batch de Imágenes</strong></p>
</div>
<div class="callout-content">
<p>Se define como un Tensor de Orden 4. En Pytorch esto se representa como N, C, H, W (Número de Imágenes, Canales, Altura y Ancho).</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/image_batch.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img data-src="img/clase-6/image_batch.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a></p>
</figure>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Luego un Tensor de Dimensiones (32,3,224,512) implica que tenemos 32 imágenes RGB de dimensiones <span class="math inline">\(224\times512\)</span>.</p>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="co">## Simulación de 2 imágenes RGB de 5x5 píxeles</span></span>
<span id="cb4-2"><a></a>torch.randint(<span class="dv">0</span>,<span class="dv">256</span>, (<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="font-size: 90%;">
<pre><code>tensor([[[[248, 240, 146,  73, 228],
          [ 79, 125, 191, 203, 133],
          [202,  12, 237, 109,  62],
          [133, 227, 148,  78, 229],
          [121, 247, 202,  51,   3]],

         [[253,  28,  20, 144, 255],
          [115, 132, 114,  45, 164],
          [ 57, 238, 117, 250,  41],
          [ 58,  73,  29, 253, 240],
          [246,  84,  93,   2, 145]],

         [[ 83,   4, 144, 126, 202],
          [ 98, 235,  55,  83, 104],
          [ 21, 185,  27, 102, 117],
          [255, 133,  23,  83, 150],
          [ 49, 152,  81, 233,  98]]],
-----------------------------------------------
        [[[216,  92, 251, 214, 178],
          [252,  48,  88,  82,  79],
          [168, 208, 223,   9, 169],
          [145, 148, 254, 128, 156],
          [238, 175, 233, 136, 118]],

         [[112,  68, 143,  93, 150],
          [ 32, 103,  97,  93, 223],
          [205,  56,  90,  24, 108],
          [ 13, 135,  98,  20,  93],
          [ 20,  91,  37,  81,  10]],

         [[109, 145,  90, 243,  63],
          [103, 134, 130,  11,  72],
          [132, 163, 153,  26, 255],
          [ 45, 228,  26, 169, 212],
          [ 34, 211, 229,  82, 201]]]])</code></pre>
</div>
</div></div>
</section>
<section id="redes-convolucionales-definición-e-inspiración" class="slide level2 smaller">
<h2>Redes Convolucionales: Definición e Inspiración</h2>
<dl>
<dt>Redes Convolucionales (CNN)</dt>
<dd>
Son un tipo de red neuronal cuyos parámetros entrenables son filtros (también llamados <strong><em>Kernels</em></strong>) que aprenden a detectar patrones en los datos de entrada.
</dd>
</dl>
<p>El resultado de una Convolucional es un <strong><em>feature map</em></strong>, el cual representa la presencia y localización de ciertos patrones visuales.</p>
<div class="columns">
<div class="column">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Existe el mito de que las Redes Convolucionales se inspiraron en el funcionamiento del Cortex Visual humano. <strong>No sé si es tan así</strong>.</p>
<p>El mito dice que las CNNs fueron diseñadas para imitar el cortex visual humano. Esto viene de los trabajos de Hubel y Wiesel (década de 1960), que estudiaron cómo las neuronas en la corteza visual de gatos respondían a estímulos:</p>
<ul>
<li>Descubrieron neuronas simples que respondían a líneas en cierta orientación y posición.</li>
<li>Descubrieron neuronas complejas que respondían a patrones similares, pero en distintas posiciones (invarianza local).</li>
</ul>
</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/visual_cat.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img data-src="img/clase-6/visual_cat.png" class="quarto-figure quarto-figure-center" style="width:90.0%"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="redes-convolucionales-definición-e-inspiración-1" class="slide level2 smaller">
<h2>Redes Convolucionales: Definición e Inspiración</h2>
<div class="callout callout-caution callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p><strong><em>¿Por qué necesitamos las Redes Convolucionales?</em></strong> Evitar la sobreparametrización. ¿Por qué esto es un problema?</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>🔔 Importancia</strong></p>
</div>
<div class="callout-content">
<p>No es exagerado afirmar que las CNNs han sido la arquitectura más influyente en Deep Learning, ya que han impulsado avances importantes en tareas de visión por computador, como clasificación de imágenes, detección de objetos y segmentación semántica. Además, contribuyeron a que el Deep Learning ganara popularidad en la industria tecnológica y superara la época conocida como el <strong><em>AI Winter</em></strong>.</p>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<h4 id="algunos-hitos-importantes">🗓️ Algunos hitos importantes:</h4>
<ul>
<li><strong>1990</strong>: Yann LeCun et al.&nbsp;propone uno de los primeros intentos de CNN, el cual va agregando features más simples en features más complejas progresivamente.</li>
<li><strong>1998</strong>: Yann LeCun, propone LeNet-5 con 2 redes convolucionales y 3 FFN.</li>
<li><strong>2012</strong>: Krizhevsky, Sutskever y Hinton proponen AlexNet (5 capas convolucionales y 3 FFN), el cual obtiene <strong>SOTA performance</strong> en ImageNet.</li>
</ul>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/alexnet-paper.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img data-src="img/clase-6/alexnet-paper.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="convolutional-neural-network-cnns" class="slide level2 smaller">
<h2>Convolutional Neural Network (CNNs)</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Architectura General</strong></p>
</div>
<div class="callout-content">
<p>Una Convolutional Neural Network (CNN) está formada por múltiples capas que colaboran para identificar y extraer características significativas de las imágenes, con el fin de clasificarlas o detectar objetos dentro de ellas.</p>
</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-7/CNN-arch.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img data-src="img/clase-7/CNN-arch.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Feature Extractor - Encoder - Backbone</strong></p>
</div>
<div class="callout-content">
<p>Corresponde al bloque en el que se detectan características o patrones relevantes de la imagen. En este bloque es donde se aplican normalmente las operaciones de <strong><em>Convolución</em></strong> y <strong><em>Pooling</em></strong>.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Flatten</strong></p>
</div>
<div class="callout-content">
<p>Corresponde a una operación intermedia que aplana los feature maps generadas para ser utilizados como features de entrada para ser utilizados por la parte final de la red.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Prediction Head - Head - MLP</strong></p>
</div>
<div class="callout-content">
<p>Corresponde a una FFN que tomará las features aprendidas por la CNN y generará una predicción.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="la-convolución" class="slide level2 smaller">
<h2>La Convolución</h2>
<div style="font-size: 80%;">
<dl>
<dt>Convolución</dt>
<dd>
Corresponde a una operación que permite extraer <strong><em>feature maps</em></strong>, donde un filtro o kernel se desplaza sobre distintas secciones de los datos, ya sea una secuencia, una imagen o un video, para capturar sus patrones más relevantes.
</dd>
</dl>
</div>
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>☝️Atención</strong></p>
</div>
<div class="callout-content">
<p>Esto es nuevamente un término marketero, porque no es una Convolucional real, sino una operación llamada <strong>Cross Correlation</strong>.</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/convolution.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img data-src="img/clase-6/convolution.gif" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
<div style="font-size: 80%;">
<dl>
<dt>Feature Map</dt>
<dd>
Corresponde a la salida de una convolución (equivalente a la Activación) y es un nuevo tensor que captura ciertas características del dato (secuencia, imagen o video). Cuando se trata de imágenes, captura features como bordes, cambios de textura, color, formas, o elementos más pequeños.
</dd>
</dl>
</div>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Es importante notar que los features maps son de una <strong>tamaño menor a la entrada</strong> debido a la operación de Convolución.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Se obtendrán tantos feature maps como filtros se apliquen. Esto es otro hiperparámetro de la Red Convolucional que se conoce como los canales de salida o <strong><em>out_channels</em></strong>.</p>
</div>
</div>
</div>
</section>
<section id="el-filtro-o-kernel" class="slide level2 smaller">
<h2>El filtro o Kernel</h2>
<div class="columns">
<div class="column">
<h5 id="gaussian-blur">Gaussian Blur</h5>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/gaussian_blur.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img data-src="img/clase-6/gaussian_blur.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></a></p>
</figure>
</div>
<h5 id="líneas-horizontales">Líneas Horizontales</h5>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/horizontal_lines.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img data-src="img/clase-6/horizontal_lines.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></a></p>
</figure>
</div>
<h5 id="bordes">Bordes</h5>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/bordes.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img data-src="img/clase-6/bordes.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></a></p>
</figure>
</div>
</div><div class="column">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Kernel</strong></p>
</div>
<div class="callout-content">
<p>Corresponde a una matriz pequeña que permite detectar patrones específicos en la imagen al aplicarse de manera móvil sobre ella. Estos Kernel solías estudiarse y diseñarse manualmente para tareas específicas como detección de bordes, desenfoque, realce de contraste, entre otros.</p>
<p>En una red convolucional, el kernel es un conjunto de pesos que se ajustan durante el proceso de entrenamiento para identificar características relevantes en las imágenes. Es decir, la CNN aprende qué Kernels son más importantes para la tarea que se está resolviendo.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>El Kernel se aplica a todos los canales a la vez, lo cuál inicialmente lo hace ver como una operación bastante costosa computacionalmente.</p>
</div>
</div>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>El Kernel introduce el primer hiperparámetro de las CNN que es el <strong>Kernel Size</strong>. En general son cuadrados, y de dimensión impar.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="feature-maps" class="slide level2 smaller">
<h2>Feature Maps</h2>
<div style="font-size: 80%;">
<dl>
<dt>Feature Map</dt>
<dd>
Corresponde a la salida de una convolución (equivalente a la Activación) y es un nuevo tensor que captura ciertas características del dato (secuencia, imagen o video). Cuando se trata de imágenes, captura features como bordes, cambios de textura, color, formas, o elementos más pequeños.
</dd>
</dl>
</div>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/feature_maps.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img data-src="img/clase-6/feature_maps.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Básicamente los feature maps son imágenes que resaltan ciertos patrones aprendidos por los kernels.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>A medida que avanzamos en las capas convolucionales, los feature maps tienden a capturar patrones más complejos y abstractos.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Cada feature map es de tamaño más pequeño que la imagen original, pero contiene información más relevante para la tarea de clasificación o detección.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Se obtendrán tantos feature maps como filtros se apliquen. Esto es otro hiperparámetro de la Red Convolucional que se conoce como los canales de salida o <strong><em>out_channels</em></strong>.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
</div></div>
</section>
<section id="hiperparámetros-de-la-convolución" class="slide level2 smaller">
<h2>Hiperparámetros de la Convolución</h2>
<div class="columns">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/stride.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img data-src="img/clase-6/stride.gif" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Stride</strong></p>
</div>
<div class="callout-content">
<p>Hace referencia al número de posiciones que el kernel se desplaza sobre la imagen de entrada en cada paso. Un <strong><em>stride</em></strong> más grande produce feature maps más pequeños y con menos detalle, mientras que un stride más pequeño preserva mayor información, aunque incrementa la cantidad de operaciones necesarias.</p>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/convolution_padding.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img data-src="img/clase-6/convolution_padding.gif" class="quarto-figure quarto-figure-center" style="width:130.0%"></a></p>
</figure>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Padding</strong></p>
</div>
<div class="callout-content">
<p>Consiste en añadir un relleno alrededor de la imagen de entrada para facilitar el desplazamiento del kernel y evitar que la convolución reduzca en exceso sus dimensiones. Este relleno también permite conservar la información presente en los bordes de la imagen. Cuando no se aplica padding, la operación se denomina <strong><em>“valid”</em></strong>, mientras que si se agregan los píxeles necesarios para mantener el tamaño original, se conoce como <strong><em>“same”</em></strong>.</p>
</div>
</div>
</div>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/dilation.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img data-src="img/clase-6/dilation.gif" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Dilation</strong></p>
</div>
<div class="callout-content">
<p>Hace referencia a los espacios o intervalos que se insertan entre los elementos del kernel durante la convolución. El uso de dilation permite ampliar el campo receptivo de la red, capturando un mayor contexto sin aumentar el tamaño del kernel. Un valor de 1 indica que no se aplica <strong><em>dilation</em></strong>.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="convolución-en-pytorch" class="slide level2 smaller">
<h2>Convolución en Pytorch</h2>
<div style="font-size: 130%;">
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>nn.Conv2d(in_channels, out_channels, kernel_size, stride<span class="op">=</span><span class="dv">1</span>,padding<span class="op">=</span><span class="dv">0</span>,dilation<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="font-size: 80%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Input</strong></p>
</div>
<div class="callout-content">
<p>Este tipo de redes no requiere que se le den las dimensiones de las entradas, pero sí espera recibir tensores de dimensión <span class="math inline">\((N,C_{in}, H_{in},W_{in})\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Output</strong></p>
</div>
<div class="callout-content">
<p>La Red convolucional devuelve un Tensor de Dimensiones <span class="math inline">\((N,C_{out}, H_{out}, W_{out})\)</span>. Donde:</p>
<p><span class="math display">\[H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - dilation[0]\cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor\]</span> <span class="math display">\[W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Es importante tener noción del tamaño de la imagen para poder escoger un <em>kernel_size</em> que recorra la imagen completa y que no deje partes sin convolucionar.</p>
</div>
</div>
</div>
</div>
</section>
<section id="partes-de-una-cnn-pooling" class="slide level2 smaller">
<h2>Partes de una CNN: Pooling</h2>
<dl>
<dt>Pooling</dt>
<dd>
El Pooling es una operación de agregación que permite ir disminuyendo el tamaño de las entradas. De esta manera la red puede comenzar a especializarse en aspectos cada vez más finos.
</dd>
</dl>
<div class="columns">
<div class="column">
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>El Pooling también se aplica de manera móvil como una convolución. Pero a diferencia de esta normalmente no genera traslape.</p>
</div>
</div>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Acá se introduce otro hiperparámetro que es el <strong><em>Pooling Size</em></strong>. En general es cuadrado y de dimensión par, y utiliza un stride del mismo tamaño que el <strong><em>Pooling Size</em></strong> para evitar traslapes.</p>
</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/pooling_gif.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img data-src="img/clase-6/pooling_gif.gif" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="pooling-in-pytorch" class="slide level2 smaller">
<h2>Pooling in Pytorch</h2>
<div style="font-size: 90%;">
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>nn.AvgPool2d(kernel_size, stride<span class="op">=</span><span class="va">None</span>,padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-2"><a></a>nn.MaxPool2d(kernel_size, stride<span class="op">=</span><span class="va">None</span>,padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Ojo</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Pytorch llama también <code>kernel_size</code> al tamaño del Pooling.</li>
<li><code>stride=None</code> implica <code>stride = kernel_size</code>.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>MaxPool</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - dilation[0]\cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor\]</span> <span class="math display">\[W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>AvgPool</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[0] - kernel\_size[0]}{stride[0]} + 1 \right\rfloor\]</span> <span class="math display">\[W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[1] - kernel\_size[1]}{stride[1]} + 1 \right\rfloor\]</span></p>
</div>
</div>
</div>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>El Average Pool no permite Dilation.</p>
</div>
</div>
</div>
</div>
</section>
<section id="adaptivepooling" class="slide level2 smaller">
<h2>AdaptivePooling</h2>
<blockquote>
<p>La mayoría de las arquitecturas CNN modernas aplican un procedimiento llamado <strong><em>Adaptive Pooling</em></strong> antes de la etapa de predicción (FFN). Independientemente del tamaño de la imagen de entrada, el Adaptive Pooling siempre genera una salida de tamaño fijo, ya que ajusta sus parámetros para asegurar que la dimensión de salida sea la deseada.</p>
</blockquote>
<div class="columns">
<div class="column">
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>nn.AdaptiveAvgPool2d(output_size)</span>
<span id="cb8-2"><a></a>nn.AdaptiveMaxPool2d(output_size, return_indices<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong><code>return_indices=True</code> permite devolver en qué posiciones se encontraron los valores máximos, lo cual es útil para operaciones de <strong><em>unpooling</em></strong> para revertir el proceso de pooling.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/pooling.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img data-src="img/clase-6/pooling.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="mnist-con-cnn" class="slide level2 smaller">
<h2>MNIST con CNN</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/CNN_MNIST_arch.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img data-src="img/clase-6/CNN_MNIST_arch.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/CNN_params.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23"><img data-src="img/clase-6/CNN_params.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
<div class="callout callout-warning callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>El número de Parámetros para una Red Convolucional con muchas más capas bajó considerablemente, de 67M a 373K de Parámetros para una imagen de <span class="math inline">\(512 \times 512\)</span>.</li>
</ul>
</div>
</div>
</div>
</div></div>
</section>
<section id="grafo-cnn-sencilla" class="slide level2 smaller">
<h2>Grafo CNN sencilla</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/conv_graph.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24"><img data-src="img/clase-6/conv_graph.png" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="co">## Una Imagen de 1 Canal de Tamaño 6x6</span></span>
<span id="cb9-2"><a></a>X <span class="op">=</span> torch.tensor([</span>
<span id="cb9-3"><a></a>        [[[<span class="fl">7.</span>, <span class="fl">6.</span>, <span class="fl">8.</span>, <span class="fl">5.</span>, <span class="fl">1.</span>, <span class="fl">3.</span>],</span>
<span id="cb9-4"><a></a>          [<span class="fl">8.</span>, <span class="fl">6.</span>, <span class="fl">5.</span>, <span class="fl">3.</span>, <span class="fl">5.</span>, <span class="fl">5.</span>],</span>
<span id="cb9-5"><a></a>          [<span class="fl">9.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">5.</span>, <span class="fl">3.</span>, <span class="fl">5.</span>],</span>
<span id="cb9-6"><a></a>          [<span class="fl">4.</span>, <span class="fl">5.</span>, <span class="fl">5.</span>, <span class="fl">9.</span>, <span class="fl">2.</span>, <span class="fl">6.</span>],</span>
<span id="cb9-7"><a></a>          [<span class="fl">9.</span>, <span class="fl">5.</span>, <span class="fl">3.</span>, <span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">2.</span>],</span>
<span id="cb9-8"><a></a>          [<span class="fl">4.</span>, <span class="fl">4.</span>, <span class="fl">8.</span>, <span class="fl">8.</span>, <span class="fl">9.</span>, <span class="fl">8.</span>]]]</span>
<span id="cb9-9"><a></a>])</span>
<span id="cb9-10"><a></a>X.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(1,1,6,6)</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>C_out <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb11-2"><a></a>N, C_in, H, W <span class="op">=</span> X.shape</span>
<span id="cb11-3"><a></a>kH, kW <span class="op">=</span> (<span class="dv">3</span>,<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="co">## 2 filtros de un Canal de tamaño 3x3</span></span>
<span id="cb12-2"><a></a>given_w <span class="op">=</span> torch.tensor([</span>
<span id="cb12-3"><a></a>        [[[<span class="op">-</span><span class="fl">1.</span>,  <span class="fl">0.</span>,  <span class="fl">1.</span>],</span>
<span id="cb12-4"><a></a>          [ <span class="fl">0.</span>, <span class="op">-</span><span class="fl">1.</span>,  <span class="fl">1.</span>],</span>
<span id="cb12-5"><a></a>          [ <span class="fl">0.</span>,  <span class="fl">0.</span>,  <span class="fl">1.</span>]]],</span>
<span id="cb12-6"><a></a></span>
<span id="cb12-7"><a></a>        [[[<span class="op">-</span><span class="fl">1.</span>, <span class="op">-</span><span class="fl">1.</span>,  <span class="fl">0.</span>],</span>
<span id="cb12-8"><a></a>          [ <span class="fl">1.</span>,  <span class="fl">0.</span>,  <span class="fl">0.</span>],</span>
<span id="cb12-9"><a></a>          [ <span class="fl">1.</span>,  <span class="fl">1.</span>,  <span class="fl">0.</span>]]]])</span>
<span id="cb12-10"><a></a>given_w.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(2,1,3,3)</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>given_bias <span class="op">=</span> torch.tensor([<span class="fl">1.</span>, <span class="fl">1.</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div>
</section>
<section id="grafo-cnn-sencilla-convolución" class="slide level2 smaller">
<h2>Grafo CNN sencilla: Convolución</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/convolution.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-25"><img data-src="img/clase-6/convolution.gif" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="kw">def</span> calculate_out(X, k_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>), stride<span class="op">=</span><span class="dv">1</span>, dilation<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb15-2"><a></a>  kH, kW <span class="op">=</span> k_size</span>
<span id="cb15-3"><a></a>  N, in_channels, H_in, W_in <span class="op">=</span> X.shape</span>
<span id="cb15-4"><a></a>  out_H <span class="op">=</span> np.floor((H_in <span class="op">+</span><span class="dv">2</span><span class="op">*</span>padding<span class="op">-</span>dilation<span class="op">*</span>(kH<span class="op">-</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>stride <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb15-5"><a></a>  out_W <span class="op">=</span> np.floor((W_in <span class="op">+</span><span class="dv">2</span><span class="op">*</span>padding<span class="op">-</span>dilation<span class="op">*</span>(kW<span class="op">-</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>stride <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb15-6"><a></a>  <span class="cf">return</span> <span class="bu">int</span>(out_H), <span class="bu">int</span>(out_W)</span>
<span id="cb15-7"><a></a></span>
<span id="cb15-8"><a></a>H_out, W_out <span class="op">=</span> calculate_out(X, k_size <span class="op">=</span> (kH,kW))</span>
<span id="cb15-9"><a></a>H_out, W_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(4,4)</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>O <span class="op">=</span> torch.zeros((N, C_out, H_out, W_out))</span>
<span id="cb17-2"><a></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb17-3"><a></a>    <span class="cf">for</span> co <span class="kw">in</span> <span class="bu">range</span>(C_out):</span>
<span id="cb17-4"><a></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(H_out):</span>
<span id="cb17-5"><a></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(W_out):</span>
<span id="cb17-6"><a></a>                <span class="co"># submatriz de tamaño kH x kW</span></span>
<span id="cb17-7"><a></a>                patch <span class="op">=</span> X[n, :, i:i<span class="op">+</span>kH, j:j<span class="op">+</span>kW]</span>
<span id="cb17-8"><a></a>                O[n, co, i, j] <span class="op">=</span> (patch <span class="op">*</span> given_w[co]).<span class="bu">sum</span>() <span class="op">+</span> given_bias[co]</span>
<span id="cb17-9"><a></a>O</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([[[[  5.,   5.,  -1.,  -4.],
          [ -4.,  -7.,  -1.,  -6.],
          [ -1., -10.,   6.,  -8.],
          [ -9.,  -8.,  -6.,  -6.]],

         [[ -3.,   5.,   3.,  -6.],
          [ -3.,  -7.,   3., -13.],
          [ -5., -12.,   4.,  -7.],
          [ -9.,  -4.,  -6.,  -5.]]]])</code></pre>
</div></div>
<div class="callout callout-caution callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Este proceso es <strong><em>extremadamente ineficiente</em></strong> computacionalmente hablando. Por lo que se utiliza un proceso equivalente llamado <strong><em>im2col</em></strong>.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
</section>
<section id="im2col" class="slide level2 smaller">
<h2>im2col</h2>
<blockquote>
<p><strong><em>im2col</em></strong> es un algoritmo que permite transformar la operación de convolución en una operación de multiplicación de matrices, lo cual es computacionalmente más eficiente. En este caso los parches que requieren la convolución se aplanan y se organizan en columnas de una nueva matriz.</p>
</blockquote>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/im2col.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-26"><img data-src="img/clase-6/im2col.gif" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<p>El procedimiento en Pytorch se realiza de la siguiente manera:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a><span class="co">## Cada columna es un patche aplanado de 3x3. 16 patches en total.</span></span>
<span id="cb19-2"><a></a>X_col <span class="op">=</span> F.unfold(X, kernel_size<span class="op">=</span>(kH, kW))  <span class="co"># (1, 9, 16) (N,kH*kW,n_patches)</span></span>
<span id="cb19-3"><a></a><span class="bu">print</span>(<span class="ss">f"X_col shape: </span><span class="sc">{</span>X_col<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-4"><a></a>X_col</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>X_col shape: torch.Size([1, 9, 16])
tensor([[[7., 6., 8., 5., 8., 6., 5., 3., 9., 1., 1., 5., 4., 5., 5., 9.],
         [6., 8., 5., 1., 6., 5., 3., 5., 1., 1., 5., 3., 5., 5., 9., 2.],
         [8., 5., 1., 3., 5., 3., 5., 5., 1., 5., 3., 5., 5., 9., 2., 6.],
         [8., 6., 5., 3., 9., 1., 1., 5., 4., 5., 5., 9., 9., 5., 3., 1.],
         [6., 5., 3., 5., 1., 1., 5., 3., 5., 5., 9., 2., 5., 3., 1., 2.],
         [5., 3., 5., 5., 1., 5., 3., 5., 5., 9., 2., 6., 3., 1., 2., 2.],
         [9., 1., 1., 5., 4., 5., 5., 9., 9., 5., 3., 1., 4., 4., 8., 8.],
         [1., 1., 5., 3., 5., 5., 9., 2., 5., 3., 1., 2., 4., 8., 8., 9.],
         [1., 5., 3., 5., 5., 9., 2., 6., 3., 1., 2., 2., 8., 8., 9., 8.]]])</code></pre>
</div></div>
</section>
<section id="im2col-1" class="slide level2 smaller">
<h2>im2col</h2>
<p>¿Qué pasa si ahora aplanamos los filtros también?</p>
<div class="columns">
<div class="column">
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a><span class="co">## Dejamos cada filtro como una fila</span></span>
<span id="cb21-2"><a></a>W_row <span class="op">=</span> given_w.reshape(C_out, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-3"><a></a><span class="bu">print</span>(W_row.shape)</span>
<span id="cb21-4"><a></a>W_row</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(2,9)
tensor([[ 1.,  1.,  0., -1.,  1., -1.,  0., -1., -1.],
        [ 1.,  1.,  0., -1.,  1., -1., -1.,  0., -1.]])</code></pre>
</div><div class="column">
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>💭 Luego La convolución se puede pensar como una transformación lineal aplicada a parches aplanados de la imagen de entrada. Es decir cada parches es transformado linealmente por los filtros aplanados, generando una nueva representación de la imagen (un feature map).</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
</div></div>
<div style="font-size: 110%;">
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Entonces podemos expresar la convolución como una multiplicación de matrices muy similar a una FFN:</p>
<p><span class="math display">\[H_{col} = W_{row} \cdot X_{col} + b \cdot 1_{C_{in}*kH*KW}^T \]</span></p>
<p>Donde <span class="math inline">\(b\)</span> tiene dimensiones <span class="math inline">\(C_{out} \times 1\)</span> y <span class="math inline">\(1^T_{C_{in}*kH*kW}\)</span> tiene dimensiones <span class="math inline">\(1 \times C_{in}*kH*kW\)</span>. Aunque es más sencillo pensar el <span class="math inline">\(b\)</span> como un vector que se le aplica <strong>Broadcasting</strong>.</p>
</div>
</div>
</div>
</div>
<div class="columns">
<div class="column">
<p>Luego para volver a la forma de la imagen basta con hacer un reshape:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a>H <span class="op">=</span> H_col.reshape(N, C_out, H_out, W_out)</span>
<span id="cb23-2"><a></a><span class="bu">print</span>(H.shape)</span>
<span id="cb23-3"><a></a>H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(1, 2, 4, 4)</code></pre>
</div><div class="column">
<pre><code>tensor([[[[  5.,   5.,  -1.,  -4.],
          [ -4.,  -7.,  -1.,  -6.],
          [ -1., -10.,   6.,  -8.],
          [ -9.,  -8.,  -6.,  -6.]],

         [[ -3.,   5.,   3.,  -6.],
          [ -3.,  -7.,   3., -13.],
          [ -5., -12.,   4.,  -7.],
          [ -9.,  -4.,  -6.,  -5.]]]])</code></pre>
</div></div>
</section>
<section id="pooling" class="slide level2 smaller">
<h2>Pooling</h2>
<div class="columns">
<div class="column" style="width:48%;">
<h4 id="nuevamente-necesitamos-hacer-un-im2col-para-poder-hacer-el-pooling-como-una-multiplicación-de-matrices.">Nuevamente necesitamos hacer un <code>im2col</code> para poder hacer el pooling como una multiplicación de matrices.</h4>
<h4 id="eso-implica-que-h-quedará-como">Eso implica que H quedará como:</h4>
<div class="sourceCode" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a>pool_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb26-2"><a></a>H_pool, W_pool <span class="op">=</span> calculate_out(H, k_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>),stride<span class="op">=</span><span class="dv">2</span>) </span>
<span id="cb26-3"><a></a><span class="co">## (2,2)</span></span>
<span id="cb26-4"><a></a>h_col <span class="op">=</span> F.unfold(H, kernel_size<span class="op">=</span>pool_size, stride<span class="op">=</span>pool_size)</span>
<span id="cb26-5"><a></a><span class="bu">print</span>(h_col.shape)</span>
<span id="cb26-6"><a></a>h_col</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(1,8,4)
tensor([[[  5.,  -1.,  -1.,   6.],
         [  5.,  -4., -10.,  -8.],
         [ -4.,  -1.,  -9.,  -6.],
         [ -7.,  -6.,  -8.,  -6.],
         [ -3.,   3.,  -5.,   4.],
         [  5.,  -6., -12.,  -7.],
         [ -3.,   3.,  -9.,  -6.],
         [ -7., -13.,  -4.,  -5.]]])</code></pre>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>😱Notar como la operación de im2col genera parches para todos los canales a la vez. Por lo tanto es necesario separar por canales para aplicar el pooling.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
</div><div class="column" style="width:52%;">
<div class="sourceCode" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a><span class="co">## Tenemos 2 canales de 4 por cada uno</span></span>
<span id="cb28-2"><a></a><span class="co">## al cuál debemos aplicar el máximo.</span></span>
<span id="cb28-3"><a></a>h_col_reshaped <span class="op">=</span> h_col.reshape(N, C_out, pool_size<span class="op">*</span>pool_size, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb28-4"><a></a>h_col_reshaped</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(1,2,4,4)
tensor([[[[  5.,  -1.,  -1.,   6.],
          [  5.,  -4., -10.,  -8.],
          [ -4.,  -1.,  -9.,  -6.],
          [ -7.,  -6.,  -8.,  -6.]],

         [[ -3.,   3.,  -5.,   4.],
          [  5.,  -6., -12.,  -7.],
          [ -3.,   3.,  -9.,  -6.],
          [ -7., -13.,  -4.,  -5.]]]])
</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a><span class="co">## Calculamos el máximo de cada columna (que es un parche de 2x2)</span></span>
<span id="cb30-2"><a></a><span class="co">## Además guardamos la posición del máximo</span></span>
<span id="cb30-3"><a></a>M_flat, pool_indices <span class="op">=</span> h_col_reshaped.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb30-4"><a></a>M_flat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([[[ 5., -1., -1.,  6.],
         [ 5.,  3., -4.,  4.]]])</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a><span class="co">## Recuperamos la forma del feature map luego del pooling</span></span>
<span id="cb32-2"><a></a>M <span class="op">=</span> M_flat.reshape(N, C_out, H_pool, W_pool)</span>
<span id="cb32-3"><a></a><span class="bu">print</span>(M.shape)</span>
<span id="cb32-4"><a></a>M</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>(1,2,2,2)
tensor([[[[ 5., -1.],
          [-1.,  6.]],

         [[ 5.,  3.],
          [-4.,  4.]]]])</code></pre>
</div></div>
</section>
<section id="flatten-y-ffn" class="slide level2 smaller">
<h2>Flatten y FFN</h2>
<div class="columns">
<div class="column" style="width:70%;">
<div class="sourceCode" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a>f <span class="op">=</span> M.reshape(N, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb34-2"><a></a><span class="bu">print</span>(f.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([[ 5., -1., -1.,  6.,  5.,  3., -4.,  4.]])</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a>Z <span class="op">=</span> f <span class="op">@</span> W_fc <span class="op">+</span> b_fc</span>
<span id="cb36-2"><a></a>Z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([[18.]])</code></pre>
<h4 id="utilizando-nn.module">Utilizando <code>nn.Module</code></h4>
<div class="sourceCode" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a><span class="kw">class</span> Conv(nn.Module):</span>
<span id="cb38-2"><a></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb38-3"><a></a>      <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb38-4"><a></a>      <span class="va">self</span>.conv <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">2</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-5"><a></a>      <span class="va">self</span>.conv.weight.data <span class="op">=</span> given_w</span>
<span id="cb38-6"><a></a>      <span class="va">self</span>.conv.bias.data <span class="op">=</span> given_bias</span>
<span id="cb38-7"><a></a>      <span class="va">self</span>.max_pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, return_indices<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-8"><a></a>      <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">8</span>, <span class="dv">1</span>)</span>
<span id="cb38-9"><a></a>      nn.init.ones_(<span class="va">self</span>.fc.weight)</span>
<span id="cb38-10"><a></a>      nn.init.ones_(<span class="va">self</span>.fc.bias)</span>
<span id="cb38-11"><a></a>      <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb38-12"><a></a></span>
<span id="cb38-13"><a></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb38-14"><a></a>      x <span class="op">=</span> <span class="va">self</span>.conv(x)</span>
<span id="cb38-15"><a></a>      x, <span class="va">self</span>.indices <span class="op">=</span> <span class="va">self</span>.max_pool(x)</span>
<span id="cb38-16"><a></a>      x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb38-17"><a></a>      x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb38-18"><a></a>      <span class="cf">return</span> x</span>
<span id="cb38-19"><a></a></span>
<span id="cb38-20"><a></a>model <span class="op">=</span> Conv()</span>
<span id="cb38-21"><a></a><span class="co"># Forward con PyTorch</span></span>
<span id="cb38-22"><a></a>logits <span class="op">=</span> model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([[18.]])</code></pre>
</div><div class="column" style="width:30%;">
<div class="sourceCode" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a><span class="co"># Linear</span></span>
<span id="cb40-2"><a></a>W_fc <span class="op">=</span> torch.ones(<span class="dv">8</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.]])
</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a>b_fc <span class="op">=</span> torch.ones(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>tensor([1.])</code></pre>
</div></div>
</section>
<section id="le-interesa-calcular-los-gradientes" class="slide level2">
<h2>¿Le interesa calcular los gradientes?</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Calcular los gradientes de una CNN se simplifica bastante utilizando el enfoque de <code>im2col</code>, ya que la convolución se ha transformado en una multiplicación de matrices. Esto permite aplicar los conceptos que aprendimos en la primera parte del curso.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Aún así aparecen conceptos que escapan del conocimiento del Cálculo que conocemos, como por ejemplo el Gradiente del <code>im2col</code> (que Spoiler, es el algoritmo <code>col2im</code>).</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Échele una miradita al notebook de la clase, hay muchas horas de esfuerzo invertidas ahí.</strong></p>
</div>
<div class="callout-content">

</div>
</div>
</div>
</section>
<section id="variante-en-1d" class="slide level2 smaller">
<h2>Variante en 1d</h2>
<div class="columns">
<div class="column" style="width:60%;">
<dl>
<dt>Conv1d</dt>
<dd>
Corresponde a la variante de una dimensión, en la cual la entrada corresponden a secuencias de elementos como podrían ser series de tiempo, audio o hasta cadenas de texto.
</dd>
</dl>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>En este caso la implementación en Pytorch es similar a la 2D sólo que esperando tensores de dimensiones <span class="math inline">\((N,C_{in}, L_{in})\)</span>, donde <span class="math inline">\(C_{in}\)</span> corresponde al número de canales, que en el caso de series de tiempo equivale a features, y <span class="math inline">\(L_{in}\)</span> corresponde al largo de la secuencia.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>La salida de la Conv1d tendrá dimensiones <span class="math inline">\((N,C_{out},L_{out})\)</span> con:</p>
<p><span class="math display">\[L_{out} = \left\lfloor \frac{L_{in} + 2 \cdot padding - dilation \cdot (kernel\_size - 1) - 1}{stride} + 1 \right\rfloor\]</span></p>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-7/time_series.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27"><img data-src="img/clase-7/time_series.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-7/audio.png" class="lightbox" data-gallery="quarto-lightbox-gallery-28"><img data-src="img/clase-7/audio.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="variante-en-3d" class="slide level2 smaller">
<h2>Variante en 3d</h2>
<div class="columns">
<div class="column" style="width:60%;">
<dl>
<dt>Conv3d</dt>
<dd>
Corresponde a la variante de tres dimensiones, en la cual la entrada corresponde a secuencias de imágenes, es decir, videos.
</dd>
</dl>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Este caso también es similar sólo que se esperan tensores de dimensiones <span class="math inline">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span> donde <span class="math inline">\(C_in\)</span> corresponde al número de canales, <span class="math inline">\(D\)</span> en el caso de un video corresponde al número de frames de tamaño <span class="math inline">\(H_{in} \times W_{in}\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-important no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>La salida de la Conv1d tendrá dimensiones <span class="math inline">\((N,C_{out},D_{out},H_{out},W_{out})\)</span> con:</p>
<p><span class="math display">\[D_{out} = \left\lfloor \frac{D_{in} + 2 \cdot padding[0] - dilation[0] \cdot (kernel\_size[0] - 1) - 1}{stride[0]} + 1 \right\rfloor\]</span> <span class="math display">\[H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot padding[1] - dilation[1]\cdot (kernel\_size[1] - 1) - 1}{stride[1]} + 1 \right\rfloor\]</span> <span class="math display">\[W_{out} = \left\lfloor \frac{W_{in} + 2 \cdot padding[2] - dilation[2]\cdot (kernel\_size[2] - 1) - 1}{stride[2]} + 1 \right\rfloor\]</span></p>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><a href="img/clase-6/frame-rates.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img data-src="img/clase-6/frame-rates.jpg" class="quarto-figure quarto-figure-center"></a></p>
</figure>
</div>
</div></div>
</section>
<section id="terminamos" class="title-slide slide level1 center">
<h1>🥵 Terminamos</h1>
<div class="footer">
<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
</p><p><span property="dct:title">Tics-579 Deep Learning</span> por Alfonso Tobar-Arancibia está licenciado bajo <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0</a></p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">
<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a>
<p></p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../logo-uai-blanco.jpeg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': true,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1366,

        height: 768,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/datacubeR\.github\.io\/clases_UAI\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
    (function() {
      let previousOnload = window.onload;
      window.onload = () => {
        if (previousOnload) {
          previousOnload();
        }
        lightboxQuarto.on('slide_before_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          const href = trigger.getAttribute('href');
          if (href !== null) {
            const imgEl = window.document.querySelector(`a[href="${href}"] img`);
            if (imgEl !== null) {
              const srcAttr = imgEl.getAttribute("src");
              if (srcAttr && srcAttr.startsWith("data:")) {
                slideConfig.href = srcAttr;
              }
            }
          } 
        });
      
        lightboxQuarto.on('slide_after_load', (data) => {
          const { slideIndex, slideNode, slideConfig, player, trigger } = data;
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(slideNode);
          }
        });
      
      };
      
    })();
              </script>
    

</body></html>