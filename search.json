[
  {
    "objectID": "tics411-labs.html",
    "href": "tics411-labs.html",
    "title": "Prácticos",
    "section": "",
    "text": "Práctico\nColab\n\n\n\n\nPreprocesamiento\n\n\n\nEDA\n\n\n\nK-Means\n\n\n\nAnálisis de Centros\n\n\n\nAglomerativo\n\n\n\nDBSCAN\n\n\n\nEvaluación de Clusters\n\n\n\nEjemplos Hopkins\n\n\n\nProyecto Clustering\n\n\n\nApriori\n\n\n\nResolución Guía\n\n\n\nKNN\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "tics411/clase-5.html#clustering-densidad",
    "href": "tics411/clase-5.html#clustering-densidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral.\n\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso utilizaremos DBSCAN (Density-Based Spatial Clustering Applications with Noise)."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-definiciones",
    "href": "tics411/clase-5.html#dbscan-definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Definiciones",
    "text": "DBSCAN: Definiciones\n\n\n\n\n\n\nHiperparámetros del Modelo\n\n\n\neps: Radio de análisis\nMinPts: Corresponde al mínimo de puntos necesarios en un Radio eps.\n\n\n\n\n\n\n\nDensidad\n\n\nDensidad es el número de puntos dentro del radio eps.\n\n\nCore Point/Punto Central\n\n\nUn punto central/core es aquel que tiene al menos MinPts puntos dentro de la esfera definida por eps (se incluye él mismo).\n\n\n\n\n\nBorder Point/Punto Borde\n\n\nUn punto de borde tiene menos puntos que MinPts del eps, pero está dentro de la esfera de un punto central.\n\n\nNoise Point/Punto Ruido\n\n\nUn punto de ruido es todo aquel que no es punto central ni de borde."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-algoritmo-categorización-de-puntos",
    "href": "tics411/clase-5.html#dbscan-algoritmo-categorización-de-puntos",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Algoritmo categorización de puntos",
    "text": "DBSCAN: Algoritmo categorización de puntos\n\nPrimeramente se aplica un algoritmo para categorizar cada punto de acuerdo a las definiciones anteriores.\n\n\nPara cada punto en el espacio:\n\nCalcular su densidad en EPS y aplicar el siguiente algoritmo:"
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-1",
    "href": "tics411/clase-5.html#ejemplo-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 1",
    "text": "Ejemplo: Iteración 1\n\nSupongamos un ejemplo con \\(MinPts=4\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Core Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-2",
    "href": "tics411/clase-5.html#ejemplo-iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 2",
    "text": "Ejemplo: Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Border Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-3",
    "href": "tics411/clase-5.html#ejemplo-iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 3",
    "text": "Ejemplo: Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-final",
    "href": "tics411/clase-5.html#ejemplo-iteración-final",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración Final",
    "text": "Ejemplo: Iteración Final\n\n\n\n\n\n\n\n\n\n\n\nAhora, ¿Cómo definimos que partes son clusters o no?"
  },
  {
    "objectID": "tics411/clase-5.html#algoritmo-de-clustering",
    "href": "tics411/clase-5.html#algoritmo-de-clustering",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo de Clustering",
    "text": "Algoritmo de Clustering\nSe aplica el siguiente algoritmo para calcular clusterings.\n\n\n\n\n\n\nAntes de aplicar se desechan los Noise Points ya que no serán considerados. (Veremos luego que ocurre con estos puntos).\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label"
  },
  {
    "objectID": "tics411/clase-5.html#iteración-1",
    "href": "tics411/clase-5.html#iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 1",
    "text": "Iteración 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nTodos los puntos cercanos a un Core reciben la misma etiqueta."
  },
  {
    "objectID": "tics411/clase-5.html#iteración-2",
    "href": "tics411/clase-5.html#iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 2",
    "text": "Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n## label ya está en 1\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nEn este caso obtuvimos 2 clusters, e indirectamente un 3er de puntos ruido."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan",
    "href": "tics411/clase-5.html#dbscan",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\n\n\n\n\n\n\n\n\n\n\n¿Sería posible replicar un proceso de Clustering similar utilizando K-Means? ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-detalles-técnicos",
    "href": "tics411/clase-5.html#dbscan-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Detalles Técnicos",
    "text": "DBSCAN: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nResistente al ruido.\nPuede lidiar con clusters de diferentes formas y tamaños.\nNo es necesario especificar cuántos clusters encontrar.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAlgoritmo de alta complejidad computacional que puede llegar \\(O(n^2)\\) en el peor caso.\nSe ve afectado por densidad de los datos y por datos con una alta dimensionalidad.\nSu óptimo resultado depende específicamente de sus Hiperparámetros.\nNo puede generalizar en datos no usados en entrenamiento."
  },
  {
    "objectID": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros",
    "href": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo encontrar los Hiperparámetros?",
    "text": "¿Cómo encontrar los Hiperparámetros?\n\n\n\n\n\n\n\n\nminPts\n\n\nPara datasets multidimensionales grandes, la regla es:\n\\[minPts \\ge dim + 1\\]\n\n\n\n\n\n\n\n\n\nOtras recomendaciones:\n\n\n\nPara dos dimensiones: \\(minPts=4\\) (Ester et al., 1996)\nPara más de 2 dimensiones: \\(minPts = 2 \\cdot dim\\) (Sander et al., 1998)"
  },
  {
    "objectID": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros-1",
    "href": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros-1",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo encontrar los Hiperparámetros?",
    "text": "¿Cómo encontrar los Hiperparámetros?\n\nPara encontrar EPS se suele utilizar el método de Vecinos más cercanos.\n\n\n\nIdea\n\nLa distancia de los puntos dentro de un cluster a su k-ésimo vecino deberían ser similares.\nLuego, los puntos atípicos (o ruidosos) tienen el k-ésimo vecino a una mayor distancia.\n\n\n\n\n\n\n\n💡 Podemos plotear la distancia ordenada de cada punto a su k-ésimo vecino y seleccionar un eps cercano al crecimiento exponencial (codo)."
  },
  {
    "objectID": "tics411/clase-5.html#implementación-en-scikit-learn",
    "href": "tics411/clase-5.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples = 5, eps = 0.5, metric = \"euclidean\")\n\n## Se entrena y se genera la predicción\ndbs.fit_predict(X)\n\n\nmin_samples: Corresponde a minPts. Por defecto 5.\neps: Corresponde al radio de la esfera en la que se buscan los puntos cercanos. Por defecto 0.5.\nmetric: Corresponde a la distancia utilizada para medir la distancia. Permite todas las distancias mencionadas acá.\n.fit_predict(): Entrenará el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento. Adicionalmente los puntos ruidosos se etiquetarán como -1.\n\n\n👀 Veamos un ejemplo."
  },
  {
    "objectID": "tics411/lab-0.html#qué-es-scikit-learn",
    "href": "tics411/lab-0.html#qué-es-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Qué es Scikit-Learn?",
    "text": "¿Qué es Scikit-Learn?\n\n\n\n\n\nScikit-Learn (sklearn para los amigos) es una librería creada por David Cournapeau, como un Google Summer Code Project y luego Matthieu Brucher en su tesis.\nEn 2010 queda a cargo de INRIA y tiene un ciclo de actualización de 3 meses.\nEs la librería más famosa y poderosa para hacer Machine Learning hoy en día.\nSu API es tan famosa, que hoy se sabe que una librería es de calidad si sigue los estándares implementados por Scikit-Learn.\nPara que un algoritmo sea parte de Scikit-Learn debe poseer 3 años desde su publicación y 200+ citaciones mostrando su utilidad y amplio uso (ver acá).\nAdemás es una librería que obliga a que sus algoritmos tengan la capacidad de generalizar."
  },
  {
    "objectID": "tics411/lab-0.html#diseño",
    "href": "tics411/lab-0.html#diseño",
    "title": "TICS-411 Minería de Datos",
    "section": "Diseño",
    "text": "Diseño\n\nScikit-Learn sigue un patrón de Programación Orientada a Objetos (POO) basado en clases.\n\n\n\n\n\n\n\n\nEn programación, una clase es un objeto que internamente contiene estados que pueden ir cambiando en el tiempo.\n\nUna clase posee:\n\nMétodos: Funciones que cambian el comportamiento de la clase.\nAtributos: Datos propios de la clase.\n\n\n\n\n\n\n\nScikit-Learn sigue el siguiente estándar:\n\nTodas las Clases se escriben en CamelCase: Ej: KMeans,LogisticRegression, StandardScaler.\nLas clases en Scikit-Learn pueden representar algoritmos, o etapas de un preprocesamiento.\n\nLos algoritmos se denominan Estimators.\nLos preprocesamientos se denominan Transformers.\n\nLas funciones se escriben como snake_case y permiten realizar algunas operaciones básicas en el proceso de modelamiento. Ej: train_test_split(), cross_val_score().\nNormalmente se utilizan letras mayúsculas para denotar Matrices o DataFrames, mientras que las letras minúsculas denotan Vectores o Series."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-no-supervisados",
    "href": "tics411/lab-0.html#estimadores-no-supervisados",
    "title": "TICS-411 Minería de Datos",
    "section": "Estimadores No supervisados",
    "text": "Estimadores No supervisados\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X) \n\ny_pred = model.predict(X) \n\n## Opcionalmente se puede entrenar y predecir a la vez.\nmodel.fit_predict(X) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperparámetros.\nL3. Entrenar o ajustar el modelo (Requiere sólo de X).\nL5. Predecir. Los modelos de clasificación tienen la capacidad de generar probabilidades.\nL7-8. Este tipo de modelos permite entrenar y predecir en un sólo paso."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-predictivos",
    "href": "tics411/lab-0.html#estimadores-predictivos",
    "title": "TICS-411 Minería de Datos",
    "section": "Estimadores Predictivos",
    "text": "Estimadores Predictivos\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X_train, y_train) \n\ny_pred = model.predict(X_test) \ny_pred_proba = model.predict_proba(X_test)\n\nmodel.score(X_test,y_test) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperparámetros.\nL3. Entrenar o ajustar el modelo (Ojo, requiere de X e y).\nL5–6. Predecir en datos nuevos. (Algunos modelos pueden predecir probabilidades).\nL8. Evaluar el modelo en los datos nuevos."
  },
  {
    "objectID": "tics411/lab-0.html#output-de-un-modelo",
    "href": "tics411/lab-0.html#output-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Output de un Modelo",
    "text": "Output de un Modelo\n\nLos modelos no entregan directamente un output sino que los dejan almacenados en su interior como un estado.\nLos Estimators tienen dos estados:\n\nNot Fitted: Modelo antes de ser entrenado\nFitted: Una vez que el modelo ya está entrenado. (Después de aplicar .fit())\n\n\n\n\n\n\n\n\n\nMuchos modelos pueden entregar información sólo luego de ser entrenados (su atributo termina con un _).\nEj: model.coef_, model.intercept_.\n\n\n\n\n\n\n\n\n\n\n\nEl modelo es una herramienta a la cual le entregamos datos (Input), y nos devuelve datos (Predicciones)."
  },
  {
    "objectID": "tics411/lab-0.html#transformers",
    "href": "tics411/lab-0.html#transformers",
    "title": "TICS-411 Minería de Datos",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\n\n\nA diferencia de los Estimators, los Transformers no son modelos.\nSu input y su output son datos.\nAlgunos Transformers permiten escalar los datos, transformar categorías en números, rellenar valores faltantes. (Veremos más acerca de esto en los Preprocesamiento).\n\n\n\n\n\n\nfrom sklearn.preprocessing import Transformer \ntr = Transformer(hp1=v1, hp2=v2,...) \ntr.fit(X) \n\nX_new = tr.transform(X) \n\n## Opcionalmente\nX_new = tr.fit_transform(X) \n\nL1. Importar la clase a utilizar (en este caso del submodulo preprocessing, aunque pueden haber otros como impute).\nL2. Instanciar el Transformer y sus hiperparámetros.\nL3. Entrenar o ajustar el Transformer.\nL5. Transformar los datos.\nL7-8. Adicionalmente se puede entrenar y transformar los datos en un sólo paso."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines",
    "href": "tics411/lab-0.html#pipelines",
    "title": "TICS-411 Minería de Datos",
    "section": "Pipelines",
    "text": "Pipelines\n\nEn ocasiones un Dataset requiere más de un preprocesamiento.\nEstas Transformaciones normalmente se hacen en serie de manera consecutiva.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl Estimator es opcional, es decir, el Pipeline puede ser para combinar sólo Transformers o Transformers + un Estimator.\n\n\n\n\n\n\n\n\n\n\nUn Pipeline puede tener sólo un Estimator."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines-código",
    "href": "tics411/lab-0.html#pipelines-código",
    "title": "TICS-411 Minería de Datos",
    "section": "Pipelines: Código",
    "text": "Pipelines: Código\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.pipeline import Pipeline \n\npipe = Pipeline(steps=[ \n    (\"ohe\", OneHotEncoder()),\n    (\"sc\", StandardScaler()),\n    (\"model\", DecisionTreeClassifier())\n])\n\npipe.fit(X_train, y_train) \ny_pred = pipe.predict(X_test) \n\npipe.score(X_test, y_test) \n\nL1-2. Importo mi modelo y mis preprocesamientos\nL3. Importo el Pipeline.\nL5-9. Instancio un Pipeline.\nL11. Entreno el Pipeline.\nL12. Predigo utilizando el Pipeline entrenado.\nL14. Evalúo el modelo en datos no vistos."
  },
  {
    "objectID": "tics411/lab-0.html#documentación",
    "href": "tics411/lab-0.html#documentación",
    "title": "TICS-411 Minería de Datos",
    "section": "Documentación",
    "text": "Documentación\n\nProbablemente Scikit-Learn tenga una de las mejores documentaciones existentes.\n\n\nVeamos el caso de la Documentación del One Hot Encoder"
  },
  {
    "objectID": "tics411/clase-6.html#evaluación",
    "href": "tics411/clase-6.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\nPensemos en la Evaluación como una medida de desempeño el cuál “evalúa” qué tan bien realizado está el clustering. El objetivo principal del Clustering debe ser siempre la generación de clusters compactos que estén diferenciados los unos a los otros.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el Clustering que mejor describe el problema."
  },
  {
    "objectID": "tics411/clase-6.html#objetivos-de-la-evaluación",
    "href": "tics411/clase-6.html#objetivos-de-la-evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivos de la Evaluación",
    "text": "Objetivos de la Evaluación"
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins",
    "href": "tics411/clase-6.html#tendencia-hopkins",
    "title": "TICS-411 Minería de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins\n\nEstadístico Hopkins\n\n\nPermite evaluar a priori si es que efectivamente existen clusters antes de aplicar un algoritmo.\n\n\n\n\n\n\\[H = \\frac{\\sum_{i = 1}^p w_i}{\\sum_{i = 1}^p u_i + \\sum_{i = 1}^p w_i}\\]\n\n\\(w_i\\): corresponde a la distancia de un punto aleatorio al vecino más cercano en los datos originales.\n\\(u_i\\): corresponde a la distancia de un punto real del dataset al vecino más cercano.\n\\(p\\): Número de puntos generados en el espacio del Dataset.\n\n\nfrom pyclustertend import hopkins\n\n1-hopkins(X, p)\n\n\n\n\n\n\n\nX: Dataset al cuál se le aplica el Estadístico.\np: Número de Puntos para el cálculo.\n\n\n\n\n\n\n\n\n\n\npyclustertend entrega el valor 1-H."
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins-1",
    "href": "tics411/clase-6.html#tendencia-hopkins-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins"
  },
  {
    "objectID": "tics411/clase-6.html#cálculo-hopkins-ejemplo-p2",
    "href": "tics411/clase-6.html#cálculo-hopkins-ejemplo-p2",
    "title": "TICS-411 Minería de Datos",
    "section": "Cálculo Hopkins: Ejemplo p=2",
    "text": "Cálculo Hopkins: Ejemplo p=2\n\n\n\n\n\n\n\nPuntos obtenidos de los Datos\n\\[u_1\\approx 0\\]\n\\[u_2\\approx 0\\]\n\nPuntos Aleatorios en el Espacio de los Datos\n\\[w_1\\approx 1.8\\]\n\\[w_2\\approx 1.12\\]\n\nCálculo Hopkins\n\\[ H = \\frac{w_1 + w_2}{u_1 + u_2 + w_1 + w_2}\\] \\[ H = \\frac{1.8 + 1.12}{0 + 0 + 1.8 + 1.8} = 1\\]"
  },
  {
    "objectID": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "href": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "title": "TICS-411 Minería de Datos",
    "section": "Visual Assesment of Tendency (VAT)",
    "text": "Visual Assesment of Tendency (VAT)\n\nCorresponde a una inspección visual de la distancia entre los puntos (matriz de distancia). Colores más oscuros indican menor distancias entre dichos puntos lo que indica mayor cohesión.\n\n\n\n\n\n\n\n\n\nSe pueden ver claramente dos bloques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo es posible ver bloques importantes.\n\n\n\n\n\n\n\n\n\n\nfrom pyclustertend import vat\n\nvat(X)"
  },
  {
    "objectID": "tics411/clase-6.html#correlación",
    "href": "tics411/clase-6.html#correlación",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación",
    "text": "Correlación\nProcedimiento:\n\nConstruir una matriz de similaridad entre todos los puntos de la siguiente manera:\n\n\\[s(i,j) = \\frac{1}{d(i,j) + 1}\\]\n\nConstruir una matriz de similaridad \"ideal\" basada en la pertenencia a un Cluster.\nSi \\(i\\) y \\(j\\) pertenecen al mismo cluster entonces \\(s(i,j)=1\\), en otro caso \\(s(i,j) = 0\\)\nCalcular la Correlación entre la matriz de similaridad y la matriz ideal (obtenidas en los pasos 1 y 2).\n\n\n\n\n\n\n\nUna correlación alta indica que los puntos que están en el mismo cluster son cercanos entre ellos."
  },
  {
    "objectID": "tics411/clase-6.html#cohesión",
    "href": "tics411/clase-6.html#cohesión",
    "title": "TICS-411 Minería de Datos",
    "section": "Cohesión",
    "text": "Cohesión\n\nCohesión\n\n\nMide cuán cercanos están los objetos dentro de un mismo cluster. Se utiliza la Suma de los Errores al Cuadrado, que es equivalente a la Inercia de K-Means (o Within Cluster).\n\n\n\n\\[ SSE_{total} = \\sum_{k = 1}^K\\sum_{x_i \\in C_k} (x_i - \\bar{C_k})^2\\]\n\n\\(C_k\\) corresponde al Centroide del Cluster \\(k\\). Dicho centroide puede ser calculado como la media/mediana de todos los puntos del Centroide.\n\\(K\\) corresponde al Número de Clusters.\n\n\n\n\n\n\n\n\nNo me gusta mucho este nombre, porque en realidad es como un inverso de la Cohesión."
  },
  {
    "objectID": "tics411/clase-6.html#separación",
    "href": "tics411/clase-6.html#separación",
    "title": "TICS-411 Minería de Datos",
    "section": "Separación",
    "text": "Separación\n\nSeparación\n\n\nMide cuán distinto es un cluster de otro. Se usa la suma de las distancias al cuadrado entre los centroides hacia el promedio de todos los puntos. (Between groups sum squares, SSB).\n\n\n\n\\[ SSB_{total} = \\sum_{k = 1}^K |C_k|(\\bar{X} - \\bar{C_k})^2\\]\n\n\\(|C_k|\\) corresponde al número de elementos (Cardinalidad) del Cluster \\(i\\).\n\\(\\bar{X}\\) corresponde al promedio de todos los puntos."
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "title": "TICS-411 Minería de Datos",
    "section": "Coeficiente de Silhouette (Coeficiente de Silueta)",
    "text": "Coeficiente de Silhouette (Coeficiente de Silueta)\n\nEl coeficiente de Silhouette es otra medida que combina la cohesión y la separación. Los valores varían entre -1 y 1, donde valores cercanos a 1 representan una mejor agrupación.\n\n\n\n\n\n\n\nValores cercanos a \\(-1\\) representan que el punto está incorrectamente asignado a un cluster.\n\n\n\n\\[S_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}\\]\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X, labels, sample_size = None, metric=\"euclidean\")"
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Coeficiente de Silhouette: Ejemplo",
    "text": "Coeficiente de Silhouette: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[C_{silueta} = \\frac{1}{n}\\sum_{i} s_i\\]\n\n\\(a_i\\): Distancia promedio del punto \\(i\\) a todos los otros puntos del mismo cluster. (Cohesión)\n\\(b_{ij}\\): Distancia promedio del punto \\(i\\) a todos los puntos del cluster \\(j\\) donde no pertenezca \\(i\\). (Separación)\n\\(b_j\\): Mínimo de \\(b_{ij}\\) tal que el punto i no pertenezca al cluster \\(j\\). (Menor Separación)"
  },
  {
    "objectID": "tics411/clase-6.html#ejercicio-propuesto",
    "href": "tics411/clase-6.html#ejercicio-propuesto",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejercicio Propuesto",
    "text": "Ejercicio Propuesto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio Propuesto\n\n\nCalcule el coeficiente de Silueta. Tabla de resultado al final de las Slides."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta",
    "href": "tics411/clase-6.html#curvas-de-silueta",
    "title": "TICS-411 Minería de Datos",
    "section": "Curvas de Silueta",
    "text": "Curvas de Silueta\nEs común mostrar los resultados del coeficiente de silueta como gráficos de este estilo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblemas\n\n\n\nSiluetas negativas.\nClusters bajo el promedio.\nMucha variabilidad de Silueta en un sólo cluster."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta-implementación",
    "href": "tics411/clase-6.html#curvas-de-silueta-implementación",
    "title": "TICS-411 Minería de Datos",
    "section": "Curvas de Silueta: Implementación",
    "text": "Curvas de Silueta: Implementación\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\nskplt.metrics.plot_silhouette(X, labels, metric=\"euclidean\", title=\"Silhouette Analysis\")\nplt.show()\n\nL1-2: Importación de Librerías Necesarias. Esta implementación está en la librería Scikit-plot. (Para instalar pip install scikit-plot)\nX: Dataset usado para el clustering.\nlabels : etiquetas obtenidos de algún proceso de Clustering.\nmetric: Métrica a utilizar, por defecto usa “euclidean”.\ntitle: Se puede agregar un Título personalizado a la curva."
  },
  {
    "objectID": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "href": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "title": "TICS-411 Minería de Datos",
    "section": "Resultados Ejercicio Propuesto",
    "text": "Resultados Ejercicio Propuesto\n\n\n\n\n\nCoeficiente de Silhouette = 0.6148\n\n\n\n\n\n\nComprobar utilizando Scikit-Learn\n\n\n\n\n\nTics-411 Minería de Datos está licenciado bajo CC BY-NC-SA 4.0"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html",
    "href": "tics411/notebooks/10-resolucion_guia.html",
    "title": "K-Means",
    "section": "",
    "text": "import pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(dict(x=[0, 0, 1, 4, 5, 6], y=[1, 0, 0, 4, 4, 6]))\ndisplay(df)\nd_matrix = pd.DataFrame(distance_matrix(df, df))\nd_matrix\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n1\n\n\n1\n0\n0\n\n\n2\n1\n0\n\n\n3\n4\n4\n\n\n4\n5\n4\n\n\n5\n6\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.000000\n1.000000\n1.414214\n5.000000\n5.830952\n7.810250\n\n\n1\n1.000000\n0.000000\n1.000000\n5.656854\n6.403124\n8.485281\n\n\n2\n1.414214\n1.000000\n0.000000\n5.000000\n5.656854\n7.810250\n\n\n3\n5.000000\n5.656854\n5.000000\n0.000000\n1.000000\n2.828427\n\n\n4\n5.830952\n6.403124\n5.656854\n1.000000\n0.000000\n2.236068\n\n\n5\n7.810250\n8.485281\n7.810250\n2.828427\n2.236068\n0.000000"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#k-means",
    "href": "tics411/notebooks/10-resolucion_guia.html#k-means",
    "title": "K-Means",
    "section": "K-Means",
    "text": "K-Means\n\ncentroides = pd.DataFrame(dict(x=[1, 5], y=[1, 6]))\ncentroides_2 = pd.DataFrame(dict(x=[1 / 3, 5], y=[1 / 3, 14 / 3]))\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(df.x, df.y)\nplt.scatter(centroides.x, centroides.y, c=\"red\")\nplt.scatter(centroides_2.x, centroides_2.y, c=\"green\")\nplt.title(\"Centroides Iter 1: Rojo, Iter 2: Verde\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Distancia Centroides 1 a Puntos\npd.DataFrame(distance_matrix(centroides, df))\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1.000000\n1.414214\n1.000000\n4.242641\n5.0\n7.071068\n\n\n1\n7.071068\n7.810250\n7.211103\n2.236068\n2.0\n1.000000\n\n\n\n\n\n\n\n\n\n## Distancia Centroides 2 a Puntos\npd.DataFrame(distance_matrix(centroides_2, df))\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.745356\n0.471405\n0.745356\n5.18545\n5.934831\n8.013877\n\n\n1\n6.200358\n6.839428\n6.146363\n1.20185\n0.666667\n1.666667"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "href": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "title": "K-Means",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=2, eps=2)\ndbs.fit_predict(df)\n\narray([ 0,  0,  0,  1,  1, -1])\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=1, eps=1)\ndbs.fit_predict(df)\n\narray([0, 0, 0, 1, 1, 2])"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "href": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "title": "K-Means",
    "section": "Jerarquico Linkage Complete",
    "text": "Jerarquico Linkage Complete\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nplot_dendogram(df, link=\"complete\")"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#cohesión-y-separación",
    "href": "tics411/notebooks/10-resolucion_guia.html#cohesión-y-separación",
    "title": "K-Means",
    "section": "Cohesión y Separación",
    "text": "Cohesión y Separación\n\nimport numpy as np\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nlabels = np.array([0, 0, 0, 1, 1, 1])\ncenters = centroides_2.values\nsse, ssb = compute_clustering_metrics(df, labels, centers, is_df=True)\nsse, ssb\n\n(6.0, 60.833333333333336)"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "href": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "title": "K-Means",
    "section": "Silhouette",
    "text": "Silhouette\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, 1]\n    return a, b, bj, n_clusters\n\n\nd_matrix = distance_matrix(df, df)\na, b, bj, n_clusters = silhouette_score_m(d_matrix, labels)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.7517302154855591\n\n\n\ns_score_table\n\n\n\n\n\n\n\n\n\na\nb0\nb1\nb\ns\n\n\n\n\n0\n1.207107\n0.000000\n6.213734\n6.213734\n0.805736\n\n\n1\n1.000000\n0.000000\n6.848420\n6.848420\n0.853981\n\n\n2\n1.207107\n0.000000\n6.155701\n6.155701\n0.803904\n\n\n3\n1.914214\n5.218951\n0.000000\n5.218951\n0.633219\n\n\n4\n1.618034\n5.963643\n0.000000\n5.963643\n0.728684\n\n\n5\n2.532248\n8.035260\n0.000000\n8.035260\n0.684858"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html",
    "href": "tics411/notebooks/07-ex-evaluation.html",
    "title": "Evaluación de Clusters",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pyclustertend import vat, hopkins, ivat\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nnp.random.seed(0)\n\nset_config(transform_output=\"pandas\")\n\nX = sns.load_dataset(\"iris\").drop(columns=\"species\")\nX_random = np.random.rand(150, 4)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#evaluación-de-clusters",
    "href": "tics411/notebooks/07-ex-evaluation.html#evaluación-de-clusters",
    "title": "Evaluación de Clusters",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pyclustertend import vat, hopkins, ivat\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nnp.random.seed(0)\n\nset_config(transform_output=\"pandas\")\n\nX = sns.load_dataset(\"iris\").drop(columns=\"species\")\nX_random = np.random.rand(150, 4)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#visualización-de-ambos-datasets",
    "href": "tics411/notebooks/07-ex-evaluation.html#visualización-de-ambos-datasets",
    "title": "Evaluación de Clusters",
    "section": "Visualización de ambos Datasets",
    "text": "Visualización de ambos Datasets\n\n!pip install pyclustertend\n\nRequirement already satisfied: pyclustertend in /home/datacuber/miniconda3/lib/python3.9/site-packages (1.8.2)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.1.2 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.4.1.post1)\nRequirement already satisfied: numba&lt;0.55.0,&gt;=0.54.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (0.54.1)\nRequirement already satisfied: matplotlib&lt;4.0.0,&gt;=3.3.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (3.7.5)\nRequirement already satisfied: numpy==1.20.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.20.3)\nRequirement already satisfied: pandas&lt;2.0.0,&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.5.3)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (9.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (23.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (0.11.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.2.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (6.4.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (4.37.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.4.4)\nRequirement already satisfied: setuptools in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (67.5.1)\nRequirement already satisfied: llvmlite&lt;0.38,&gt;=0.37.0rc1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (0.37.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pandas&lt;2.0.0,&gt;=1.2.0-&gt;pyclustertend) (2022.2.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (3.1.0)\nRequirement already satisfied: scipy&gt;=1.6.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.3.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.11.0)\nRequirement already satisfied: six&gt;=1.5 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.16.0)\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X)\npca = PCA(n_components=2)\npca_random = pca.fit_transform(X_random)\n\n\ndef compute_hopkins(X, p):\n    h_s = 1 - hopkins(X, p)\n    print(f\"Hopskins para p={p} es: {h_s}\")\n    return h_s\n\n\nhs_X = compute_hopkins(X, p=50)\nhs_random = compute_hopkins(X_random, p=50)\n\nfig, ax = plt.subplot_mosaic([[\"iris\", \"random\"]], figsize=(15, 6))\n\nax[\"iris\"].scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nax[\"random\"].scatter(pca_random[\"pca0\"], pca_random[\"pca1\"])\nax[\"random\"].set_title(\n    f\"Reducción a 2D de nuestros puntos aleatorios. H = {hs_random:.2f}\"\n)\nax[\"iris\"].set_title(f\"Reducción a 2D de Iris. H = {hs_X:.2f}\")\nplt.show()\n\nHopskins para p=50 es: 0.8241582644992403\nHopskins para p=50 es: 0.48048319214476964"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "title": "Evaluación de Clusters",
    "section": "VAT: Iris",
    "text": "VAT: Iris\n\nimport matplotlib.pyplot as plt\n\nvat(X_sc)\nplt.title(\"VAT para Iris Escalado\")\nivat(X_sc)\nplt.title(\"iVAT para Iris Escalado\")\n\nText(0.5, 1.0, 'iVAT para Iris Escalado')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "title": "Evaluación de Clusters",
    "section": "VAT: Random",
    "text": "VAT: Random\n\nvat(X_random)\nplt.title(\"VAT para Dataset Random\")\nivat(X_random)\nplt.title(\"iVAT para Dataset Random\")\n\nText(0.5, 1.0, 'iVAT para Dataset Random')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#correlación",
    "href": "tics411/notebooks/07-ex-evaluation.html#correlación",
    "title": "Evaluación de Clusters",
    "section": "Correlación",
    "text": "Correlación\n\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance_matrix\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X_sc)\n\n\ndef cluster_correlation(X, labels, p=2):\n    \"\"\"p corresponde al nivel de la distancia de Minkowski\"\"\"\n    ideal_sim = (labels == labels.reshape(-1, 1)).astype(np.float32)\n\n    d_matrix = distance_matrix(X, X, p=p)\n    S = 1 / (d_matrix + 1)\n    return np.corrcoef(S.flatten(), ideal_sim.flatten()).min()\n\n\ncluster_correlation(X_sc, labels)\n\n0.6856891998862197"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#cohesión-y-separación",
    "href": "tics411/notebooks/07-ex-evaluation.html#cohesión-y-separación",
    "title": "Evaluación de Clusters",
    "section": "Cohesión y Separación",
    "text": "Cohesión y Separación\n\ncenters = km.cluster_centers_\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nsse, ssb = compute_clustering_metrics(X_sc, labels, centers, is_df=True)\nsse, ssb\n\n(222.36170496502297, 377.638295034977)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "href": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "title": "Evaluación de Clusters",
    "section": "Ejemplo de Clases",
    "text": "Ejemplo de Clases\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        x=[2, 3, 4, 8, 9, 10, 6, 7, 8],\n        y=[5, 4, 6, 3, 2, 5, 10, 8, 9],\n        c=[0, 0, 0, 1, 1, 1, 2, 2, 2],\n    )\n)\n\nd_matrix = distance_matrix(df[[\"x\", \"y\"]], df[[\"x\", \"y\"]], p=2)\nplt.scatter(df.x, df.y, c=df.c, s=200, edgecolors=\"k\")\n\ndf\n\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n0\n2\n5\n0\n\n\n1\n3\n4\n0\n\n\n2\n4\n6\n0\n\n\n3\n8\n3\n1\n\n\n4\n9\n2\n1\n\n\n5\n10\n5\n1\n\n\n6\n6\n10\n2\n\n\n7\n7\n8\n2\n\n\n8\n8\n9\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(df[[\"x\", \"y\"]], df.c)\n\n0.614855027897113\n\n\n\n## Esta función se hizo sólo para mostrar los pasos intermedios\n## Usen esta función para revisar sus resultados cuando estudien para la prueba.\n\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, 1]\n    return a, b, bj, n_clusters\n\n\na, b, bj, n_clusters = silhouette_score_m(d_matrix, df.c.values)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.6148550289904339"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "href": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "title": "Evaluación de Clusters",
    "section": "Silhouette Curve",
    "text": "Silhouette Curve\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X_sc, labels)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html",
    "href": "tics411/notebooks/08-proyecto_clustering.html",
    "title": "Preparación de los Datos",
    "section": "",
    "text": "# En caso que de ejecutar esto en Colab, van a tener que instalar Scikit-Plot para poder ver la curva de Silhouette.\n#!pip install scikit-plot\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\nRANDOM_STATE = 0\nnp.random.seed(RANDOM_STATE)\nN = np.random.randint(5, 15, size=1)[0]\nn_samples = np.random.randint(100, 1000, size=N)\nX, _ = make_blobs(\n    n_samples=n_samples,\n    n_features=9,\n    cluster_std=2.5,\n    random_state=RANDOM_STATE,\n)\ndf = pd.DataFrame(X)\ndict_cat = {\n    0: \"Cat 1\",\n    1: \"Cat 2\",\n    2: \"Cat 3\",\n}\nrng = np.random.default_rng()\ndf[\"cat_var\"] = rng.choice(a=[0, 1, 2], size=len(df), p=[0.2, 0.3, 0.5])\ndf[\"cat_var\"] = df[\"cat_var\"].map(dict_cat)\n\ndf.columns = [f\"x{i}\" for i, _ in enumerate(df.columns, start=1)]\ndf[\"x1\"] += 100\ndf[\"x5\"] *= 327\ndf[\"x9\"] /= 15\n\ndf.to_csv(\"proyecto_clustering.csv\", index=False)\n## Acá comienza oficialmente el código.\ndf = pd.read_csv(\"proyecto_clustering.csv\")\ndf.dtypes.value_counts().plot(\n    kind=\"bar\", title=\"Tipos de Datos en el Dataset\", edgecolor=\"k\"\n)\nplt.tight_layout()\ndf.hist(figsize=(20, 6), edgecolor=\"k\", grid=False)\nplt.tight_layout()\ndf[\"x10\"].value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Distribución de las Variables Categóricas\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#variables-categóricas",
    "href": "tics411/notebooks/08-proyecto_clustering.html#variables-categóricas",
    "title": "Preparación de los Datos",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\ndummy_vars = ohe.fit_transform(df[[\"x10\"]])\n\nX = pd.concat([df.drop(columns=\"x10\"), dummy_vars], axis=1)\nX\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10_Cat 1\nx10_Cat 2\nx10_Cat 3\n\n\n\n\n0\n105.576134\n4.823419\n3.409904\n-11.687494\n-1532.613468\n-4.589218\n-6.854641\n-8.877022\n-0.449964\n0.0\n0.0\n1.0\n\n\n1\n100.479786\n-4.876628\n-5.404970\n6.932649\n-4092.341900\n12.163845\n-6.502116\n10.874025\n0.348683\n0.0\n0.0\n1.0\n\n\n2\n97.357744\n8.467431\n-0.865210\n4.353712\n1444.577125\n-1.992772\n-12.223474\n-9.100414\n0.407230\n0.0\n0.0\n1.0\n\n\n3\n95.857842\n5.931475\n0.278352\n3.413013\n1959.773064\n-10.248761\n-8.136656\n-9.158037\n0.478212\n0.0\n0.0\n1.0\n\n\n4\n99.772427\n-2.876912\n4.499859\n1.308382\n-2318.502069\n2.062409\n-13.469304\n-0.236395\n0.478002\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6418\n98.951606\n6.649525\n1.869195\n1.765821\n4348.322822\n-6.866256\n-1.578755\n-12.749590\n0.454056\n0.0\n0.0\n1.0\n\n\n6419\n94.996949\n-6.638457\n3.999433\n0.989885\n-1811.824888\n-0.859185\n-7.422772\n3.293839\n0.558469\n0.0\n0.0\n1.0\n\n\n6420\n95.495497\n6.664764\n0.019823\n1.825686\n2845.172238\n-7.376139\n-8.056029\n-10.066078\n0.224961\n0.0\n0.0\n1.0\n\n\n6421\n99.435967\n5.469512\n6.342347\n-1.182801\n-358.410366\n-1.205160\n2.248149\n6.840680\n0.748647\n0.0\n1.0\n0.0\n\n\n6422\n109.218858\n-6.367392\n-0.857113\n-6.749834\n1913.311965\n-4.103422\n0.343194\n-5.460592\n0.121518\n0.0\n0.0\n1.0\n\n\n\n\n6423 rows × 12 columns\n\n\n\n\n\npca = PCA(n_components=2, random_state=42)\npca_X = pca.fit_transform(X)\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nplt.title(\"Visualización PCA del Dataset\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "href": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "title": "Preparación de los Datos",
    "section": "K-Means",
    "text": "K-Means\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"Número de Clústers\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X, k_max=20, color=\"blue\", title=\"Curva del Codo para K-Means\"\n)\n\n\n\n\n\n\n\n\n\nmetricas = dict()\n\n\nK_KMEANS = 10\nkm = KMeans(n_clusters=K_KMEANS, n_init=10, random_state=RANDOM_STATE)\nlabels_km = km.fit_predict(X)\n\n\ns_km = silhouette_score(X, labels_km)\nmetricas[\"km_10\"] = s_km\nmetricas\n\n{'km_10': 0.39419687509752793}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#jerárquico",
    "href": "tics411/notebooks/08-proyecto_clustering.html#jerárquico",
    "title": "Preparación de los Datos",
    "section": "Jerárquico",
    "text": "Jerárquico\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlinkage_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in linkage_list:\n    plot_dendogram(X, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef train_hierarchical(K_H, linkage):\n    hc = AgglomerativeClustering(n_clusters=K_H, linkage=linkage)\n    labels_h = hc.fit_predict(X)\n    s_h = silhouette_score(X, labels_h)\n    print(f\"El coeficiente de Silueta es {s_h}\")\n    return labels_h, s_h\n\n\nlabels_c9, s_c9 = train_hierarchical(K_H=9, linkage=\"complete\")\nlabels_a9, s_a9 = train_hierarchical(K_H=9, linkage=\"average\")\nlabels_w4, s_w4 = train_hierarchical(K_H=4, linkage=\"ward\")\n\nEl coeficiente de Silueta es 0.37657025597625804\nEl coeficiente de Silueta es 0.3812263959798965\nEl coeficiente de Silueta es 0.2852126845283154\n\n\n\nmetricas[\"s_c9\"] = s_c9\nmetricas[\"s_a9\"] = s_a9\nmetricas[\"s_w4\"] = s_w4\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "href": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "title": "Preparación de los Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = X.shape[1] + 1\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino más cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino más cercano\")\n    plt.title(f\"Búsqueda de EPS para DBSCAN con k={k}\")\n\n\ndbscan_elbow_plot(X, k=MIN_SAMPLES)\n\nEPS = 1.6\n\n\n\n\n\n\n\n\n\ndbs = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\nlabels_dbs = dbs.fit_predict(X)\ns_dbs = silhouette_score(X, labels_dbs)\nmetricas[\"s_dbs\"] = s_dbs\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154,\n 's_dbs': 0.1818560991479739}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#evaluación",
    "href": "tics411/notebooks/08-proyecto_clustering.html#evaluación",
    "title": "Preparación de los Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\npd.Series(metricas.values(), index=metricas.keys()).plot(\n    kind=\"bar\",\n    rot=0,\n    edgecolor=\"k\",\n    title=\"Silhouette Score para los modelos generados\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nlabels_km\n\narray([5, 9, 2, ..., 2, 0, 1], dtype=int32)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i])\n    plt.suptitle(title, fontsize=15)\n    plt.tight_layout()\n\n\ncenter_analysis_viz(\n    df,\n    n_clusters=10,\n    labels=labels_km,\n    columns=num_vars,\n    title=\"Análisis de Centro\",\n)\n\n\n\n\n\n\n\n\n\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"], c=labels_km)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X, labels_km)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/legacy_code.html",
    "href": "tics411/notebooks/legacy_code.html",
    "title": "Clases UAI",
    "section": "",
    "text": "## Otra forma de calcular lo mismo pero mucho más ineficiente. No usar!!\n# def compute_ideal_sim(labels):\n#     labels = pd.Series(labels, name=\"labels\")\n#     labels_df = labels.to_frame().reset_index()\n#     return (\n#         labels_df.merge(labels_df, how=\"outer\", on=\"labels\")\n#         .add(1)\n#         .set_index([\"index_x\", \"index_y\"])\n#         .unstack(level=1)\n#         .fillna(0)\n#         .astype(bool)\n#         .astype(int)\n#     )\n\n\n# ideal_sim_pd = compute_ideal_sim(labels).to_numpy()\n\n\n## Es para demostrar que dan lo mismo.\n# np.array_equal(ideal_sim_np, ideal_sim_pd)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/viz.html",
    "href": "tics411/notebooks/viz.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\na = np.array([4.5, 4, 4.1, 1, 2.3, 2.2, 2.4, 5, 5.5, 6.2, 6, 6, 6, 6])\nb = np.append(a, a + 1)\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(b, bins=5)\nax[\"A\"].set_title(\"Bins 5\")\nax[\"A\"].set_xlabel(\"Notas\")\nax[\"A\"].set_ylabel(\"Número de Estudiantes\")\nax[\"B\"].hist(b, bins=15)\nax[\"B\"].set_title(\"Bins 15\")\nax[\"B\"].set_xlabel(\"Notas\")\nax[\"C\"].hist(b, bins=30)\nax[\"C\"].set_title(\"Bins 30\")\nax[\"C\"].set_xlabel(\"Notas\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import KernelDensity\n\n\nkd_gauss = KernelDensity(kernel=\"epanechnikov\")\nkd_gauss.fit(b[:, np.newaxis])\nx_grid = np.linspace(1, 6, 1000)\nb_gauss = np.exp(kd_gauss.score_samples(x_grid[:, np.newaxis]))\nplt.hist(b)\nplt.plot(b_gauss)\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nnp.random.seed(0)\nx_grid = np.linspace(-4.5, 3.5, 1000)\nx = np.concatenate([norm(-1, 1.0).rvs(400), norm(1, 0.3).rvs(100)])\n\n\ndef kde_sklearn(x, x_grid, bandwidth=0.2, **kwargs):\n    \"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n    kde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n    kde_skl.fit(x[:, np.newaxis])\n    # score_samples() returns the log-likelihood of the samples\n    log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n    return np.exp(log_pdf)\n\n\npdf = kde_sklearn(x, x_grid, bandwidth=0.2)\n\nplt.hist(x)\nplt.plot(x_grid, pdf)\n\n\n\n\n\n\n\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"tophat\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_tophat = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_gaussian = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"epanechnikov\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_epa = np.exp(log_densidad_pred)\n\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"A\"].plot(X_grid, densidad_pred_tophat, color=\"red\", label=\"predicción\")\nax[\"A\"].set_title(\"Kernel Uniforme/Tophat h=0.2\")\n\nax[\"B\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"B\"].plot(X_grid, densidad_pred_gaussian, color=\"red\", label=\"predicción\")\nax[\"B\"].set_title(\"Kernel Gaussiano h=0.2\")\n\nax[\"C\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"C\"].plot(X_grid, densidad_pred_epa, color=\"red\", label=\"predicción\")\nax[\"C\"].set_title(\"Kernel Epanechnikov h=0.2\")\n\nText(0.5, 1.0, 'Kernel Epanechnikov h=0.2')\n\n\n\n\n\n\n\n\n\n\nn = 1000\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2))\n\nX_grid = np.linspace(-3, 4, 1000)\n\nmodelo_kde = KernelDensity(kernel=\"linear\", bandwidth=1)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred = np.exp(log_densidad_pred)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax.plot(X_grid, densidad_pred, color=\"red\", label=\"predicción\")\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df[\"embarked\"].value_counts().plot(\n    kind=\"bar\", rot=0, title=\"Personas embarcadas por puerto del Titanic\"\n)\n\n\n\n\n\n\n\n\n\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\n\n\n\ng = sns.catplot(\n    data=titanic_df, y=\"fare\", x=\"pclass\", kind=\"bar\", errorbar=None, hue=\"sex\"\n)\ng.figure.suptitle(\"Tarifa Promedio de Pasajeros del Titanic\\n por Clase y Sexo.\")\n\n\n\n\n\n\n\n\n\ndata = titanic_df[[\"age\", \"fare\"]].melt(value_vars=[\"age\", \"fare\"])\ndata\n\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nage\n22.00\n\n\n1\nage\n38.00\n\n\n2\nage\n26.00\n\n\n3\nage\n35.00\n\n\n4\nage\n35.00\n\n\n...\n...\n...\n\n\n1777\nfare\n13.00\n\n\n1778\nfare\n30.00\n\n\n1779\nfare\n23.45\n\n\n1780\nfare\n30.00\n\n\n1781\nfare\n7.75\n\n\n\n\n1782 rows × 2 columns\n\n\n\n\n\nsns.catplot(\n    kind=\"box\", x=\"variable\", y=\"value\", data=data, height=6, aspect=0.5, hue=\"variable\"\n)\n\n\n\n\n\n\n\n\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\ndata = iris_df.melt(\n    value_vars=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n)\ndata\n\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nsepal_length\n5.1\n\n\n1\nsepal_length\n4.9\n\n\n2\nsepal_length\n4.7\n\n\n3\nsepal_length\n4.6\n\n\n4\nsepal_length\n5.0\n\n\n...\n...\n...\n\n\n595\npetal_width\n2.3\n\n\n596\npetal_width\n1.9\n\n\n597\npetal_width\n2.0\n\n\n598\npetal_width\n2.3\n\n\n599\npetal_width\n1.8\n\n\n\n\n600 rows × 2 columns\n\n\n\n\n\ng = sns.catplot(x=\"variable\", y=\"value\", kind=\"box\", hue=\"variable\", data=data)\ng.figure.suptitle(\"Distribución de Medidas de Flores\")\ng._legend.remove()\ng.set(xlabel=None)\ng.set(ylabel=None)\n\n\n\n\n\n\n\n\n\nplt.scatter(iris_df.sepal_length, iris_df.petal_length)\nplt.title(\"Relación entre Largo del Sépalo y del Pétalo\")\nplt.xlabel(\"Largo del Sépalo\")\nplt.ylabel(\"Largo del Pétalo\")\n\nText(0, 0.5, 'Largo del Pétalo')\n\n\n\n\n\n\n\n\n\n\nanscombe = sns.load_dataset(\"anscombe\")\n\nfig = plt.figure(figsize=(10, 9))\nax = fig.subplot_mosaic(\n    \"\"\"AB\n                        CD\"\"\"\n)\nsns.regplot(data=anscombe.query(\"dataset == 'I'\"), x=\"x\", y=\"y\", ax=ax[\"A\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'II'\"), x=\"x\", y=\"y\", ax=ax[\"B\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'III'\"), x=\"x\", y=\"y\", ax=ax[\"C\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'IV'\"), x=\"x\", y=\"y\", ax=ax[\"D\"], ci=None)\nplt.suptitle(\"Cuarteto de Anscombe\")\n\nText(0.5, 0.98, 'Cuarteto de Anscombe')\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\ndata = pca.fit_transform(iris_df.drop(columns=\"species\"))\nx, y = data[:,0], data[:,1]\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nc = iris_df.species.astype(\"category\").cat.codes\nplt.scatter(x,y, c = c)\nplt.title(\"3 Clusters\");\n\n\n\n\n\n\n\n\n\nc_prima = (c == 0).astype(\"int64\")\nplt.scatter(x,y, c= c_prima)\nplt.title(\"2 Clusters\")\n\nText(0.5, 1.0, '2 Clusters')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/kmeans.html",
    "href": "tics411/notebooks/kmeans.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\n\n## Primera fila son coordenadas X\n## Segunda fila son coordenadas Y\n## Cada columna es el punto.\npuntos = np.array([[1, 2, 4, 5], [1, 1, 3, 4]])\npuntos\n\nc_1 = np.array([1, 1])\nc_2 = np.array([2, 1])\npuntos\n\narray([[1, 2, 4, 5],\n       [1, 1, 3, 4]])\n\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_clusters(puntos, c_1, c_2, c=None):\n    plt.scatter(puntos[0], puntos[1], s=500, c=c)\n    plt.scatter(\n        c_1[0],\n        c_1[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 1\",\n        edgecolors=\"k\",\n        s=200,\n        color=\"orange\",\n    )\n    plt.scatter(\n        c_2[0],\n        c_2[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 2\",\n        edgecolors=\"k\",\n        s=200,\n        c=\"green\",\n    )\n    plt.grid(alpha=0.5)\n    plt.legend()\n\n\nplot_clusters(puntos, c_1, c_2)\n\n\n\n\n\n\n\n\n\ndef distancia(p0, p1):\n    x0 = p0[0]\n    x1 = p1[0]\n    y0 = p0[1]\n    y1 = p1[1]\n    return np.sqrt((x1 - x0) ** 2 + (y1 - y0) ** 2)\n\n\ndef calculate_distances(puntos, c_1, c_2):\n\n    distancia_mat = np.zeros((2, 4))\n    distancia_mat\n\n    for i in range(4):\n        p = puntos[:, i]\n        distancia_mat[0, i] = distancia(p, c_1)\n        distancia_mat[1, i] = distancia(p, c_2)\n\n    return distancia_mat\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\ndistancia_mat\n\narray([[0.        , 1.        , 3.60555128, 5.        ],\n       [1.        , 0.        , 2.82842712, 4.24264069]])\n\n\n\ndef calculate_clusters(distancia_mat):\n    min_distancia_mat = np.min(distancia_mat, axis=0)\n\n    clusters = distancia_mat == min_distancia_mat\n    return clusters\n\n\nclusters = calculate_clusters(distancia_mat)\nclusters.astype(\"int64\")\n\n## Solo el punto 1 pertenece al cluster 1, todo el resto al cluster 2\n\narray([[1, 0, 0, 0],\n       [0, 1, 1, 1]])\n\n\n\ndef calculate_centroids(clusters):\n    c_1 = puntos[:, clusters[0]].mean(axis=1)\n    c_2 = puntos[:, clusters[1]].mean(axis=1)\n\n    return c_1, c_2\n\n\nc_1, c_2 = calculate_centroids(clusters)\nc_1, c_2\n\n(array([1., 1.]), array([3.66666667, 2.66666667]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"blue\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\nclusters = calculate_clusters(distancia_mat)\nc_1, c_2 = calculate_centroids(clusters)\nclusters\n\narray([[ True,  True, False, False],\n       [False, False,  True,  True]])\n\n\n\nc_1, c_2\n\n(array([1.5, 1. ]), array([4.5, 3.5]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"red\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/preguntas-prueba.html",
    "href": "tics411/notebooks/preguntas-prueba.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        x=[2, 8, 13, 16, 19, 2, 8, 13, 16, 19],\n        y=[7, 7, 7, 7, 7, 3, 3, 5, 5, 5],\n    )\n)\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n2\n7\n\n\n1\n8\n7\n\n\n2\n13\n7\n\n\n3\n16\n7\n\n\n4\n19\n7\n\n\n5\n2\n3\n\n\n6\n8\n3\n\n\n7\n13\n5\n\n\n8\n16\n5\n\n\n9\n19\n5\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\nimport numpy as np\n\nc1 = df.iloc[0]\nc2 = df.iloc[9]\n\nvals = np.arange(1, 11)\nd_m = distance_matrix(df.to_numpy(), df.to_numpy(), p=2)\nd_m = pd.DataFrame(d_m, index=vals, columns=vals).round(2)\nd_m\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n0.00\n6.00\n11.00\n14.00\n17.00\n4.00\n7.21\n11.18\n14.14\n17.12\n\n\n2\n6.00\n0.00\n5.00\n8.00\n11.00\n7.21\n4.00\n5.39\n8.25\n11.18\n\n\n3\n11.00\n5.00\n0.00\n3.00\n6.00\n11.70\n6.40\n2.00\n3.61\n6.32\n\n\n4\n14.00\n8.00\n3.00\n0.00\n3.00\n14.56\n8.94\n3.61\n2.00\n3.61\n\n\n5\n17.00\n11.00\n6.00\n3.00\n0.00\n17.46\n11.70\n6.32\n3.61\n2.00\n\n\n6\n4.00\n7.21\n11.70\n14.56\n17.46\n0.00\n6.00\n11.18\n14.14\n17.12\n\n\n7\n7.21\n4.00\n6.40\n8.94\n11.70\n6.00\n0.00\n5.39\n8.25\n11.18\n\n\n8\n11.18\n5.39\n2.00\n3.61\n6.32\n11.18\n5.39\n0.00\n3.00\n6.00\n\n\n9\n14.14\n8.25\n3.61\n2.00\n3.61\n14.14\n8.25\n3.00\n0.00\n3.00\n\n\n10\n17.12\n11.18\n6.32\n3.61\n2.00\n17.12\n11.18\n6.00\n3.00\n0.00\n\n\n\n\n\n\n\n\n\nprint(d_m.to_latex())\n\n\\begin{tabular}{lrrrrrrrrrr}\n\\toprule\n{} &     1  &     2  &     3  &     4  &     5  &     6  &     7  &     8  &     9  &     10 \\\\\n\\midrule\n1  &   0.00 &   6.00 &  11.00 &  14.00 &  17.00 &   4.00 &   7.21 &  11.18 &  14.14 &  17.12 \\\\\n2  &   6.00 &   0.00 &   5.00 &   8.00 &  11.00 &   7.21 &   4.00 &   5.39 &   8.25 &  11.18 \\\\\n3  &  11.00 &   5.00 &   0.00 &   3.00 &   6.00 &  11.70 &   6.40 &   2.00 &   3.61 &   6.32 \\\\\n4  &  14.00 &   8.00 &   3.00 &   0.00 &   3.00 &  14.56 &   8.94 &   3.61 &   2.00 &   3.61 \\\\\n5  &  17.00 &  11.00 &   6.00 &   3.00 &   0.00 &  17.46 &  11.70 &   6.32 &   3.61 &   2.00 \\\\\n6  &   4.00 &   7.21 &  11.70 &  14.56 &  17.46 &   0.00 &   6.00 &  11.18 &  14.14 &  17.12 \\\\\n7  &   7.21 &   4.00 &   6.40 &   8.94 &  11.70 &   6.00 &   0.00 &   5.39 &   8.25 &  11.18 \\\\\n8  &  11.18 &   5.39 &   2.00 &   3.61 &   6.32 &  11.18 &   5.39 &   0.00 &   3.00 &   6.00 \\\\\n9  &  14.14 &   8.25 &   3.61 &   2.00 &   3.61 &  14.14 &   8.25 &   3.00 &   0.00 &   3.00 \\\\\n10 &  17.12 &  11.18 &   6.32 &   3.61 &   2.00 &  17.12 &  11.18 &   6.00 &   3.00 &   0.00 \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n/tmp/ipykernel_8516/2921418078.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(d_m.to_latex())\n\n\n\nD = d_m.loc[[1, 10]]\nD\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n0.00\n6.00\n11.00\n14.00\n17.0\n4.00\n7.21\n11.18\n14.14\n17.12\n\n\n10\n17.12\n11.18\n6.32\n3.61\n2.0\n17.12\n11.18\n6.00\n3.00\n0.00\n\n\n\n\n\n\n\n\n\nG = D == D.min()\nG.astype(\"int64\")\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n10\n0\n0\n1\n1\n1\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\n\n\nG\n\n\nA = np.array([-5, -1, 1, 2, 3])\nA.mean()\n\n0.0\n\n\n\nA.std(ddof=0)\n\n2.8284271247461903\n\n\n\nprint(A.mean())\n(A**2).sum()\n\n0.0\n\n\n40\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nsc = StandardScaler()\nmm = MinMaxScaler()\nsc.fit_transform(A.reshape(-1, 1))\n\narray([[-1.76776695],\n       [-0.35355339],\n       [ 0.35355339],\n       [ 0.70710678],\n       [ 1.06066017]])\n\n\n\nmm.fit_transform(A.reshape(-1, 1))\n\narray([[0.   ],\n       [0.5  ],\n       [0.75 ],\n       [0.875],\n       [1.   ]])\n\n\n\ndf = pd.DataFrame(\n    dict(\n        Item=[1, 2, 3, 4, 5, 6, 7, 8],\n        Forma=[\n            \"cuadrado\",\n            \"cuadrado\",\n            \"circulo\",\n            \"circulo\",\n            \"triangulo\",\n            \"cuadrado\",\n            \"circulo\",\n            \"circulo\",\n        ],\n        Color=[\n            \"rojo\",\n            \"azul\",\n            \"rojo\",\n            \"verde\",\n            \"rojo\",\n            \"amarillo\",\n            \"amarillo\",\n            \"verde\",\n        ],\n    )\n)\n\nprint(df.to_latex(index=False, caption=\"Dataset\"))\n\n\\begin{table}\n\\centering\n\\caption{Dataset}\n\\begin{tabular}{rll}\n\\toprule\n Item &     Forma &    Color \\\\\n\\midrule\n    1 &  cuadrado &     rojo \\\\\n    2 &  cuadrado &     azul \\\\\n    3 &   circulo &     rojo \\\\\n    4 &   circulo &    verde \\\\\n    5 & triangulo &     rojo \\\\\n    6 &  cuadrado & amarillo \\\\\n    7 &   circulo & amarillo \\\\\n    8 &   circulo &    verde \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n/tmp/ipykernel_8516/3999806737.py:27: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(df.to_latex(index=False, caption=\"Dataset\"))\n\n\n\ndata = [\n    [0, 0.5, 1, 1.6, 0.8, 1.3, 3.6, 3.6, 4.3, 4.5, 5, 4],\n    [0.5, 0, 0.5, 1.1, 0.8, 1.4, 3.5, 3.3, 4, 4.2, 4.7, 4.2],\n    [1, 0.5, 0, 0.8, 0.8, 1.3, 3, 2.8, 3.5, 3.7, 4.2, 4.1],\n    [1.6, 1.1, 0.8, 0, 1.6, 2.1, 3.4, 3, 3.7, 3.8, 4.4, 4.8],\n    [0.8, 0.8, 0.8, 1.6, 0, 0.6, 2.9, 2.9, 3.6, 3.8, 4.3, 3.4],\n    [1.3, 1.4, 1.3, 2.1, 0.6, 0, 2.5, 2.7, 3.3, 3.6, 4, 2.8],\n    [3.6, 3.5, 3, 3.4, 2.9, 2.5, 0, 0.9, 1, 1.4, 1.6, 2.7],\n    [3.6, 3.3, 2.8, 3, 2.9, 2.7, 0.9, 0, 0.7, 0.9, 1.4, 3.6],\n    [4.3, 4, 3.5, 3.7, 3.6, 3.3, 1, 0.7, 0, 0.4, 0.7, 3.7],\n    [4.5, 4.2, 3.7, 3.8, 3.8, 3.6, 1.4, 0.9, 0.4, 0, 0.6, 4.1],\n    [5, 4.7, 4.2, 4.4, 4.3, 4, 1.6, 1.4, 0.7, 0.6, 0, 4.1],\n    [4, 4.2, 4.1, 4.8, 3.4, 2.8, 2.7, 3.6, 3.7, 4.1, 4.1, 0],\n]\n\n\ndf = pd.DataFrame(data)\ndf.index = [*range(1, 13)]\ndf.columns = [*range(1, 13)]\ndisplay(df)\nprint(df.to_latex(caption=\"DBSCAN\"))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n1\n0.0\n0.5\n1.0\n1.6\n0.8\n1.3\n3.6\n3.6\n4.3\n4.5\n5.0\n4.0\n\n\n2\n0.5\n0.0\n0.5\n1.1\n0.8\n1.4\n3.5\n3.3\n4.0\n4.2\n4.7\n4.2\n\n\n3\n1.0\n0.5\n0.0\n0.8\n0.8\n1.3\n3.0\n2.8\n3.5\n3.7\n4.2\n4.1\n\n\n4\n1.6\n1.1\n0.8\n0.0\n1.6\n2.1\n3.4\n3.0\n3.7\n3.8\n4.4\n4.8\n\n\n5\n0.8\n0.8\n0.8\n1.6\n0.0\n0.6\n2.9\n2.9\n3.6\n3.8\n4.3\n3.4\n\n\n6\n1.3\n1.4\n1.3\n2.1\n0.6\n0.0\n2.5\n2.7\n3.3\n3.6\n4.0\n2.8\n\n\n7\n3.6\n3.5\n3.0\n3.4\n2.9\n2.5\n0.0\n0.9\n1.0\n1.4\n1.6\n2.7\n\n\n8\n3.6\n3.3\n2.8\n3.0\n2.9\n2.7\n0.9\n0.0\n0.7\n0.9\n1.4\n3.6\n\n\n9\n4.3\n4.0\n3.5\n3.7\n3.6\n3.3\n1.0\n0.7\n0.0\n0.4\n0.7\n3.7\n\n\n10\n4.5\n4.2\n3.7\n3.8\n3.8\n3.6\n1.4\n0.9\n0.4\n0.0\n0.6\n4.1\n\n\n11\n5.0\n4.7\n4.2\n4.4\n4.3\n4.0\n1.6\n1.4\n0.7\n0.6\n0.0\n4.1\n\n\n12\n4.0\n4.2\n4.1\n4.8\n3.4\n2.8\n2.7\n3.6\n3.7\n4.1\n4.1\n0.0\n\n\n\n\n\n\n\n\n\\begin{table}\n\\centering\n\\caption{DBSCAN}\n\\begin{tabular}{lrrrrrrrrrrrr}\n\\toprule\n{} &   1  &   2  &   3  &   4  &   5  &   6  &   7  &   8  &   9  &   10 &   11 &   12 \\\\\n\\midrule\n1  &  0.0 &  0.5 &  1.0 &  1.6 &  0.8 &  1.3 &  3.6 &  3.6 &  4.3 &  4.5 &  5.0 &  4.0 \\\\\n2  &  0.5 &  0.0 &  0.5 &  1.1 &  0.8 &  1.4 &  3.5 &  3.3 &  4.0 &  4.2 &  4.7 &  4.2 \\\\\n3  &  1.0 &  0.5 &  0.0 &  0.8 &  0.8 &  1.3 &  3.0 &  2.8 &  3.5 &  3.7 &  4.2 &  4.1 \\\\\n4  &  1.6 &  1.1 &  0.8 &  0.0 &  1.6 &  2.1 &  3.4 &  3.0 &  3.7 &  3.8 &  4.4 &  4.8 \\\\\n5  &  0.8 &  0.8 &  0.8 &  1.6 &  0.0 &  0.6 &  2.9 &  2.9 &  3.6 &  3.8 &  4.3 &  3.4 \\\\\n6  &  1.3 &  1.4 &  1.3 &  2.1 &  0.6 &  0.0 &  2.5 &  2.7 &  3.3 &  3.6 &  4.0 &  2.8 \\\\\n7  &  3.6 &  3.5 &  3.0 &  3.4 &  2.9 &  2.5 &  0.0 &  0.9 &  1.0 &  1.4 &  1.6 &  2.7 \\\\\n8  &  3.6 &  3.3 &  2.8 &  3.0 &  2.9 &  2.7 &  0.9 &  0.0 &  0.7 &  0.9 &  1.4 &  3.6 \\\\\n9  &  4.3 &  4.0 &  3.5 &  3.7 &  3.6 &  3.3 &  1.0 &  0.7 &  0.0 &  0.4 &  0.7 &  3.7 \\\\\n10 &  4.5 &  4.2 &  3.7 &  3.8 &  3.8 &  3.6 &  1.4 &  0.9 &  0.4 &  0.0 &  0.6 &  4.1 \\\\\n11 &  5.0 &  4.7 &  4.2 &  4.4 &  4.3 &  4.0 &  1.6 &  1.4 &  0.7 &  0.6 &  0.0 &  4.1 \\\\\n12 &  4.0 &  4.2 &  4.1 &  4.8 &  3.4 &  2.8 &  2.7 &  3.6 &  3.7 &  4.1 &  4.1 &  0.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n/tmp/ipykernel_8516/1107703924.py:21: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(df.to_latex(caption=\"DBSCAN\"))\n\n\n\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        ai=np.random.rand(5),\n        biA=np.random.rand(5),\n        biB=np.random.rand(5),\n        bi=np.random.rand(5),\n        si=np.random.rand(5),\n    )\n)\nprint(df.to_latex(caption=\"Tabla Silhouette\"))\n\n\\begin{table}\n\\centering\n\\caption{Tabla Silhouette}\n\\begin{tabular}{lrrrrr}\n\\toprule\n{} &        ai &       biA &       biB &        bi &        si \\\\\n\\midrule\n0 &  0.096053 &  0.128639 &  0.841865 &  0.942217 &  0.000843 \\\\\n1 &  0.856134 &  0.549423 &  0.721880 &  0.292111 &  0.063252 \\\\\n2 &  0.436851 &  0.914593 &  0.261301 &  0.486813 &  0.830686 \\\\\n3 &  0.877781 &  0.886552 &  0.762853 &  0.487446 &  0.127603 \\\\\n4 &  0.667224 &  0.334679 &  0.423831 &  0.758756 &  0.887252 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n/tmp/ipykernel_8516/7425570.py:12: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(df.to_latex(caption=\"Tabla Silhouette\"))\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html",
    "href": "tics411/notebooks/11-ex-knn.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\ndf.dtypes.value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Tipos de Variable presente en Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "href": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "title": "Clases UAI",
    "section": "Supongamos que utilizaremos las siguientes variables",
    "text": "Supongamos que utilizaremos las siguientes variables\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#eda",
    "href": "tics411/notebooks/11-ex-knn.html#eda",
    "title": "Clases UAI",
    "section": "EDA",
    "text": "EDA\n\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nprint(f\"Variables Numéricas: {num_cols}\")\nprint(f\"Variables Categóricas: {cat_cols}\")\n\n\nValores Faltantes (Nulos)\n\nX.isnull().sum().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Cantidad de Valores Nulos en el Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-numéricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-numéricas",
    "title": "Clases UAI",
    "section": "Variables Numéricas",
    "text": "Variables Numéricas\n\nX.hist(grid=False, edgecolor=\"k\")\nplt.suptitle(\"Distribución de Variables Numéricas\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-categóricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-categóricas",
    "title": "Clases UAI",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\n\ncolor = [\"red\", \"blue\", \"green\"]\nfor cat, color in zip(cat_cols, color):\n    df[cat].value_counts().plot(\n        kind=\"bar\",\n        edgecolor=\"k\",\n        color=color,\n        title=f\"Categorías para '{cat}'\",\n    )\n    plt.show()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "href": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "title": "Clases UAI",
    "section": "Preprocesamiento",
    "text": "Preprocesamiento\n\nfrom feature_engine.imputation import CategoricalImputer\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X)\nX_imp\n\n\nfrom feature_engine.imputation import MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X_imp)\nX_imp\n\n\nfrom feature_engine.encoding import OneHotEncoder\n\nohe = OneHotEncoder()\nX_ohe = ohe.fit_transform(X_imp)\nX_ohe\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_ohe)\nX_sc_all\n\n\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"fare\", \"age\"])\nX_sc = sc.fit_transform(X_ohe)\nX_sc"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "href": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "title": "Clases UAI",
    "section": "Entrenamiento del Modelo",
    "text": "Entrenamiento del Modelo\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn_clf(X, y, k=5, prep=\"\"):\n    knn = KNeighborsClassifier(\n        n_neighbors=k, metric=\"euclidean\", n_jobs=-1\n    )\n    ## Notar que es posible utilizar Variables categóricas como Etiquetas...\n    knn.fit(X, y)\n    y_pred = knn.predict(X)\n    print(\n        f\"Score k = {k}, y Preprocesamiento: {prep}: {knn.score(X,y):.4f}\"\n    )\n    return y_pred\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    print(\n        \"=================================================================\"\n    )\n    y_pred_sc = knn_clf(X_sc, y, k=k, prep=\"StandardScaler Numérico\")\n    y_pred_sc_all = knn_clf(X_sc_all, y, k=k, prep=\"StandardScaler a todo\")\n    y_pred_ohe = knn_clf(X_ohe, y, k=k, prep=\"Sin Escalar\")\n\n\nConclusión: Los Preprocesamientos afectan de manera importante el entrenamiento de un modelo."
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "href": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "title": "Clases UAI",
    "section": "Uso de Pipelines",
    "text": "Uso de Pipelines\n\nfrom sklearn.pipeline import Pipeline\n\n\ndef model_pipeline(num_method, cat_method, k=5):\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", OneHotEncoder()),\n            (\"sc\", StandardScaler()),\n            (\"model\", KNeighborsClassifier(n_neighbors=5, n_jobs=-1)),\n        ]\n    )\n\n    return pipe\n\n\npipe = model_pipeline(num_method=\"mean\", cat_method=\"frequent\", k=5)\npipe\n\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\npipe.score(X, y)\n\n\ny_pred"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html",
    "href": "tics411/notebooks/03-ex_kmeans.html",
    "title": "Ejemplo K-Means",
    "section": "",
    "text": "import seaborn as sns\n\n# Importamos el Dataset Iris\ndf = sns.load_dataset(\"iris\")\ndf\n\n\ndf[\"species\"].value_counts()\n\n\nSupongamos que utilizaremos sólo las variables numéricas… “Species”, es de hecho la respuesta correcta (la etiqueta).\n\n\n# Definimos X como una Matriz sin la variable Species.\nX = df.drop(columns=\"species\")\nX"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ejemplo-k-means",
    "href": "tics411/notebooks/03-ex_kmeans.html#ejemplo-k-means",
    "title": "Ejemplo K-Means",
    "section": "",
    "text": "import seaborn as sns\n\n# Importamos el Dataset Iris\ndf = sns.load_dataset(\"iris\")\ndf\n\n\ndf[\"species\"].value_counts()\n\n\nSupongamos que utilizaremos sólo las variables numéricas… “Species”, es de hecho la respuesta correcta (la etiqueta).\n\n\n# Definimos X como una Matriz sin la variable Species.\nX = df.drop(columns=\"species\")\nX"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "href": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "title": "Ejemplo K-Means",
    "section": "Ayuda Visual",
    "text": "Ayuda Visual\nVamos a utilizar PCA para poder reducir las dimensiones a un tamaño el cual podamos visualizar: 2D.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n## Esto es sólo una ayuda para poder visualizar datos\n# que están en más dimensiones de las que podemos ver.\npca = PCA(n_components=2, random_state=1)\npca_X = pca.fit_transform(X)\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(pca_X[:, 0], pca_X[:, 1])\nplt.title(\"Visualización de Iris en 2D.\")\nplt.tight_layout()\n\n\n## Esta es una función que nos permitirá visualizar nuestras etiquetas en un espacio reducido por PCA.\n## Además permite la visualización de los centroides de nuestro proceso...\n\n\ndef pca_viz(pca_X, pca_centroids, labels, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    plt.scatter(\n        pca_centroids[:, 0],\n        pca_centroids[:, 1],\n        marker=\"*\",\n        c=\"red\",\n        s=150,\n    )\n    plt.title(title)\n\n\nImplementación de K-Means\n\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\n\npca_viz(\n    pca_X,\n    pca_centroids,\n    labels=labels,\n    title=\"Visualización de K-Means en Iris 2D\",\n)\n\n\n\nEfecto del Escalamiento en K-Means\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_sc = pca.fit_transform(X_sc)\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nsc_labels = km.fit_predict(X_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\npca_viz(\n    pca_X_sc,\n    pca_centroids,\n    sc_labels,\n    title=\"K-Means de Iris en 2D luego de Estandarizar los datos. \",\n)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\nX_mm = mm.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_mm = pca.fit_transform(X_mm)\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nmm_labels = km.fit_predict(X_mm)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(\n    pca_X_mm,\n    pca_centroids,\n    mm_labels,\n    title=\"K-Means de Iris en 2D luego de Normalizar los datos.\",\n)"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ejemplo-más-avanzado-sin-entrenar-con-todos-los-datos",
    "href": "tics411/notebooks/03-ex_kmeans.html#ejemplo-más-avanzado-sin-entrenar-con-todos-los-datos",
    "title": "Ejemplo K-Means",
    "section": "Ejemplo más avanzado sin entrenar con todos los datos…",
    "text": "Ejemplo más avanzado sin entrenar con todos los datos…\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(X, test_size=0.25, random_state=1)\n\n\nEstamos dejando un 25% de los datos fuera para poder chequear cuál sería la predicción que se le dan a dichos datos.\n\n\npca = PCA(n_components=2)\nkm = KMeans(n_clusters=2, n_init=10)\nsc = StandardScaler()\n## Fit siempre se hace con datos de `Entrenamiento`.\n\n## Escalamos los datos...\nsc.fit(X_train)\nX_train_sc = sc.transform(X_train)\nX_test_sc = sc.transform(X_test)\n\n# Generamos las coordenadas del PCA para visualizar\npca.fit(X_train_sc)\npca_train = pca.transform(X_train_sc)\npca_test = pca.transform(X_test_sc)\n\ntrain_labels = km.fit_predict(X_train_sc)\ntest_labels = km.predict(X_test_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(pca_train, pca_centroids, train_labels)\npca_viz(pca_test, pca_centroids, test_labels, cmap=\"tab20b\")"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#cuál-es-el-k-óptimo",
    "href": "tics411/notebooks/03-ex_kmeans.html#cuál-es-el-k-óptimo",
    "title": "Ejemplo K-Means",
    "section": "Cuál es el K óptimo?",
    "text": "Cuál es el K óptimo?\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"Número de Clústers\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X_train,\n    k_max=15,\n    color=\"red\",\n    title=\"Curva del Codo para el Dataset Iris, sólo con Train Set.\",\n)\n\n\nwc"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html",
    "href": "tics411/notebooks/06-ex-DBSCAN.html",
    "title": "DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\n\n\ndf = sns.load_dataset(\"iris\")\nX = df.drop(columns=\"species\")\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#dbscan",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#dbscan",
    "title": "DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\n\n\ndf = sns.load_dataset(\"iris\")\nX = df.drop(columns=\"species\")\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#función-para-visualizar",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#función-para-visualizar",
    "title": "DBSCAN",
    "section": "Función para Visualizar",
    "text": "Función para Visualizar\n\nimport matplotlib.pyplot as plt\n\n\n## Función ligeramente modificada para no requerir centroides en caso que no sea aplicable.\ndef pca_viz(pca_X, labels, pca_centroids=None, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    if pca_centroids is not None:\n        plt.scatter(\n            pca_centroids[:, 0],\n            pca_centroids[:, 1],\n            marker=\"*\",\n            c=\"red\",\n            s=150,\n        )\n    plt.title(title)\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=13, eps=0.6)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\n\n\nfrom sklearn.decomposition import PCA\n\n## Probar minPts = 4 y eps = 0.2\n## Probar minPts = 10 y eps = 0.5\n## Probar minPts = 13 y eps = 0.6\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X_sc)\n\npca_viz(\n    pca_X,\n    labels=labels,\n    title=\"Visualización de DBSCAN para Iris en 2D\",\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino más cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino más cercano\")\n    plt.title(f\"Búsqueda de EPS para DBSCAN con k={k}\")\n\n\n# k = 5 escogido ya que tenemos 4 dimensiones.\ndbscan_elbow_plot(X_sc, k=5)"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperparámetros-óptimos",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperparámetros-óptimos",
    "title": "DBSCAN",
    "section": "Modelo entrenado con Hiperparámetros Óptimos",
    "text": "Modelo entrenado con Hiperparámetros Óptimos\n\nMIN_PTS = 20\nEPS = 0.75\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=MIN_PTS, eps=EPS)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\npca_viz(\n    pca_X,\n    labels=labels,\n    title=f\"Visualización de DBSCAN para Iris en 2D con los mejores Hiperparámetros: MinPts: {MIN_PTS} y eps = {EPS}\",\n)"
  },
  {
    "objectID": "tics411/clase-0.html#quién-soy",
    "href": "tics411/clase-0.html#quién-soy",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Quién soy?",
    "text": "¿Quién soy?\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia, estudié Ingeniería Civil pero llevo 9 años trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nTerminando mi Msc. y empezando mi PhD en la UAI.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqué mi primer paper el año pasado sobre Hate Speech en Español.\nJuego Tenis de Mesa, hago Agility con mi perrita Kira y escribo en mi Blog."
  },
  {
    "objectID": "tics411/clase-0.html#objetivos-del-curso",
    "href": "tics411/clase-0.html#objetivos-del-curso",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\n\nIdentificar Elementos Claves del Machine Learning (Terminología, Nomenclatura, Intuición).\nEntender como interactúan los algoritmos más importantes.\nAprender a seleccionar el mejor Algoritmo para el Problema.\nEjecutar y aplicar algoritmos clásicos de Machine Learning.\nEvaluar el desempeño esperado del Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#tópicos",
    "href": "tics411/clase-0.html#tópicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tópicos",
    "text": "Tópicos\n\n\n\n\n\n\n\nIntroducción a la Minería de Datos\nAnálisis Exploratorio de Datos (EDA)\nModelos No Supervisados/Descriptivos\nModelos Supervisados/Predictivos\n\n\n\n\n\n\nModelos no Supervisados\n\nK-Means\nHierarchical Clustering\nDBScan\nApriori\n\n\nModelos Supervisados\n\nKNN\nÁrboles de Decisión\nNaive Bayes\nRegresión Logística"
  },
  {
    "objectID": "tics411/clase-0.html#sobre-las-clases",
    "href": "tics411/clase-0.html#sobre-las-clases",
    "title": "TICS-411 Minería de Datos",
    "section": "Sobre las clases",
    "text": "Sobre las clases\n\n\nClases presenciales, con participación activa de los estudiantes.\nEs un curso coordinado.\nCanal oficial será Webcursos.\nMucha terminología y material de estudio será en Inglés.\nHorario: Jueves.\n\n15:30 a 16:40 (Cátedra)\n17:00 a 18:10 (Práctico)\nIdealmente!!\n\nAsistencia es voluntaria, pero altamente recomendada."
  },
  {
    "objectID": "tics411/clase-0.html#materiales-de-clases",
    "href": "tics411/clase-0.html#materiales-de-clases",
    "title": "TICS-411 Minería de Datos",
    "section": "Materiales de Clases",
    "text": "Materiales de Clases\n\nDiapositivas\nPrácticos\n\n\n\n\n\n\n\n\nSlides interactivas (Código se puede copiar e imágenes se pueden ver en grande).\nSe puede buscar contenido en las diapositivas mediante un buscador.\nSe dejarán copias en PDF en Webcursos (levemente distintas).\n\n\n\n\n\n\n\n\n\n\nSe espera que los estudiantes dominen las siguientes tecnologías:\n\nPython\nGoogle Colab\nPandas/Numpy\nScikit-Learn (Se enseñará a lo largo del curso)."
  },
  {
    "objectID": "tics411/clase-0.html#material-complementario",
    "href": "tics411/clase-0.html#material-complementario",
    "title": "TICS-411 Minería de Datos",
    "section": "Material Complementario",
    "text": "Material Complementario\n\n\n\n\n\n\nCurso de Scikit-Learn \n\nTutorial Colab\nAgregar Datos Externos a Colab"
  },
  {
    "objectID": "tics411/clase-0.html#evaluación",
    "href": "tics411/clase-0.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\n\n\n\n\n\n\nDos Evaluaciones Escritas (P1, P2) coordinadas y cuatro tareas prácticas en parejas (T1, T2, T3, T4) \\[NP = 0.35 \\cdot P1 + 0.35 \\cdot P2 + 0.3 \\cdot \\bar{T}\\] \\[ \\bar{T} = (T1 + T2 + T3 + T4)/4 \\]\n\n\n\n\n\n\n\n\n\n\nSi NP &gt; 5\n\n\n\\[NF = NP\\]\n\n\n\n\n\n\n\n\n\nEn caso contrario:\n\n\n\\[NF = 0.7 \\cdot NP + 0.3 \\cdot E\\]"
  },
  {
    "objectID": "tics411/clase-0.html#ayudantías",
    "href": "tics411/clase-0.html#ayudantías",
    "title": "TICS-411 Minería de Datos",
    "section": "Ayudantías",
    "text": "Ayudantías\nAyudante: TBD\nemail: TBD\n\n\n\n\n\n\n\nLas ayudantías serán en la manera que sean necesarias.\nEstarán enfocadas principalmente en aplicaciones y código."
  },
  {
    "objectID": "tics411/clase-0.html#revolución-de-los-datos",
    "href": "tics411/clase-0.html#revolución-de-los-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Revolución de los Datos",
    "text": "Revolución de los Datos\n\n\n\n\n\n\n\nHablar de los distintos tipos de Datos.\nTodo es datos, y está lleno de ellos en Internet y el mundo."
  },
  {
    "objectID": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "href": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Nace el Data Science (Ciencia de Datos)",
    "text": "Nace el Data Science (Ciencia de Datos)\n\n\n\n\n\n\n\nExplicar las distintas etapas. Qué son cada uno de ellos.\nExplicar que no estoy de acuerdo con todas las definiciones."
  },
  {
    "objectID": "tics411/clase-0.html#cómo-aprovechar-la-información-que-tenemos",
    "href": "tics411/clase-0.html#cómo-aprovechar-la-información-que-tenemos",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo aprovechar la información que tenemos?",
    "text": "¿Cómo aprovechar la información que tenemos?\n\n\nData Mining (Minería de Datos)\n\n\n“The process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.” (Fayyad, Piatetsky-Shapiro & Smith 1996)\n\n\n\n\n\n\nMachine Learning (Aprendizaje Automático)\n\n\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” (Mitchell, 2006)\n\n\n\n\n\n\nExplicar que estos son dos tipos de Approaches con el que hoy en día se enfrentan los datos.\nEl primero más enfocado en un análisis manual.\nEl segundo en un enfoque más automático."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos",
    "href": "tics411/clase-0.html#tipos-de-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos",
    "text": "Tipos de Datos\n\n\n\n\n\nDatos Estructurados\n\n\n\n\n\nDatos No Estructurados"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, instancias, registros. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos más amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo. También el que más disponible está en las empresas.\nDistintos data types, por lo que normalmente requiere de algún tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "href": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Series de Tiempo",
    "text": "Tipos de Datos: Series de Tiempo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Instancias temporales (Normalmente interdependientes).\nColumnas: Variables, Atributos, Features (Univariada o Multivariada).\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo.\nPropiedad temporal requiere preprocesamiento y modelos especiales."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-imágenes",
    "href": "tics411/clase-0.html#tipos-de-datos-imágenes",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Imágenes",
    "text": "Tipos de Datos: Imágenes\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste es el tipo de Datos que disparó la Inteligencia Artificial.\n¿Cuántos computadores para identificar un Gato? 16,000\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplicar el concepto de Tensor, extensión de las matrices. Diferencia entre Grayscale y RGB."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "href": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Texto Libre",
    "text": "Tipos de Datos: Texto Libre\n\n\n\n\n\n\n\n\n\n\n\n\nDatos Masivos.\nDificiles de lidiar ya que deben ser llevarse a una representación numérica.\nAlto nivel de Sesgo y Subjetividad.\n\n\n\n\n\n\n\n\n\n\n\nGracias a este tipo de datos se han producido los avances más increíbles del último tiempo: Transformers"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-videos",
    "href": "tics411/clase-0.html#tipos-de-datos-videos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Videos",
    "text": "Tipos de Datos: Videos\n\n\n\n\n\n\n\n\nLos videos no son más que arreglos de imágenes.\nSon un tipo de dato muy pesado y difícil de lidiar.\nRequiere alto poder de Procesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-aprendizaje",
    "href": "tics411/clase-0.html#tipos-de-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Aprendizaje",
    "text": "Tipos de Aprendizaje"
  },
  {
    "objectID": "tics411/clase-0.html#reinforcement-learning",
    "href": "tics411/clase-0.html#reinforcement-learning",
    "title": "TICS-411 Minería de Datos",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este tipo de aprendizaje se enseña por refuerzo. Es decir se da una recompensa si el sistema aprende lo que queremos.\n\n\n\n\n\n\n\n\n\n\n\nSi el premio es mayor, se pueden obtener aprendizajes mayores.\n\n\n\n\n\n\n\n\n\n\n\nUn ejemplo de esto es AlphaTensor en el cual un modelo aprendió una nueva manera de multiplicar matrices que es más eficiente.\n\n\n\n\n\n\n\n\n\n\n\nOtro ejemplo es AlphaFold donde el modelo aprendió/descubrió cómo se doblan las proteínas cuando se vuelven aminoácidos."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-supervisados-regresión-y-clasificación",
    "href": "tics411/clase-0.html#problemas-supervisados-regresión-y-clasificación",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas Supervisados: Regresión y Clasificación",
    "text": "Problemas Supervisados: Regresión y Clasificación\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión: Se busca estimar un valor continuo.\n\n(Estimar el valor de una casa).\n\nClasificación: Se busca encontrar una categoría o un valor discreto.\n\n(Clasificar una imagen como Perro o Gato).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara entrenar este tipo de modelos se necesitan etiquetas, es decir, la respuesta esperada del modelo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmbos ejemplos se pueden realizar utilizando Largo (Eje Y) y Peso (Eje X)."
  },
  {
    "objectID": "tics411/clase-0.html#clustering",
    "href": "tics411/clase-0.html#clustering",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusters: Una categoría en la que sus componentes son similares. Los clusters normalmente no tienen un nombre propio, sino que uno les asigna uno.\nTambién se les llama segmentos. No usar la palabra clase.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo requiere de etiquetas, por lo tanto, no es posible evaluar su desempeño de manera 100% acertada."
  },
  {
    "objectID": "tics411/clase-0.html#reducción-de-dimensionalidad",
    "href": "tics411/clase-0.html#reducción-de-dimensionalidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Reducción de Dimensionalidad",
    "text": "Reducción de Dimensionalidad\n\n\n\n\n\n\n\n\n\n\n\n\nReducción de la Dimensionalidad: Eliminar complejidad sin perder información clave para poder entender su comportamiento."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML",
    "text": "Nuestro Sistema de ML\nCreemos un Sistema de ML que sea capaz de ver una imágen y pronunciar correctamente el uso de la letra C.\n\n\n\n\n\n\nVamos a Entrenar un Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Entrenamiento",
    "text": "Nuestro Sistema de ML: Entrenamiento\n\n\n\n\n\n\nKasa\n\n\n\n\n\n\n\nKokodrilo\n\n\n\n\n\n\n\nKubo\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué patrones está aprendiendo el modelo?\n\n\n\n\n\nEntrenamiento\n\n\nEs el proceso en el cuál se permite al modelo aprender. En este proceso se le entregan ejemplos (Train Set) para que el modelo de manera autónoma pueda aprender patrones que le permitan resolver la tarea dada."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Inferencia",
    "text": "Nuestro Sistema de ML: Inferencia\n\nInferencia/Predicción\n\n\nSe refiere al proceso en el que el modelo tiene que demostrar cuál sería su decisión de acuerdo a los patrones aprendidos en el proceso de entrenamiento. Los ejemplos en los que se prueba se le denomina Test Set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKukillo\n\n\nBikikleta\n\n\n\n\n\nGeneralización\n\n\nSe le llama generalización a la capacidad del modelo de aplicar lo aprendido de manera correcta en ejemplos no vistos."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento",
    "text": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento\n\n\n\n\n\n\nKuchillo\n\n\n\n\n\n\n\nChokolate\n\n\n\n\n\n\n\nSinsel\n\n\n\n\n\n\n\n\n\n\n\n\nNo es bueno entrenar con las mismas instancias de de Test, es decir, con las cuales se evalúa el modelo. ¿Por qué?\n\n\n\n\n\nMencionar el caso de error de ImageNet."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo",
    "text": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKuchillo\n\n\nBisikleta\n\n\n\n\n\nEvaluación\n\n\nUtilizar una métrica que permita ponerle nota al modelo.\n\n\n\n\n\n\n1er Modelo: 2 correctas de 4, es decir 50%.\n\n\n\n\n2do Modelo: 4 correctas de 4, es decir 100%."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas del Aprendizaje",
    "text": "Problemas del Aprendizaje\n\nSupongamos que queremos utilizar nuestro modelo para pronunciar palabras en otro idioma (otro Test Set).\n¿Qué problemas podemos encontrar?\n\n\n\n\nStomach \\(\\rightarrow\\) Stomak\nArcher \\(\\rightarrow\\) Archer\nChurch \\(\\rightarrow\\) Churk\n\nChurch.\n\nArcheology \\(\\rightarrow\\) Archeology\n\nArkeology.\n\nChicago \\(\\rightarrow\\) Chicago\n\nShicago.\n\nMuscle \\(\\rightarrow\\) Muskle\n\nMus_le.\n\nIch mag Schweinefleisch \\(\\rightarrow\\) Ich mag Schweinefleisk.\n\nIj mag Shvaineflaish.\n\n\n\n\n\n\n\n\n\n\nClaramente tenemos un problema. ¿A qué se debe esto?"
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas del Aprendizaje: Definiciones",
    "text": "Problemas del Aprendizaje: Definiciones\n\nOverfitting (Sobreajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, porque se ajusta demasiado bien (llegando a memorizar) a los datos de entrenamiento. ¿Cómo se puede mitigar este problema?\n\n\n\n\n\n\n\n\n\n\nSe le tiende a llamar sobreentrenamiento, pero no es del todo correcto para el caso de modelos de Machine Learning. Lo más correcto es que el sobreentrenamiento provoca overfitting.\n\n\n\n\n\nMostrar ejemplos en Pizarra de manera gráfica. Ejemplos típicos de Excel.\n\n\n\nUnderfitting (Subajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, pero a diferencia del overfitting no se ha ajustado correctamente a los datos. ¿Cómo se vería el underfitting en nuestro ejemplo?"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Crisp-DM",
    "text": "Etapas del Modelamiento: Crisp-DM"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: KDD",
    "text": "Etapas del Modelamiento: KDD"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Semma",
    "text": "Etapas del Modelamiento: Semma"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-metodología-propia",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-metodología-propia",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Metodología Propia",
    "text": "Etapas del Modelamiento: Metodología Propia"
  },
  {
    "objectID": "tics411/clase-4.html#definiciones",
    "href": "tics411/clase-4.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nClustering Jerárquico\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender. Se basa en la construcción de Jerarquías para ir construyendo clusters.\n\n\nDendograma\n\n\nCorresponde a un diagrama en el que se muestran las distancias de atributos entre clases que son parte de un mismo cluster."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerarquía",
    "href": "tics411/clase-4.html#clustering-jerarquía",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Jerarquía",
    "text": "Clustering: Jerarquía\n\nLos algoritmos basados en jerarquía pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Algoritmo",
    "text": "Clustering Aglomerativo: Algoritmo\nAlgoritmo\n\nInicialmente se considera cada punto como un cluster.\nCalcula la matriz de proximidad/distancia entre cada cluster.\nRepetir (hasta que exista un solo cluster):\n\nUnir los cluster más cercanos.\nActualizar la matriz de proximidad/distancia.\n\n\n\n\n\n\n\n\nLo más importante de este proceso es el cálculo de la matriz de proximidad/distancia entre clusters\n\n\n\n\n\n\n\n\n\nDistintos enfoques de distancia entre clusters, segmentan los datos en forma distinta."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Ejemplo",
    "text": "Clustering Aglomerativo: Ejemplo\nSupongamos que tenemos cinco tipos de genes cuya expresión ha sido determinada por 3 caracteríticas. Las siguientes expresiones pueden ser vistas como la expresión dados los genes en tres experimentos. ​\n\nApliquemos un Clustering Jerárquico Aglomerativo utilizando como medida de similaridad la Distancia Euclideana.\n\n\n\n\n\n\n\nOtros tipos de distancia también son aplicables siguiendo un procedimiento análogo."
  },
  {
    "objectID": "tics411/clase-4.html#algoritmo-1era-iteración",
    "href": "tics411/clase-4.html#algoritmo-1era-iteración",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo: 1era Iteración",
    "text": "Algoritmo: 1era Iteración\n\n\n\n\n\n\nEl algoritmo considerará que todos los puntos inicialmente son un cluster. Por lo tanto, tratará de encontrar los 2 puntos más cercanos e intentará unirnos en un sólo cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblema: ¿Cómo actualizamos la matriz de Distancias?\n\n\n\n\n\n\n\n\n\n\n\nEntonces crearemos un nuevo cluster: bcl2-Caspade."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Single Linkage",
    "text": "Clustering Aglomerativo: Single Linkage\n\n\n\n\n\n\n\n\n\nDistancia entre clusters determinada por los puntos más similares entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = min\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nGenera Clusters largos y delgados.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nAfectado por Outliers"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Complete Linkage",
    "text": "Clustering Aglomerativo: Complete Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por la distancia ente los puntos más disímiles entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = max\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a quebrar Clusters Grandes.\nTiene tendencia a generar Clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Average Linkage",
    "text": "Clustering Aglomerativo: Average Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el promedio de las distancias que componen los clusters.\nPunto intermedio entre Single y Complete.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = avg\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a datos atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Ward Linkage",
    "text": "Clustering Aglomerativo: Ward Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el incremento del Within cluster distance.\nMinimiza la distancia intra cluster y maximiza la distancia entre clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = wc(Cij) - wc(C_i) - wc(C_j) = \\frac{n_i\\cdot n_j}{n_i + n_j}||\\bar{C_i} - \\bar{C_j}||^2\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#hiperparámetros",
    "href": "tics411/clase-4.html#hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "Hiperparámetros",
    "text": "Hiperparámetros\nLos Hiperparámetros de este modelo serán:\n\n\n\n\n\n\nNote\n\n\n\nlinkage: La forma de calcular la distancia entre clusters.\ndistancia: La distancia utilizada como similaridad entre los clusters.\n\n\n\n\n\n\n\n\n\n\nA diferencia de K-Means, este método no requiere definir el número de Clusters a priori."
  },
  {
    "objectID": "tics411/clase-4.html#volvamos-a-la-iteración-1",
    "href": "tics411/clase-4.html#volvamos-a-la-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Volvamos a la Iteración 1",
    "text": "Volvamos a la Iteración 1\n\nSupongamos que por simplicidad utilizaremos Average Linkage. (El proceso para utilizar otro linkage es análogo).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos a extraer una Matriz entre los puntos a fusionar y los puntos de los clusters restantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 1era Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#iteración-2",
    "href": "tics411/clase-4.html#iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 2",
    "text": "Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 2da Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#iteración-3",
    "href": "tics411/clase-4.html#iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 3",
    "text": "Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 3ra Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#dendograma-resultante",
    "href": "tics411/clase-4.html#dendograma-resultante",
    "title": "TICS-411 Minería de Datos",
    "section": "Dendograma Resultante",
    "text": "Dendograma Resultante\n\n\n\n\n\n\nNo es necesario realizar la última iteración ya que se entiende que ambos clusters se unen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo encontramos los clusters una vez que tenemos el Dendograma?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos escoger un umbral de distancia y ver cuántos clusters se forman.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo regla general se deben escoger clusters más distanciados entre sí."
  },
  {
    "objectID": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "href": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "title": "TICS-411 Minería de Datos",
    "section": "Efecto del Linkage Escogido",
    "text": "Efecto del Linkage Escogido"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerárquico-detalles-técnicos",
    "href": "tics411/clase-4.html#clustering-jerárquico-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Jerárquico: Detalles Técnicos",
    "text": "Clustering Jerárquico: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nNo requiere definir el número de Clusters a priori.\nAl tener distintas variantes es posible que los puntos sean agrupados de manera completamente distintas.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nMuy ineficiente computacionalmente debido a que genera una nueva matriz de distancia en cada iteración lo que entrega una complejidad \\(O(n^2)\\) o \\(O(n^3)\\) dependiendo del linkage.\nUna vez que se decide combinar 2 clusters no es posible revertir esta decisión.\nNo tiene capacidad de generalización, ya que no es posible aplicarlo a datos nuevos."
  },
  {
    "objectID": "tics411/clase-4.html#implementación-en-scikit-learn",
    "href": "tics411/clase-4.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=2, metric=\"euclidean\",linkage=\"ward\")\n\n## Se entrena y se genera la predicción\nac.fit_predict(X)\n\n\nn_clusters: Define el número de clusters a crear, por defecto 2.\nmetric: Permite distancias L1, L2 y coseno. Por defecto “euclidean”.\nlinkage: Permite single, complete, average y ward. Por defecto “ward”.\n.fit_predict(): Entrenará el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento.\n\n\n\n\n\n\n\n\n\nSi bien el método de Aglomeración no requiere el número de clusters a generar, Scikit-Learn lo exige de modo de poder etiquetar cada elemento.\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué no existen los métodos .fit() y .predict() por separado?"
  },
  {
    "objectID": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "href": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "title": "TICS-411 Minería de Datos",
    "section": "Otras implementaciones (Dendograma)",
    "text": "Otras implementaciones (Dendograma)\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Genera los cálculos necesarios para construir el Histograma.\nZ = linkage(X, method='single', metric=\"euclidean\") \n\n# Graficar el Dendograma\nplt.figure(figsize=(10, 5)) # Define el tamaño del Gráfico\nplt.title('Dendograma Clustering Jerárquico') # Define un título para el dendograma\nplt.xlabel('Iris Samples')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\nplt.show()\n\n\nPrincipalmente este código permite graficar el Dendograma completo.\nL4: Genera una instancia del Dendograma. (Sería equivalente al .fit() de Scikit-Learn).\nL5-L12: Corresponde al código necesario para graficar el Dendograma."
  },
  {
    "objectID": "tics411/clase-4.html#sugerencias",
    "href": "tics411/clase-4.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que el clustering aglomerativo también es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala común.\n\n\n\n\n\n\n\n\n\n\nOtras técnicas como merge y split, no aplican a este tipo de clustering debido a las limitaciones del algoritmo."
  },
  {
    "objectID": "tics411/clase-4.html#variantes",
    "href": "tics411/clase-4.html#variantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes",
    "text": "Variantes\n\nEn casos en los que no es posible calcular distancias debido a la presencia de datos categóricos, es posible utilizar el Gower Dissimilarity como medida de similitud.\n\n\n\n\n\n\n\n\n\nGower\n\nSe define como la proporción de variables que tienen distinto valor con respecto al total sin considerar donde ambos son ceros.\n\n\n\n\\[Gower(p1,p2) = \\frac{3}{9}\\]"
  },
  {
    "objectID": "tics411/clase-7.html#introducción",
    "href": "tics411/clase-7.html#introducción",
    "title": "TICS-411 Minería de Datos",
    "section": "Introducción",
    "text": "Introducción\n\nGracias a los planes de fidelización (juntar puntos, dar RUT, acumular millas, etc.) las empresas son capaces de detectar patrones:\n\n\nQué nos gusta,\nQué compramos,\nCon qué frecuencia lo compramos,\nJunto con qué lo compramos\netc.\n\n\n\n\n\n\n\nMarket Basket Analysis\n\n\nCorresponde al estudio de nuestra canasta de compras. De modo que podamos entender qué cosas son las que como clientes preferimos y una empresa pueda Recomendar de manera más apropiadas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones",
    "href": "tics411/clase-7.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nPatrón\n\n\nPredicado (output True/False) para verificar si una estructura buscada ocurre o no.\n\n\nTarea\n\n\nEncontrar reglas de asociación basado en patrones.\n\n\n\nEjemplos\n\nDatasets de supermercados:\n\n10% de los clientes totales compran vino y quedo (patrón: si compro vino, también llevo queso).\n\nDatasets de Alarmas:\n\nSi la alarma A y B suenan en un intervalo de 30 segundos, entonces la alarma C sonará dentro de un intervalo de 60 segundos con 50% de probabilidad."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado\n\nDatos Transaccionales\n\n\nUna transacción involucra un conjunto de elementos. Una boleta de supermercado muestra el conjunto de elementos comprados por un cliente. Los productos involucrados en una transacción se denominan items."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado"
  },
  {
    "objectID": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "href": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivo y Aplicaciones",
    "text": "Objetivo y Aplicaciones\n\n\n\n\n\n\nObjetivo\n\n\nEncontrar asociaciones entre elementos u objetos de bases de datos transaccionales.\n\n\n\n\n\n\n\n\n\nAplicaciones\n\n\n\nApoyo a toma de decisiones.\nAnálisis de Información de Ventas.\nDistribución y ubicación de Mercaderías.\nSegmentación de Clientes en base de patrones de compra.\nDiágnostico y predicción de alarmas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones-medidas",
    "href": "tics411/clase-7.html#definiciones-medidas",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones: Medidas",
    "text": "Definiciones: Medidas\n\n\n\n\n\n\nSupport (Soporte)\n\nFracción de Transacciones que contienen a \\(X\\). Probabilidad de que una transacción contenga a \\(X\\).\n\n\n\\[Supp(X) = P(X)\\]\n\n\n\n\n\n\n\nSupport Count\n\nNúmero de Transacciones que contienen a \\(X\\).\n\n\n\\[SuppCount(X) = Count(X)\\]\n\n\n\n\n\n\n\n\nConfidence (Confianza o Eficiencia)\n\nFracción de las Transacciones en las que aparece \\(X\\) que también incluyen \\(Z\\).\n\n\n\\[Conf(X \\implies Z) = \\frac{Supp(X \\cup Z)}{Supp(X)}\\] \\[Conf(X \\implies Z) = \\frac{SuppCount(X \\cup Z)}{SuppCount(X)}\\]\n\n\n\n\n\n\n\n\n\n\n\nOjo con la Notación \\(\\cup\\). En este caso significa que tanto el producto X como el Producto Z sean parte de la transacción."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "href": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplos: Support y Confidence",
    "text": "Ejemplos: Support y Confidence\n\n\n\n\n\n\n\n\n\\[ Supp({Pan}) = 4/7\\] \\[ Supp({Leche}) = 3/7\\] \\[ Supp({Pan, Huevo}) = 2/7\\]\n\\[ Conf({Pan} \\implies {Huevo}) = \\frac{Supp({Pan, Huevo})}{Supp(Pan)} = \\frac{2/7}{4/7}\\]\n\\[ Conf({Pan} \\implies {Leche}) = \\frac{Supp({Pan, Leche})}{Supp(Pan)} = \\frac{1/7}{4/7}\\] \\[ Conf({Leche} \\implies {Pan}) = \\frac{Supp({Pan, Leche})}{Supp(Leche)} = \\frac{1/7}{3/7}\\]"
  },
  {
    "objectID": "tics411/clase-7.html#problema",
    "href": "tics411/clase-7.html#problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Problema",
    "text": "Problema\n\nEn un dataset transaccional de n productos totales y \\(|U_i|\\) elementos para la Transacción \\(i\\).\n\nSe pueden generar un total de \\(N_{reglas}\\) de asociación:\n\\[N_{reglas} = \\sum_{i=1}^{2^{n}} \\sum_{j=0}^{|U_i|}\\binom{|U_i|}{j}\\]\n\n\n\n\n\n\n\n\n\nSi suponemos un supermercado que tiene 1000 productos, y transacciones que pueden ir entre 1 y 50 productos. El problema es muy costoso, y se podrían eventualmente generar demasiadas combinaciones."
  },
  {
    "objectID": "tics411/clase-7.html#algoritmo-apriori",
    "href": "tics411/clase-7.html#algoritmo-apriori",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo Apriori",
    "text": "Algoritmo Apriori\n\nApriori\n\n\nEs un algoritmo para aprender reglas de asociación que utiliza el principio Apriori para buscar de forma eficiente las reglas que satisfacen los límites de soporte y confianza.\n\n\n\n\nAlgoritmo\n\nFijar \\(k=1\\) y determinar lista de candidatos de tamaño \\(k\\).\n\nCalcular la frecuencia del conjunto.\nEliminar conjuntos con baja frecuencia (utilizando un umbral de soporte).\nUnir los conjuntos frecuentes para generar conjuntos de tamaño \\(k+1\\).\nSi existe la posibilidad de seguir creando combinaciones volver al paso a y repetir.\n\nUsar todos los conjuntos frecuentes para generar reglas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori",
    "href": "tics411/clase-7.html#ejemplo-apriori",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori",
    "text": "Ejemplo Apriori\n\nSupongamos el siguiente dataset transaccional:\n\nSupongamos que queremos calcular las reglas de asociación que tengan un MinSupp=40% y un MinConf=70%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodríamos pensar que MinSupp y MinConf son los hiperparámetros de este algoritmo."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 1",
    "text": "Ejemplo Apriori: Iteración 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGalletas NO CUMPLE con el Soporte Mínimo solicitado. Por lo tanto, lo elimino y genero relaciones de 2 productos sin considerar Galletas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-2",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 2",
    "text": "Ejemplo Apriori: Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcá NO SE ELIMINA ningún producto, ya que en los itemsets que sobrevivieron hay Pan, Mantequilla, Leche, Pañales y Cerveza."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-3",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 3",
    "text": "Ejemplo Apriori: Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe puede apreciar que los únicos 3 productos que sobreviven son Pan, Mantequilla y Leche. Por lo tanto, NO ES POSIBLE generar reglas con 4 productos."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas",
    "href": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Generación de Reglas",
    "text": "Ejemplo Apriori: Generación de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pan, Mantequilla}:\n\n\\(Conf(Pan \\implies Mantequilla) = \\frac{Supp(Pan, Mantequilla)}{Supp(Pan)} = \\frac{3}{3}\\)✅ \\(Conf(Mantequilla \\implies Pan) = \\frac{Supp(Pan, Mantequilla)}{Supp(Mantequilla)} = \\frac{3}{3}\\)✅\n\n\n\nPara {Pan, Leche}:\n\n\\(Conf(Pan \\implies Leche) = \\frac{Supp(Pan, Leche)}{Supp(Pan)} = \\frac{2}{3}\\) ❌ \\(Conf(Leche \\implies Pan) = \\frac{Supp(Pan, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ✅\n\n\n\nPara {Mantequilla, Leche}:\n\n\\(Conf(Mantequilla \\implies Leche) = \\frac{Supp(Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\) ❌ \\(Conf(Leche \\implies Mantequilla) = \\frac{Supp(Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ✅"
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Generación de Reglas",
    "text": "Ejemplo Apriori: Generación de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pañales, Cerveza}:\n\n\\(Conf(Pañales \\implies Cerveza) = \\frac{Supp(Pañales, Cerveza)}{Supp(Pañales)} = \\frac{2}{3}\\)❌ \\(Conf(Cerveza \\implies Pañales) = \\frac{Supp(Pañales, Cerveza)}{Supp(Cerveza)} = \\frac{2}{2}\\)✅\n\n\n\nPara {Pan, Mantequilla, Leche}:\n\n\\(Conf({Pan, Mantequilla} \\implies {Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Mantequilla)} = \\frac{2}{3}\\)❌ \\(Conf({Pan, Leche} \\implies {Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Leche)} = \\frac{2}{2}\\)✅ \\(Conf({Mantequilla, Leche} \\implies {Pan}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla, Leche)} = \\frac{2}{2}\\)✅\n\n\\(Conf({Leche} \\implies {Pan, Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\)✅ \\(Conf({Mantequilla} \\implies {Pan, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\)❌ \\(Conf({Pan} \\implies {Mantequilla, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan)} = \\frac{2}{3}\\)❌"
  },
  {
    "objectID": "tics411/clase-7.html#resultado-final",
    "href": "tics411/clase-7.html#resultado-final",
    "title": "TICS-411 Minería de Datos",
    "section": "Resultado Final",
    "text": "Resultado Final\n\n\nItemset MinSupp = 40%\n\n\n\n\n\n\nReglas Finales MinConf = 70%\n\\[Pan \\implies Mantequilla\\] \\[Mantequilla \\implies Pan\\] \\[Leche \\implies Pan\\] \\[Leche \\implies Mantequilla\\] \\[Cerveza \\implies Pañales\\] \\[\\{Pan, Leche\\} \\implies Mantequilla\\]\n\\[\\{Mantequilla, Leche\\} \\implies Pan\\] \\[Leche \\implies \\{Pan, Mantequilla\\}\\]\n\n\n\n\n\n\nInsights:\n\n\n\nEl Pan, la Leche y la Mantequilla están relacionados.\nParece ser que si llevo Cervezas también llevo Pañales."
  },
  {
    "objectID": "tics411/clase-7.html#evaluación-de-reglas-de-asociación",
    "href": "tics411/clase-7.html#evaluación-de-reglas-de-asociación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación de Reglas de Asociación",
    "text": "Evaluación de Reglas de Asociación\n\nLift\n\nMide qué tan lejos de la independencia están \\(X\\) e \\(Y\\). Lift varía entre 0 y \\(\\infty\\).\n\n\n\\[Lift(X,Y) = \\frac{Conf(X \\implies Y)}{s(Y)}\\]\n\n\\(Lift(X,Y) \\sim 1\\) implica independencia y la regla no es importante.\n\\(Lift(X,Y) &lt; 1\\) implica una asociación negativa de la regla.\n\\(Lift(X,Y) &gt; 1\\) implica una asociativa de la regla. Un mayor Lift implica que la regla es potencialmente útil para el futuro.\n\nEjemplo:\n\\[Lift(Cerveza, Pañales) = \\frac{Conf(Cerveza \\implies Pañales)}{Supp(Pañales)} = \\frac{1}{0.6} = 1.67\\]\n\n\n\n\n\n\nUna persona que compra Cerveza tiene 1.67 más chances de comprar Pañales."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-preprocesamiento",
    "href": "tics411/clase-7.html#implementación-en-python-preprocesamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Preprocesamiento",
    "text": "Implementación en Python: Preprocesamiento\nPre-procesamiento\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns = tre.columns_)\nL4: transactions debe ser una lista de listas. Cada fila, son distintas transacciones. Cada transaccion puede tener distinto número de elementos. L5: tre.columns_ extrae los nombres de los productos para que el DataFrame sea más entendible.\n\n\n\n\n\n\ndf_encoded es un DataFrame tipo OneHotEncoder pero con valores Booleanos (Esto es solicitado por la documentación)."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-itemsets",
    "href": "tics411/clase-7.html#implementación-en-python-itemsets",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Itemsets",
    "text": "Implementación en Python: Itemsets\nfrom mlxtend.frequent_patterns import apriori \n\nitemset = apriori(df_encoded, min_support=0.5, use_colnames = True)\nL3: df_encoded es el DataFrame preprocesado.\n\nmin_support: Corresponde al Soporte Mínimo para generar itemsets. Por defecto 0.5.\nuse_colnames: Permite que las reglas usen los nombres de las columnas para referirse a los productos. Por defecto es False, pero conviene usarlo como True.\nitemset será un DataFrame con los itemsets generados."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-reglas",
    "href": "tics411/clase-7.html#implementación-en-python-reglas",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Reglas",
    "text": "Implementación en Python: Reglas\nfrom mlxtend.frequent_patterns import association_rules\n\nrules = association_rules(itemsets, metric=\"confidence\", min_threshold=0.8)\nL3: itemset es el dataframe generado en el paso anterior.\n\nmetric: Métrica para definir reglas, puede ser “confidence” y otras definidas acá\nmin_threshold: Corresponde al umbral de la métrica a utilizar. Por defecto 0.8.\nrules corresponde a un Dataset que tiene las Reglas de Asociación detectadas y muchas métricas asociadas."
  },
  {
    "objectID": "tics411/clase-8.html#introducción-al-aprendizaje-supervisado",
    "href": "tics411/clase-8.html#introducción-al-aprendizaje-supervisado",
    "title": "TICS-411 Minería de Datos",
    "section": "Introducción al Aprendizaje Supervisado",
    "text": "Introducción al Aprendizaje Supervisado\nLos modelos Predictivos/Supervisados tienen la capacidad de predecir valores en datos no vistos.\n\nChip Huyen (Profesora de Stanford)\n\n\n“Models do not predict the future but encode the past…”\n\n\n\nAprenden mediante un proceso de entrenamiento en un train set y evalúan su performance/rendimiento utilizando un test set."
  },
  {
    "objectID": "tics411/clase-8.html#definiciones",
    "href": "tics411/clase-8.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nFeatures\n\n\nTambién llamadas variables o atributos. Corresponden al Input del Modelo y con el cuál el modelo aprende y predice. Normalmente es representado mediante una Matriz denominada \\(X\\).\n\n\nLabels o Etiquetas\n\nCorresponde a las respuestas que el modelo necesita mapear para poder descubrir patrones de manera automática. Normalmente se representa mediante un vector denominado \\(y\\)."
  },
  {
    "objectID": "tics411/clase-8.html#ejemplo",
    "href": "tics411/clase-8.html#ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nQueremos generar un algoritmo de aprendizaje tal que dado un cierto set de datos predigamos si es que a un niño se le dará o no permiso para jugar.\n\n\n\n\n\n\n\nProblema de Clasificación Binaria (Dos clases opuestas).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLlamaremos Features a la Matriz variables que intentarán predecir la etiqueta (con título morado).\nLlamaremos etiquetas al vector de valores a predecir (en azul)."
  },
  {
    "objectID": "tics411/clase-8.html#definición-del-problema",
    "href": "tics411/clase-8.html#definición-del-problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Definición del Problema",
    "text": "Definición del Problema\n\\[h_\\theta(X) = f(X, \\theta)\\]\n\n\n\n\n\n\n\nA \\(h_\\theta(\\cdot)\\) la denominaremos hipótesis o simplemente modelo.\n\\(X\\) será nuestro set de features (\\(n\\times m\\) donde \\(n\\) es el número de observaciones y \\(m\\) el número de features).\n\nCada fila de \\(X\\) corresponde a un vector \\(x_i\\) que representa una observación de nuestro set de features.\n\\(\\theta\\) corresponde a los parámetros del modelo (existen modelos paramétricos y no paramétricos).\nCada algoritmo tendrá su propio mapeo \\(f(\\cdot)\\) para tratar de predecir una etiqueta.\n\n\n\n\n\n\n\n\n\n\nTipos de Hipótesis\n\n\n\nSi \\(h_\\theta(X)\\) devuelve valores discretos (o categóricos) hablaremos de un modelo de Clasificación.\nSi \\(h_\\theta(X)\\) devuelve valores continuos hablaremos de un modelo de Regresión."
  },
  {
    "objectID": "tics411/clase-8.html#tipos-de-problemas",
    "href": "tics411/clase-8.html#tipos-de-problemas",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Problemas",
    "text": "Tipos de Problemas\n\n\n\n\n\n\nClasificación:\n\n\n\nBinaria: La Clasificación es dicotómica, Perro o Gato, Sí o No, 1 o 0, Clase Positiva o Negativa.\nMulticlase: La clasificación puede tener más de 2 clases, pero sólo una es posible.\n\nEj: Perro, Gato o Canario; 0, 1, 2, 3, 4.\n\nMultilabel: La clasificación puede tener más de 2 clases, y más de una es posible a la vez.\n\nEj: Categorías de Libro: Puede ser Romance y Drama, Películas: Fantasía, Animación y Acción.\n\n\n\n\n\n\n\n\n\n\n\nRegresión:\n\n\n\nSimple: Predigo sólo un valor. Ej: Predecir la Temperatura.\nMultiple: Predigo varios valores continuos a la vez.\n\nEj: Modelo para intentar estimar Temperatura y Humedad a la vez.\n\nForecast: Donde se utilizan valores pasados para estimar valores futuros.\n\nDadas mis ganancias pasadas, estimar las futuras."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-intuición",
    "href": "tics411/clase-8.html#clasificación-intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Intuición",
    "text": "Clasificación: Intuición\n\n\n\n\n\n\nSupongamos el siguiente problema de clasificación. Tenemos un algoritmo, que dadas las variables Largo y Peso sean capaces de predecir si es que un Pez es una Reineta o una Sardina."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-intuición-1",
    "href": "tics411/clase-8.html#clasificación-intuición-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Intuición",
    "text": "Clasificación: Intuición\n\n\n\n\n\n\n\n\n\n\n\n\nQueremos encontrar una Regla de Decisión (Decision Rule) que permita clasificar correctamente un punto nuevo.\nDistintos modelos son capaces de encontrar distintas reglas de decisión. Por lo tanto, sus predicciones pueden ser completamente distintas."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-detalles",
    "href": "tics411/clase-8.html#clasificación-detalles",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Detalles",
    "text": "Clasificación: Detalles\nEs importante mencionar que un modelo de clasificación puede generar:\n\nHard Predictions: Es decir, la instancia a predecir es clase 0 o clase 1.\nSoft Prediction: Es decir, la instancia a predecir tiene una probabilidad \\(p\\) de pertenecer a la clase 1 y de \\(1-p\\) de pertenecer a la clase 0.\n\n\n\n\n\n\n\n\nCuando se hace predicción binaria, lo común es usar un Threshold de 0.5 para elegir la clase. Es decir si \\(p&lt;0\\) entonces clase 0, si \\(p \\ge 0.5\\) entonces clase 1.\n\n\n\n\n\n\n\n\n\n\nEn el caso de predicción multiclase o multilabel. Se hace tiene la probabilidad para cada clase. Por lo tanto se se asigna la clase de mayor probabilidad."
  },
  {
    "objectID": "tics411/clase-8.html#k-nearest-neighbors",
    "href": "tics411/clase-8.html#k-nearest-neighbors",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nEl modelo de vecinos más cercanos, o KNN por sus siglas en Inglés es un modelo basado en distancias. Su regla de decisión se basa en imitar el comportamiento de sus \\(K\\) vecinos más cercanos por votación (para clasificación) o la media (para regresión).\n\n\n\n\n\n\n\nK es un hiperparámetro de este modelo.\n\n\n\n\n\n\n\n\n\n\n\n\nSupongamos \\(K = 3\\).\nEs decir, tomaremos los 3 vecinos más cercanos.\n\n\n\n\n\n\n\nEn general es una buena idea elegir vecinos impares. ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-1-training-time",
    "href": "tics411/clase-8.html#knn-paso-1-training-time",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Paso 1 (Training Time)",
    "text": "KNN: Paso 1 (Training Time)\n\nTraining Time\n\nCorresponde al periodo donde el modelo aprende de los datos. Toma un patrón y ese modelo es utilizado para predecir.\n\n\n\n\n\n\n\n\nEn el caso de un KNN NO HAY APRENDIZAJE en esta etapa."
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-2-test-time",
    "href": "tics411/clase-8.html#knn-paso-2-test-time",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Paso 2 (Test Time)",
    "text": "KNN: Paso 2 (Test Time)\n\nInference Time\n\nCorresponde al periodo donde el modelo debe emitir una predicción.\n\n\nEn este caso, KNN calcula las distancias del punto a predecir (en verde) a todos los otros puntos existentes (proceso caro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa predicción corresponderá a la etiqueta mayoritaria por votacioń\n\n\n\n\n\n\n\n\nLa predicción corresponderá a la etiqueta mayoritaria por votacioń.\n¿Cuál sería una buena estrategia de predicción para un modelo de Regresión?"
  },
  {
    "objectID": "tics411/clase-8.html#fronteras-de-decisión",
    "href": "tics411/clase-8.html#fronteras-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Fronteras de Decisión",
    "text": "Fronteras de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\nImplicitamente, todo modelo de Machine Learning generará lo que se llama una Frontera de Decisión.\nSi un punto no visto cae dentro de su frontera entonces se le asigna dicha etiqueta."
  },
  {
    "objectID": "tics411/clase-8.html#implementación-clasificación-en-scikit-learn",
    "href": "tics411/clase-8.html#implementación-clasificación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación Clasificación en Scikit-Learn",
    "text": "Implementación Clasificación en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClasifier(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicción...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) número de vecinos a utilizar. Por defecto 5.\nmetric: Métrica de distancia. Por defecto “Minkowski”.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un parámetro interno para poder paralelizar los cálculos. Se recomienda utilizar -1 para utilizar todos sus cores."
  },
  {
    "objectID": "tics411/clase-8.html#implementación-regresión-en-scikit-learn",
    "href": "tics411/clase-8.html#implementación-regresión-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación Regresión en Scikit-Learn",
    "text": "Implementación Regresión en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn_clf = KNeighborsRegressor(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicción...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) número de vecinos a utilizar. Por defecto 5.\nmetric: Métrica de distancia. Por defecto “Minkowski”.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un parámetro interno para poder paralelizar los cálculos. Se recomienda utilizar -1 para utilizar todos sus cores.\n\n\n\n\n\n\n\n\n¿Cómo se encuentran las predicciones en un modelo de Regresión?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-detalles-técnicos",
    "href": "tics411/clase-8.html#knn-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Detalles Técnicos",
    "text": "KNN: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nModelo muy simple de implementar y entender.\nMuy eficiente en el aprendizaje.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nInferencia ineficiente.\nCurse of Dimensionality: A medida que el número de dimensiones del problema crece, se requiere un incremento exponencial en la cantidad de datos para asegurar que existen suficientes vecinos cercanos para cualquier punto."
  },
  {
    "objectID": "tics411/clase-2.html#eda",
    "href": "tics411/clase-2.html#eda",
    "title": "TICS-411 Minería de Datos",
    "section": "EDA",
    "text": "EDA\n\nEl Analisis Exploratorio de Datos (EDA, por sus siglas en inglés) es procedimiento en el cual se analiza un dataset para explorar sus características principales.\n\n\nSu objetivo principal es poder familiarizarse con los datos además de encontrar potenciales problemas en su calidad.\nPrincipalmente hace uso de técnicas de manipulación de datos y visualizaciones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos hallazgos importantes dentro del proceso se les denomina insights.\n\n\n\n\n\n\n\n\n\nEl uso de visualizaciones inadecuadas podría llevar a conclusiones erróneas.\n\n\n\n\n\n\n\n\n\n\nSummary.\nVisualización."
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-tendencia-central",
    "href": "tics411/clase-2.html#medidas-de-tendencia-central",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas de Tendencia Central",
    "text": "Medidas de Tendencia Central"
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-dispersión-y-asimetría",
    "href": "tics411/clase-2.html#medidas-de-dispersión-y-asimetría",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas de Dispersión y Asimetría",
    "text": "Medidas de Dispersión y Asimetría"
  },
  {
    "objectID": "tics411/clase-2.html#eda-visualización",
    "href": "tics411/clase-2.html#eda-visualización",
    "title": "TICS-411 Minería de Datos",
    "section": "EDA: Visualización",
    "text": "EDA: Visualización\n\nLa visualización de datos es la presentación de datos en forma gráfica. Permite simplificar conceptos más complejos en especial a altos mandos.\n\n\nGracias a la evolución del cerebro humano somos capaces de detectar patrones complejos en la naturaleza a partir de la Visión.\n\n\n\n\n\n\n\n\nPuede ser difícil de aplicar si el tamaño de los datos es grande (sea en instancias o atributos). Por ejemplo, si los datos están en 4 dimensiones.\n\n\n\n\n\n\n\n\n\n\n\n\nSe suelen resumir los datos en estadísticas simples.\nGraficar datos en 1D, 2D y 3D (evitar dentro de lo posible).\nLa visualización debe ser comprensible ojalá sin ninguna explicación.\n\n\n\n\n\n\n\n\n\n\n\n\nEn caso de datos de alta dimensionalidad puede ser una buena idea reducir dimensiones mediante técnicas como:\n\nPCA\nUMAP\netc."
  },
  {
    "objectID": "tics411/clase-2.html#caso-de-visualización",
    "href": "tics411/clase-2.html#caso-de-visualización",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso de Visualización",
    "text": "Caso de Visualización\n\n\n\n\n\n\n\nFiguras\nEscala de Colores.\nTamaño de los puntos.\nDemasiada información en un sólo gráfico.\nNo se entiende el mensaje."
  },
  {
    "objectID": "tics411/clase-2.html#canales-visuales",
    "href": "tics411/clase-2.html#canales-visuales",
    "title": "TICS-411 Minería de Datos",
    "section": "Canales Visuales",
    "text": "Canales Visuales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe les llama canales visuales a elementos visuales que pueden utilizarse para expresar información (Clase Visualizacion Andreas Mueller).\nLa idea es poder mapear cada uno de estos canales a valores que queremos visualizar.\n\n\n\n\n\n\n\n\n\n\n\nNo todos los canales son igual de útiles ni fáciles de entender."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nHistograma\n\n\nEl histograma permite visualizar distribuciones univariadas acumulando los datos en rangos de igual tamaño (bins).\n\n\n\n\nPermite visualizar el centro, la extensión, la asimetría y outliers.\n\n\n\n\n\n\n\n\nEl histograma puede ser “engañoso” para conjuntos de datos pequeños.\nLa visualización puede resultar de manera muy distintas dependiendo del número de bins."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nKernel Density\n\n\nCorresponde a un suavizamiento de un Histograma en el cuál se usa un Kernel (función no negativa que suma 1 y tiene media 0) para agrupar los puntos vecinos.\n\n\n\n\n\nLa función estimada es:\n\\[f(x) = \\frac{1}{n} = \\sum_{i=1}^n K \\left(\\frac{x - x(i)}{h}\\right)\\]\n\n\\(K(u)\\) es el Kernel.\n\\(h\\) es el ancho de banda."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nBoxplot (Caja y Bigotes)\n\nEs un tipo de gráfico que muestra la distribución de manera univariada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiene la capacidad de mostrar varias distribuciones a la vez.\nAdemás presenta estadísticos de interés: Mediana, IQR y outliers.\nLos puntos fuera de los bigotes son considerados Outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos bigotes pueden representar:\n\nMínimo y Máximo. (En este caso no hay outliers).\n\\(\\mu \\pm 3\\sigma\\)\nPercentiles 5 y 95.\nOtros valores."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-barras",
    "href": "tics411/clase-2.html#visualizaciones-barras",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Barras",
    "text": "Visualizaciones: Barras\n\nBar Plot\n\n\nLa altura de la barra (normalmente Eje y) representa una agregación asociada a una categoría (normalmente Eje x).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOtras convenciones llaman a este gráfico Column Plot, mientras que el Bar Plot tiene las barras de manera horizontal."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-puntos",
    "href": "tics411/clase-2.html#visualizaciones-puntos",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Puntos",
    "text": "Visualizaciones: Puntos\n\nScatter\n\n\nGráfico empleado para mostrar distribución de datos bivariados\n\n\n\n\nMuestra la relación entre una variable independiente (Eje X) y una variable dependiente (Eje Y).\nPermite mostrar relaciones lineales o no-lineales (Correlaciones).\nOutliers.\nSimplemente ubicación de Puntos en el Espacio."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-líneas",
    "href": "tics411/clase-2.html#visualizaciones-líneas",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Líneas",
    "text": "Visualizaciones: Líneas\n\nLineplot\n\n\nGráfico empleado para visualizar tendencias y su evolución de una medida (Eje Y) en el tiempo (Eje X).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi bien es posible utilizarlo para gráficar dos medidas continuas, las buenas prácticas indican que el eje X siempre debería contener una componente temporal."
  },
  {
    "objectID": "tics411/clase-2.html#estadísticos-vs-visualizaciones",
    "href": "tics411/clase-2.html#estadísticos-vs-visualizaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Estadísticos vs Visualizaciones",
    "text": "Estadísticos vs Visualizaciones"
  },
  {
    "objectID": "tics411/clase-2.html#otras-visualizaciones",
    "href": "tics411/clase-2.html#otras-visualizaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Otras Visualizaciones?",
    "text": "¿Otras Visualizaciones?"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html",
    "href": "tics411/notebooks/01-Preprocesamiento.html",
    "title": "Clases UAI",
    "section": "",
    "text": "%%capture\n## Ejecutar esta celda para instalar o actualizar Feature_Engine\n!pip install -U feature_engine\n## Chequear que la versión de Feature Engine sea al menos 1.7\nimport feature_engine\n\nfeature_engine.__version__\n\n'1.7.0'\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import set_config\n\n## (Opcional) Este comando permite que el output de Scikit-Learn sean Pandas DataFrames.\n## Por dejecto, Scikit-Learn transforma todo a Numpy, ya que es más eficiente computacionalmente.\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "href": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "title": "Clases UAI",
    "section": "Valores Faltantes",
    "text": "Valores Faltantes\n\n## Para detectar valores faltantes se utiliza el siguiente comando.\ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n## Opcionalmente se puede obtener el % o la fracción de nulos utilizando la siguiente variante.\ndf.isnull().mean()\n\nsurvived       0.000000\npclass         0.000000\nsex            0.000000\nage            0.198653\nsibsp          0.000000\nparch          0.000000\nfare           0.000000\nembarked       0.002245\nclass          0.000000\nwho            0.000000\nadult_male     0.000000\ndeck           0.772166\nembark_town    0.002245\nalive          0.000000\nalone          0.000000\ndtype: float64\n\n\nPandas: Es posible imputar valores usando Pandas con el comando .fillna().\n\nmedia = df[\"age\"].mean()\nmediana = df[\"age\"].median()\nprint(f\"Promedio de Edad: {media}\")\nprint(\n    f'Promedio de Edad con Imputación con Ceros: {df[\"age\"].fillna(0).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputación por Media: {df[\"age\"].fillna(media).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputación por Mediana: {df[\"age\"].fillna(mediana).mean()}'\n)\n\nPromedio de Edad: 29.69911764705882\nPromedio de Edad con Imputación con Ceros: 23.79929292929293\nPromedio de Edad con Imputación por Media: 29.69911764705882\nPromedio de Edad con Imputación por Mediana: 29.36158249158249\n\n\nScikit-Learn: Utiliza la clase SimpleImputer, el cual permite distintas estrategias de Imputación: \"mean\", \"median\", \"most_frequent\", \"constant\".\n\nfrom sklearn.impute import SimpleImputer\n\nsc = SimpleImputer(strategy=\"mean\")\n## En este caso uso [[]] ya que Scikit Learn espera Matrices o DataFrames.\n## Utilizar [[]] fuerza a que AGE sea un DataFrame de una Columna y no una Serie.\n\ndata_imputed = sc.fit_transform(df[[\"age\"]])\n## Se puede ver que los nuevos datos ya no poseen valores Perdidos.\ndata_imputed.isnull().sum()\n\nage    0\ndtype: int64"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "href": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "title": "Clases UAI",
    "section": "Outliers",
    "text": "Outliers\npandas: En Pandas se pueden acotar los outliers utilizando .clip()\n\nprint(f\"Promedio de Tarifas: {df.fare.mean()}\")\ndf[\"fare\"].agg([\"min\", \"max\"])\n\nPromedio de Tarifas: 32.204207968574636\n\n\nmin      0.0000\nmax    512.3292\nName: fare, dtype: float64\n\n\n\nlower: Define la cota inferior.\nupper: Define la cota superior.\n\n\nclipped_data = df[[\"fare\"]].clip(lower=10, upper=50)\nclipped_data.agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n10.0\n\n\nmax\n50.0\n\n\n\n\n\n\n\n\n\ndf[[\"fare\"]]\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows × 1 columns\n\n\n\n\n\n## Los valores menores a 10 fueron reemplazados por 10.\n## Los valores mayores a 50 fueron reemplazados por 50.\nclipped_data\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows × 1 columns\n\n\n\n\nsklearn: Para este caso nos apoyaremos de la librería feature_engine la cual posee herramientas para acotar. feature_engine sigue exactamente la misma lógica de Scikit-Learn.\n\nfrom feature_engine.outliers import ArbitraryOutlierCapper, Winsorizer\n\ncapper = ArbitraryOutlierCapper(\n    max_capping_dict=dict(fare=50), min_capping_dict=dict(fare=10)\n)\ncapper.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows × 1 columns\n\n\n\n\n\ncapping_method: Define la Estragegia a utilizar para el Winsorizer. Ver Docs.\n\n\n## \"gaussian\" permite acotar por mu +/- 3*std\n## \"iqr\" permite rellenar por Q1 - 3*iqr y Q3 + 3*iqr\nwin = Winsorizer(capping_method=\"gaussian\")\nwin.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows × 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#variables-categóricas",
    "href": "tics411/notebooks/01-Preprocesamiento.html#variables-categóricas",
    "title": "Clases UAI",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\npandas:\n\nOne Hot Encoding\nPara la conversión de variables categóricas utilizamos pd.get_dummies(). * drop_first: Si es True se elimina la primera categoría.\n\npd.get_dummies(df[\"embark_town\"], drop_first=False)\n\n\n\n\n\n\n\n\n\nCherbourg\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nFalse\nTrue\n\n\n1\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n\n\n886\nFalse\nFalse\nTrue\n\n\n887\nFalse\nFalse\nTrue\n\n\n888\nFalse\nFalse\nTrue\n\n\n889\nTrue\nFalse\nFalse\n\n\n890\nFalse\nTrue\nFalse\n\n\n\n\n891 rows × 3 columns\n\n\n\n\n\npd.get_dummies(df[\"embark_town\"], drop_first=True)\n# Una ventaja de este procedimiento es que no considera los Nulos como otra categoría...\n\n\n\n\n\n\n\n\n\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nTrue\n\n\n1\nFalse\nFalse\n\n\n2\nFalse\nTrue\n\n\n3\nFalse\nTrue\n\n\n4\nFalse\nTrue\n\n\n...\n...\n...\n\n\n886\nFalse\nTrue\n\n\n887\nFalse\nTrue\n\n\n888\nFalse\nTrue\n\n\n889\nFalse\nFalse\n\n\n890\nTrue\nFalse\n\n\n\n\n891 rows × 2 columns\n\n\n\n\n\n\nOrdinal Encoder\nSe utiliza pd.factorize(). * sort: Usar True ya que coloca las categorías en orden. Además de esta manera se comporta igual que OrdinalEncoder de Scikit-Learn.\n\npd.DataFrame(\n    pd.factorize(df[\"embark_town\"], sort=True)[0], columns=[\"new_column\"]\n)\n\n\n\n\n\n\n\n\n\nnew_column\n\n\n\n\n0\n2\n\n\n1\n0\n\n\n2\n2\n\n\n3\n2\n\n\n4\n2\n\n\n...\n...\n\n\n886\n2\n\n\n887\n2\n\n\n888\n2\n\n\n889\n0\n\n\n890\n1\n\n\n\n\n891 rows × 1 columns\n\n\n\n\nScikit-Learn:\n\n\nOne Hot Encoding\n\nsparse_output: Se debe fijar como False para poder ver el output como Pandas\ndrop: Se debe colocar \"first\" o el nombre de una sóla categoría a eliminar.\n\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nohe = OneHotEncoder(drop=\"first\", sparse_output=False)\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\n\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n\n\n890\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 3 columns\n\n\n\n\n\n\nOrdinal Encoder\n\nohe = OneHotEncoder(\n    drop=[\"Queenstown\"], sparse_output=False\n)  # También se puede colocar np.nan.\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\n\nembark_town_Cherbourg\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n1.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n\n\n\n\n891 rows × 3 columns\n\n\n\n\n\noe = OrdinalEncoder()\noe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\n2.0\n\n\n1\n0.0\n\n\n2\n2.0\n\n\n3\n2.0\n\n\n4\n2.0\n\n\n...\n...\n\n\n886\n2.0\n\n\n887\n2.0\n\n\n888\n2.0\n\n\n889\n0.0\n\n\n890\n1.0\n\n\n\n\n891 rows × 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "href": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "title": "Clases UAI",
    "section": "Escalamiento",
    "text": "Escalamiento\nEl escalamiento normalmente se realiza sólo en Scikit-Learn. Se mostrarán la Estandarización y Normalización.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n## Llamaremos esto Estandarización... (sólo por convención del curso)\nsc = StandardScaler()\ndata = sc.fit_transform(df[[\"fare\"]])\ndata.agg([\"mean\", \"std\"])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmean\n3.987333e-18\n\n\nstd\n1.000562e+00\n\n\n\n\n\n\n\n\n\n## Llamaremos esto Normalización... (sólo por convención del curso)\nmms = MinMaxScaler()\nmms.fit_transform(df[[\"fare\"]]).agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n0.0\n\n\nmax\n1.0"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-sólo-a-algunas-variables.",
    "href": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-sólo-a-algunas-variables.",
    "title": "Clases UAI",
    "section": "Aplicar Preprocesamientos sólo a algunas variables.",
    "text": "Aplicar Preprocesamientos sólo a algunas variables.\nScikit-Learn fue diseñado para el entrenamiento eficiente de modelos. Para ello, se basó en Numpy, el cuál no cuenta con nombre de columnas, por lo que para poder aplicar pre-procesamientos a ciertas partes del Dataset utiliza lo que se llama el ColumnTransformer(), el cuál va más allá del alcance del curso.\nPara simplificar el proceso de elegir ciertas columnas, feature_engine posee una el SklearnTransformerWrapper que permite elegir qué variables queremos pasar por cierta transformación.\n\n## Sin SklearnTransformerWrapper\n\nohe = OneHotEncoder(sparse_output=False)\nohe.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea columnas dummies incluso para las variables numéricas.\n\n\n\n\n\n\n\n\n\nage_0.42\nage_0.67\nage_0.75\nage_0.83\nage_0.92\nage_1.0\nage_2.0\nage_3.0\nage_4.0\nage_5.0\n...\nage_70.0\nage_70.5\nage_71.0\nage_74.0\nage_80.0\nage_nan\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 93 columns\n\n\n\n\n\n## Aplicar preprocesamientos a ciertas variables...\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nohe_w = SklearnTransformerWrapper(\n    OneHotEncoder(sparse_output=False), variables=\"embark_town\"\n)\nohe_w.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea dummies sólo para la variable embark_town y deja age como estaba.\n\n\n\n\n\n\n\n\n\nage\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n22.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n38.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\n27.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n19.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\nNaN\n0.0\n0.0\n1.0\n0.0\n\n\n889\n26.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n32.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 5 columns"
  },
  {
    "objectID": "tics411/notebooks/distancia.html",
    "href": "tics411/notebooks/distancia.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(dict(x=[0, 2, 3, 5], y=[2, 0, 1, 1]))\ndf.index = [1, 2, 3, 4]\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n1\n0\n2\n\n\n2\n2\n0\n\n\n3\n3\n1\n\n\n4\n5\n1\n\n\n\n\n\n\n\n\n\nnp.zeros((4, 4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\ndef distancia_l1(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n\n    return np.abs(x1 - x2) + np.abs(y1 - y2)\n\n\ndef distancia_l2(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n\n\ndef distancia_linf(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    d_x = np.abs(x1 - x2)\n    d_y = np.abs(y1 - y2)\n    return np.max([d_x, d_y])\n\n\ndef calculate_matrix(distance, n_puntos):\n    m = np.zeros((n_puntos, n_puntos))\n    for i in range(n_puntos):\n        for j in range(n_puntos):\n            p = df.iloc[i]\n            q = df.iloc[j]\n            m[i, j] = distance(p, q)\n\n    return m\n\n\nm_m = calculate_matrix(distancia_l1, 4)\nm_e = calculate_matrix(distancia_l2, 4)\nm_c = calculate_matrix(distancia_linf, 4)\n\n\nm_m\n\narray([[0., 4., 4., 6.],\n       [4., 0., 2., 4.],\n       [4., 2., 0., 2.],\n       [6., 4., 2., 0.]])\n\n\n\nm_e\n\narray([[0.        , 2.82842712, 3.16227766, 5.09901951],\n       [2.82842712, 0.        , 1.41421356, 3.16227766],\n       [3.16227766, 1.41421356, 0.        , 2.        ],\n       [5.09901951, 3.16227766, 2.        , 0.        ]])\n\n\n\nm_c\n\narray([[0., 2., 3., 5.],\n       [2., 0., 1., 3.],\n       [3., 1., 0., 2.],\n       [5., 3., 2., 0.]])\n\n\n\n\na = pd.Series(\n    [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0]\n)\nb = pd.Series(\n    [1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0]\n)\n\na.var(ddof=0)  # Varianza Poblacional\n\n3.5\n\n\n\na.var(ddof=1)  # Varianza Muestral\n\n3.7916666666666665\n\n\n\nb.var(ddof=1)\n\n1.5916666666666668\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/hopkins.html",
    "href": "tics411/notebooks/hopkins.html",
    "title": "Ejemplos de Hopkins",
    "section": "",
    "text": "from pyclustertend import hopkins\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef blobs_examples(\n    n_centers, cluster_std, n_samples=4000, p=100, center_box=(-10, 10)\n):\n    df_spread, labels = make_blobs(\n        n_samples=n_samples,\n        centers=n_centers,\n        n_features=2,\n        random_state=42,\n        center_box=center_box,\n        cluster_std=cluster_std,\n    )\n    df_spread = pd.DataFrame(df_spread, columns=[\"x\", \"y\"])\n    plt.scatter(df_spread.x, df_spread.y, c=labels)\n    plt.title(f\"H = {1-hopkins(df_spread, p)}\")\n    plt.tight_layout()\n\n\n## Una sola nube, muy compacta...\nblobs_examples(n_centers=1, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchos nubes muy compactos...\nblobs_examples(n_centers=5, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchas nubes extremadamente compactos\nblobs_examples(n_centers=5, cluster_std=0.001, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso utilizamos la función make_blobs para simular clusters ficticios. Los clusters siempre son esféricos, es por eso que el Hopkins tiende a dar valores bastante buenos. Aunque, dependiendo de qué tan compacto sea la nube tiende a 1 de manera muy fuerte.\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\n\ndef random_examples(n_samples=4000, p=100):\n    df_random = pd.DataFrame(\n        np.random.rand(n_samples, 2), columns=[\"x\", \"y\"]\n    )\n    plt.scatter(df_random.x, df_random.y)\n    plt.title(f\"H = {1-hopkins(df_random, p)}\")\n    plt.tight_layout()\n\n\n## Puntos más dispersos\nrandom_examples(n_samples=100, p=10)\n\n\n\n\n\n\n\n\n\n## Más denso, pero aún aleatorio...\nrandom_examples(n_samples=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso estamos usando np.random.rand para simular sólo valores aleatorios. Se puede ver que entre más lleno está el espacio, Hopkins tiende a 0.5.\n\n\n## valores uniformemente distribuidos y con poca tendencia a agruparse (normalmente pocos puntos)\n## tienen H más pequeños, pero es díficil obtenerlos...\nnp.random.seed(0)\n\n\ndef uniform_example(n_samples=10, max_val=10, p=10):\n    df_uniform = pd.DataFrame(\n        dict(\n            x=np.random.randint(0, max_val + 1, size=n_samples),\n            y=np.random.randint(0, max_val, size=n_samples),\n        )\n    )\n\n    plt.scatter(df_uniform.x, df_uniform.y)\n    plt.title(f\"H = {1-hopkins(df_uniform, p)}\")\n\n\nuniform_example(n_samples=11, max_val=10, p=10)\n\n\n\n\n\n\n\n\n\nuniform_example(n_samples=4000, max_val=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso también estamos forzando aleatoriedad pero con uniformidad de distancia. Para eso simulamos usando np.random.randint para generar valores aleatorios pero más o menos equiespaciados uniformemente. Es bien interesante este caso, porque si usamos muchos datos, se tiende a valores completamente aleatorios, es decir H \\(\\sim\\) 0.5."
  },
  {
    "objectID": "tics411/notebooks/hopkins.html#ejemplos-de-hopkins",
    "href": "tics411/notebooks/hopkins.html#ejemplos-de-hopkins",
    "title": "Ejemplos de Hopkins",
    "section": "",
    "text": "from pyclustertend import hopkins\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef blobs_examples(\n    n_centers, cluster_std, n_samples=4000, p=100, center_box=(-10, 10)\n):\n    df_spread, labels = make_blobs(\n        n_samples=n_samples,\n        centers=n_centers,\n        n_features=2,\n        random_state=42,\n        center_box=center_box,\n        cluster_std=cluster_std,\n    )\n    df_spread = pd.DataFrame(df_spread, columns=[\"x\", \"y\"])\n    plt.scatter(df_spread.x, df_spread.y, c=labels)\n    plt.title(f\"H = {1-hopkins(df_spread, p)}\")\n    plt.tight_layout()\n\n\n## Una sola nube, muy compacta...\nblobs_examples(n_centers=1, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchos nubes muy compactos...\nblobs_examples(n_centers=5, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchas nubes extremadamente compactos\nblobs_examples(n_centers=5, cluster_std=0.001, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso utilizamos la función make_blobs para simular clusters ficticios. Los clusters siempre son esféricos, es por eso que el Hopkins tiende a dar valores bastante buenos. Aunque, dependiendo de qué tan compacto sea la nube tiende a 1 de manera muy fuerte.\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\n\ndef random_examples(n_samples=4000, p=100):\n    df_random = pd.DataFrame(\n        np.random.rand(n_samples, 2), columns=[\"x\", \"y\"]\n    )\n    plt.scatter(df_random.x, df_random.y)\n    plt.title(f\"H = {1-hopkins(df_random, p)}\")\n    plt.tight_layout()\n\n\n## Puntos más dispersos\nrandom_examples(n_samples=100, p=10)\n\n\n\n\n\n\n\n\n\n## Más denso, pero aún aleatorio...\nrandom_examples(n_samples=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso estamos usando np.random.rand para simular sólo valores aleatorios. Se puede ver que entre más lleno está el espacio, Hopkins tiende a 0.5.\n\n\n## valores uniformemente distribuidos y con poca tendencia a agruparse (normalmente pocos puntos)\n## tienen H más pequeños, pero es díficil obtenerlos...\nnp.random.seed(0)\n\n\ndef uniform_example(n_samples=10, max_val=10, p=10):\n    df_uniform = pd.DataFrame(\n        dict(\n            x=np.random.randint(0, max_val + 1, size=n_samples),\n            y=np.random.randint(0, max_val, size=n_samples),\n        )\n    )\n\n    plt.scatter(df_uniform.x, df_uniform.y)\n    plt.title(f\"H = {1-hopkins(df_uniform, p)}\")\n\n\nuniform_example(n_samples=11, max_val=10, p=10)\n\n\n\n\n\n\n\n\n\nuniform_example(n_samples=4000, max_val=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso también estamos forzando aleatoriedad pero con uniformidad de distancia. Para eso simulamos usando np.random.randint para generar valores aleatorios pero más o menos equiespaciados uniformemente. Es bien interesante este caso, porque si usamos muchos datos, se tiende a valores completamente aleatorios, es decir H \\(\\sim\\) 0.5."
  },
  {
    "objectID": "tics411/notebooks/jerarquico.html",
    "href": "tics411/notebooks/jerarquico.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        alpha=[9, 10, 1, 6, 1],\n        beta=[3, 2, 9, 5, 10],\n        gamma=[7, 9, 4, 5, 3],\n    )\n)\n\nindices = [\"p53\", \"mdm2\", \"bcl2\", \"cylinE\", \"Caspade\"]\ndf.index = indices\ndf\n\n\n\n\n\n\n\n\n\nalpha\nbeta\ngamma\n\n\n\n\np53\n9\n3\n7\n\n\nmdm2\n10\n2\n9\n\n\nbcl2\n1\n9\n4\n\n\ncylinE\n6\n5\n5\n\n\nCaspade\n1\n10\n3\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\n\ndef calculate_global_min(dm):\n    data = np.triu(dm)\n\n    min_val = np.nanmin(data[np.nonzero(data)])\n    position = [dm.index[val[0]] for val in np.where(data == min_val)]\n    return min_val, position\n\n\noriginal_dm = distance_matrix(df, df, p=2)\noriginal_dm = pd.DataFrame(\n    original_dm, index=df.index, columns=df.index\n).round(2)\noriginal_dm.index = np.arange(5).astype(str)\noriginal_dm.columns = np.arange(5).astype(str)\noriginal_dm\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\n1\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\n2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\n3\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\n4\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\n\ndata = original_dm.copy()\ndata.index = indices\ndata.columns = indices\ndata\n\n\n\n\n\n\n\n\n\np53\nmdm2\nbcl2\ncylinE\nCaspade\n\n\n\n\np53\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\nmdm2\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\nbcl2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\ncylinE\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\nCaspade\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\n\ndef clean_position(position):\n    pos = []\n    for p in position:\n        pos.extend(p.split(\",\"))\n    return pos\n\n\ndef new_iteration(dm, original_dm, linkage=np.nanmean):\n    min_val, position = calculate_global_min(dm)\n    print(f\"El valor mínimo encontrado es: {min_val}\")\n    print(f\"Clusters a fusionar: {position}\")\n    non_position = [col for col in dm.columns if col not in position]\n    print(f\"Clusters que no se fusionan: {non_position}\")\n    new_position = \",\".join(position)\n    new_dm = dm.copy()\n    values = []\n    clean_pos = clean_position(position)\n    for n_p in non_position:\n        n_p = n_p.split(\",\")\n        v = linkage(original_dm.loc[n_p, clean_pos])\n        values.append(v)\n\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    new_dm = new_dm.T\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    return new_dm.drop(index=position, columns=position)\n\n\ndm_1 = new_iteration(original_dm, original_dm)\ndm_1\n\nEl valor mínimo encontrado es: 1.41\nClusters a fusionar: ['2', '4']\nClusters que no se fusionan: ['0', '1', '3']\n\n\n\n\n\n\n\n\n\n\n0\n1\n3\n2,4\n\n\n\n\n0\n0.00\n2.45\n4.120\n10.900\n\n\n1\n2.45\n0.00\n6.400\n12.950\n\n\n3\n4.12\n6.40\n0.000\n6.915\n\n\n2,4\n10.90\n12.95\n6.915\nNaN\n\n\n\n\n\n\n\n\n\ndm_2 = new_iteration(dm_1, original_dm)\ndm_2\n\nEl valor mínimo encontrado es: 2.45\nClusters a fusionar: ['0', '1']\nClusters que no se fusionan: ['3', '2,4']\n\n\n\n\n\n\n\n\n\n\n3\n2,4\n0,1\n\n\n\n\n3\n0.000\n6.915\n5.260\n\n\n2,4\n6.915\nNaN\n11.925\n\n\n0,1\n5.260\n11.925\nNaN\n\n\n\n\n\n\n\n\n\ndm_3 = new_iteration(dm_2, original_dm)\ndm_3\n\nEl valor mínimo encontrado es: 5.26\nClusters a fusionar: ['3', '0,1']\nClusters que no se fusionan: ['2,4']\n\n\n\n\n\n\n\n\n\n\n2,4\n3,0,1\n\n\n\n\n2,4\nNaN\n10.255\n\n\n3,0,1\n10.255\nNaN\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/05-ex-jerarquico.html",
    "href": "tics411/notebooks/05-ex-jerarquico.html",
    "title": "Ejemplo Clustering Aglomerativo",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"iris\")\ndf\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\nX = df.drop(columns=\"species\")\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n3\n-1.506521\n0.098217\n-1.283389\n-1.315444\n\n\n4\n-1.021849\n1.249201\n-1.340227\n-1.315444\n\n\n...\n...\n...\n...\n...\n\n\n145\n1.038005\n-0.131979\n0.819596\n1.448832\n\n\n146\n0.553333\n-1.282963\n0.705921\n0.922303\n\n\n147\n0.795669\n-0.131979\n0.819596\n1.053935\n\n\n148\n0.432165\n0.788808\n0.933271\n1.448832\n\n\n149\n0.068662\n-0.131979\n0.762758\n0.790671\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_iris = pca.fit_transform(X_sc)\n\n\ndef pca_viz(pca, color=None, title=\"\"):\n    plt.scatter(pca_iris[\"pca0\"], pca_iris[\"pca1\"], c=color)\n    plt.title(title)\n    plt.show()\n\n\npca_viz(pca_iris, title=\"Visualización de Iris en 2 dimensiones\")\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlink_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in link_list:\n    plot_dendogram(X_sc, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nagc = AgglomerativeClustering(\n    n_clusters=3, metric=\"euclidean\", linkage=\"ward\"\n)\nlabels = agc.fit_predict(X_sc)\npca_viz(\n    pca_iris,\n    color=labels,\n    title=\"Clustering Iris. Método Average, 3 Clusters.\",\n)\n\n## Transformarlo en función para probar muchas combinaciones..."
  },
  {
    "objectID": "tics411/notebooks/05-ex-jerarquico.html#ejemplo-clustering-aglomerativo",
    "href": "tics411/notebooks/05-ex-jerarquico.html#ejemplo-clustering-aglomerativo",
    "title": "Ejemplo Clustering Aglomerativo",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"iris\")\ndf\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\nX = df.drop(columns=\"species\")\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n3\n-1.506521\n0.098217\n-1.283389\n-1.315444\n\n\n4\n-1.021849\n1.249201\n-1.340227\n-1.315444\n\n\n...\n...\n...\n...\n...\n\n\n145\n1.038005\n-0.131979\n0.819596\n1.448832\n\n\n146\n0.553333\n-1.282963\n0.705921\n0.922303\n\n\n147\n0.795669\n-0.131979\n0.819596\n1.053935\n\n\n148\n0.432165\n0.788808\n0.933271\n1.448832\n\n\n149\n0.068662\n-0.131979\n0.762758\n0.790671\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_iris = pca.fit_transform(X_sc)\n\n\ndef pca_viz(pca, color=None, title=\"\"):\n    plt.scatter(pca_iris[\"pca0\"], pca_iris[\"pca1\"], c=color)\n    plt.title(title)\n    plt.show()\n\n\npca_viz(pca_iris, title=\"Visualización de Iris en 2 dimensiones\")\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlink_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in link_list:\n    plot_dendogram(X_sc, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nagc = AgglomerativeClustering(\n    n_clusters=3, metric=\"euclidean\", linkage=\"ward\"\n)\nlabels = agc.fit_predict(X_sc)\npca_viz(\n    pca_iris,\n    color=labels,\n    title=\"Clustering Iris. Método Average, 3 Clusters.\",\n)\n\n## Transformarlo en función para probar muchas combinaciones..."
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html",
    "href": "tics411/notebooks/04-analisis_centros.html",
    "title": "Análisis de Centros",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\ndf = sns.load_dataset(\"iris\")\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(df.drop(columns=\"species\"))\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nlabels = km.fit_predict(df.drop(columns=\"species\"))\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\nmean_table, std_table = create_tables(\n    df,\n    labels,\n    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n)\n\n\n## Corresponde a los valores promedios de cada variable por Cluster (los Centroides)\nmean_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n5.901613\n2.748387\n4.393548\n1.433871\n\n\n1\n5.006000\n3.428000\n1.462000\n0.246000\n\n\n2\n6.850000\n3.073684\n5.742105\n2.071053\n\n\n\n\n\n\n\n\n\n## Corresponde a la Desviación Estándar de cada variable por Cluster\nstd_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n0.466410\n0.296284\n0.508895\n0.297500\n\n\n1\n0.352490\n0.379064\n0.173664\n0.105386\n\n\n2\n0.494155\n0.290092\n0.488590\n0.279872"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html#análisis-de-centros",
    "href": "tics411/notebooks/04-analisis_centros.html#análisis-de-centros",
    "title": "Análisis de Centros",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\ndf = sns.load_dataset(\"iris\")\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(df.drop(columns=\"species\"))\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nlabels = km.fit_predict(df.drop(columns=\"species\"))\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\nmean_table, std_table = create_tables(\n    df,\n    labels,\n    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n)\n\n\n## Corresponde a los valores promedios de cada variable por Cluster (los Centroides)\nmean_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n5.901613\n2.748387\n4.393548\n1.433871\n\n\n1\n5.006000\n3.428000\n1.462000\n0.246000\n\n\n2\n6.850000\n3.073684\n5.742105\n2.071053\n\n\n\n\n\n\n\n\n\n## Corresponde a la Desviación Estándar de cada variable por Cluster\nstd_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n0.466410\n0.296284\n0.508895\n0.297500\n\n\n1\n0.352490\n0.379064\n0.173664\n0.105386\n\n\n2\n0.494155\n0.290092\n0.488590\n0.279872"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html#representación-gráfica",
    "href": "tics411/notebooks/04-analisis_centros.html#representación-gráfica",
    "title": "Análisis de Centros",
    "section": "Representación Gráfica",
    "text": "Representación Gráfica\nAcá les dejo una Función con la cual pueden realizar el Análisis de Centros. Para ello requieren un DataFrame que contenga las variables a analizar y su etiqueta.\nSe debe indicar, el df, el número de Clusters creados, la columna de la etiqueta, y las columnas a analizar. Adicionalmente se puede agregar un título y cambiar las dimensiones del gráfico.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i].title())\n    plt.suptitle(title, fontsize=15)\n    plt.show()\n\n\ncolumns = df.drop(columns=[\"species\", \"labels\"]).columns.tolist()\ncenter_analysis_viz(\n    df,\n    n_clusters=3,\n    labels=labels,\n    columns=columns,\n    title=\"Análisis de Centros para Iris\",\n)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html",
    "href": "tics411/notebooks/02-EDA.html",
    "title": "EDA",
    "section": "",
    "text": "El siguiente notebook tiene por propósito mostrar algunos comandos básicos para poder realizar Exploración de Datos utilizando Pandas.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Vamos a cargar los siguientes datos para poder explorarlos.\niris_df = sns.load_dataset(\"iris\")\ntitanic_df = sns.load_dataset(\"titanic\")\nts_df = sns.load_dataset(\"dowjones\")\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\n\nLos comandos .mean() y .median() permiten calcular la media y la mediana en datos numéricos. Como se ve en los ejemplos permite llamar una Serie de Pandas y calcular un valor.\n\nTip: En caso de querer aplicar estos comandos a un DataFrame se recomienda utilizar el flag numeric_only = True para evitar calcular estos valores en Datos Categóricos donde no hacen sentido.\n\n\nprint(f\"Promedio de Ancho de Petalo {iris_df['sepal_width'].mean()}\")\nprint(f\"Mediana de Largo de Petalo {iris_df['sepal_length'].median()}\")\n\nPromedio de Ancho de Petalo 3.0573333333333337\nMediana de Largo de Petalo 5.8\n\n\nPandas también cuenta con el comando .mode() el cuál devuelve la moda. A diferencia de los comandos anteriores, .mode() puede utilizarse tanto para datos categóricos como datos numéricos.\n\nprint(f\"Moda de Especies: \")\niris_df[\"species\"].mode()\n\nModa de Especies: \n\n\n0        setosa\n1    versicolor\n2     virginica\nName: species, dtype: object\n\n\nEl comando .quantile() permite calcular algún percentil de interés. q es un valor que va entre 0 y 1 para indicar el percentil requerido. Recordar que la mediana es equivalente al Percentil 50.\n\np25 = iris_df[\"sepal_width\"].quantile(q=0.25)\np50 = iris_df[\"sepal_width\"].quantile(q=0.50)\np75 = iris_df[\"sepal_width\"].quantile(q=0.75)\niris_df[\"sepal_width\"].median(), p25, p50, p75\n\n(3.0, 2.8, 3.0, 3.3)\n\n\n\n\n\nPandas permite el cálculo de distintas medidas de dispersión. Al igual que los comandos anteriores contiene el flag numeric_only = True para evitar inconvenientes en DataFrames con distintos data types. Además contiene el comando ddof el cuál permitirá diferenciar si se quiere la medida poblacional (ddof = 0) o la muestral (ddof = 1).\n\n# Varianza Poblacional\niris_df.var(numeric_only=True, ddof=0)\n\nsepal_length    0.681122\nsepal_width     0.188713\npetal_length    3.095503\npetal_width     0.577133\ndtype: float64\n\n\n\n# Varianza Muestral\niris_df.var(numeric_only=True, ddof=1)\n\nsepal_length    0.685694\nsepal_width     0.189979\npetal_length    3.116278\npetal_width     0.581006\ndtype: float64\n\n\n\n# Desviación Estándar Muestral\niris_df.std(numeric_only=True, ddof=1)\n\nsepal_length    0.828066\nsepal_width     0.435866\npetal_length    1.765298\npetal_width     0.762238\ndtype: float64\n\n\n\n# Función para calcular el Rango Intercuartil...\ndef calculate_IQR(column):\n    quantiles = iris_df.quantile([0.25, 0.75], numeric_only=True)\n    iqr_sl = quantiles.loc[0.75, column] - quantiles.loc[0.25, column]\n    return iqr_sl\n\n\ncalculate_IQR(\"sepal_length\")\ncalculate_IQR(\"petal_width\")\n\n1.5\n\n\n\n# Coeficiente de Skewness o Asimetría.\niris_df.skew(numeric_only=True)\n\nsepal_length    0.314911\nsepal_width     0.318966\npetal_length   -0.274884\npetal_width    -0.102967\ndtype: float64\n\n\n\n\n\nA continuación se mostrarán comandos propios de Pandas para poder generar los gráficos visto a lo largo de las clases. Se sugiere este tipo de gráficos cuando se trabaje con DataFrames ya que poseen buena documentación y una interfaz común para todos los gráficos.\nOpciones:\n\nkind: Permite indicar mediante un string el tipo de gráfico a mostrar.\nfigsize = (w,h): Permite fijar el tamaño de la figura. Notar que primero se entrega el ancho y luego el alto. Yo normalmente uso (20,6) ya que considero que queda bastante bien.\nedgecolor: Permite indicar el color del borde de las barras mediante un string. Tiene sentido para histogramás y bar plots.\ngrid = True/False: Permite mostrar o no una grilla.\nbins = n: Opción sólo para histogramas que permite indicar en cuántos bins se dividen los datos en el Histograma.\nalpha = 0.5: Corresponde al grado de transparecencia. Es un valor que va entre 0 y 1. Entre más pequeño el valor, más transparente.\ntitle: Permite agregar un Título como String.\nxlabel: Permite agregar un Título al Eje X.\nylabel: Permite agregar un Título al Eje Y.\n\n\n\n\niris_df.plot(\n    kind=\"hist\", alpha=0.5, bins=30, figsize=(20, 6), edgecolor=\"black\"\n)\n# Notar que este genera todos los histogramas superpuestos...\n\nPor alguna razón Pandas tiene el comando .hist(). Este comando es bastante útil porque a diferencia del anterior no superpone los histogramas, lo cual la mayoría de las veces es lo que se busca.\n\niris_df.hist(figsize=(20, 6), bins=30, edgecolor=\"black\", grid=False)\n# tight_layout es opcional y a veces evita que hayan traslapes de títulos.\n# Usarlo si es que es necesario.\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nA diferencia de los Histográmas, los Barplots son utilizados para aplicar una agregación antes de gráficar. Esta agregación se puede utilizar mediante .value_counts() que permite contar valores, o mediante .groupby() el cuál permite aplicar otros tipos de agregación.\n\n# Acá por ejemplos contamos la cantidad de pasajeros por Sexo\ntitanic_df[\"sex\"].value_counts()\n\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\n\n# Una vez que tenemos contados los elementos podemos graficar...\ntitanic_df[\"sex\"].value_counts().plot(\n    kind=\"bar\",\n    figsize=(5, 6),\n    title=\"Número de Pasajeros por Sexo...\",\n    edgecolor=\"black\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Otro ejemplo, en este caso calculando el promedio por de Edad y Tarifa por Año.\ntitanic_df.groupby(\"pclass\")[[\"age\", \"fare\"]].mean()\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\npclass\n\n\n\n\n\n\n1\n38.233441\n84.154687\n\n\n2\n29.877630\n20.662183\n\n\n3\n25.140620\n13.675550\n\n\n\n\n\n\n\n\n\n## En este caso, el índice Pclass irá al Eje X y los valores agregados de Age y Fare irán como barras.\niris_df.groupby(\"species\").mean().plot(kind=\"bar\", edgecolor=\"black\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\niris_df.drop(columns=\"species\").plot(kind=\"box\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nNotar que a diferencia de los casos anteriores, el gráfico de puntos requiere que se definan qué columna irá en x y en y respectivamente.\n\n\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n)\n\n\n\n\n\nEl lineplot es el gráfico por defecto de Pandas, por lo tanto no es necesario definir el parámetro kind. Al igual que el gráfico de Puntos se debe definir las variables x e y. Se recomienda siempre que x sea una variable de tipo temporal.\n\nts_df.plot(x=\"Date\", y=\"Price\", title=\"Evolución del Dow Jones\")\n\n\n## Este es un ejemplo de varias series de tiempo en conjunto.\n## Este código sólo genera datos sintéticos.\nfrom scipy.stats import norm\n\nts_df[\"AA\"] = ts_df[\"Price\"] + norm.rvs(size=649) * 55 + 1000\nts_df[\"BB\"] = -norm.rvs(size=649) * 55\n\nts_df.set_index(\"Date\").plot(title=\"Comparación distintas Tendencias\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nEn muchas ocaciones nosotros queremos mostrar una compilación de todos nuestros gráficos más que cada uno por separado. Para eso Matplotlib cuenta con la opción Mosaico.\nMosaico permite generar una grilla definida como un String. Si se fijan nuestra grilla se define por el string:\n\"\"\"AAA\n   BCC\"\"\"\nEn este caso nuestro canvas se divide en 6 partes, el gráfico que asigne a A utilizará las 3 secciones superiores, B utilizará sólo la sección de abajo a la izquierda y C utilizará las 2 restantes.\nPara asignar cada sección .plot() de pandas posee el parámetro ax donde se debe generar la asignación.\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.subplot_mosaic(\n    \"\"\"AAA\n       BCC\"\"\"\n)\n\n# Gráfico asignado a C\niris_df.drop(columns=\"species\").plot(\n    kind=\"box\", ax=ax[\"C\"], title=\"Distribución de Datos por Variable\"\n)\n\n## Gráfico asignado a B\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n    ax=ax[\"B\"],\n)\n\n## Gráfico asignado a A\niris_df.groupby(\"species\").mean().plot(\n    kind=\"bar\",\n    edgecolor=\"black\",\n    ax=ax[\"A\"],\n    rot=0,\n    title=\"Valores promedio por Especie\",\n)\n\n## Permite Agregar un título general a todo el Gráfico\nplt.suptitle(\"Este será un título para todo el Gráfico\", fontsize=20)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nLos comandos mostrados anteriormente son una adaptación de Matplotlib a Pandas. La gracia que tienen es que son fáciles de aprender y funcionarán directamente en Pandas que será nuestra principal fuente de datos.\nEn el caso de trabajar con Numpy, estos comandos NO FUNCIONARÁN. Por lo tanto es necesario utilizar la API de Matplotlib. La traducción no es 100% directa, pero normalmente todos los parámetros de .plot() se cambiarán por comandos del tipo plt.---\n\n\n\nplt.plot(x,y, c = \"red\") #Existe también plt.bar, plt.hist, plt.scatter, plt.boxplot.\nplt.title(\"Este va a ser un título\")\nplt.xlabel(\"Este será una etiqueta del Eje X\")\nAprender Matplotlib es bastante más complicado pero tiene funcionalidades muchísimo más avanzadas que Pandas. Para este curso, no será necesario especializarse en Matplotlib, pero sí más adelante utilizaremos algunos gráficos que no se pueden hacer tan fácilmente en Pandas (pero serán casos puntuales)."
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "title": "EDA",
    "section": "",
    "text": "Los comandos .mean() y .median() permiten calcular la media y la mediana en datos numéricos. Como se ve en los ejemplos permite llamar una Serie de Pandas y calcular un valor.\n\nTip: En caso de querer aplicar estos comandos a un DataFrame se recomienda utilizar el flag numeric_only = True para evitar calcular estos valores en Datos Categóricos donde no hacen sentido.\n\n\nprint(f\"Promedio de Ancho de Petalo {iris_df['sepal_width'].mean()}\")\nprint(f\"Mediana de Largo de Petalo {iris_df['sepal_length'].median()}\")\n\nPromedio de Ancho de Petalo 3.0573333333333337\nMediana de Largo de Petalo 5.8\n\n\nPandas también cuenta con el comando .mode() el cuál devuelve la moda. A diferencia de los comandos anteriores, .mode() puede utilizarse tanto para datos categóricos como datos numéricos.\n\nprint(f\"Moda de Especies: \")\niris_df[\"species\"].mode()\n\nModa de Especies: \n\n\n0        setosa\n1    versicolor\n2     virginica\nName: species, dtype: object\n\n\nEl comando .quantile() permite calcular algún percentil de interés. q es un valor que va entre 0 y 1 para indicar el percentil requerido. Recordar que la mediana es equivalente al Percentil 50.\n\np25 = iris_df[\"sepal_width\"].quantile(q=0.25)\np50 = iris_df[\"sepal_width\"].quantile(q=0.50)\np75 = iris_df[\"sepal_width\"].quantile(q=0.75)\niris_df[\"sepal_width\"].median(), p25, p50, p75\n\n(3.0, 2.8, 3.0, 3.3)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-dispersión",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-dispersión",
    "title": "EDA",
    "section": "",
    "text": "Pandas permite el cálculo de distintas medidas de dispersión. Al igual que los comandos anteriores contiene el flag numeric_only = True para evitar inconvenientes en DataFrames con distintos data types. Además contiene el comando ddof el cuál permitirá diferenciar si se quiere la medida poblacional (ddof = 0) o la muestral (ddof = 1).\n\n# Varianza Poblacional\niris_df.var(numeric_only=True, ddof=0)\n\nsepal_length    0.681122\nsepal_width     0.188713\npetal_length    3.095503\npetal_width     0.577133\ndtype: float64\n\n\n\n# Varianza Muestral\niris_df.var(numeric_only=True, ddof=1)\n\nsepal_length    0.685694\nsepal_width     0.189979\npetal_length    3.116278\npetal_width     0.581006\ndtype: float64\n\n\n\n# Desviación Estándar Muestral\niris_df.std(numeric_only=True, ddof=1)\n\nsepal_length    0.828066\nsepal_width     0.435866\npetal_length    1.765298\npetal_width     0.762238\ndtype: float64\n\n\n\n# Función para calcular el Rango Intercuartil...\ndef calculate_IQR(column):\n    quantiles = iris_df.quantile([0.25, 0.75], numeric_only=True)\n    iqr_sl = quantiles.loc[0.75, column] - quantiles.loc[0.25, column]\n    return iqr_sl\n\n\ncalculate_IQR(\"sepal_length\")\ncalculate_IQR(\"petal_width\")\n\n1.5\n\n\n\n# Coeficiente de Skewness o Asimetría.\niris_df.skew(numeric_only=True)\n\nsepal_length    0.314911\nsepal_width     0.318966\npetal_length   -0.274884\npetal_width    -0.102967\ndtype: float64"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#visualizaciones",
    "href": "tics411/notebooks/02-EDA.html#visualizaciones",
    "title": "EDA",
    "section": "",
    "text": "A continuación se mostrarán comandos propios de Pandas para poder generar los gráficos visto a lo largo de las clases. Se sugiere este tipo de gráficos cuando se trabaje con DataFrames ya que poseen buena documentación y una interfaz común para todos los gráficos.\nOpciones:\n\nkind: Permite indicar mediante un string el tipo de gráfico a mostrar.\nfigsize = (w,h): Permite fijar el tamaño de la figura. Notar que primero se entrega el ancho y luego el alto. Yo normalmente uso (20,6) ya que considero que queda bastante bien.\nedgecolor: Permite indicar el color del borde de las barras mediante un string. Tiene sentido para histogramás y bar plots.\ngrid = True/False: Permite mostrar o no una grilla.\nbins = n: Opción sólo para histogramas que permite indicar en cuántos bins se dividen los datos en el Histograma.\nalpha = 0.5: Corresponde al grado de transparecencia. Es un valor que va entre 0 y 1. Entre más pequeño el valor, más transparente.\ntitle: Permite agregar un Título como String.\nxlabel: Permite agregar un Título al Eje X.\nylabel: Permite agregar un Título al Eje Y.\n\n\n\n\niris_df.plot(\n    kind=\"hist\", alpha=0.5, bins=30, figsize=(20, 6), edgecolor=\"black\"\n)\n# Notar que este genera todos los histogramas superpuestos...\n\nPor alguna razón Pandas tiene el comando .hist(). Este comando es bastante útil porque a diferencia del anterior no superpone los histogramas, lo cual la mayoría de las veces es lo que se busca.\n\niris_df.hist(figsize=(20, 6), bins=30, edgecolor=\"black\", grid=False)\n# tight_layout es opcional y a veces evita que hayan traslapes de títulos.\n# Usarlo si es que es necesario.\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nA diferencia de los Histográmas, los Barplots son utilizados para aplicar una agregación antes de gráficar. Esta agregación se puede utilizar mediante .value_counts() que permite contar valores, o mediante .groupby() el cuál permite aplicar otros tipos de agregación.\n\n# Acá por ejemplos contamos la cantidad de pasajeros por Sexo\ntitanic_df[\"sex\"].value_counts()\n\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\n\n# Una vez que tenemos contados los elementos podemos graficar...\ntitanic_df[\"sex\"].value_counts().plot(\n    kind=\"bar\",\n    figsize=(5, 6),\n    title=\"Número de Pasajeros por Sexo...\",\n    edgecolor=\"black\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Otro ejemplo, en este caso calculando el promedio por de Edad y Tarifa por Año.\ntitanic_df.groupby(\"pclass\")[[\"age\", \"fare\"]].mean()\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\npclass\n\n\n\n\n\n\n1\n38.233441\n84.154687\n\n\n2\n29.877630\n20.662183\n\n\n3\n25.140620\n13.675550\n\n\n\n\n\n\n\n\n\n## En este caso, el índice Pclass irá al Eje X y los valores agregados de Age y Fare irán como barras.\niris_df.groupby(\"species\").mean().plot(kind=\"bar\", edgecolor=\"black\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#boxplots",
    "href": "tics411/notebooks/02-EDA.html#boxplots",
    "title": "EDA",
    "section": "",
    "text": "iris_df.drop(columns=\"species\").plot(kind=\"box\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nNotar que a diferencia de los casos anteriores, el gráfico de puntos requiere que se definan qué columna irá en x y en y respectivamente.\n\n\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#lineplot",
    "href": "tics411/notebooks/02-EDA.html#lineplot",
    "title": "EDA",
    "section": "",
    "text": "El lineplot es el gráfico por defecto de Pandas, por lo tanto no es necesario definir el parámetro kind. Al igual que el gráfico de Puntos se debe definir las variables x e y. Se recomienda siempre que x sea una variable de tipo temporal.\n\nts_df.plot(x=\"Date\", y=\"Price\", title=\"Evolución del Dow Jones\")\n\n\n## Este es un ejemplo de varias series de tiempo en conjunto.\n## Este código sólo genera datos sintéticos.\nfrom scipy.stats import norm\n\nts_df[\"AA\"] = ts_df[\"Price\"] + norm.rvs(size=649) * 55 + 1000\nts_df[\"BB\"] = -norm.rvs(size=649) * 55\n\nts_df.set_index(\"Date\").plot(title=\"Comparación distintas Tendencias\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#mosaico",
    "href": "tics411/notebooks/02-EDA.html#mosaico",
    "title": "EDA",
    "section": "",
    "text": "En muchas ocaciones nosotros queremos mostrar una compilación de todos nuestros gráficos más que cada uno por separado. Para eso Matplotlib cuenta con la opción Mosaico.\nMosaico permite generar una grilla definida como un String. Si se fijan nuestra grilla se define por el string:\n\"\"\"AAA\n   BCC\"\"\"\nEn este caso nuestro canvas se divide en 6 partes, el gráfico que asigne a A utilizará las 3 secciones superiores, B utilizará sólo la sección de abajo a la izquierda y C utilizará las 2 restantes.\nPara asignar cada sección .plot() de pandas posee el parámetro ax donde se debe generar la asignación.\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.subplot_mosaic(\n    \"\"\"AAA\n       BCC\"\"\"\n)\n\n# Gráfico asignado a C\niris_df.drop(columns=\"species\").plot(\n    kind=\"box\", ax=ax[\"C\"], title=\"Distribución de Datos por Variable\"\n)\n\n## Gráfico asignado a B\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n    ax=ax[\"B\"],\n)\n\n## Gráfico asignado a A\niris_df.groupby(\"species\").mean().plot(\n    kind=\"bar\",\n    edgecolor=\"black\",\n    ax=ax[\"A\"],\n    rot=0,\n    title=\"Valores promedio por Especie\",\n)\n\n## Permite Agregar un título general a todo el Gráfico\nplt.suptitle(\"Este será un título para todo el Gráfico\", fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#matplotlib",
    "href": "tics411/notebooks/02-EDA.html#matplotlib",
    "title": "EDA",
    "section": "",
    "text": "Los comandos mostrados anteriormente son una adaptación de Matplotlib a Pandas. La gracia que tienen es que son fáciles de aprender y funcionarán directamente en Pandas que será nuestra principal fuente de datos.\nEn el caso de trabajar con Numpy, estos comandos NO FUNCIONARÁN. Por lo tanto es necesario utilizar la API de Matplotlib. La traducción no es 100% directa, pero normalmente todos los parámetros de .plot() se cambiarán por comandos del tipo plt.---"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#ejemplo",
    "href": "tics411/notebooks/02-EDA.html#ejemplo",
    "title": "EDA",
    "section": "",
    "text": "plt.plot(x,y, c = \"red\") #Existe también plt.bar, plt.hist, plt.scatter, plt.boxplot.\nplt.title(\"Este va a ser un título\")\nplt.xlabel(\"Este será una etiqueta del Eje X\")\nAprender Matplotlib es bastante más complicado pero tiene funcionalidades muchísimo más avanzadas que Pandas. Para este curso, no será necesario especializarse en Matplotlib, pero sí más adelante utilizaremos algunos gráficos que no se pueden hacer tan fácilmente en Pandas (pero serán casos puntuales)."
  },
  {
    "objectID": "tics411/notebooks/09-ex-apriori.html",
    "href": "tics411/notebooks/09-ex-apriori.html",
    "title": "Algoritmo Apriori",
    "section": "",
    "text": "%%capture\n!pip install mlxtend"
  },
  {
    "objectID": "tics411/notebooks/09-ex-apriori.html#algoritmo-apriori",
    "href": "tics411/notebooks/09-ex-apriori.html#algoritmo-apriori",
    "title": "Algoritmo Apriori",
    "section": "Algoritmo Apriori",
    "text": "Algoritmo Apriori\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\nimport pandas as pd\n\n## Escribir acá las Transacciones\ntransactions = [\n    [\"Pan\", \"Mantequilla\", \"Leche\"],\n    [\"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Galletas\", \"Pañales\"],\n    [\"Leche\", \"Pañales\", \"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Pañales\"],\n]\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns=tre.columns_)\ndf_encoded\n\n\n\n\n\n\n\n\n\nCerveza\nGalletas\nLeche\nMantequilla\nPan\nPañales\n\n\n\n\n0\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n2\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n\ndef apriori_algorithm(\n    df,\n    min_supp=0.4,\n    min_conf=0.7,\n    variables=[\n        \"antecedents\",\n        \"consequents\",\n        \"support\",\n        \"confidence\",\n        \"lift\",\n    ],\n):\n\n    frequent_itemsets = apriori(\n        df, min_support=min_supp, use_colnames=True\n    )\n    rules = association_rules(\n        frequent_itemsets, metric=\"confidence\", min_threshold=min_conf\n    )\n\n    return frequent_itemsets, rules[variables]\n\n\nfrequent_itemsets, rules = apriori_algorithm(\n    df_encoded, min_supp=0.4, min_conf=0.7\n)\ndisplay(frequent_itemsets)\ndisplay(rules)\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.4\n(Cerveza)\n\n\n1\n0.4\n(Leche)\n\n\n2\n0.6\n(Mantequilla)\n\n\n3\n0.6\n(Pan)\n\n\n4\n0.6\n(Pañales)\n\n\n5\n0.4\n(Pañales, Cerveza)\n\n\n6\n0.4\n(Mantequilla, Leche)\n\n\n7\n0.4\n(Leche, Pan)\n\n\n8\n0.6\n(Mantequilla, Pan)\n\n\n9\n0.4\n(Mantequilla, Leche, Pan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\n\n\n\n\n0\n(Cerveza)\n(Pañales)\n0.4\n1.0\n1.666667\n\n\n1\n(Leche)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n2\n(Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n3\n(Mantequilla)\n(Pan)\n0.6\n1.0\n1.666667\n\n\n4\n(Pan)\n(Mantequilla)\n0.6\n1.0\n1.666667\n\n\n5\n(Mantequilla, Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n6\n(Leche, Pan)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n7\n(Leche)\n(Mantequilla, Pan)\n0.4\n1.0\n1.666667"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html",
    "href": "tics411/notebooks/pandas_basics.html",
    "title": "Seleccionar Filas, y columnas…",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df.shape, titanic_df.columns\n\n((891, 15),\n Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n        'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n        'alive', 'alone'],\n       dtype='object'))\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\ntitanic_df[\"survived_new\"] = titanic_df[\"survived\"].astype(\"float64\")\ntitanic_df\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\nsurvived_new\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n1.0\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n1.0\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n1.0\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n1.0\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n1.0\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n0.0\n\n\n\n\n891 rows × 16 columns"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html#seleccionar-filas-y-columnas",
    "href": "tics411/notebooks/pandas_basics.html#seleccionar-filas-y-columnas",
    "title": "Seleccionar Filas, y columnas…",
    "section": "Seleccionar Filas, y columnas…",
    "text": "Seleccionar Filas, y columnas…\n\n## Mostrar la diferencia entre una Serie y un DataFrame.\ntitanic_df.loc[10]\n\nsurvived                 1\npclass                   3\nsex                 female\nage                    4.0\nsibsp                    1\nparch                    1\nfare                  16.7\nembarked                 S\nclass                Third\nwho                  child\nadult_male           False\ndeck                     G\nembark_town    Southampton\nalive                  yes\nalone                False\nName: 10, dtype: object\n\n\n\ntitanic_df[\"embark_town\"].to_frame()\n\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\nSouthampton\n\n\n1\nCherbourg\n\n\n2\nSouthampton\n\n\n3\nSouthampton\n\n\n4\nSouthampton\n\n\n...\n...\n\n\n886\nSouthampton\n\n\n887\nSouthampton\n\n\n888\nSouthampton\n\n\n889\nCherbourg\n\n\n890\nQueenstown\n\n\n\n\n891 rows × 1 columns\n\n\n\n\n\n## Explicar que va una lista de elementos... no es un \"doble\" paréntesis.\ntitanic_df[[\"embark_town\", \"class\"]]\n\n\n\n\n\n\n\n\n\nembark_town\nclass\n\n\n\n\n0\nSouthampton\nThird\n\n\n1\nCherbourg\nFirst\n\n\n2\nSouthampton\nThird\n\n\n3\nSouthampton\nFirst\n\n\n4\nSouthampton\nThird\n\n\n...\n...\n...\n\n\n886\nSouthampton\nSecond\n\n\n887\nSouthampton\nFirst\n\n\n888\nSouthampton\nThird\n\n\n889\nCherbourg\nFirst\n\n\n890\nQueenstown\nThird\n\n\n\n\n891 rows × 2 columns\n\n\n\n\n\ntitanic_df.loc[[10, 15], [\"embark_town\", \"fare\", \"age\"]]\n\n\n\n\n\n\n\n\n\nembark_town\nfare\nage\n\n\n\n\n10\nSouthampton\n16.7\n4.0\n\n\n15\nSouthampton\n16.0\n55.0\n\n\n\n\n\n\n\n\n\ntitanic_df_shuffle = titanic_df.sample(frac=1)\ntitanic_df_shuffle\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n133\n1\n2\nfemale\n29.0\n1\n0\n26.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n748\n0\n1\nmale\n19.0\n1\n0\n53.1000\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n876\n0\n3\nmale\n20.0\n0\n0\n9.8458\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n226\n1\n2\nmale\n19.0\n0\n0\n10.5000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n342\n0\n2\nmale\n28.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283\n1\n3\nmale\n19.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n863\n0\n3\nfemale\nNaN\n8\n2\n69.5500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n124\n0\n1\nmale\n54.0\n0\n1\n77.2875\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n583\n0\n1\nmale\n36.0\n0\n0\n40.1250\nC\nFirst\nman\nTrue\nA\nCherbourg\nno\nTrue\n\n\n85\n1\n3\nfemale\n33.0\n3\n0\n15.8500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n\n\n891 rows × 15 columns\n\n\n\n\n\n# Esto es un error... si es que no se separa...\ntitanic_df_shuffle.iloc[3][[\"who\", \"adult_male\"]]\n\nwho            man\nadult_male    True\nName: 226, dtype: object\n\n\n\n## Algunos métodos importante...\ntitanic_df.describe(percentiles=[0.05, 0.25, 0.75, 0.95])\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n5%\n0.000000\n1.000000\n4.000000\n0.000000\n0.000000\n7.225000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\n95%\n1.000000\n3.000000\n56.000000\n3.000000\n2.000000\n112.079150\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n\ntitanic_df.mean(numeric_only=True)\n\nsurvived       0.383838\npclass         2.308642\nage           29.699118\nsibsp          0.523008\nparch          0.381594\nfare          32.204208\nadult_male     0.602694\nalone          0.602694\ndtype: float64\n\n\n\ntitanic_df.median(numeric_only=True)\n\nsurvived       0.0000\npclass         3.0000\nage           28.0000\nsibsp          0.0000\nparch          0.0000\nfare          14.4542\nadult_male     1.0000\nalone          1.0000\ndtype: float64\n\n\n\ntitanic_df.mode()\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n24.0\n0\n0\n8.05\nS\nThird\nman\nTrue\nC\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n\nMostrar que este tipo de métodos también se pueden utilizar en Series."
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html#agrupar",
    "href": "tics411/notebooks/pandas_basics.html#agrupar",
    "title": "Seleccionar Filas, y columnas…",
    "section": "Agrupar",
    "text": "Agrupar\n\ndf = pd.DataFrame(\n    dict(a=[1, 1, 1, 1, 2, 2, 2, 2], b=[1, 2, 3, 4, 5, 6, 7, 8])\n)\n\ndf\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1\n\n\n1\n1\n2\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n2\n5\n\n\n5\n2\n6\n\n\n6\n2\n7\n\n\n7\n2\n8\n\n\n\n\n\n\n\n\n\nfor i in [df.shape, df.columns, df.index, df.dtypes]:\n    print(i)\n\n(8, 2)\nIndex(['a', 'b'], dtype='object')\nRangeIndex(start=0, stop=8, step=1)\na    int64\nb    int64\ndtype: object\n\n\n\ngroups = df.groupby(\"a\")\nfor id, g in groups:\n    print(f\"Este es el grupo: {id}\")\n    display(g)\n\nEste es el grupo: 1\nEste es el grupo: 2\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1.0\n\n\n1\n1\n2.0\n\n\n2\n1\n3.0\n\n\n3\n1\n4.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n4\n2\n5.0\n\n\n5\n2\n6.0\n\n\n6\n2\n7.0\n\n\n7\n2\n8.0\n\n\n\n\n\n\n\n\n\ndf.groupby(\"a\")[\"b\"].mean()\n\na\n1    2.5\n2    6.5\nName: b, dtype: float64\n\n\n\ntitanic_df.groupby(\"sex\")[\"fare\"].mean()\n\nsex\nfemale    44.479818\nmale      25.523893\nName: fare, dtype: float64\n\n\n\ntitanic_df.groupby([\"sex\", \"pclass\"])[[\"age\", \"fare\"]].median()\n\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\nsex\npclass\n\n\n\n\n\n\nfemale\n1\n35.0\n82.66455\n\n\n2\n28.0\n22.00000\n\n\n3\n21.5\n12.47500\n\n\nmale\n1\n40.0\n41.26250\n\n\n2\n30.0\n13.00000\n\n\n3\n25.0\n7.92500"
  },
  {
    "objectID": "tics411/clase-1.html#avisos-1",
    "href": "tics411/clase-1.html#avisos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Avisos",
    "text": "Avisos\n\n\n\n\n\n\nAyudantía\n\n\nAvisos\nTenemos (posible) ayudante, pero tenemos un problema de horario.\n\nHorario Actual: Viernes 20:00 a 21:10 hrs.\nHorario Propuesto: Lunes 11:45 a 12:55 hrs.\n\n\n\n\n\n\n\n\n\n\n\n\nTarea 1\n\n\n\nEntrega el 7 de Abril: Parejas inscribirse en Webcursos.\nPlazo para inscribir parejas: Este Domingo.\n\n\n\n\n\n\n\n\n\n\n\n\nFechas de Prueba\n\n\n\nPrueba 1: Martes 30 de Abril 18:30 a 21:00\nPrueba 2: Martes 11 de Julio 18:30 a 21:00"
  },
  {
    "objectID": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, registros, instancias. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos más amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo.\nDistintos tipos, por lo que normalmente requiere de algún tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-numéricos",
    "href": "tics411/clase-1.html#data-types-numéricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Numéricos",
    "text": "Data Types: Numéricos\n\nNuméricos\n\n\nValores a los que se les puede aplicar alguna operación matemática.\n\n\n\n\n\n\n\n\n\n\nDiscretas: Número finito o contable de valores. Integers (Enteros). Ej: Número de Hijos, Cantidad de Productos, Edad.\nContinuas: Existen infinitos puntos entre dos puntos. Floats (punto flotando o decimales). Ej. Temperatura, Peso."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-categóricos",
    "href": "tics411/clase-1.html#data-types-categóricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Categóricos",
    "text": "Data Types: Categóricos\n\nCategóricos\n\n\nDatos que representan una categoría.\n\n\n\n\n\n\n\n\n\n\nNominales: Sólo nombres que no representan ningún orden. Ej: Nacionalidad, género, ocupación.\nOrdinales: Que tienen un orden o jerarquía inherente. Ej: Nivel de Escolaridad, tamaño.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo todas las operaciones matemáticas son aplicables. Ej: Media, Mediana, Sumas, Restas, etc."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-otros",
    "href": "tics411/clase-1.html#data-types-otros",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Otros",
    "text": "Data Types: Otros\n\nStrings\n\n\nDatos de texto, los cuales podrían eventualmente ser tratados y representar algo. Ej: Rescatar comunas de una dirección, rescatar sexo desde el nombre, etc.\n\n\nFechas\n\n\nDatos tipo fecha, los cuales podrían eventualmente ser tratados y representar variables de algún tipo. Ej: Rescatar Años, meses, días, semanas, trimestres (quarters), etc.\n\n\nDatos Geográficos\n\n\nDatos que representan la ubicación geográfica de un elemento. Ej: Latitud, Longitud, Coordenadas.\n\n\n\n\n\n\n\n\n\n\nSin importar el tipo de dato el mayor problema es su calidad."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "href": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Ruido",
    "text": "Calidad de los Datos: Ruido\n\nRuido\n\nCorresponde al error y extrema variabilidad en la medición en los datos. Este error puede ser aleatorio o sistemático.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe le llama Señal a la tendencia principal y representa la información significativa y valiosa de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "href": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Outliers",
    "text": "Calidad de los Datos: Outliers\n\nOutliers\n\nSon datos considerablemente diferentes a la mayoría del dataset. Dependiendo del caso pueden indicar casos \"interesantes\" o errores de medición.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que dependiendo del caso puede ser una buena idea deshacerse de ellos. ¿En qué casos podría no ser necesario eliminarlos?"
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "href": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Valores Faltantes",
    "text": "Calidad de los Datos: Valores Faltantes\n\nMissing Values\n\n\nSon valores que por alguna razón no están presentes.\n\n\n\n\n\nMissing at Random (MAR): Son valores que no están presentes por causas que no se pueden controlar. Ej: No se registró, no se preguntó, fallas en el sistema de recolección de datos, etc.\nInformative Missing: Es un valor no aplicable. Ej: Sueldo en niños, Precio de la entrada de un concierto si es que NO compró entrada."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "href": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Datos Duplicados",
    "text": "Calidad de los Datos: Datos Duplicados\n\nDuplicates\n\nSe refiere a registros que pueden estar total o parcialmente duplicados.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsto genera problemas en la confiabilidad de los datos. ¿Cuál es el registro correcto?\nEj: Caso particular de una Jooycar (una startup de seguros)."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "href": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Dominio del Problema",
    "text": "Calidad de los Datos: Dominio del Problema\n\n\n\n\n\n\n\n\n\n\n\n\n\nPor lejos el problema de calidad más difícil de encontrar.\nSe requiere experiencia y conocimiento profundo del negocio para detectarlo.\n\nEj: Caso de Super Avances en Cencosud."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-1",
    "href": "tics411/clase-1.html#feature-engineering-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nFeature Engineering\n\n\nTambién conocida como Ingeniería de Atributos, es el arte de trabajar las features existentes para limpiar o corregir variables existentes o crear nuevas variables.\n\n\nPreprocesamiento\n\n\nSe refiere al proceso de preparación de los datos para su ingreso a un modelo. En una primera parte puede incluir limpieza de datos corruptos, redundantes y/o irrelevantes. Por otra parte, también hace referencia a la transformación de datos para que puedan ser consumidos por un algoritmo."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-2",
    "href": "tics411/clase-1.html#feature-engineering-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nNo existe un procedimiento estándar.\nRevisar los datos y ver potenciales errores que puedan afectar el funcionamiento de un modelo."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "href": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Valores Faltantes",
    "text": "Preprocesamiento: Valores Faltantes\n\nImputación: Se refiere al proceso de rellenar datos faltantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependiendo del nivel de valores faltantes, es necesario evaluar la eliminación de registros o atributos completos de ser necesario."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Manejo de Outliers",
    "text": "Preprocesamiento: Manejo de Outliers\n\nCapping\n\nSe refiere al proceso de acotar un atributo eliminando los valores extremos o atípicos (outliers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAl igual que en el caso anterior, es necesario evaluar la eliminación de registros si es que representan valores atípicos."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categóricas",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categóricas",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Manejo de Variables Categóricas",
    "text": "Preprocesamiento: Manejo de Variables Categóricas\n\nLa mayoría de los modelos no tienen la capacidad de poder lidiar con variables categóricas por lo que deben ser transformadas en una representación numérica antes de ingresar a un modelo.\n\n\n\n\n\n\nOne Hot Encoder\n\n\n\n\n\n\nOrdinal Encoder\n\n\n\n\n\n\n\n\n\n\n\n\nOne Hot Encoder suele dar mejores resultados en modelos lineales modelos que dependan de distancias.\nOrdinal Encoder suele dar mejores resultados en modelos de árbol.\n\n\n\n\n\n\n\n¿Son necesarias todas las columnas en un One Hot Encoder?"
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\nEl escalamiento se refiere al proceso de llevar distintas variables a una misma escala.\n\n\n\n\n\n\n\n\n\nEvitar que la escala de una “sobre-importancia” a una cierta variable.\nPermitir una mejor convergencia de los algoritmos.\n\n\n\nStandardScaler (Normalización)\n\\[x_j=\\frac{x_j-\\mu_x}{\\sigma_x}\\]\n\n\n\n\n\n\n\nEste proceso fuerza (en la medida de lo posible) a tener media 0 y std 1.\nNotar que \\(\\sigma_x\\) hace referencia a la varianza poblacional.\n\n\n\n\nMinMax Scaler\n\\[x_j=\\frac{x_j-min(x_j)}{max(x_j)-min(x_j)}\\]\n\n\n\n\n\n\nEste proceso fuerza a los datos a distribuirse entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia: 0.75\nStd: 3.1875\nMin: -2\nMax: 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentering (Centrado): Se le llama a la diferencia entre la variable y su media.\nScaling (Escalado): Se le llama al cuociente entre la variable y su Desviación Estándar.\nStandardScaler (Normalización): Es Centrado y Escalado."
  },
  {
    "objectID": "tics411/clase-1.html#creación-de-variables",
    "href": "tics411/clase-1.html#creación-de-variables",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de Variables",
    "text": "Creación de Variables\n\nCombinación\n\n\nCombinar 2 o más variables. Ej: Calcular el área de un sitio a partir del ancho y largo.\n\n\nTransformación\n\n\nAplicar una operación a una variable. Ej: El logaritmo de las ganancias.\n\n\n\n\n\n\nDiscretización (Binning)\n\n\nGenerar categorías a partir de una variable continua."
  },
  {
    "objectID": "tics411/clase-1.html#creación-de-variables-1",
    "href": "tics411/clase-1.html#creación-de-variables-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de Variables",
    "text": "Creación de Variables\n\nRatios\n\nEs una medida que expresa la relación entre dos cantidades. Ej: Puntos por partido, cantidad de transacciones por mes, etc.\n\nAgregación\n\nAgregar o agrupar información resumida de ciertas variables. Ej: Promedio de tiempo en aprobar un tipo de crédito."
  },
  {
    "objectID": "tics411/clase-1.html#selección-de-variables",
    "href": "tics411/clase-1.html#selección-de-variables",
    "title": "TICS-411 Minería de Datos",
    "section": "Selección de Variables",
    "text": "Selección de Variables\n\nSe refiere al proceso de eliminar variables que pueden ser irrelevantes o poco significativas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcesos Manuales.\nProcesos Automáticos:\n\nPCA (Principal Component Analysis).\nRecursive Feature Elimination.\nRecursive Feature Addition.\nEliminación mediante alguna medida.\n\n\n\n\n\n\n\n\n\n\n\n\nObjetivo\n\n\n\nPuede ser una técnica apropiada para combatir la Maldición de la Dimensionalidad (Curse of Dimensionality)."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-1",
    "href": "tics411/clase-1.html#medidas-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas",
    "text": "Medidas\n\nSon métricas que permiten cuantificar la relación existente entre dos o más objetos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad",
    "href": "tics411/clase-1.html#medidas-similaridad",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad",
    "text": "Medidas: Similaridad"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-nominal",
    "href": "tics411/clase-1.html#medidas-similaridad-nominal",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Nominal",
    "text": "Medidas: Similaridad Nominal\n\n\n\nDisimilaridad: \\[D =\n\\begin{cases}\n0,  & \\text{if $p=q$} \\\\[2ex]\n1, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\nSimilaridad:\n\\[S =\n\\begin{cases}\n1,  & \\text{if $p=q$} \\\\[2ex]\n0, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 0\\] \\[D(p,q) = 1\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "href": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Ordinal",
    "text": "Medidas: Similaridad Ordinal\n\n\n\nDisimilaridad: \\[D = \\frac{|p-q|}{n}\\]\n\n\n\nSimilaridad:\n\\[S = 1 - \\frac{|p-q|}{n}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 1 - \\frac{5 - 4}{5} = 0.8\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "href": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Intervalo o Ratio",
    "text": "Medidas: Similaridad Intervalo o Ratio\n\n\n\nDisimilaridad: \\[D = |p-q|\\]\n\n\n\nSimilaridad:\n\\[S = -D\\] \\[S = \\frac{1}{1+D}\\]\n\n\n\n\nSea \\(p=35 °C\\) y \\(q = 40 °C\\). Luego:\n\\[ S(p,q) = -5\\] \\[S(p,q) = \\frac{1}{1 + 5} = 0.17\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos\n\nSea p y q vectores de dimensión \\(m\\) con sólo atributos categóricos. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\\[Sim(p,q) = \\sum_{i=1}^m S(p_i,q_i)\\]\n\n\n\n\nOverlap: \\[S(p_{a_i}, q_{a_i}) =\n\\begin{cases}\n1,  & \\text{if $p_{a_i} = q_{a_i}$} \\\\[2ex]\n0, & \\text{if $p_i\\neq q_i$}\n\\end{cases}\n\\]\n\n\n\nFrecuencia de Ocurrencia Inversa \\[S(p_i, q_i) = \\frac{1}{p_k(p_i)^2}\\]\n\n\n\nMedida de Goodall\n\n\\[S(p_i, q_i) = 1 - p_k(p_i)^2\\]\n\n\n\n\n\n\n\n\n\n\\(p_k()\\) se refiere a la probabilidad de ocurrencia del atributo k.\nTodas estas medidas son 0 si \\(p_i \\neq q_i\\)"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos\n\n\n\n\n\nEjercicio Propuesto: ¿Cuánto vale la similaridad entre los siguientes registros?\n\n1-4\n2-5\n7-8"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\nSea p y q vectores de dimensión \\(m\\) con sólo atributos binarios. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}}\\]\n\nSimple Matching Coefficient = Número de Coincidencias / Total de Atributos\n\n\n\\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}}\\]\n\nJaccard Coefficient = Número de Coincidencias 11 / Número de Atributos distintos de Ceros."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\np_i\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nq_i\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}} = \\] \\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}} = \\]\n\n\n\\[\\frac{7 + 0}{7 + 2 + 1 + 0} = 0.7\\]\n\n\n\\[\\frac{0}{2 + 1 + 0} = 0\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\nSean \\(d_1\\) y \\(d_2\\) dos vectores. La distancia coseno se calcula como:\n\n\\[cos(d_1, d_2) = \\frac{d_1 \\cdot d_2}{||d_1||||d_2||}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\nEjercicio Propuesto: ¿Cuánto vale \\(cos(d_1,d_2)\\) y \\(cos(d_1,d_3)\\)?"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-1",
    "href": "tics411/clase-1.html#distancias-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias",
    "text": "Distancias\n\nUna métrica o función de distancia es una función que define una distancia para cada par de elementos de un conjunto. Sean dos puntos x e y, una métrica o función de distancia debe satisfacer las siguientes condiciones:\n\n\nNo Negatividad:\n\n\\(d(x,y) = \\ge 0\\)\n\nIdentidad:\n\n\\(d(x,y) = 0 \\Leftrightarrow x = y\\)\n\nSimetría:\n\n\\(d(x,y) = d(y,x)\\)\n\nDesigualdad Triangular:\n\n\\(d(x,z) \\le d(x,y) + d(y,z)\\)"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Minkowski",
    "text": "Distancias: Distancia Minkowski\n\\[d(p,q) = \\left(\\sum_{k=1}^m |p_k - q_k|^r\\right)^{1/r}\\]\n\n\n\n\n\n\n\n\n\n\\(r=1 \\rightarrow\\) Distancia Manhattan (L1).\n\\(r=2 \\rightarrow\\) Distancia Euclideana (L2).\n\\(r=\\infty \\rightarrow\\) Distancia Chebyshev (L\\(\\infty\\)). \\[D_{ch}(p,q) = \\underset{k}{max} |p_k - q_k|\\]\n\n\n\n\n\n\n\nResolvamos en Colab\n\n\n\n\n\n\n\n\n\n\n\n\nSe denomina Matriz de Distancias a la Matriz que contiene la distancia \\(d(p_i,p_j)\\) en la coordenada \\(i,j\\)."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Minkowski (Resultados)",
    "text": "Distancias: Distancia Minkowski (Resultados)"
  },
  {
    "objectID": "tics411/clase-1.html#ayudantías",
    "href": "tics411/clase-1.html#ayudantías",
    "title": "TICS-411 Minería de Datos",
    "section": "Ayudantías",
    "text": "Ayudantías\nAyudante: Sofía Alvarez\nemail: sofalvarez@alumnos.uai.cl\n\n\n\n\n\n\n\nLas ayudantías serán en la manera que sean necesarias.\nEstarán enfocadas principalmente en aplicaciones, código y dudas sobre Tarea."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Mahalanobis",
    "text": "Distancias: Distancia Mahalanobis\n\\[d(p,q) = \\sqrt{(p-q)^T \\Sigma^{-1}(p-q)}\\]\ndonde \\(\\Sigma\\) es la Matriz de Covarianza de los datos de entrada.\n\\[cov(x,y) = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nPara 2 variables p y q:\n\n\\[\\Sigma = \\begin{bmatrix}\ncov(p,p) & cov(p,q) \\\\\ncov(q,p) & cov(q,q)\n\\end{bmatrix}\n\\]\nEjercicio: Supongamos las siguientes escalas de notas. Calcular la distancia entre la nota (1.0 y 7.0)\n\ntest #1: 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0\ntest #2: 1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Mahalanobis (Resultados)",
    "text": "Distancias: Distancia Mahalanobis (Resultados)\n\n\n\n\n\n\ntest #1: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{3.79}(7-1)} = 3.08\\)\ntest #2: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{1.59}(7-1)} = 4.76\\)\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que la covarianza existente entre los datos influye en la distancia."
  },
  {
    "objectID": "tics411/clase-1.html#correlación-1",
    "href": "tics411/clase-1.html#correlación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación",
    "text": "Correlación\n\nLa correlación mide la relación lineal entre 2 atributos.\n\n\n\n\nCorrelación Poblacional\n\n\\[\\rho(X,Y) = corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}\\]\n\n\n\n\nCorrelación Muestral o Pearson\n\n\\[r(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{S_xS_y}\\]"
  },
  {
    "objectID": "tics411/clase-1.html#correlación-no-es-causalidad",
    "href": "tics411/clase-1.html#correlación-no-es-causalidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación no es Causalidad",
    "text": "Correlación no es Causalidad\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante recalcar que Causalidad no es igual a Correlación. Ver video.\nLa Correlación no se ve afectada por la escala de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-2",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\n\\[d_1 \\cdot d_2 = 5\\] \\[d_1 \\cdot d_3 = 84\\]\n\\[||d_1|| = \\sqrt{42} = 6.481\\] \\[||d_2|| = \\sqrt{6} = 2.449\\] \\[||d_3|| = \\sqrt{168} = 12.962\\]\n\\[cos(d_1, d_2) = 0.3150\\] \\[cos(d_1, d_3) = 0.9999\\]\n\n\n\n\n\n\n\n\nOne Hot Encoder\nOrdinal Encoder"
  },
  {
    "objectID": "tics411/clase-9.html#intuición",
    "href": "tics411/clase-9.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\nSi tengo que estudiar para una prueba donde tengo que calcular el Coeficiente de Silueta.\n\n\nQué pasa si sólo les entrego una pregunta para estudiar y no tiene respuesta.\n¿Qué pasa si ahora les doy la respuesta?\n¿Qué pasa si te doy más ejercicios?\n¿Qué pasa luego de que haces muchos ejercicios?\n\n\n\n\n\n\n\n\n\nVoy aprendiendo mejor la tarea de calcular el coeficiente de Silueta. Lo mismo pasa con los modelos.\n\n\n\n\n\n\n\n\n\n\n\nPero no puedo medir qué tan bien aprendiste en los ejercicios que yo ya entregué para practicar. Tengo que hacer una prueba que tú no hayas visto, para ver si realmente aprendiste."
  },
  {
    "objectID": "tics411/clase-9.html#uso-de-un-modelo",
    "href": "tics411/clase-9.html#uso-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Uso de un Modelo",
    "text": "Uso de un Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo saber que el modelo está funcionando como esperamos?"
  },
  {
    "objectID": "tics411/clase-9.html#métricas",
    "href": "tics411/clase-9.html#métricas",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas",
    "text": "Métricas\nEl Rendimieto de un Modelo de Clasificación permite evaluar el error asociado al proceso de predicción.\n\n\n\n\nClase Positiva\n\nCorresponde a la clase/evento de interés. Ej: Tiene cancer, va a pagar su deuda, es un gato. Normalmente se denota como la Clase 1.\n\nClase Negativa\n\nCorresponde a la clase/evento contrario al de interés. Ej: No tiene cancer, no va a pagar su deuda, no es un gato. Normalmente se denota como la Clase 0."
  },
  {
    "objectID": "tics411/clase-9.html#métricas-matriz-de-confusión",
    "href": "tics411/clase-9.html#métricas-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas: Matriz de Confusión",
    "text": "Métricas: Matriz de Confusión\n\nLa Matriz de Confusión ordena los valores correctamente predichos y también los distintos errores que el modelo puede cometer.\n\n\n\n\n\n\n\n\n\n\nTP (Verdaderos Positivos)\n\nCorresponde a valores reales de la clase 1 que fueron correctamente predichos como clase 1.\n\nTN (Verdaderos Negativos)\n\nCorresponde a valores reales de la clase 0 que fueron correctamente predichos como clase 0.\n\nFP (Falsos Positivos)\n\nCorresponde a valores reales de la clase 0 que fueron incorrectamente predichos como clase 1.\n\nFN (Falsos Negativos)\n\nCorresponde a valores reales de la clase 1 que fueron incorrectamente predichos como clase 0."
  },
  {
    "objectID": "tics411/clase-9.html#métricas-a-partir-de-la-matriz-de-confusión",
    "href": "tics411/clase-9.html#métricas-a-partir-de-la-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas: A partir de la Matriz de Confusión",
    "text": "Métricas: A partir de la Matriz de Confusión\n\n\n\n\n\nAccuracy\n\n\n\\[\\frac{TP + TN}{TP + TN + FP + FN}\\]\n\n\n\n\n\n\nPrecision\n\n\n\\[\\frac{TP}{TP + FP}\\]\n\n\n\n\n\n\n\nRecall\n\n\n\\[\\frac{TP}{TP + FN}\\]\n\n\n\n\n\n\nF1-Score\n\n\n\\[\\frac{2\\cdot Precision \\cdot Recall}{Precision + Recall} = \\frac{2 \\cdot TP}{2\\cdot TP + FP + FN}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy es probablemente la métrica más sencilla y más utilizada.\nPrecision y Recall ponderarán distintos errores (FP y FN respectivamente) con mayor severidad. Ambas métricas son Antagonistas.\nF1-Score corresponde a la media armónica del Precision y Recall, y tiende a ponderar los errores de manera más balanceada.\n\n\n\n\n¿Cuándo utilizar cada tipo de error? (Pendiente de ejemplos)."
  },
  {
    "objectID": "tics411/clase-9.html#curva-roc",
    "href": "tics411/clase-9.html#curva-roc",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva ROC",
    "text": "Curva ROC\nLa curva ROC fue desarrollada en 1950 para analizar señales ruidosas. La curva ROC permite al operador contrapesar la tasa de verdaderos positivos (Eje \\(y\\)) versus los falsos positivos (Eje x).\n\nEl área bajo la curva representa la calidad del modelo. Una manera de interpretarla es como la probabilidad de que una predicción de la clase positiva tenga mayor probabilidad que una de clase negativa. En otras palabras, que las probabilidades se encuentren correctamente ordenadas. Por lo tanto varía entre 0.5 y 1.\n\n\n\n\n\n\n\n\n\nROC \\(\\sim\\) 0.5\n\n\n\n\n\nROC \\(\\sim\\) 1"
  },
  {
    "objectID": "tics411/clase-9.html#evaluación",
    "href": "tics411/clase-9.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\nLa evaluación de modelos supervisados es fundamental. De no hacerlo de forma correcta podemos quedarnos con una idea muy equivocada del rendimiento del modelo.\n\n\nEsto es importante ya que un modelo con un rendimiento incorrecto puede entregar predicciones completamente inútiles.\n\n\nCross Validation (Validación Cruzada)\n\n\nSe debe evaluar el rendimiento de un modelo en un dataset diferente al que fue entrenado. Esta es la única manera en la que se puede medir el poder de generalización.\n\n\nGeneralización\n\n\nCorresponde a la habilidad de un modelo de adaptarse apropiadamente a datos no vistos previamente.\n\n\n\n\n\n\n\n\n\nPara esto se asume que todos los datos son i.i.d (idependent and identically distributed). De no lograr esto, lograr buenos rendimientos es más difícil."
  },
  {
    "objectID": "tics411/clase-9.html#validación-cruzada-holdout",
    "href": "tics411/clase-9.html#validación-cruzada-holdout",
    "title": "TICS-411 Minería de Datos",
    "section": "Validación Cruzada: Holdout",
    "text": "Validación Cruzada: Holdout\n\nTambién es conocido como Train Test Split o simplemente Split. Corresponde a la separacion de nuestra data cuando con el proposito de aislar observaciones que el modelo no vea para una correcta evaluación.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set es la porción de los datos que se utilizará exclusivamente para entrenar los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl test set es la porción de los datos que se utilizará exclusivamente para validar los datos.\nEl test set simula los datos que eventualmente entrarán el modelo para obtener una predicción.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utilizan splits del tipo 70/30, 80/20 o 90/10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el problema con este tipo de validación?"
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout",
    "href": "tics411/clase-9.html#variante-holdout",
    "title": "TICS-411 Minería de Datos",
    "section": "Variante Holdout",
    "text": "Variante Holdout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe agrega un validation set el cuál se utilizará para escoger los hiperparámetros que muestren un mejor poder de generalización.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set y el test set cumplen la misma función que tenían antes."
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout-procedimiento",
    "href": "tics411/clase-9.html#variante-holdout-procedimiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Variante Holdout: Procedimiento",
    "text": "Variante Holdout: Procedimiento\n\n\n\n\n\n\n\n\n\nProcedimiento\n\nRepetir para cada Modelo a probar.\n\n\n\n\n\n\n\n\nVamos a entender un modelo como la combinación de un Algoritmo de Aprendizaje + Hiperparámetros + Preprocesamiento.\n\n\n\n\n\n\nSe entrena cada Modelo en el train set. Se mide una métrica de Evaluación apropiada utilizando el Validation Set. La llamaremos métrica de Validación.\nSe escoge el mejor Modelo como el que tenga la mejor métrica de Validación.\nSe reentrena el modelo escogido pero ahora en un “nuevo set” compuesto por el Train set + el Validation set.\nSe reporta el rendimiento final del mejor modelo (al momento del diseño) utilizando métricas medidas en el Test Set."
  },
  {
    "objectID": "tics411/clase-9.html#k-fold-cv",
    "href": "tics411/clase-9.html#k-fold-cv",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Fold CV",
    "text": "K-Fold CV\n\n\n\n\n\n\n\nEl proceso de Holdout podría llevar a un proceso de overfitting del Test Set si el modelo no es lo suficientemente robusto.\nDefiniremos la robustez más adelante.\n\n\n\n\n\n\n\n\n\n\n\n\nEl K-Fold CV se aplica sólo al Train Set y la métrica final que se reporta utilizando el Test Set.\n\n\n\n\n\n\n\n\n\n\nFold\n\nEntenderemos Folds como divisiones que haremos a nuestro dataset. (En el ejemplo se divide el dataset en 5 Folds).\n\nSplit\n\nEntenderemos Splits, como iteraciones. En cada iteración utilizaremos un Fold como Validation Set y todos los Folds restantes como Train Set.\n\n\n\n\n\n\n\n\n\nLa métrica final se calculará como el promedio de las Métricas de Validación para cada Split.\nA veces la variabilidad (medido a través de la Desviación Estándar) también es usado como criterio para elegir el mejor modelo."
  },
  {
    "objectID": "tics411/clase-9.html#bootstrap",
    "href": "tics411/clase-9.html#bootstrap",
    "title": "TICS-411 Minería de Datos",
    "section": "Bootstrap",
    "text": "Bootstrap\nConsiste en generar subgrupos aleatorios con repetición. Normalmente requiere específicar el tamaño de la muestra de entrenamiento. Y la cantidad de repeticiones que del proceso. Los sets de validación (en morado) acá se denominan out-of-bag samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa métrica final a reportar se mide como el promedio de los out-of-bag samples."
  },
  {
    "objectID": "tics411/clase-9.html#variantes-y-consejos",
    "href": "tics411/clase-9.html#variantes-y-consejos",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes y Consejos",
    "text": "Variantes y Consejos\n\nStratified K-Fold\n\nEs la variante más utilizada de K-Fold el cual genera los folds considerando que se mantenga la proporción de etiquetas en cada Fold.\n\nLeave One Out\n\nSería una variante con \\(K=n\\). Por lo tanto, el Validation Set tiene sólo una observación.\n\n\n\n\n\n\n\n\n\n¿Cuando usar cada uno?\n\n\n\nSi se tiene una cantidad de datos suficiente (normalmente tamaños muy grandes se prefiere) el Holdout.\n\nEntre más registros, menos % de Validation Set se deja.\n\nSi se requiere robustez, o hay Test sets que son muy variables se prefiere K-Fold.\n\nSi es que hay desbalance de clases, se prefiere la versión Stratified.\n\n\nSi se tienen muy pocos datos, entonces utilizar Leave-One-Out.\nBootstrap también es utilizado cuando se tengan pocos datos. Aunque suele ser un approach más estadístico."
  },
  {
    "objectID": "tics411/clase-9.html#data-leakage",
    "href": "tics411/clase-9.html#data-leakage",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Leakage",
    "text": "Data Leakage\n\nFuga de Datos\n\nSe refiere al proceso donde el modelo por alguna razón conoce información que no debería conocer. Puede ser información del Test Set o variables que revelan información primordial sobre la etiqueta.\n\n\n\n\n\n\n\n\nSe recomienda siempre que sea posible utilizar Pipelines para poder evitar el Data Leakage."
  },
  {
    "objectID": "tics411/clase-3.html#definiciones",
    "href": "tics411/clase-3.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nAprendizaje No supervisado\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender.\n\n\n\n\n\n\n\n\n\nEn nuestro caso nos enfocaremos en un caso particular de Modelación Descriptiva llamada Clustering.\n\n\n\n\nClustering\n\n\nConsiste en agrupar los datos en un menor número de entidades o grupos. A estos grupos se les conoce como clusters y pueden ser generados de manera global, o modelando las principales características de los datos."
  },
  {
    "objectID": "tics411/clase-3.html#intuición",
    "href": "tics411/clase-3.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n¿Cuántos clusters se pueden apreciar?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-introducción",
    "href": "tics411/clase-3.html#clustering-introducción",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Introducción",
    "text": "Clustering: Introducción\n\n\n\n\n\n\nClustering: Consiste en buscar grupos de objetos tales que la similaridad intra-grupo sea alta, mientras que la similaridad inter-grupos sea baja. Normalmente la distancia es usada para determinar qué tan similares son estos grupos."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-evaluación",
    "href": "tics411/clase-3.html#clustering-evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Evaluación",
    "text": "Clustering: Evaluación\n\n\n\n\n\n\n\nEvaluar el nivel del éxito o logro del Clustering es complicado. ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-tipos",
    "href": "tics411/clase-3.html#clustering-tipos",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Tipos",
    "text": "Clustering: Tipos"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-partición",
    "href": "tics411/clase-3.html#clustering-partición",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Partición",
    "text": "Clustering: Partición\n\nLos datos son separados en K clusters, donde cada punto pertenece exclusivamente a un único cluster."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-densidad",
    "href": "tics411/clase-3.html#clustering-densidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-jerarquía",
    "href": "tics411/clase-3.html#clustering-jerarquía",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Jerarquía",
    "text": "Clustering: Jerarquía\n\nLos algoritmos basados en jerarquía pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-probabilístico",
    "href": "tics411/clase-3.html#clustering-probabilístico",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Probabilístico",
    "text": "Clustering: Probabilístico\nSe ajusta cada punto a una distribución de probabilidades que indica cuál es la probabilidad de pertenencia a dicho cluster."
  },
  {
    "objectID": "tics411/clase-3.html#partición",
    "href": "tics411/clase-3.html#partición",
    "title": "TICS-411 Minería de Datos",
    "section": "Partición",
    "text": "Partición\n\nLos datos son separados en K Clusters, donde cada punto pertenece exclusivamente a un único cluster. A K se le considera como un hiperparámetro.\n\n\n\n\n\n\n\n\nCluster Compactos: Minimizar la distancia intra-cluster (within cluster).\nClusters bien separados: Maximizar la distancia inter-cluster (between cluster).\n\n\n\n\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\nEl puntaje/score mide la calidad del clustering \\(C\\) para el Dataset \\(D\\)."
  },
  {
    "objectID": "tics411/clase-3.html#score",
    "href": "tics411/clase-3.html#score",
    "title": "TICS-411 Minería de Datos",
    "section": "Score",
    "text": "Score\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\n\n\n\n\nDistancia Between-Cluster: \\[bc(C) = \\sum_{1 \\le j \\le k \\le K} d(r_j, r_k)\\]\n\ndonde \\(r_k\\) representa el centro del cluster \\(k\\): \\[r_k = \\frac{1}{n_k} \\sum_{x_i \\in C_k} x_i\\]\n\n\nDistancia Within-Cluster (Inercia): \\[wc(C) = \\sum_{k=1}^K \\sum_{x_i \\in C_k} d(x_i, r_k)\\]\n\n\n\n\n\n\n\n\n\n\n\nDistancia entre los centros de cada cluster.\n\n\n\n\n\n\n\n\n\n\nDistancia entre todos los puntos del cluster y su respectivo centro."
  },
  {
    "objectID": "tics411/clase-3.html#k-means",
    "href": "tics411/clase-3.html#k-means",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means",
    "text": "K-Means\n\nK-Means\n\n\nDado un número de clusters \\(K\\) (determinado por el usuario), cada cluster es asociado a un centro (centroide). Luego, cada punto es asociado al cluster con el centroide más cercano.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utiliza la Distancia Euclideana como medida de similaridad.\n\n\nSe seleccionan \\(K\\) puntos como centroides iniciales.\nRepite:\n\nForma K clusters asignando todos los puntos al centroide más cercano.\nRecalcula el centroide para cada clase como la media de todos los puntos de dicho cluster.\n\n\n\nSe repite este procedimiento por un número finito de iteraciones o hasta que los centroides no cambien."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo",
    "href": "tics411/clase-3.html#k-means-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\nResolvamos el siguiente ejemplo.\nSupongamos que tenemos tipos de manzana, y cada una de ellas tiene 2 atributos (features). Agrupemos estos objetos en 2 grupos de manzanas basados en sus características."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-1",
    "href": "tics411/clase-3.html#k-means-ejemplo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n1era Iteración\n\n\n\n\nSupongamos los siguientes centroides iniciales: \\[C_1 = (1,1)\\] \\[C_2 = (2,1)\\]\n\n\n\n\n\n\n\n\n\nMatriz de Distancias al Centroide: (coordenada i,j representa distancia del punto j al centroide i)\n\n\n\n\n\n\\[D^1 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n1 & 0 & 2.83 & 4.24\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son: \\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-2",
    "href": "tics411/clase-3.html#k-means-ejemplo-2",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n2da Iteración\n\n\n\n\nLos nuevos centroides son:\n\n\\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]\n\n\n\nCalculamos la Matriz de Distancias al Centroide:\n\n\n\n\\[D^2 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n3.14 & 2.26 & 0.47 & 1.89\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^2 = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son:\n\\(C_1 = (\\frac{3}{2}, 1)\\) y \\(C_2 = (\\frac{9}{2}, \\frac{7}{2})\\)"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-3",
    "href": "tics411/clase-3.html#k-means-ejemplo-3",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\nSi seguimos iterando notaremos que ya no hay cambios en los clusters. El algoritmo converge.\nEste es el resultado de usar \\(K=2\\). Utilizar otro valor de \\(K\\) entregará valores distintos.\n¿Es este el número de clusters óptimos?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-número-de-clusters-óptimos",
    "href": "tics411/clase-3.html#k-means-número-de-clusters-óptimos",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Número de Clusters Óptimos",
    "text": "K-Means: Número de Clusters Óptimos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSiempre es posible encontrar el número de clusters indicados.\nEntonces,\n\n¿Cómo debería escoger el valor de \\(K\\)?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-número-de-clusters-óptimos-1",
    "href": "tics411/clase-3.html#k-means-número-de-clusters-óptimos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Número de Clusters Óptimos",
    "text": "K-Means: Número de Clusters Óptimos\n\nCurva del Codo\n\nEs una heurísitca en la cual gráfica el valor de una métrica de distancia (e.g. within distance) para distintos valores de \\(K\\). El valor óptimo de \\(K\\) será el codo de la curva, que es el valor donde se estabiliza la métrica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste valor del codo muchas veces es subjetivo y distintas apreciaciones pueden llegar a distintos \\(K\\) óptimos.\n\n\n\n\n\n\n\n\n\n\nEventualmente otras métricas distintas al within cluster distance podrían también ser usadas.\n\n\n\n\n\n\n\n\n\n¿Cuál es el efecto que está buscando la curva del codo? ¿Qué implica el valor de K escogido?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-detalles-técnicos",
    "href": "tics411/clase-3.html#k-means-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Detalles Técnicos",
    "text": "K-Means: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nAlgoritmo relativamente eficiente (\\(O(k \\cdot n \\cdot i)\\)). Donde \\(k\\) es el número de clusters, \\(n\\) el número de puntos, e \\(i\\) el número de iteraciones.\nEncuentra “clusters esféricos”.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nSensible al punto de inicio.\nSolo se puede aplicar cuando el promedio es calculable.\nSe requiere definir K a priori (K es un hiperparámetro).\nSuceptible al ruido y a mínimos locales (podría no converger)."
  },
  {
    "objectID": "tics411/clase-3.html#implementación-en-scikit-learn",
    "href": "tics411/clase-3.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=8, n_init=10,random_state=None)\nkm.fit(X)\nkm.predict(X)\n\n## opcionalmente\nkm.fit_predict(X)\n\n\nn_clusters: Define el número de clusters a crear, por defecto 8.\nn_init: Cuántas veces se ejecuta el algoritmo, por defecto 10.\nrandom_state: Define la semilla aleatoria. Por defecto sin semilla.\ninit: Permite agregar centroides de manera manual.\n.fit(): Entrenará el modelo en los datos suministrados.\n.predict() Entregará las clusters asignados a cada dato suministrado.\n.clusters_centers_: Entregará las coordenadas de los centroides de cada Cluster.\n.inertia_: Entrega valores correspondiente a la within cluster distance.\n\n\n👀 Veamos un ejemplo en Colab."
  },
  {
    "objectID": "tics411/clase-3.html#sugerencias",
    "href": "tics411/clase-3.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que K-Means es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala común."
  },
  {
    "objectID": "tics411/clase-3.html#interpretación-clusters",
    "href": "tics411/clase-3.html#interpretación-clusters",
    "title": "TICS-411 Minería de Datos",
    "section": "Interpretación Clusters",
    "text": "Interpretación Clusters\n\n\n\n\n\n\nRecordar, que el clustering no clasifica. Por lo tanto, a pesar de que K-Means nos indica a qué cluster pertenece cierto punto, debemos interpretar cada cluster para entender qué es lo que se agrupó.\n\n\n\n\n\n\n\n\n\nLa interpretación del cluster es principalmente intuición y exploración, por lo tanto el EDA puede ser de utilidad para analizar clusters."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge",
    "href": "tics411/clase-3.html#post-procesamiento-merge",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\nPost-Procesamiento\n\n\nSe define como el tratamiento que podemos realizar al algoritmo luego de haber entregado ya sus predicciones.\n\n\n\nEs posible generar más clusters de los necesarios y luego ir agrupando los más cercanos."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge-1",
    "href": "tics411/clase-3.html#post-procesamiento-merge-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el problema con este caso de Post-Procesamiento?"
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-split",
    "href": "tics411/clase-3.html#post-procesamiento-split",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Split",
    "text": "Post-Procesamiento: Split\n\n\n\n\n\n\n\n\n\n\n\nEn Scikit-Learn esto puede conseguirse utilizando el parámetro init. Se entregan los nuevos centroides para forzar a K-Means que separe ciertos clusters."
  },
  {
    "objectID": "tics411/clase-3.html#variantes-k-means",
    "href": "tics411/clase-3.html#variantes-k-means",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes K-Means",
    "text": "Variantes K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SMD(p_1,p_2) = 4\\]\n\n\n\n\n\n\n\n\nAcá pueden encontrar una implementación de K-Modes en Python."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alfonso Tobar",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     datacuber.cl\n  \n\n      \nSoy Alfonso y he trabajado como Científico de Datos por los últimos 9 años. Además me gusta el Machine Learning Competitivo y hasta el momento he ganado 2 competencias.\nActualmente me encuentro a punto de terminar mi Msc. y además comencé a cursar mi PhD. en Data Science. Mis intereses de investigación tienen que ver con Machine Learning y Deep Learning enfocándome principalmente en la aplicación de Transformers.\nEn mi tiempo libre practico Tenis de Mesa y escribo sobre Machine Learning en mi blog: datacuber.cl.\n\n\nUniversidad Adolfo Ibañez, Viña del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Ibañez, Viña del Mar | Msc. in Data Science | 2022 - 2023\nUniversidad Técnica Federico Santa María | Ingeniería Civil | 2005 - 2013\nPuedes ver más detalles de mi carrera acá.\n\n\n\n\nHate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "index.html#educación",
    "href": "index.html#educación",
    "title": "Alfonso Tobar",
    "section": "",
    "text": "Universidad Adolfo Ibañez, Viña del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Ibañez, Viña del Mar | Msc. in Data Science | 2022 - 2023\nUniversidad Técnica Federico Santa María | Ingeniería Civil | 2005 - 2013\nPuedes ver más detalles de mi carrera acá."
  },
  {
    "objectID": "index.html#publicaciones",
    "href": "index.html#publicaciones",
    "title": "Alfonso Tobar",
    "section": "",
    "text": "Hate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "tics411.html",
    "href": "tics411.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentación del Curso\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 1\n\n\nCalidad de los Datos y Feature Engineering\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 2\n\n\nExploratory Data Analysis (EDA)\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase Bonus\n\n\nIntroducción a Scikit-Learn\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 3\n\n\nModelación Descriptiva y K-Means\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 4\n\n\nClustering Jerárquico\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 5\n\n\nDBSCAN\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 6\n\n\nEvaluación de Clusters\n\n\n\nApr 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 7\n\n\nAlgoritmo Apriori\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 8\n\n\nIntroducción al Aprendizaje Supervisado\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  }
]