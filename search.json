[
  {
    "objectID": "charlas.html",
    "href": "charlas.html",
    "title": "Charlas",
    "section": "",
    "text": "Hate Speech UAI\n\n\nPresentación del HateStack\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike a Needle in the HateStack\n\n\nPresentación final para la Datatón 2022\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nRFM-Superlag\n\n\nPresentación Final del Desafío Itaú-Binnario\n\n\n\nDec 5, 2020\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "tics411/lab-0.html#qué-es-scikit-learn",
    "href": "tics411/lab-0.html#qué-es-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Qué es Scikit-Learn?",
    "text": "¿Qué es Scikit-Learn?\n\n\n\n\n\nScikit-Learn (sklearn para los amigos) es una librería creada por David Cournapeau, como un Google Summer Code Project y luego Matthieu Brucher en su tesis.\nEn 2010 queda a cargo de INRIA y tiene un ciclo de actualización de 3 meses.\nEs la librería más famosa y poderosa para hacer Machine Learning hoy en día.\nSu API es tan famosa, que hoy se sabe que una librería es de calidad si sigue los estándares implementados por Scikit-Learn.\nPara que un algoritmo sea parte de Scikit-Learn debe poseer 3 años desde su publicación y 200+ citaciones mostrando su utilidad y amplio uso (ver acá).\nAdemás es una librería que obliga a que sus algoritmos tengan la capacidad de generalizar."
  },
  {
    "objectID": "tics411/lab-0.html#diseño",
    "href": "tics411/lab-0.html#diseño",
    "title": "TICS-411 Minería de Datos",
    "section": "Diseño",
    "text": "Diseño\n\nScikit-Learn sigue un patrón de Programación Orientada a Objetos (POO) basado en clases.\n\n\n\n\n\n\n\n\nEn programación, una clase es un objeto que internamente contiene estados que pueden ir cambiando en el tiempo.\n\nUna clase posee:\n\nMétodos: Funciones que cambian el comportamiento de la clase.\nAtributos: Datos propios de la clase.\n\n\n\n\n\n\n\nScikit-Learn sigue el siguiente estándar:\n\nTodas las Clases se escriben en CamelCase: Ej: KMeans,LogisticRegression, StandardScaler.\nLas clases en Scikit-Learn pueden representar algoritmos, o etapas de un preprocesamiento.\n\nLos algoritmos se denominan Estimators.\nLos preprocesamientos se denominan Transformers.\n\nLas funciones se escriben como snake_case y permiten realizar algunas operaciones básicas en el proceso de modelamiento. Ej: train_test_split(), cross_val_score().\nNormalmente se utilizan letras mayúsculas para denotar Matrices o DataFrames, mientras que las letras minúsculas denotan Vectores o Series."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-no-supervisados",
    "href": "tics411/lab-0.html#estimadores-no-supervisados",
    "title": "TICS-411 Minería de Datos",
    "section": "Estimadores No supervisados",
    "text": "Estimadores No supervisados\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X) \n\ny_pred = model.predict(X) \n\n## Opcionalmente se puede entrenar y predecir a la vez.\nmodel.fit_predict(X) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperparámetros.\nL3. Entrenar o ajustar el modelo (Requiere sólo de X).\nL5. Predecir. Los modelos de clasificación tienen la capacidad de generar probabilidades.\nL7-8. Este tipo de modelos permite entrenar y predecir en un sólo paso."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-predictivos",
    "href": "tics411/lab-0.html#estimadores-predictivos",
    "title": "TICS-411 Minería de Datos",
    "section": "Estimadores Predictivos",
    "text": "Estimadores Predictivos\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X_train, y_train) \n\ny_pred = model.predict(X_test) \ny_pred_proba = model.predict_proba(X_test)\n\nmodel.score(X_test,y_test) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperparámetros.\nL3. Entrenar o ajustar el modelo (Ojo, requiere de X e y).\nL5–6. Predecir en datos nuevos. (Algunos modelos pueden predecir probabilidades).\nL8. Evaluar el modelo en los datos nuevos."
  },
  {
    "objectID": "tics411/lab-0.html#output-de-un-modelo",
    "href": "tics411/lab-0.html#output-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Output de un Modelo",
    "text": "Output de un Modelo\n\nLos modelos no entregan directamente un output sino que los dejan almacenados en su interior como un estado.\nLos Estimators tienen dos estados:\n\nNot Fitted: Modelo antes de ser entrenado\nFitted: Una vez que el modelo ya está entrenado. (Después de aplicar .fit())\n\n\n\n\n\n\n\n\n\nMuchos modelos pueden entregar información sólo luego de ser entrenados (su atributo termina con un _).\nEj: model.coef_, model.intercept_.\n\n\n\n\n\n\n\n\n\n\n\nEl modelo es una herramienta a la cual le entregamos datos (Input), y nos devuelve datos (Predicciones)."
  },
  {
    "objectID": "tics411/lab-0.html#transformers",
    "href": "tics411/lab-0.html#transformers",
    "title": "TICS-411 Minería de Datos",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\n\n\nA diferencia de los Estimators, los Transformers no son modelos.\nSu input y su output son datos.\nAlgunos Transformers permiten escalar los datos, transformar categorías en números, rellenar valores faltantes. (Veremos más acerca de esto en los Preprocesamiento).\n\n\n\n\n\n\nfrom sklearn.preprocessing import Transformer \ntr = Transformer(hp1=v1, hp2=v2,...) \ntr.fit(X) \n\nX_new = tr.transform(X) \n\n## Opcionalmente\nX_new = tr.fit_transform(X) \n\nL1. Importar la clase a utilizar (en este caso del submodulo preprocessing, aunque pueden haber otros como impute).\nL2. Instanciar el Transformer y sus hiperparámetros.\nL3. Entrenar o ajustar el Transformer.\nL5. Transformar los datos.\nL7-8. Adicionalmente se puede entrenar y transformar los datos en un sólo paso."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines",
    "href": "tics411/lab-0.html#pipelines",
    "title": "TICS-411 Minería de Datos",
    "section": "Pipelines",
    "text": "Pipelines\n\nEn ocasiones un Dataset requiere más de un preprocesamiento.\nEstas Transformaciones normalmente se hacen en serie de manera consecutiva.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl Estimator es opcional, es decir, el Pipeline puede ser para combinar sólo Transformers o Transformers + un Estimator.\n\n\n\n\n\n\n\n\n\n\nUn Pipeline puede tener sólo un Estimator."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines-código",
    "href": "tics411/lab-0.html#pipelines-código",
    "title": "TICS-411 Minería de Datos",
    "section": "Pipelines: Código",
    "text": "Pipelines: Código\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.pipeline import Pipeline \n\npipe = Pipeline(steps=[ \n    (\"ohe\", OneHotEncoder()),\n    (\"sc\", StandardScaler()),\n    (\"model\", DecisionTreeClassifier())\n])\n\npipe.fit(X_train, y_train) \ny_pred = pipe.predict(X_test) \n\npipe.score(X_test, y_test) \n\nL1-2. Importo mi modelo y mis preprocesamientos\nL3. Importo el Pipeline.\nL5-9. Instancio un Pipeline.\nL11. Entreno el Pipeline.\nL12. Predigo utilizando el Pipeline entrenado.\nL14. Evalúo el modelo en datos no vistos."
  },
  {
    "objectID": "tics411/lab-0.html#documentación",
    "href": "tics411/lab-0.html#documentación",
    "title": "TICS-411 Minería de Datos",
    "section": "Documentación",
    "text": "Documentación\n\nProbablemente Scikit-Learn tenga una de las mejores documentaciones existentes.\n\n\nVeamos el caso de la Documentación del One Hot Encoder"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión",
    "href": "tics411/clase-10.html#árboles-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión",
    "text": "Árboles de Decisión\n\nTécnica de clasificación supervisada que genera una decisión basada en árboles de decisión para clasificar instancias no conocidas."
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-ejemplo",
    "href": "tics411/clase-10.html#árboles-de-decisión-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Ejemplo",
    "text": "Árboles de Decisión: Ejemplo\n\nVisualmente, un árbol de decisión segmenta el espacio separando los datos en subgrupos.\n\n\n\n\n\n\n\nEsto permite la generacion de fronteras de decisión sumamente complejas.\n\n\n\nSupongamos el siguiente ejemplo:"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión",
    "href": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Frontera de Decisión",
    "text": "Árboles de Decisión: Frontera de Decisión"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión-1",
    "href": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Frontera de Decisión",
    "text": "Árboles de Decisión: Frontera de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál sería el Nivel de Ajuste de un modelo de este tipo?"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-inferencia",
    "href": "tics411/clase-10.html#árboles-de-decisión-inferencia",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Inferencia",
    "text": "Árboles de Decisión: Inferencia\nUna vez construido el árbol de decisión basta con recorrerlo para poder generar la predicción para una instancia dada:"
  },
  {
    "objectID": "tics411/clase-10.html#características-de-árboles",
    "href": "tics411/clase-10.html#características-de-árboles",
    "title": "TICS-411 Minería de Datos",
    "section": "Características de Árboles",
    "text": "Características de Árboles\n\nPueden trabajar con valores discretos o continuos. Además pueden ser usados como modelos de Clasificación o Regresíon.\nUna vez seleccionado un atributo no es posible devolverse (backtracking).\nDebido al poder de un árbol de Decisión la mayoría de las veces tienden al Overfitting. Una forma de evitar esto es usar técnicas de Pruning.\nEs preferible usar árboles cortos (Principio de Parsimonia o Occam's Razor).\n\n\n\n\n\n\n\nEl principio de Parsimonia recomienda encontrar soluciones a problemas utilizando la menor cantidad de elementos/parámetros."
  },
  {
    "objectID": "tics411/clase-10.html#tipos-de-árboles-de-decisión",
    "href": "tics411/clase-10.html#tipos-de-árboles-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Árboles de Decisión",
    "text": "Tipos de Árboles de Decisión\n\n\n\n\n\n\n\n\n\nBinary Split\n\n\n\n\n\n\n\n\n\n\nMulti-way Split\n\n\n\n\n\n\n\nHunt’s Algorithm \\(\\implies\\) Primer Método.\nID3 \\(\\implies\\) Sólo utiliza variables categóricas.\nC4.5 \\(\\implies\\) incluye variables continuas.\nC5.0 \\(\\implies\\) Permite separación en Múltiples Splits (No ha sido implementado en Sklearn).\nCART (Classification and Regression Trees) \\(\\implies\\) Permite que el output sea continuo pero solo utilizando Splits binarios.\n\n\n\n\n\n\n\n\n\n\nLos CARTs son por lejos los árboles más utilizados en las librerías más famosas y potentes: Scikit-Learn, XGboost, LightGBM, Catboost."
  },
  {
    "objectID": "tics411/clase-10.html#creación-de-un-árbol-de-decisión",
    "href": "tics411/clase-10.html#creación-de-un-árbol-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de un Árbol de Decisión",
    "text": "Creación de un Árbol de Decisión\n\nPureza\n\nCorresponde a la probabilidad de no sacar dos registros de un Nodo que pertenezcan a la misma clase.\n\n\n\n\n\n\n\n\nEl árbol de Decisión busca crear Nodos lo más puro posible. Para ello puede utilizar las siguentes métricas:\n\n\n\n\n\nÍndice Gini\n\\[Gini(X) = 1 - \\sum_{x_i}p(x_i)^2\\]\n\nEntropía\n\\[H(X) = -\\sum_{x_i}p(x_i)log_2p(x_i)\\]\n\n\n\n\n\n\n\nA mayor valor, mayor nivel de impureza. 0 implica Nodo completamente puro."
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-ejemplo",
    "href": "tics411/clase-10.html#árbol-de-decisión-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Ejemplo",
    "text": "Árbol de Decisión: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCálculo de Impureza en Hoja\n\n\n\\[Gini_{(leaf)} = 1 - p(Yes)^2 - p(No)^2\\]\n\n\n\n\n\n\n\n\n\nCálculo de Impureza en Split\n\n\n\\[ Gini_{(split)} = \\frac{n_{(yes)}}{n} Gini_{(yes)} + \\frac{n_{(no)}}{n} Gini_{(no)}\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-popcorn",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-popcorn",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Popcorn",
    "text": "Árbol de Decisión: Raíz Popcorn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{4}\\right)^2 - \\left(\\frac{3}{4}\\right)^2 = 0.375\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.375 + \\frac{3}{7} \\cdot 0.444 = 0.405\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-soda",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-soda",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Soda",
    "text": "Árbol de Decisión: Raíz Soda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{4}\\right)^2 - \\left(\\frac{1}{4}\\right)^2 = 0.375\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{3}\\right)^2 - \\left(\\frac{3}{3}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.375 = 0.214\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5\\]\n\n\n\n\\[Gini_{(split)} = \\frac{6}{7}\\cdot 0.5 = 0.429\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-1",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{2}\\right)^2 - \\left(\\frac{2}{2}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{5}\\right)^2 - \\left(\\frac{2}{5}\\right)^2 = 0.48\\]\n\n\n\n\\[Gini_{(split)} = \\frac{5}{7}\\cdot 0.48 = 0.343\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-2",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{4}\\right)^2 - \\left(\\frac{2}{4}\\right)^2 = 0.5\\]\n\n\n\n\\[Gini_{(split)} = \\frac{3}{7}\\cdot 0.444 + \\frac{4}{7} \\cdot 0.5 = 0.476\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-3",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{2}{4}\\right)^2 - \\left(\\frac{2}{4}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.5 + \\frac{3}{7} \\cdot 0.444 = 0.476\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-4",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-4",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{2}\\right)^2 - \\left(\\frac{2}{5}\\right)^2 = 0.48\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{2}\\right)^2 - \\left(\\frac{2}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{5}{7}\\cdot 0.48 = 0.343\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-5",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-5",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{6}{7}\\cdot 0.5 = 0.429\\]"
  },
  {
    "objectID": "tics411/clase-10.html#qué-split-elegiremos",
    "href": "tics411/clase-10.html#qué-split-elegiremos",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Qué Split elegiremos?",
    "text": "¿Qué Split elegiremos?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEscogeremos el Split más pequeño que representa el que genera más pureza.\n\n\n\n\n\n\n\n\n\n\n\nEl nodo que no le gusta la Soda quedó completamente puro. Por lo tanto, no puede seguir dividiéndose. Seguiremos trabajando sólo con aquellos que sí les gusta la Soda."
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{2}\\right)^2 - \\left(\\frac{0}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{2}{4}\\cdot 0.5 = 0.25\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-1",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{3}\\right)^2 - \\left(\\frac{0}{3}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = 0\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-2",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{2}\\right)^2 - \\left(\\frac{0}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{2}{4} \\cdot 0.5 = 0.25\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-3",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\\] \\[Gini_{(no)} = 1 - \\left(\\frac{1}{1}\\right)^2 - \\left(\\frac{0}{1}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{3}{4} \\cdot 0.444 = 0.333\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión",
    "href": "tics411/clase-10.html#árbol-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión",
    "text": "Árbol de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál sería la predicción?"
  },
  {
    "objectID": "tics411/clase-10.html#crecimiento-de-un-árbol",
    "href": "tics411/clase-10.html#crecimiento-de-un-árbol",
    "title": "TICS-411 Minería de Datos",
    "section": "Crecimiento de un Árbol",
    "text": "Crecimiento de un Árbol\n\nUn árbol sólo dejará de crecer si:\n\nNo hay más puntos a separar.\n\nTodas las muestras de un nodo pertenecen a la misma clase.\n\nNo hay más variables a separar.\n\n\n\n\n\n\n\n\n\nEsto normalmente termina en Overfitting.\n\n\n\n\n\n\n\n\n\n\nPara solucionar esto se aplica regularización. En el caso de Árboles esto se denomina prunning."
  },
  {
    "objectID": "tics411/clase-10.html#pruning",
    "href": "tics411/clase-10.html#pruning",
    "title": "TICS-411 Minería de Datos",
    "section": "Pruning",
    "text": "Pruning\nPrepruning: Define/Evita que el árbol crezca hasta:\n\nUn cierto nivel o número de hojas.\nAplicar un test estadístico (normalmente un proceso muy costoso).\nUsar medidas de complejidad para penalizar árboles de gran tamaño.\n\nPostpruning: Decide eliminar nodos, luego de que el árbol crezca.\n\nUsar un parámetro de Costo de Impureza."
  },
  {
    "objectID": "tics411/clase-10.html#hiperparámetros",
    "href": "tics411/clase-10.html#hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "Hiperparámetros",
    "text": "Hiperparámetros\n\n\n\n\n\n\n\ncriterion: Elegir bajo qué criterio se mide la impureza.\nmax_depth: El nivel es la altura que tendrá el árbol. Niveles más bajos generan árboles más simples.\nmin_samples_split: Número de instancias necesarias para generar un split. Un mayor número o proporción generará árboles más simples.\nmin_samples_leaf: Número mínimo de instancias necesarias para que un nodo sea hoja. Un número o proporción más alta generará árboles más simples.\nccp_alpha: Está asociado a la pureza total del árbol. Para más información ver acá.\n\n\n\n\n\n\n\n\n\n\n¿Cómo se ve la complejidad/simplicidad en un árbol de Decisión?"
  },
  {
    "objectID": "tics411/clase-10.html#implementación-en-scikit-learn",
    "href": "tics411/clase-10.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndt = DecisionTreeClassifier(criterion=\"gini\", max_depth=None, min_sample_split=2, \n                            min_samples_leaf=1,min_impurity_decrease=0, \n                            ccp_alpha=0, random_state=42)\ndt.fit(X_train, y_train)\n\ny_pred = dt.predict(X_test)\ny_proba = dt.predict_proba(X_test)\n\n## Permite Visualizar el Árbol de Decisión\nplt_tree(dt, filled = True, feature_names=None, class_names=None)\n\n\ncriterion: Puede ser gini o entropía. Por defecto \"gini\".\nmax_depth: Número de niveles que se permita que crezca el nivel, por defecto None, significa todos los que pueda.\nmin_samples_split: El número mínimo de elementos dentro de un nodo para permitir el split. Por defecto 2.\nmin_samples_leaf: El número mínimo de elementos para que un nodo pueda ser considerado hoja. Por defecto 1.\nmin_impurity_decreased: Decrecimiento mínimo de la impureza. Si no se cumple, no hay Split. Por defecto 0.\nccp_alpha: Parámetro de Post-Pruning. Valores más altos genera la poda de más nodos."
  },
  {
    "objectID": "tics411/clase-13.html#definición",
    "href": "tics411/clase-13.html#definición",
    "title": "TICS-411 Minería de Datos",
    "section": "Definición",
    "text": "Definición\n\nAnomalías\n\n\nConjunto de puntos que son considerablemente diferentes al resto.\n\n\n\n\n\n\n\n\n\nPor definición las Anomalías son relativamente raras. * Pueden ocurrir en proporciones extremadamente bajas en los datos. Ej: 1 entre mil. * El contexto es importante. Ej: Temperaturas bajo cero en Verano.\n\n\n\nEjemplos:\n\nTelecomunicaciones: Detección de Abusos de Roaming.\nBanca: Compras/Ventas inusualmente elevados.\nFinanzas y Seguros: Detectar y prevenir patrones de gastos fraudulentos.\nMantención: Predicción de comportamiento irregular/fallas.\nSmart Homes: Detecciones de fugas de Energía\netc."
  },
  {
    "objectID": "tics411/clase-13.html#más-ejemplos",
    "href": "tics411/clase-13.html#más-ejemplos",
    "title": "TICS-411 Minería de Datos",
    "section": "Más ejemplos",
    "text": "Más ejemplos\n\n\n\n\n\n\nLa definición de Anomalía es altamente subjetiva y depende mucho del Dominio en el cuál se está trabajando."
  },
  {
    "objectID": "tics411/clase-13.html#tipos-de-anomalías-series-de-tiempo",
    "href": "tics411/clase-13.html#tipos-de-anomalías-series-de-tiempo",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Anomalías (Series de Tiempo)",
    "text": "Tipos de Anomalías (Series de Tiempo)"
  },
  {
    "objectID": "tics411/clase-13.html#desafíos",
    "href": "tics411/clase-13.html#desafíos",
    "title": "TICS-411 Minería de Datos",
    "section": "Desafíos",
    "text": "Desafíos\n\n\n\n\n\n\nDesafíos\n\n\n\n¿Cuántos Atributos/Variables usamos para definir un Outlier?\n¿Cuántos Outliers existen?\nEste tipo de problema suele ser complicado de Etiquetar, por lo que es difícil resolverlo como un problema supervisado.\nPuede ser como “Encontrar una aguja en un pajar”."
  },
  {
    "objectID": "tics411/clase-13.html#enfoques",
    "href": "tics411/clase-13.html#enfoques",
    "title": "TICS-411 Minería de Datos",
    "section": "Enfoques",
    "text": "Enfoques"
  },
  {
    "objectID": "tics411/clase-13.html#técnicas-visuales",
    "href": "tics411/clase-13.html#técnicas-visuales",
    "title": "TICS-411 Minería de Datos",
    "section": "Técnicas Visuales",
    "text": "Técnicas Visuales\n\n\n\n\n\n\nEstas técnicas son muy subjetivas ya que dependen del criterio/apreciación del usuario.\n\n\n\n\n\nBox Plots\n\n\n\n\n\n\nScatter Plots"
  },
  {
    "objectID": "tics411/clase-13.html#técnicas-estadísticas-test-de-grubbs",
    "href": "tics411/clase-13.html#técnicas-estadísticas-test-de-grubbs",
    "title": "TICS-411 Minería de Datos",
    "section": "Técnicas Estadísticas: Test de Grubbs",
    "text": "Técnicas Estadísticas: Test de Grubbs\n\n\n\n\n\n\nEl test de Grubbs detecta si algún dato es un outlier sobre una variable asumiendo que se distribuyen de manera normal.\n\n\n\n\\[G = \\frac{\\underset{i = 1,2,...,n}{max}|x_i - \\bar{X}|}{S_x}\\]\ndonde \\(\\bar{X}\\) y \\(S_x\\) corresponden a la media y Desviación Estándar Muestral.\n\nEso implica que \\(G\\) se distribuye como una t-student de \\(n-2\\) grados de libertad, por lo tanto si:\n\n\\[ G_{critico} = \\frac{n-1}{\\sqrt{n}}\\sqrt{\\frac{t^2_{(\\alpha/n, n-2)}}{n-2+t_{(\\alpha/n, n-2)^2}}}\\]\n\n\n\n\n\n\nSi \\(G &gt; G_{critico}\\), \\(x_i\\) es considerado un outlier con una significancia \\(\\alpha/n\\) para una t-student con \\(n-2\\) grados de libertad."
  },
  {
    "objectID": "tics411/clase-13.html#test-de-grubs-en-python",
    "href": "tics411/clase-13.html#test-de-grubs-en-python",
    "title": "TICS-411 Minería de Datos",
    "section": "Test de Grubs en Python",
    "text": "Test de Grubs en Python\n\n\n\n\n\n\nEste código debiera entregar una lista de todos los puntos que son considerados outliers.\n\n\n\nfrom scipy import stats\nimport numpy as np\n\nn = 16 # Número de Datos\nalpha = 0.05 # nivel de confianza\n\nt_crit = stats.t.ppf(1-alpha/n, n-2)\nG_crit = (n-1)/np.sqrt(n)*np.sqrt(t_crit**2/(n-2 + t_crit**2))\n\ndata = np.array([5,14,15,15,19,17,16,20,22,8,21,28,11,9,29,40])\nG_test = np.abs(data-np.mean(data)/np.std(data))\n\ntest_grubbs = np.where(G_test&gt;G_crit)\nprint(f\"Outliers: {data[test_grubbs]}\")"
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado",
    "href": "tics411/clase-13.html#caso-multivariado",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\n\n\n\n\nLa idea es calcular la distancia de cada punto al centro tomando en consideración la covarianza."
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-1",
    "href": "tics411/clase-13.html#caso-multivariado-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 1\n\n\n\nCalcular el punto central de todos los puntos (Promedio) \\[\\mu = (3.16, 3.16)\\]\n\n\n\n\n\n\n\n\n\n\n\nPaso 2\n\n\n\nCalcular la Inversa de la Matriz de Covarianza:"
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-continuación",
    "href": "tics411/clase-13.html#caso-multivariado-continuación",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado: Continuación",
    "text": "Caso Multivariado: Continuación\n\n\n\n\n\n\nPaso 3\n\n\nCalcular la distancia de cada punto con respecto a la media y la inversa de la Covarianza.\n\n\n\n\\[d_i = (p_i - \\mu)^T \\Sigma^{-1}(p_i - \\mu)\\]\n\\[d_1 = (p_1 - \\mu)^T \\sigma^{-1}(p_1 - \\mu)\\]\n\\[d_1 = ([0,0] - [3.16, 3.16])^T \\begin{bmatrix}\n                            0.147 & -0.147  \\\\\n                            -0.147 & 1.911  \\\\\n                            \\end{bmatrix}\n                            ([0,0] - [3.16, 3.16])\\]\n\n\n\n\n\n\nSe debe repetir este procedimiento para cada punto."
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-continuación-1",
    "href": "tics411/clase-13.html#caso-multivariado-continuación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado: Continuación",
    "text": "Caso Multivariado: Continuación\n\n\n\n\n\n\nPaso 4:\n\n\nSe debe calcular el punto crítico según t-student con 95% confianza, y orden de magnitud \\(m\\) dimensiones.\n\n\n\n\\[t_{(\\alpha = 0.95,2)} = 5.99\\]\n\n\n\n\n\n\nPaso 5\n\n\nComparar, Si \\(d_i&gt;t_{crit}\\) entonces \\(d_i\\) es Outlier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso ningún valor de \\(d_i\\) es mayor al $t_{(crit)}, por lo tanto, no hay outliers."
  },
  {
    "objectID": "tics411/clase-13.html#distancia-de-mahalanobis",
    "href": "tics411/clase-13.html#distancia-de-mahalanobis",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancia de Mahalanobis",
    "text": "Distancia de Mahalanobis\nLa distancia de Mahalanobis corresponde a:\n\\[d_i = \\sqrt{(p_i - \\mu)^T \\Sigma^{-1}(p_i - \\mu)}\\]\n\n\n\n\n\n\nSe puede repetir el mismo procedimiento anterior, sólo que se define una Distancia de Mahalonobis umbral. Las que superen dicho umbral son considerados como Outliers."
  },
  {
    "objectID": "tics411/clase-13.html#dbscan",
    "href": "tics411/clase-13.html#dbscan",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\nPodemos utilizar el procedimiento que aprendimos de DBSCAN. Todos los puntos Noise serán considerados como Anomalías.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point. Lo cuál en nuestro caso particular se considerará una Anomalía."
  },
  {
    "objectID": "tics411/clase-13.html#k-nearest-neighbor",
    "href": "tics411/clase-13.html#k-nearest-neighbor",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbor",
    "text": "K-Nearest Neighbor\n\n\n\n\n\n\nSe puede utilizar los modelos de vecinos más cercanos para determinar outliers siguiendo el siguiente procedimiento:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 1: Definir el valor de \\(k\\) para encontrar los vecinos más cercanos.\nEj: Sea \\(k=3\\)\nPaso 2: Calcular la Matriz de Distancias y determinar los vecinos más cercanos."
  },
  {
    "objectID": "tics411/clase-13.html#k-nearest-neighbor-continuación",
    "href": "tics411/clase-13.html#k-nearest-neighbor-continuación",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbor: Continuación",
    "text": "K-Nearest Neighbor: Continuación\n\n\n\nPaso 3: Calcular la distancias Promedio.\n\n\n\n\n\n\nPaso 4: Escoger un Umbral. Si es que la distancia es mayor al Umbral entonces es un Outlier. Ej: \\(Dist_crit: 3\\)"
  },
  {
    "objectID": "tics411/clase-13.html#local-outlier-factor-lof",
    "href": "tics411/clase-13.html#local-outlier-factor-lof",
    "title": "TICS-411 Minería de Datos",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\n\nLocal Outlier Factor (LOF) detecta anomalías con sus vecindarios locales, en lugar de la distribución glocal de los datos.\n\n\n\n\n\n\n\nEn la Figura, \\(O1\\) y \\(O2\\) son anomalías locales en comparación con \\(C1\\), \\(O3\\) es una anomalía global, y \\(O4\\) no es una anomalía."
  },
  {
    "objectID": "tics411/clase-13.html#algoritmo",
    "href": "tics411/clase-13.html#algoritmo",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo",
    "text": "Algoritmo\n\nDeterminar \\(N(x,k)\\), los k-vecinos más cercanos de cada punto x.\nPara todo punto \\(y\\), calcular la distancia a su k-ésimo vecino más cercano.\nCalcular la reach-distance entre todos los puntos: \\[reach-distance_k(x,y) = max\\{k-distance(y), d(x,y)\\}\\]\nCalcule la densidad del vecindario local sobre sus \\(k\\) vecinos, donde \\(|N(x,k)| = k\\).\n\n\\[density(x,k) = lrd_k(x) = \\left(\\frac{\\sum_{y \\in N(x,k)} reach-distance_k(x,y)}{|N(x,k)|}\\right)^{-1}\\]\n\nCalcule el Local Outlier Factor para el punto x como la proporción de la densidad de sus \\(k\\) vecinos más cercanos, con respecto a la densidad del punto \\(x\\).\n\n\n\n\\[ LOF(x) = \\frac{\\sum_{y \\in N(x,k) density(y,k)}}{|N(x,k)|density(x,k)} \\]\n\n\n\n\n\n\n\nLOF(X) &gt;&gt; 1 implica anomalía."
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor",
    "text": "Ejemplo Local Outlier Factor\n\n\n\nConsideremos los siguientes 4 puntos de datos: a(0,0), b(0,1), c(1,1), d(3,0). Calcular el LOF para cada punto y mostrar la anomalía principal.\n\n\n\n\n\n\n\n\n\nUtilizar \\(K = 2\\) y Distancia Manhattan.\n\n\n\n\n\nPaso 1: Calcular Distancias\n\ndist(a,b) = 1\ndist(a,c) = 2\ndist(a,d) = 3\ndist(b,c) = 1\ndist(b,d) = 4\ndist(c,d) = 3"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 2: Para todo punto \\(y\\), calcule la distancia a su k-ésimo vecino más cercano.\n\n\\(dist_2(a) = dist(a,c) = 2\\) (c es el 2do vecino más cercano)\n\\(dist_2(b) = dist(b,a) = 1\\) (a/c es el 2do vecino más cercano)\n\\(dist_2(c) = dist(c,a) = 2\\) (a es el 2do vecino más cercano)\n\\(dist_2(d) = dist(d,a) = 3\\) (a/c es el 2do vecino más cercano)"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-1",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 3: Calcular la reach-distance entre todos los puntos, es decir, los puntos vecindarios a una distancia k.\n\n\n\n\\(N_k(o)\\): Vecindario de \\(k\\)-distancia de \\(o\\), \\(N_k(o)=\\{o'\\|o' \\in D, dist(o,o') \\le dist_k(o)\\}\\)\n\n\n\n\n\\(N_2(a) = \\{b,c\\}\\)\n\\(N_2(b) = \\{a,c\\}\\)\n\\(N_2(c) = \\{b,a\\}\\)\n\\(N_2(d) = \\{a,c\\}\\)"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-2",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\n\n\nPaso 4: Calcular la densidad del vecinadario local sobre sus \\(k\\) vecinos.\n\n\n\n\n\n\n\n\n\\(reach-dist_2(b \\leftarrow a) = max\\{dist_2(b), dist(b,a)\\} = max\\{1,1\\} = 1\\)\n\\(reach-dist_2(c \\leftarrow a) = max\\{dist_2(c), dist(c,a)\\} = max\\{2,2\\} = 2\\)\n\n\n\n\n\n\n\nCalcular el resto de manera análoga."
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-3",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nEntonces, \\(lrd_k(o)\\): Densidad de alcanzabilidad local de \\(o\\).\n\\[lrd_2(a) = \\frac{|\\mathcal{N}(a)|}{reach-dist_2(b\\leftarrow a) + reach-dist_2(c \\leftarrow a)} = \\frac{2}{1 + 2} = 0.667\\] \\[lrd_2(b) = \\frac{|\\mathcal{N}(b)|}{reach-dist_2(a\\leftarrow b) + reach-dist_2(b \\leftarrow b)} = \\frac{2}{2 + 2} = 0.5\\] \\[lrd_2(c) = \\frac{|\\mathcal{N}(c)|}{reach-dist_2(b\\leftarrow c) + reach-dist_2(a \\leftarrow c)} = \\frac{2}{1 + 2} = 0.667\\] \\[lrd_2(d) = \\frac{|\\mathcal{N}(d)|}{reach-dist_2(a\\leftarrow d) + reach-dist_2(c \\leftarrow d)} = \\frac{2}{1 + 2} = 0.33\\]"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-4",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-4",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 5: Calcular el Local Outlier Factor para el punto \\(x\\) como la proporción de la densidad de sus \\(k\\) vecinos más cercanos, con respecto a la densidad del punto \\(x\\).\n\n\n\\[LOF(x) = \\frac{\\sum_{y \\in N(x,k)} density(y,k)}{|N(x,k)|density(x,k)}\\]\n\\[LOF_2(a) = \\frac{lrd_2(b) + lrd_2(c)}{N_2(a) \\cdot lrd_2(a)} = \\frac{0.5 + 0.667}{2 \\cdot 0.667} = 0.87\\] \\[LOF_2(b) = \\frac{lrd_2(a) + lrd_2(c)}{N_2(b) \\cdot lrd_2(b)} = \\frac{0.667 + 0.667}{2 \\cdot 0.5} = 1.334\\] \\[LOF_2(c) = \\frac{lrd_2(b) + lrd_2(a)}{N_2(c) \\cdot lrd_2(c)} = \\frac{0.5 + 0.667}{2 \\cdot 0.667} = 0.87\\] \\[LOF_2(d) = \\frac{lrd_2(a) + lrd_2(c)}{N_2(d) \\cdot lrd_2(d)} = \\frac{0.667 + 0.667}{2 \\cdot 0.33} = 2\\]"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-5",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-5",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 6: Ordena todas las LOF_k(o)\n\nLOF_2(d) = 2 \\(\\implies\\) el punto paraecer una anomalía (LOF &gt;&gt; 1)\nLOF_2(b) = 1.334\nLOF_2(a) = 0.87\nLOF_2(c) = 0.87\n\n\n\n\n\n\n\nDetalles Técnicos\n\n\n\nDado que esto sigue un enfoque local, la resolución depende de la elección del usuario para \\(k\\).\nGenera una puntuación (Anomaly Score) para cada punto.\nComo \\(LOF\\) es una razón, es difícil de interpretar. No existe un valor umbral específico por encima del cual un punto se define como un valor atípico. La identificación de un valor atípico depende del problema y del usuario\nComo \\(LOF\\) es una razón, es difícil de interpretar. No existe un valor umbral específico por encima del cual un punto se define como un valor atípico. La identificación de un valor atípico depende del problema y del usuario."
  },
  {
    "objectID": "tics411/clase-9.html#intuición",
    "href": "tics411/clase-9.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\nSupongamos que tengo que estudiar para la prueba de Minería de Datos y tengo que aprender a calcular el Coeficiente de Silueta.\n\nQué pasa si sólo les entrego una pregunta para estudiar y no tiene respuesta.\n¿Qué pasa si ahora les doy la respuesta?\n¿Qué pasa si te doy más ejercicios?\n¿Qué pasa luego de que haces muchos ejercicios?\n\n\n\n\n\n\n\n\nVoy aprendiendo mejor la tarea de calcular el coeficiente de Silueta. Lo mismo pasa con los modelos.\n\n\n\n\n\n\n\n\n\n\n\nPero no puedo medir qué tan bien aprendiste en los ejercicios que yo ya entregué para practicar. Tengo que hacer una prueba que tú no hayas visto, para ver si realmente aprendiste."
  },
  {
    "objectID": "tics411/clase-9.html#uso-de-un-modelo",
    "href": "tics411/clase-9.html#uso-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Uso de un Modelo",
    "text": "Uso de un Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo saber que el modelo está funcionando como esperamos?"
  },
  {
    "objectID": "tics411/clase-9.html#métricas",
    "href": "tics411/clase-9.html#métricas",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas",
    "text": "Métricas\nEl Rendimieto de un Modelo de Clasificación permite evaluar el error asociado al proceso de predicción.\n\n\n\n\nClase Positiva\n\nCorresponde a la clase/evento de interés. Ej: Tiene cancer, va a pagar su deuda, es un gato. Normalmente se denota como la Clase 1.\n\nClase Negativa\n\nCorresponde a la clase/evento contrario al de interés. Ej: No tiene cancer, no va a pagar su deuda, no es un gato. Normalmente se denota como la Clase 0.\n\n\n\n\n\n\n\n\n\n\n\nScikit-Learn usa la siguiente convención:\n\nSi se llama *_score un mayor puntaje es mejor.\nSi se llama *_error o *_loss un mejor puntaje es mejor."
  },
  {
    "objectID": "tics411/clase-9.html#métricas-matriz-de-confusión",
    "href": "tics411/clase-9.html#métricas-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas: Matriz de Confusión",
    "text": "Métricas: Matriz de Confusión\n\nLa Matriz de Confusión ordena los valores correctamente predichos y también los distintos errores que el modelo puede cometer.\n\n\n\n\n\n\n\n\n\n\nTP (Verdaderos Positivos)\n\nCorresponde a valores reales de la clase 1 que fueron correctamente predichos como clase 1.\n\nTN (Verdaderos Negativos)\n\nCorresponde a valores reales de la clase 0 que fueron correctamente predichos como clase 0.\n\nFP (Falsos Positivos)\n\nCorresponde a valores reales de la clase 0 que fueron incorrectamente predichos como clase 1.\n\nFN (Falsos Negativos)\n\nCorresponde a valores reales de la clase 1 que fueron incorrectamente predichos como clase 0."
  },
  {
    "objectID": "tics411/clase-9.html#métricas-a-partir-de-la-matriz-de-confusión",
    "href": "tics411/clase-9.html#métricas-a-partir-de-la-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas: A partir de la Matriz de Confusión",
    "text": "Métricas: A partir de la Matriz de Confusión\n\n\n\n\n\nAccuracy\n\n\n\\[\\frac{TP + TN}{TP + TN + FP + FN}\\]\n\n\n\n\n\n\nPrecision\n\n\n\\[\\frac{TP}{TP + FP}\\]\n\n\n\n\n\n\n\nRecall\n\n\n\\[\\frac{TP}{TP + FN}\\]\n\n\n\n\n\n\nF1-Score\n\n\n\\[\\frac{2\\cdot Precision \\cdot Recall}{Precision + Recall} = \\frac{2 \\cdot TP}{2\\cdot TP + FP + FN}\\]\n\n\n\n\n\n\n\n\n\n\n\nAccuracy es probablemente la métrica más sencilla y más utilizada.\nPrecision y Recall ponderarán distintos errores (FP y FN respectivamente) con mayor severidad. Ambas métricas son Antagonistas.\nF1-Score corresponde a la media armónica del Precision y Recall, y tiende a ponderar los errores de manera más balanceada.\n\n\n\n\n¿Cuándo utilizar cada tipo de error?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-roc",
    "href": "tics411/clase-9.html#curva-roc",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva ROC",
    "text": "Curva ROC\nLa curva ROC fue desarrollada en 1950 para analizar señales ruidosas. La curva ROC permite al operador contrapesar la tasa de verdaderos positivos (Eje \\(y\\)) versus los falsos positivos (Eje x).\n\nEl área bajo la curva representa la calidad del modelo. Una manera de interpretarla es como la probabilidad de que una predicción de la clase positiva tenga mayor probabilidad que una de clase negativa. En otras palabras, mide que las probabilidades se encuentren correctamente ordenadas. Por lo tanto varía entre 0.5 y 1.\n\n\n\n\n\n\n\n\n\nROC \\(\\sim\\) 0.5\n\n\n\n\n\nROC \\(\\sim\\) 1"
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python",
    "href": "tics411/clase-9.html#implementación-en-python",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python",
    "text": "Implementación en Python\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\naccuracy_score(y_true, y_pred)\nprecision_score(y_true, y_pred)\nrecall_score(y_true, y_pred)\nf1_score(y_true, y_pred)\nroc_auc_score(y_true, y_proba)\n\ny_true: Corresponde a las etiquetas reales del Dataset.\ny_pred: Corresponde a las predicciones realizadas por el modelo.\ny_proba: Corresponden a las probabilidades predichas por el modelo (si es que el modelo lo permite)."
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python-matriz-de-confusión",
    "href": "tics411/clase-9.html#implementación-en-python-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Matriz de Confusión",
    "text": "Implementación en Python: Matriz de Confusión\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python-curva-roc",
    "href": "tics411/clase-9.html#implementación-en-python-curva-roc",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Curva ROC",
    "text": "Implementación en Python: Curva ROC\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_predictions(y_true, y_proba)"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-training",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-training",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva de Aprendizaje: Training",
    "text": "Curva de Aprendizaje: Training\n\n\n\n\n\n\n\n\n\n\n\n¿Qué sería la Complejidad del Modelo?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-validación",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-validación",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva de Aprendizaje: Validación",
    "text": "Curva de Aprendizaje: Validación\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué el modelo pierde rendimiento cuando aumenta su Complejidad?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-mejor-ajuste",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-mejor-ajuste",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva de Aprendizaje: Mejor Ajuste",
    "text": "Curva de Aprendizaje: Mejor Ajuste\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting:\n\n\nGran diferencia entre Training y Validation Score.\n\n\n\n\n\n\n\n\n\nUnderfitting:\n\n\nPoca diferencia entre Training y Validation Score, pero con ambos puntajes “relativamente bajos”.\n\n\n\n\n\n\n\n\n\nProper fitting o Sweet Spot:\n\n\nCorresponde al mejor puntaje en el set de Validación. Donde también la distancia entre Train y Test es poca."
  },
  {
    "objectID": "tics411/clase-9.html#complejidad-de-un-modelo",
    "href": "tics411/clase-9.html#complejidad-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Complejidad de un Modelo",
    "text": "Complejidad de un Modelo\n¿Qué modelo es un mejor clasificador?"
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff",
    "href": "tics411/clase-9.html#bias-variance-tradeoff",
    "title": "TICS-411 Minería de Datos",
    "section": "Bias Variance Tradeoff",
    "text": "Bias Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos puntos azules serán puntos que usaremos para entrenar.\nLos puntos verdes serán puntos que usaremos para validar."
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff-bias",
    "href": "tics411/clase-9.html#bias-variance-tradeoff-bias",
    "title": "TICS-411 Minería de Datos",
    "section": "Bias Variance Tradeoff: Bias",
    "text": "Bias Variance Tradeoff: Bias\n\nBias\n\n\nSe refiere a la incapacidad de un modelo de capturar la verdadera relación entre los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo está “sesgado” a tomar una cierta relación que no necesariamente existe."
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff-variance",
    "href": "tics411/clase-9.html#bias-variance-tradeoff-variance",
    "title": "TICS-411 Minería de Datos",
    "section": "Bias Variance Tradeoff: Variance",
    "text": "Bias Variance Tradeoff: Variance\n\nVariance\n\n\nSe refiere a la diferencia de ajuste entre datasets (Train y Validación).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo varía demasiado su comportamiento entre Training y Testing Time."
  },
  {
    "objectID": "tics411/clase-9.html#complejidad-de-un-modelo-1",
    "href": "tics411/clase-9.html#complejidad-de-un-modelo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Complejidad de un Modelo",
    "text": "Complejidad de un Modelo\n\n\nOverfitting\n\n\n\n\n\n\n\n\n\n\n\n\nRegularización: Se refiere a una penalización para disminuir su complejidad.\n\nModelos más simples: Utilizar modelos con una Frontera de Decisión más simple.\nMás datos!!! Más datos más dificil aprender, por lo tanto, modelos complejos se ven más beneficiados de esto.\n\n\n\n\n\n\nUnderfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuitar Regularización\nModelos más complejos\nMás variabilidad en los datos!!! Podría ser que los datos no permitan aprender patrones más complejos."
  },
  {
    "objectID": "tics411/clase-9.html#cómo-generamos-sets-de-validación",
    "href": "tics411/clase-9.html#cómo-generamos-sets-de-validación",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo generamos sets de Validación?",
    "text": "¿Cómo generamos sets de Validación?\n\nLa evaluación de modelos supervisados es fundamental. De no hacerlo de forma correcta podemos quedarnos con una idea muy equivocada del rendimiento del modelo.\n\n\nCross Validation (Validación Cruzada)\n\n\nSe debe evaluar el rendimiento de un modelo en un dataset diferente al que fue entrenado. Esta es la única manera en la que se puede medir el poder de generalización de un modelo.\n\n\nGeneralización\n\n\nCorresponde a la habilidad de un modelo de adaptarse apropiadamente a datos no vistos previamente.\n\n\n\n\n\n\n\n\n\nUtilizar una estrategia incorrecta de Validación puede llevar a problemas de generalización. La estrategia de Validación debe ser lo más parecida posible a cómo se utilizará el modelo en Producción.\n\n\n\n\n\n\n\n\n\nPara esto se asume que todos los datos son i.i.d (independent and identically distributed). De no lograr esto, lograr buenos rendimientos es más difícil."
  },
  {
    "objectID": "tics411/clase-9.html#validación-cruzada-holdout",
    "href": "tics411/clase-9.html#validación-cruzada-holdout",
    "title": "TICS-411 Minería de Datos",
    "section": "Validación Cruzada: Holdout",
    "text": "Validación Cruzada: Holdout\n\nTambién es conocido como Train Test Split o simplemente Split. Corresponde a la separacion de nuestra data cuando con el proposito de aislar observaciones que el modelo no vea para una correcta evaluación.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set es la porción de los datos que se utilizará exclusivamente para entrenar los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl test set es la porción de los datos que se utilizará exclusivamente para validar los datos.\nEl test set simula los datos que eventualmente entrarán el modelo para obtener una predicción.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utilizan splits del tipo 70/30, 80/20 o 90/10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el problema con este tipo de validación?"
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout",
    "href": "tics411/clase-9.html#variante-holdout",
    "title": "TICS-411 Minería de Datos",
    "section": "Variante Holdout",
    "text": "Variante Holdout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe agrega un validation set el cuál se utilizará para escoger los hiperparámetros que muestren un mejor poder de generalización.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set y el test set cumplen la misma función que tenían antes."
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout-procedimiento",
    "href": "tics411/clase-9.html#variante-holdout-procedimiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Variante Holdout: Procedimiento",
    "text": "Variante Holdout: Procedimiento\n\n\n\n\n\n\n\n\nProcedimiento\n\nRepetir para cada Modelo a probar.\n\n\n\n\n\n\n\n\nVamos a entender un modelo como la combinación de un Algoritmo de Aprendizaje + Hiperparámetros + Preprocesamiento.\n\n\n\n\n\nSe entrena cada Modelo en el train set. Se mide una métrica de Evaluación apropiada utilizando el Validation Set. La llamaremos métrica de Validación.\nSe escoge el mejor Modelo como el que tenga la mejor métrica de Validación.\nSe reentrena el modelo escogido pero ahora en un “nuevo set” compuesto por el Train set + el Validation set.\nSe reporta el rendimiento final del mejor modelo (al momento del diseño) utilizando métricas medidas en el Test Set."
  },
  {
    "objectID": "tics411/clase-9.html#k-fold-cv",
    "href": "tics411/clase-9.html#k-fold-cv",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Fold CV",
    "text": "K-Fold CV\n\n\n\n\n\n\n\nEl proceso de Holdout podría llevar a un proceso de overfitting del Test Set si el modelo no es lo suficientemente robusto.\n\n\n\n\n\n\n\n\n\n\n\n\nEl K-Fold CV se aplica sólo al Train Set y la métrica final que se reporta utilizando el Test Set.\n\n\n\n\n\n\n\n\n\n\nFold\n\nEntenderemos Folds como divisiones que haremos a nuestro dataset. (En el ejemplo se divide el dataset en 5 Folds).\n\nSplit\n\nEntenderemos Splits, como iteraciones. En cada iteración utilizaremos un Fold como Validation Set y todos los Folds restantes como Train Set.\n\n\n\n\n\n\n\n\n\nLa métrica final se calculará como el promedio de las Métricas de Validación para cada Split.\nA veces la variabilidad (medido a través de la Desviación Estándar) también es usado como criterio para elegir el mejor modelo.\n\n\n\n\n\n\n\n\n\n\n\nEn la práctica se le llama incorrectamente Cross Validation al K-Fold."
  },
  {
    "objectID": "tics411/clase-9.html#bootstrap",
    "href": "tics411/clase-9.html#bootstrap",
    "title": "TICS-411 Minería de Datos",
    "section": "Bootstrap",
    "text": "Bootstrap\nConsiste en generar subgrupos aleatorios con repetición. Normalmente requiere específicar el tamaño de la muestra de entrenamiento. Y la cantidad de repeticiones que del proceso. Los sets de validación (en morado) acá se denominan out-of-bag samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa métrica final a reportar se mide como el promedio de los out-of-bag samples."
  },
  {
    "objectID": "tics411/clase-9.html#variantes-y-consejos",
    "href": "tics411/clase-9.html#variantes-y-consejos",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes y Consejos",
    "text": "Variantes y Consejos\n\nStratified K-Fold\n\nEs la variante más utilizada de K-Fold el cual genera los folds considerando que se mantenga la proporción de etiquetas en cada Fold.\n\nLeave One Out\n\nSería una variante con \\(K=n\\). Por lo tanto, el Validation Set tiene sólo una observación.\n\n\n\n\n\n\n\n\n\n¿Cuando usar cada uno?\n\n\n\nSi se tiene una cantidad de datos suficiente (normalmente tamaños muy grandes se prefiere) el Holdout.\n\nEntre más registros, menos % de Validation Set se deja.\n\nSi se requiere robustez, o hay Test sets que son muy variables se prefiere K-Fold.\n\nSi es que hay desbalance de clases, se prefiere la versión Stratified.\n\n\nSi se tienen muy pocos datos, entonces utilizar Leave-One-Out.\nBootstrap también es utilizado cuando se tengan pocos datos. Aunque suele ser un approach más estadístico."
  },
  {
    "objectID": "tics411/clase-9.html#baseline",
    "href": "tics411/clase-9.html#baseline",
    "title": "TICS-411 Minería de Datos",
    "section": "Baseline",
    "text": "Baseline\n\nUn modelo Baseline es un modelo simple, normalmente sin aprendizaje asociado o con poder de aprendizaje más limitado, el cuál será utilizado como medida de referencia para ver si algoritmos más complejos efectivamente están aprendiendo.\n\n\n\n\n\n\n\nSi estamos probando un nuevo modelo y éste es capaz de superar el rendimiento de un Baseline, se considera como que estamos aprendiendo algo nuevo.\n\n\n\n\n\n\n\n\n\nModelos que no superaron el puntaje de un modelo Baseline normalmente son deshechados."
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python-baselines",
    "href": "tics411/clase-9.html#implementación-en-python-baselines",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Baselines",
    "text": "Implementación en Python: Baselines\nfrom sklearn.dummy import DummyClassifier\n\ndc = DummyClassifier(strategy=\"prior\", random_state = 42, constant=None)\ndc.fit(X_train,y_train)\ny_pred = dc.predict(X_test)\n\n\n\nstrategy: Corresponde a estrategias “dummy” con las cuales generar predicciones.\n\n“prior”: predice siempre la clase más frecuente observada en el entrenamiento. Si se predice la probabilidad, se devuelve la probabilidad empírica.\n“constant”: Devuelve un valor constante provisto por el usuario.\n“uniform”: Predice probabilidades aleatorios obtenidas mediante una distribución uniforme."
  },
  {
    "objectID": "tics411/clase-9.html#data-leakage",
    "href": "tics411/clase-9.html#data-leakage",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Leakage",
    "text": "Data Leakage\n\nFuga de Datos\n\n\nSe refiere al proceso donde el modelo por alguna razón conoce información que no debería conocer. Puede ser información del Test Set o variables que revelan información primordial sobre la etiqueta.\n\n\n\n\n\n\n\n\n\nCuando existe Data Leakage es posible que los resultados del modelo no reflejen correctamente su rendimiento dando una falsa sensación de optimismo.\n\n\n\nEjemplos\n\nEstandarizar o aplicar preprocesamientos antes del Split de la Data.\nUtilizar variables que tienen directa relación con el Target.\n\n\n\n\n\n\n\n\nSe recomienda siempre que sea posible utilizar Pipelines para poder evitar el Data Leakage."
  },
  {
    "objectID": "tics411/clase-6.html#evaluación",
    "href": "tics411/clase-6.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\nPensemos en la Evaluación como una medida de desempeño el cuál “evalúa” qué tan bien realizado está el clustering. El objetivo principal del Clustering debe ser siempre la generación de clusters compactos que estén diferenciados los unos a los otros.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el Clustering que mejor describe el problema."
  },
  {
    "objectID": "tics411/clase-6.html#objetivos-de-la-evaluación",
    "href": "tics411/clase-6.html#objetivos-de-la-evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivos de la Evaluación",
    "text": "Objetivos de la Evaluación"
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins",
    "href": "tics411/clase-6.html#tendencia-hopkins",
    "title": "TICS-411 Minería de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins\n\nEstadístico Hopkins\n\n\nPermite evaluar a priori si es que efectivamente existen clusters antes de aplicar un algoritmo.\n\n\n\n\n\n\\[H = \\frac{\\sum_{i = 1}^p w_i}{\\sum_{i = 1}^p u_i + \\sum_{i = 1}^p w_i}\\]\n\n\\(w_i\\): corresponde a la distancia de un punto aleatorio al vecino más cercano en los datos originales.\n\\(u_i\\): corresponde a la distancia de un punto real del dataset al vecino más cercano.\n\\(p\\): Número de puntos generados en el espacio del Dataset.\n\n\nfrom pyclustertend import hopkins\n\n1-hopkins(X, p)\n\n\n\n\n\n\n\nX: Dataset al cuál se le aplica el Estadístico.\np: Número de Puntos para el cálculo.\n\n\n\n\n\n\n\n\n\n\npyclustertend entrega el valor 1-H."
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins-1",
    "href": "tics411/clase-6.html#tendencia-hopkins-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins"
  },
  {
    "objectID": "tics411/clase-6.html#cálculo-hopkins-ejemplo-p2",
    "href": "tics411/clase-6.html#cálculo-hopkins-ejemplo-p2",
    "title": "TICS-411 Minería de Datos",
    "section": "Cálculo Hopkins: Ejemplo p=2",
    "text": "Cálculo Hopkins: Ejemplo p=2\n\n\n\n\n\n\n\nPuntos obtenidos de los Datos\n\\[u_1\\approx 0\\]\n\\[u_2\\approx 0\\]\n\nPuntos Aleatorios en el Espacio de los Datos\n\\[w_1\\approx 1.8\\]\n\\[w_2\\approx 1.12\\]\n\nCálculo Hopkins\n\\[ H = \\frac{w_1 + w_2}{u_1 + u_2 + w_1 + w_2}\\] \\[ H = \\frac{1.8 + 1.12}{0 + 0 + 1.8 + 1.8} = 1\\]"
  },
  {
    "objectID": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "href": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "title": "TICS-411 Minería de Datos",
    "section": "Visual Assesment of Tendency (VAT)",
    "text": "Visual Assesment of Tendency (VAT)\n\nCorresponde a una inspección visual de la distancia entre los puntos (matriz de distancia). Colores más oscuros indican menor distancias entre dichos puntos lo que indica mayor cohesión.\n\n\n\n\n\n\n\n\n\nSe pueden ver claramente dos bloques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo es posible ver bloques importantes.\n\n\n\n\n\n\n\n\n\nfrom pyclustertend import vat\n\nvat(X)"
  },
  {
    "objectID": "tics411/clase-6.html#correlación",
    "href": "tics411/clase-6.html#correlación",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación",
    "text": "Correlación\nProcedimiento:\n\nConstruir una matriz de similaridad entre todos los puntos de la siguiente manera:\n\n\\[s(i,j) = \\frac{1}{d(i,j) + 1}\\]\n\nConstruir una matriz de similaridad \"ideal\" basada en la pertenencia a un Cluster.\nSi \\(i\\) y \\(j\\) pertenecen al mismo cluster entonces \\(s(i,j)=1\\), en otro caso \\(s(i,j) = 0\\)\nCalcular la Correlación entre la matriz de similaridad y la matriz ideal (obtenidas en los pasos 1 y 2).\n\n\n\n\n\n\n\nUna correlación alta indica que los puntos que están en el mismo cluster son cercanos entre ellos."
  },
  {
    "objectID": "tics411/clase-6.html#cohesión",
    "href": "tics411/clase-6.html#cohesión",
    "title": "TICS-411 Minería de Datos",
    "section": "Cohesión",
    "text": "Cohesión\n\nCohesión\n\n\nMide cuán cercanos están los objetos dentro de un mismo cluster. Se utiliza la Suma de los Errores al Cuadrado, que es equivalente a la Inercia de K-Means (o Within Cluster).\n\n\n\n\\[ SSE_{total} = \\sum_{k = 1}^K\\sum_{x_i \\in C_k} (x_i - \\bar{C_k})^2\\]\n\n\\(C_k\\) corresponde al Centroide del Cluster \\(k\\). Dicho centroide puede ser calculado como la media/mediana de todos los puntos del Centroide.\n\\(K\\) corresponde al Número de Clusters.\n\n\n\n\n\n\n\n\nNo me gusta mucho este nombre, porque en realidad es como un inverso de la Cohesión."
  },
  {
    "objectID": "tics411/clase-6.html#separación",
    "href": "tics411/clase-6.html#separación",
    "title": "TICS-411 Minería de Datos",
    "section": "Separación",
    "text": "Separación\n\nSeparación\n\n\nMide cuán distinto es un cluster de otro. Se usa la suma de las distancias al cuadrado entre los centroides hacia el promedio de todos los puntos. (Between groups sum squares, SSB).\n\n\n\n\\[ SSB_{total} = \\sum_{k = 1}^K |C_k|(\\bar{X} - \\bar{C_k})^2\\]\n\n\\(|C_k|\\) corresponde al número de elementos (Cardinalidad) del Cluster \\(i\\).\n\\(\\bar{X}\\) corresponde al promedio de todos los puntos."
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "title": "TICS-411 Minería de Datos",
    "section": "Coeficiente de Silhouette (Coeficiente de Silueta)",
    "text": "Coeficiente de Silhouette (Coeficiente de Silueta)\n\nEl coeficiente de Silhouette es otra medida que combina la cohesión y la separación. Los valores varían entre -1 y 1, donde valores cercanos a 1 representan una mejor agrupación.\n\n\n\n\n\n\n\nValores cercanos a \\(-1\\) representan que el punto está incorrectamente asignado a un cluster.\n\n\n\n\\[S_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}\\]\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X, labels, sample_size = None, metric=\"euclidean\")"
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Coeficiente de Silhouette: Ejemplo",
    "text": "Coeficiente de Silhouette: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[C_{silueta} = \\frac{1}{n}\\sum_{i} s_i\\]\n\n\\(a_i\\): Distancia promedio del punto \\(i\\) a todos los otros puntos del mismo cluster. (Cohesión)\n\\(b_{ij}\\): Distancia promedio del punto \\(i\\) a todos los puntos del cluster \\(j\\) donde no pertenezca \\(i\\). (Separación)\n\\(b_j\\): Mínimo de \\(b_{ij}\\) tal que el punto i no pertenezca al cluster \\(j\\). (Menor Separación)"
  },
  {
    "objectID": "tics411/clase-6.html#ejercicio-propuesto",
    "href": "tics411/clase-6.html#ejercicio-propuesto",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejercicio Propuesto",
    "text": "Ejercicio Propuesto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio Propuesto\n\n\nCalcule el coeficiente de Silueta. Tabla de resultado al final de las Slides."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta",
    "href": "tics411/clase-6.html#curvas-de-silueta",
    "title": "TICS-411 Minería de Datos",
    "section": "Curvas de Silueta",
    "text": "Curvas de Silueta\nEs común mostrar los resultados del coeficiente de silueta como gráficos de este estilo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblemas\n\n\n\nSiluetas negativas.\nClusters bajo el promedio.\nMucha variabilidad de Silueta en un sólo cluster."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta-implementación",
    "href": "tics411/clase-6.html#curvas-de-silueta-implementación",
    "title": "TICS-411 Minería de Datos",
    "section": "Curvas de Silueta: Implementación",
    "text": "Curvas de Silueta: Implementación\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\nskplt.metrics.plot_silhouette(X, labels, metric=\"euclidean\", title=\"Silhouette Analysis\")\nplt.show()\n\nL1-2: Importación de Librerías Necesarias. Esta implementación está en la librería Scikit-plot. (Para instalar pip install scikit-plot)\nX: Dataset usado para el clustering.\nlabels : etiquetas obtenidos de algún proceso de Clustering.\nmetric: Métrica a utilizar, por defecto usa “euclidean”.\ntitle: Se puede agregar un Título personalizado a la curva."
  },
  {
    "objectID": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "href": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "title": "TICS-411 Minería de Datos",
    "section": "Resultados Ejercicio Propuesto",
    "text": "Resultados Ejercicio Propuesto\n\n\n\n\n\nCoeficiente de Silhouette = 0.6148\n\n\n\n\n\n\nComprobar utilizando Scikit-Learn\n\n\n\n\n\nTics-411 Minería de Datos está licenciado bajo CC BY-NC-SA 4.0"
  },
  {
    "objectID": "tics411/clase-5.html#clustering-densidad",
    "href": "tics411/clase-5.html#clustering-densidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral.\n\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso utilizaremos DBSCAN (Density-Based Spatial Clustering Applications with Noise)."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-definiciones",
    "href": "tics411/clase-5.html#dbscan-definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Definiciones",
    "text": "DBSCAN: Definiciones\n\n\n\n\n\n\nHiperparámetros del Modelo\n\n\n\neps: Radio de análisis\nMinPts: Corresponde al mínimo de puntos necesarios en un Radio eps.\n\n\n\n\n\n\n\nDensidad\n\n\nDensidad es el número de puntos dentro del radio eps.\n\n\nCore Point/Punto Central\n\n\nUn punto central/core es aquel que tiene al menos MinPts puntos dentro de la esfera definida por eps (se incluye él mismo).\n\n\n\n\n\nBorder Point/Punto Borde\n\n\nUn punto de borde tiene menos puntos que MinPts del eps, pero está dentro de la esfera de un punto central.\n\n\nNoise Point/Punto Ruido\n\n\nUn punto de ruido es todo aquel que no es punto central ni de borde."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-algoritmo-categorización-de-puntos",
    "href": "tics411/clase-5.html#dbscan-algoritmo-categorización-de-puntos",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Algoritmo categorización de puntos",
    "text": "DBSCAN: Algoritmo categorización de puntos\n\nPrimeramente se aplica un algoritmo para categorizar cada punto de acuerdo a las definiciones anteriores.\n\n\nPara cada punto en el espacio:\n\nCalcular su densidad en EPS y aplicar el siguiente algoritmo:"
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-1",
    "href": "tics411/clase-5.html#ejemplo-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 1",
    "text": "Ejemplo: Iteración 1\n\nSupongamos un ejemplo con \\(MinPts=4\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Core Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-2",
    "href": "tics411/clase-5.html#ejemplo-iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 2",
    "text": "Ejemplo: Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Border Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-3",
    "href": "tics411/clase-5.html#ejemplo-iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 3",
    "text": "Ejemplo: Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-final",
    "href": "tics411/clase-5.html#ejemplo-iteración-final",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración Final",
    "text": "Ejemplo: Iteración Final\n\n\n\n\n\n\n\n\n\n\n\nAhora, ¿Cómo definimos que partes son clusters o no?"
  },
  {
    "objectID": "tics411/clase-5.html#algoritmo-de-clustering",
    "href": "tics411/clase-5.html#algoritmo-de-clustering",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo de Clustering",
    "text": "Algoritmo de Clustering\nSe aplica el siguiente algoritmo para calcular clusterings.\n\n\n\n\n\n\nAntes de aplicar se desechan los Noise Points ya que no serán considerados. (Veremos luego que ocurre con estos puntos).\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label"
  },
  {
    "objectID": "tics411/clase-5.html#iteración-1",
    "href": "tics411/clase-5.html#iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 1",
    "text": "Iteración 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nTodos los puntos cercanos a un Core reciben la misma etiqueta."
  },
  {
    "objectID": "tics411/clase-5.html#iteración-2",
    "href": "tics411/clase-5.html#iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 2",
    "text": "Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n## label ya está en 1\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nEn este caso obtuvimos 2 clusters, e indirectamente un 3er de puntos ruido."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan",
    "href": "tics411/clase-5.html#dbscan",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\n\n\n\n\n\n\n\n\n\n\n¿Sería posible replicar un proceso de Clustering similar utilizando K-Means? ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-detalles-técnicos",
    "href": "tics411/clase-5.html#dbscan-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Detalles Técnicos",
    "text": "DBSCAN: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nResistente al ruido.\nPuede lidiar con clusters de diferentes formas y tamaños.\nNo es necesario especificar cuántos clusters encontrar.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAlgoritmo de alta complejidad computacional que puede llegar \\(O(n^2)\\) en el peor caso.\nSe ve afectado por densidad de los datos y por datos con una alta dimensionalidad.\nSu óptimo resultado depende específicamente de sus Hiperparámetros.\nNo puede generalizar en datos no usados en entrenamiento."
  },
  {
    "objectID": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros",
    "href": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo encontrar los Hiperparámetros?",
    "text": "¿Cómo encontrar los Hiperparámetros?\n\n\n\n\n\n\n\n\nminPts\n\n\nPara datasets multidimensionales grandes, la regla es:\n\\[minPts \\ge dim + 1\\]\n\n\n\n\n\n\n\n\n\nOtras recomendaciones:\n\n\n\nPara dos dimensiones: \\(minPts=4\\) (Ester et al., 1996)\nPara más de 2 dimensiones: \\(minPts = 2 \\cdot dim\\) (Sander et al., 1998)"
  },
  {
    "objectID": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros-1",
    "href": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros-1",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo encontrar los Hiperparámetros?",
    "text": "¿Cómo encontrar los Hiperparámetros?\n\nPara encontrar EPS se suele utilizar el método de Vecinos más cercanos.\n\n\n\nIdea\n\nLa distancia de los puntos dentro de un cluster a su k-ésimo vecino deberían ser similares.\nLuego, los puntos atípicos (o ruidosos) tienen el k-ésimo vecino a una mayor distancia.\n\n\n\n\n\n\n\n💡 Podemos plotear la distancia ordenada de cada punto a su k-ésimo vecino y seleccionar un eps cercano al crecimiento exponencial (codo)."
  },
  {
    "objectID": "tics411/clase-5.html#implementación-en-scikit-learn",
    "href": "tics411/clase-5.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples = 5, eps = 0.5, metric = \"euclidean\")\n\n## Se entrena y se genera la predicción\ndbs.fit_predict(X)\n\n\nmin_samples: Corresponde a minPts. Por defecto 5.\neps: Corresponde al radio de la esfera en la que se buscan los puntos cercanos. Por defecto 0.5.\nmetric: Corresponde a la distancia utilizada para medir la distancia. Permite todas las distancias mencionadas acá.\n.fit_predict(): Entrenará el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento. Adicionalmente los puntos ruidosos se etiquetarán como -1.\n\n\n👀 Veamos un ejemplo."
  },
  {
    "objectID": "tics411/clase-2.html#eda",
    "href": "tics411/clase-2.html#eda",
    "title": "TICS-411 Minería de Datos",
    "section": "EDA",
    "text": "EDA\n\nEl Analisis Exploratorio de Datos (EDA, por sus siglas en inglés) es procedimiento en el cual se analiza un dataset para explorar sus características principales.\n\n\nSu objetivo principal es poder familiarizarse con los datos además de encontrar potenciales problemas en su calidad.\nPrincipalmente hace uso de técnicas de manipulación de datos y visualizaciones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos hallazgos importantes dentro del proceso se les denomina insights.\n\n\n\n\n\n\n\n\n\nEl uso de visualizaciones inadecuadas podría llevar a conclusiones erróneas.\n\n\n\n\n\n\n\n\n\n\nSummary.\nVisualización."
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-tendencia-central",
    "href": "tics411/clase-2.html#medidas-de-tendencia-central",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas de Tendencia Central",
    "text": "Medidas de Tendencia Central"
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-dispersión-y-asimetría",
    "href": "tics411/clase-2.html#medidas-de-dispersión-y-asimetría",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas de Dispersión y Asimetría",
    "text": "Medidas de Dispersión y Asimetría"
  },
  {
    "objectID": "tics411/clase-2.html#eda-visualización",
    "href": "tics411/clase-2.html#eda-visualización",
    "title": "TICS-411 Minería de Datos",
    "section": "EDA: Visualización",
    "text": "EDA: Visualización\n\nLa visualización de datos es la presentación de datos en forma gráfica. Permite simplificar conceptos más complejos en especial a altos mandos.\n\n\nGracias a la evolución del cerebro humano somos capaces de detectar patrones complejos en la naturaleza a partir de la Visión.\n\n\n\n\n\n\n\n\nPuede ser difícil de aplicar si el tamaño de los datos es grande (sea en instancias o atributos). Por ejemplo, si los datos están en 4 dimensiones.\n\n\n\n\n\n\n\n\n\n\n\n\nSe suelen resumir los datos en estadísticas simples.\nGraficar datos en 1D, 2D y 3D (evitar dentro de lo posible).\nLa visualización debe ser comprensible ojalá sin ninguna explicación.\n\n\n\n\n\n\n\n\n\n\n\n\nEn caso de datos de alta dimensionalidad puede ser una buena idea reducir dimensiones mediante técnicas como:\n\nPCA\nUMAP\netc."
  },
  {
    "objectID": "tics411/clase-2.html#caso-de-visualización",
    "href": "tics411/clase-2.html#caso-de-visualización",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso de Visualización",
    "text": "Caso de Visualización\n\n\n\n\n\n\n\nFiguras\nEscala de Colores.\nTamaño de los puntos.\nDemasiada información en un sólo gráfico.\nNo se entiende el mensaje."
  },
  {
    "objectID": "tics411/clase-2.html#canales-visuales",
    "href": "tics411/clase-2.html#canales-visuales",
    "title": "TICS-411 Minería de Datos",
    "section": "Canales Visuales",
    "text": "Canales Visuales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe les llama canales visuales a elementos visuales que pueden utilizarse para expresar información (Clase Visualizacion Andreas Mueller).\nLa idea es poder mapear cada uno de estos canales a valores que queremos visualizar.\n\n\n\n\n\n\n\n\n\n\n\nNo todos los canales son igual de útiles ni fáciles de entender."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nHistograma\n\n\nEl histograma permite visualizar distribuciones univariadas acumulando los datos en rangos de igual tamaño (bins).\n\n\n\n\nPermite visualizar el centro, la extensión, la asimetría y outliers.\n\n\n\n\n\n\n\n\nEl histograma puede ser “engañoso” para conjuntos de datos pequeños.\nLa visualización puede resultar de manera muy distintas dependiendo del número de bins."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nKernel Density\n\n\nCorresponde a un suavizamiento de un Histograma en el cuál se usa un Kernel (función no negativa que suma 1 y tiene media 0) para agrupar los puntos vecinos.\n\n\n\n\n\nLa función estimada es:\n\\[f(x) = \\frac{1}{n} = \\sum_{i=1}^n K \\left(\\frac{x - x(i)}{h}\\right)\\]\n\n\\(K(u)\\) es el Kernel.\n\\(h\\) es el ancho de banda."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nBoxplot (Caja y Bigotes)\n\nEs un tipo de gráfico que muestra la distribución de manera univariada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiene la capacidad de mostrar varias distribuciones a la vez.\nAdemás presenta estadísticos de interés: Mediana, IQR y outliers.\nLos puntos fuera de los bigotes son considerados Outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos bigotes pueden representar:\n\nMínimo y Máximo. (En este caso no hay outliers).\n\\(\\mu \\pm 3\\sigma\\)\nPercentiles 5 y 95.\nOtros valores."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-barras",
    "href": "tics411/clase-2.html#visualizaciones-barras",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Barras",
    "text": "Visualizaciones: Barras\n\nBar Plot\n\n\nLa altura de la barra (normalmente Eje y) representa una agregación asociada a una categoría (normalmente Eje x).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOtras convenciones llaman a este gráfico Column Plot, mientras que el Bar Plot tiene las barras de manera horizontal."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-puntos",
    "href": "tics411/clase-2.html#visualizaciones-puntos",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Puntos",
    "text": "Visualizaciones: Puntos\n\nScatter\n\n\nGráfico empleado para mostrar distribución de datos bivariados\n\n\n\n\nMuestra la relación entre una variable independiente (Eje X) y una variable dependiente (Eje Y).\nPermite mostrar relaciones lineales o no-lineales (Correlaciones).\nOutliers.\nSimplemente ubicación de Puntos en el Espacio."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-líneas",
    "href": "tics411/clase-2.html#visualizaciones-líneas",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Líneas",
    "text": "Visualizaciones: Líneas\n\nLineplot\n\n\nGráfico empleado para visualizar tendencias y su evolución de una medida (Eje Y) en el tiempo (Eje X).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi bien es posible utilizarlo para gráficar dos medidas continuas, las buenas prácticas indican que el eje X siempre debería contener una componente temporal."
  },
  {
    "objectID": "tics411/clase-2.html#estadísticos-vs-visualizaciones",
    "href": "tics411/clase-2.html#estadísticos-vs-visualizaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Estadísticos vs Visualizaciones",
    "text": "Estadísticos vs Visualizaciones"
  },
  {
    "objectID": "tics411/clase-2.html#otras-visualizaciones",
    "href": "tics411/clase-2.html#otras-visualizaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Otras Visualizaciones?",
    "text": "¿Otras Visualizaciones?"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html",
    "href": "tics411/notebooks/04-analisis_centros.html",
    "title": "Análisis de Centros",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\ndf = sns.load_dataset(\"iris\")\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(df.drop(columns=\"species\"))\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nlabels = km.fit_predict(df.drop(columns=\"species\"))\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\nmean_table, std_table = create_tables(\n    df,\n    labels,\n    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n)\n## Corresponde a los valores promedios de cada variable por Cluster (los Centroides)\nmean_table\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n5.901613\n2.748387\n4.393548\n1.433871\n\n\n1\n5.006000\n3.428000\n1.462000\n0.246000\n\n\n2\n6.850000\n3.073684\n5.742105\n2.071053\n## Corresponde a la Desviación Estándar de cada variable por Cluster\nstd_table\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n0.466410\n0.296284\n0.508895\n0.297500\n\n\n1\n0.352490\n0.379064\n0.173664\n0.105386\n\n\n2\n0.494155\n0.290092\n0.488590\n0.279872"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html#representación-gráfica",
    "href": "tics411/notebooks/04-analisis_centros.html#representación-gráfica",
    "title": "Análisis de Centros",
    "section": "Representación Gráfica",
    "text": "Representación Gráfica\nAcá les dejo una Función con la cual pueden realizar el Análisis de Centros. Para ello requieren un DataFrame que contenga las variables a analizar y su etiqueta.\nSe debe indicar, el df, el número de Clusters creados, la columna de la etiqueta, y las columnas a analizar. Adicionalmente se puede agregar un título y cambiar las dimensiones del gráfico.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i].title())\n    plt.suptitle(title, fontsize=15)\n    plt.show()\n\n\ncolumns = df.drop(columns=[\"species\", \"labels\"]).columns.tolist()\ncenter_analysis_viz(\n    df,\n    n_clusters=3,\n    labels=labels,\n    columns=columns,\n    title=\"Análisis de Centros para Iris\",\n)"
  },
  {
    "objectID": "tics411/notebooks/legacy_code.html",
    "href": "tics411/notebooks/legacy_code.html",
    "title": "Clases UAI",
    "section": "",
    "text": "## Otra forma de calcular lo mismo pero mucho más ineficiente. No usar!!\n# def compute_ideal_sim(labels):\n#     labels = pd.Series(labels, name=\"labels\")\n#     labels_df = labels.to_frame().reset_index()\n#     return (\n#         labels_df.merge(labels_df, how=\"outer\", on=\"labels\")\n#         .add(1)\n#         .set_index([\"index_x\", \"index_y\"])\n#         .unstack(level=1)\n#         .fillna(0)\n#         .astype(bool)\n#         .astype(int)\n#     )\n\n\n# ideal_sim_pd = compute_ideal_sim(labels).to_numpy()\n\n\n## Es para demostrar que dan lo mismo.\n# np.array_equal(ideal_sim_np, ideal_sim_pd)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/14-ex-NB.html",
    "href": "tics411/notebooks/14-ex-NB.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\"]]\ny = df.survived\n\n\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\n\n\ndef make_pipeline(parameters):\n    if parameters[\"sc_variables\"] == \"all\":\n        scaler = StandardScaler()\n    elif parameters[\"sc_variables\"] is not None:\n        scaler = SklearnTransformerWrapper(\n            StandardScaler(), variables=parameters[\"sc_variables\"]\n        )\n    else:\n        scaler = \"passthrough\"\n\n    if parameters[\"num_method\"] is None:\n        num_imp = \"passthrough\"\n    else:\n        num_imp = MeanMedianImputer(\n            imputation_method=parameters[\"num_method\"]\n        )\n\n    if parameters[\"cat_method\"] is None:\n        cat_imp = \"passthrough\"\n    else:\n        cat_imp = CategoricalImputer(\n            imputation_method=parameters[\"cat_method\"]\n        )\n\n    print(\n        f\"Entrenamiento para Naive Bayes y threshold = {parameters['threshold']}\"\n    )\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\"cat_imp\", cat_imp),\n            (\"num_imp\", num_imp),\n            (\"ohe\", parameters[\"encoder\"]),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    parameters[\"model\"],\n                    threshold=parameters[\"threshold\"],\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=MultinomialNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7844311377245509\nTest Accuracy 0.757847533632287\n===================================\nTrain Precision 0.7137254901960784\nTest Precision 0.6732673267326733\n===================================\nTrain Recall 0.7193675889328063\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7784431137724551\nTest Accuracy 0.7488789237668162\n===================================\nTrain Precision 0.6996197718631179\nTest Precision 0.6601941747572816\n===================================\nTrain Recall 0.7272727272727273\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=\"all\",\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7784431137724551\nTest Accuracy 0.7488789237668162\n===================================\nTrain Precision 0.6996197718631179\nTest Precision 0.6601941747572816\n===================================\nTrain Recall 0.7272727272727273\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"age\", \"fare\"]]\ny = df.survived\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=\"mean\",\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=MultinomialNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.6811377245508982\nTest Accuracy 0.7309417040358744\n===================================\nTrain Precision 0.6063829787234043\nTest Precision 0.7230769230769231\n===================================\nTrain Recall 0.4505928853754941\nTest Recall 0.5280898876404494\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=\"mean\",\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7844311377245509\nTest Accuracy 0.7623318385650224\n===================================\nTrain Precision 0.7056603773584905\nTest Precision 0.6730769230769231\n===================================\nTrain Recall 0.7391304347826086\nTest Recall 0.7865168539325843\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html",
    "href": "tics411/notebooks/07-ex-evaluation.html",
    "title": "Evaluación de Clusters",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pyclustertend import vat, hopkins, ivat\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nnp.random.seed(0)\n\nset_config(transform_output=\"pandas\")\n\nX = sns.load_dataset(\"iris\").drop(columns=\"species\")\nX_random = np.random.rand(150, 4)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#visualización-de-ambos-datasets",
    "href": "tics411/notebooks/07-ex-evaluation.html#visualización-de-ambos-datasets",
    "title": "Evaluación de Clusters",
    "section": "Visualización de ambos Datasets",
    "text": "Visualización de ambos Datasets\n\n!pip install pyclustertend\n\nRequirement already satisfied: pyclustertend in /home/datacuber/miniconda3/lib/python3.9/site-packages (1.8.2)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.1.2 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.4.1.post1)\nRequirement already satisfied: numba&lt;0.55.0,&gt;=0.54.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (0.54.1)\nRequirement already satisfied: matplotlib&lt;4.0.0,&gt;=3.3.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (3.7.5)\nRequirement already satisfied: numpy==1.20.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.20.3)\nRequirement already satisfied: pandas&lt;2.0.0,&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.5.3)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (9.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (23.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (0.11.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.2.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (6.4.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (4.37.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.4.4)\nRequirement already satisfied: setuptools in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (67.5.1)\nRequirement already satisfied: llvmlite&lt;0.38,&gt;=0.37.0rc1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (0.37.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pandas&lt;2.0.0,&gt;=1.2.0-&gt;pyclustertend) (2022.2.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (3.1.0)\nRequirement already satisfied: scipy&gt;=1.6.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.3.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.11.0)\nRequirement already satisfied: six&gt;=1.5 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.16.0)\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X)\npca = PCA(n_components=2)\npca_random = pca.fit_transform(X_random)\n\n\ndef compute_hopkins(X, p):\n    h_s = 1 - hopkins(X, p)\n    print(f\"Hopskins para p={p} es: {h_s}\")\n    return h_s\n\n\nhs_X = compute_hopkins(X, p=50)\nhs_random = compute_hopkins(X_random, p=50)\n\nfig, ax = plt.subplot_mosaic([[\"iris\", \"random\"]], figsize=(15, 6))\n\nax[\"iris\"].scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nax[\"random\"].scatter(pca_random[\"pca0\"], pca_random[\"pca1\"])\nax[\"random\"].set_title(\n    f\"Reducción a 2D de nuestros puntos aleatorios. H = {hs_random:.2f}\"\n)\nax[\"iris\"].set_title(f\"Reducción a 2D de Iris. H = {hs_X:.2f}\")\nplt.show()\n\nHopskins para p=50 es: 0.8241582644992403\nHopskins para p=50 es: 0.48048319214476964"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "title": "Evaluación de Clusters",
    "section": "VAT: Iris",
    "text": "VAT: Iris\n\nimport matplotlib.pyplot as plt\n\nvat(X_sc)\nplt.title(\"VAT para Iris Escalado\")\nivat(X_sc)\nplt.title(\"iVAT para Iris Escalado\")\n\nText(0.5, 1.0, 'iVAT para Iris Escalado')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "title": "Evaluación de Clusters",
    "section": "VAT: Random",
    "text": "VAT: Random\n\nvat(X_random)\nplt.title(\"VAT para Dataset Random\")\nivat(X_random)\nplt.title(\"iVAT para Dataset Random\")\n\nText(0.5, 1.0, 'iVAT para Dataset Random')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#correlación",
    "href": "tics411/notebooks/07-ex-evaluation.html#correlación",
    "title": "Evaluación de Clusters",
    "section": "Correlación",
    "text": "Correlación\n\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance_matrix\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X_sc)\n\n\ndef cluster_correlation(X, labels, p=2):\n    \"\"\"p corresponde al nivel de la distancia de Minkowski\"\"\"\n    ideal_sim = (labels == labels.reshape(-1, 1)).astype(np.float32)\n\n    d_matrix = distance_matrix(X, X, p=p)\n    S = 1 / (d_matrix + 1)\n    return np.corrcoef(S.flatten(), ideal_sim.flatten()).min()\n\n\ncluster_correlation(X_sc, labels)\n\n0.6856891998862197"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#cohesión-y-separación",
    "href": "tics411/notebooks/07-ex-evaluation.html#cohesión-y-separación",
    "title": "Evaluación de Clusters",
    "section": "Cohesión y Separación",
    "text": "Cohesión y Separación\n\ncenters = km.cluster_centers_\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nsse, ssb = compute_clustering_metrics(X_sc, labels, centers, is_df=True)\nsse, ssb\n\n(222.36170496502297, 377.638295034977)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "href": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "title": "Evaluación de Clusters",
    "section": "Ejemplo de Clases",
    "text": "Ejemplo de Clases\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        x=[2, 3, 4, 8, 9, 10, 6, 7, 8],\n        y=[5, 4, 6, 3, 2, 5, 10, 8, 9],\n        c=[0, 0, 0, 1, 1, 1, 2, 2, 2],\n    )\n)\n\nd_matrix = distance_matrix(df[[\"x\", \"y\"]], df[[\"x\", \"y\"]], p=2)\nplt.scatter(df.x, df.y, c=df.c, s=200, edgecolors=\"k\")\n\ndf\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n0\n2\n5\n0\n\n\n1\n3\n4\n0\n\n\n2\n4\n6\n0\n\n\n3\n8\n3\n1\n\n\n4\n9\n2\n1\n\n\n5\n10\n5\n1\n\n\n6\n6\n10\n2\n\n\n7\n7\n8\n2\n\n\n8\n8\n9\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(df[[\"x\", \"y\"]], df.c)\n\n0.614855027897113\n\n\n\n## Esta función se hizo sólo para mostrar los pasos intermedios\n## Usen esta función para revisar sus resultados cuando estudien para la prueba.\n\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, -1]\n    return a, b, bj, n_clusters\n\n\na, b, bj, n_clusters = silhouette_score_m(d_matrix, df.c.values)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.6395238095238095"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "href": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "title": "Evaluación de Clusters",
    "section": "Silhouette Curve",
    "text": "Silhouette Curve\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X_sc, labels)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/15-ex-LR.html",
    "href": "tics411/notebooks/15-ex-LR.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.survived\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nX_train.shape, X_test.shape\n\n((668, 5), (223, 5))\n\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\n\n\ndef make_pipeline(\n    num_method, cat_method, encoder, sc_variables, C, threshold=0.5\n):\n    scaler = SklearnTransformerWrapper(\n        StandardScaler(), variables=sc_variables\n    )\n\n    print(f\"Entrenamiento para C={C} y threshold = {threshold}\")\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", encoder),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    LogisticRegression(\n                        C=C, random_state=42, max_iter=10000\n                    ),\n                    threshold=threshold,\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    print(\"===================================\")\n    print(f\"Coeficientes: {model[-1].estimator_.coef_}\")\n    print(f\"Coeficientes: {model[-1].estimator_.intercept_}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\npipe = make_pipeline(\n    num_method=\"mean\",\n    cat_method=\"missing\",\n    encoder=OneHotEncoder(),\n    sc_variables=[\"age\", \"fare\"],\n    C=10,\n    threshold=0.6,\n)\n\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para C=10 y threshold = 0.6\n===================================\nTrain Accuracy 0.8068862275449101\nTest Accuracy 0.8026905829596412\n===================================\nTrain Precision 0.8444444444444444\nTest Precision 0.8082191780821918\n===================================\nTrain Recall 0.6007905138339921\nTest Recall 0.6629213483146067\n===================================\nCoeficientes: [[ 0.08468769 -0.35930833  0.91502373 -1.02074655  0.36843929 -1.13894327\n   1.40165975 -0.60754774  0.03083727 -0.10686131  0.94628825]]\nCoeficientes: [0.3325951]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportante, normalemente es buena idea escoger sólo una métrica. O darle más preponderancia a una métrica, ya que el mejor modelo puede ser muy distinto dependiendo de la métrica a utilizar.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/hopkins.html",
    "href": "tics411/notebooks/hopkins.html",
    "title": "Ejemplos de Hopkins",
    "section": "",
    "text": "from pyclustertend import hopkins\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef blobs_examples(\n    n_centers, cluster_std, n_samples=4000, p=100, center_box=(-10, 10)\n):\n    df_spread, labels = make_blobs(\n        n_samples=n_samples,\n        centers=n_centers,\n        n_features=2,\n        random_state=42,\n        center_box=center_box,\n        cluster_std=cluster_std,\n    )\n    df_spread = pd.DataFrame(df_spread, columns=[\"x\", \"y\"])\n    plt.scatter(df_spread.x, df_spread.y, c=labels)\n    plt.title(f\"H = {1-hopkins(df_spread, p)}\")\n    plt.tight_layout()\n\n\n## Una sola nube, muy compacta...\nblobs_examples(n_centers=1, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchos nubes muy compactos...\nblobs_examples(n_centers=5, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchas nubes extremadamente compactos\nblobs_examples(n_centers=5, cluster_std=0.001, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso utilizamos la función make_blobs para simular clusters ficticios. Los clusters siempre son esféricos, es por eso que el Hopkins tiende a dar valores bastante buenos. Aunque, dependiendo de qué tan compacto sea la nube tiende a 1 de manera muy fuerte.\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\n\ndef random_examples(n_samples=4000, p=100):\n    df_random = pd.DataFrame(\n        np.random.rand(n_samples, 2), columns=[\"x\", \"y\"]\n    )\n    plt.scatter(df_random.x, df_random.y)\n    plt.title(f\"H = {1-hopkins(df_random, p)}\")\n    plt.tight_layout()\n\n\n## Puntos más dispersos\nrandom_examples(n_samples=100, p=10)\n\n\n\n\n\n\n\n\n\n## Más denso, pero aún aleatorio...\nrandom_examples(n_samples=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso estamos usando np.random.rand para simular sólo valores aleatorios. Se puede ver que entre más lleno está el espacio, Hopkins tiende a 0.5.\n\n\n## valores uniformemente distribuidos y con poca tendencia a agruparse (normalmente pocos puntos)\n## tienen H más pequeños, pero es díficil obtenerlos...\nnp.random.seed(0)\n\n\ndef uniform_example(n_samples=10, max_val=10, p=10):\n    df_uniform = pd.DataFrame(\n        dict(\n            x=np.random.randint(0, max_val + 1, size=n_samples),\n            y=np.random.randint(0, max_val, size=n_samples),\n        )\n    )\n\n    plt.scatter(df_uniform.x, df_uniform.y)\n    plt.title(f\"H = {1-hopkins(df_uniform, p)}\")\n\n\nuniform_example(n_samples=11, max_val=10, p=10)\n\n\n\n\n\n\n\n\n\nuniform_example(n_samples=4000, max_val=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso también estamos forzando aleatoriedad pero con uniformidad de distancia. Para eso simulamos usando np.random.randint para generar valores aleatorios pero más o menos equiespaciados uniformemente. Es bien interesante este caso, porque si usamos muchos datos, se tiende a valores completamente aleatorios, es decir H \\(\\sim\\) 0.5.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html",
    "href": "tics411/notebooks/08-proyecto_clustering.html",
    "title": "Preparación de los Datos",
    "section": "",
    "text": "# En caso que de ejecutar esto en Colab, van a tener que instalar Scikit-Plot para poder ver la curva de Silhouette.\n#!pip install scikit-plot\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\nRANDOM_STATE = 0\nnp.random.seed(RANDOM_STATE)\nN = np.random.randint(5, 15, size=1)[0]\nn_samples = np.random.randint(100, 1000, size=N)\nX, _ = make_blobs(\n    n_samples=n_samples,\n    n_features=9,\n    cluster_std=2.5,\n    random_state=RANDOM_STATE,\n)\ndf = pd.DataFrame(X)\ndict_cat = {\n    0: \"Cat 1\",\n    1: \"Cat 2\",\n    2: \"Cat 3\",\n}\nrng = np.random.default_rng()\ndf[\"cat_var\"] = rng.choice(a=[0, 1, 2], size=len(df), p=[0.2, 0.3, 0.5])\ndf[\"cat_var\"] = df[\"cat_var\"].map(dict_cat)\n\ndf.columns = [f\"x{i}\" for i, _ in enumerate(df.columns, start=1)]\ndf[\"x1\"] += 100\ndf[\"x5\"] *= 327\ndf[\"x9\"] /= 15\n\ndf.to_csv(\"proyecto_clustering.csv\", index=False)\n## Acá comienza oficialmente el código.\ndf = pd.read_csv(\"proyecto_clustering.csv\")\ndf.dtypes.value_counts().plot(\n    kind=\"bar\", title=\"Tipos de Datos en el Dataset\", edgecolor=\"k\"\n)\nplt.tight_layout()\ndf.hist(figsize=(20, 6), edgecolor=\"k\", grid=False)\nplt.tight_layout()\ndf[\"x10\"].value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Distribución de las Variables Categóricas\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#variables-categóricas",
    "href": "tics411/notebooks/08-proyecto_clustering.html#variables-categóricas",
    "title": "Preparación de los Datos",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\ndummy_vars = ohe.fit_transform(df[[\"x10\"]])\n\nX = pd.concat([df.drop(columns=\"x10\"), dummy_vars], axis=1)\nX\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10_Cat 1\nx10_Cat 2\nx10_Cat 3\n\n\n\n\n0\n105.576134\n4.823419\n3.409904\n-11.687494\n-1532.613468\n-4.589218\n-6.854641\n-8.877022\n-0.449964\n0.0\n0.0\n1.0\n\n\n1\n100.479786\n-4.876628\n-5.404970\n6.932649\n-4092.341900\n12.163845\n-6.502116\n10.874025\n0.348683\n0.0\n0.0\n1.0\n\n\n2\n97.357744\n8.467431\n-0.865210\n4.353712\n1444.577125\n-1.992772\n-12.223474\n-9.100414\n0.407230\n0.0\n0.0\n1.0\n\n\n3\n95.857842\n5.931475\n0.278352\n3.413013\n1959.773064\n-10.248761\n-8.136656\n-9.158037\n0.478212\n0.0\n0.0\n1.0\n\n\n4\n99.772427\n-2.876912\n4.499859\n1.308382\n-2318.502069\n2.062409\n-13.469304\n-0.236395\n0.478002\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6418\n98.951606\n6.649525\n1.869195\n1.765821\n4348.322822\n-6.866256\n-1.578755\n-12.749590\n0.454056\n0.0\n0.0\n1.0\n\n\n6419\n94.996949\n-6.638457\n3.999433\n0.989885\n-1811.824888\n-0.859185\n-7.422772\n3.293839\n0.558469\n0.0\n0.0\n1.0\n\n\n6420\n95.495497\n6.664764\n0.019823\n1.825686\n2845.172238\n-7.376139\n-8.056029\n-10.066078\n0.224961\n0.0\n0.0\n1.0\n\n\n6421\n99.435967\n5.469512\n6.342347\n-1.182801\n-358.410366\n-1.205160\n2.248149\n6.840680\n0.748647\n0.0\n1.0\n0.0\n\n\n6422\n109.218858\n-6.367392\n-0.857113\n-6.749834\n1913.311965\n-4.103422\n0.343194\n-5.460592\n0.121518\n0.0\n0.0\n1.0\n\n\n\n\n6423 rows × 12 columns\n\n\n\n\npca = PCA(n_components=2, random_state=42)\npca_X = pca.fit_transform(X)\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nplt.title(\"Visualización PCA del Dataset\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "href": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "title": "Preparación de los Datos",
    "section": "K-Means",
    "text": "K-Means\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"Número de Clústers\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X, k_max=20, color=\"blue\", title=\"Curva del Codo para K-Means\"\n)\n\n\n\n\n\n\n\n\n\nmetricas = dict()\n\n\nK_KMEANS = 10\nkm = KMeans(n_clusters=K_KMEANS, n_init=10, random_state=RANDOM_STATE)\nlabels_km = km.fit_predict(X)\n\n\ns_km = silhouette_score(X, labels_km)\nmetricas[\"km_10\"] = s_km\nmetricas\n\n{'km_10': 0.39419687509752793}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#jerárquico",
    "href": "tics411/notebooks/08-proyecto_clustering.html#jerárquico",
    "title": "Preparación de los Datos",
    "section": "Jerárquico",
    "text": "Jerárquico\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlinkage_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in linkage_list:\n    plot_dendogram(X, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef train_hierarchical(K_H, linkage):\n    hc = AgglomerativeClustering(n_clusters=K_H, linkage=linkage)\n    labels_h = hc.fit_predict(X)\n    s_h = silhouette_score(X, labels_h)\n    print(f\"El coeficiente de Silueta es {s_h}\")\n    return labels_h, s_h\n\n\nlabels_c9, s_c9 = train_hierarchical(K_H=9, linkage=\"complete\")\nlabels_a9, s_a9 = train_hierarchical(K_H=9, linkage=\"average\")\nlabels_w4, s_w4 = train_hierarchical(K_H=4, linkage=\"ward\")\n\nEl coeficiente de Silueta es 0.37657025597625804\nEl coeficiente de Silueta es 0.3812263959798965\nEl coeficiente de Silueta es 0.2852126845283154\n\n\n\nmetricas[\"s_c9\"] = s_c9\nmetricas[\"s_a9\"] = s_a9\nmetricas[\"s_w4\"] = s_w4\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "href": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "title": "Preparación de los Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = X.shape[1] + 1\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino más cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino más cercano\")\n    plt.title(f\"Búsqueda de EPS para DBSCAN con k={k}\")\n\n\ndbscan_elbow_plot(X, k=MIN_SAMPLES)\n\nEPS = 1.6\n\n\n\n\n\n\n\n\n\ndbs = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\nlabels_dbs = dbs.fit_predict(X)\ns_dbs = silhouette_score(X, labels_dbs)\nmetricas[\"s_dbs\"] = s_dbs\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154,\n 's_dbs': 0.1818560991479739}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#evaluación",
    "href": "tics411/notebooks/08-proyecto_clustering.html#evaluación",
    "title": "Preparación de los Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\npd.Series(metricas.values(), index=metricas.keys()).plot(\n    kind=\"bar\",\n    rot=0,\n    edgecolor=\"k\",\n    title=\"Silhouette Score para los modelos generados\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nlabels_km\n\narray([5, 9, 2, ..., 2, 0, 1], dtype=int32)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i])\n    plt.suptitle(title, fontsize=15)\n    plt.tight_layout()\n\n\ncenter_analysis_viz(\n    df,\n    n_clusters=10,\n    labels=labels_km,\n    columns=num_vars,\n    title=\"Análisis de Centro\",\n)\n\n\n\n\n\n\n\n\n\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"], c=labels_km)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X, labels_km)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html",
    "href": "tics411/notebooks/pandas_basics.html",
    "title": "Seleccionar Filas, y columnas…",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df.shape, titanic_df.columns\n\n((891, 15),\n Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n        'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n        'alive', 'alone'],\n       dtype='object'))\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\ntitanic_df[\"survived_new\"] = titanic_df[\"survived\"].astype(\"float64\")\ntitanic_df\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\nsurvived_new\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n1.0\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n1.0\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n1.0\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n1.0\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n1.0\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n0.0\n\n\n\n\n891 rows × 16 columns\n## Mostrar la diferencia entre una Serie y un DataFrame.\ntitanic_df.loc[10]\n\nsurvived                 1\npclass                   3\nsex                 female\nage                    4.0\nsibsp                    1\nparch                    1\nfare                  16.7\nembarked                 S\nclass                Third\nwho                  child\nadult_male           False\ndeck                     G\nembark_town    Southampton\nalive                  yes\nalone                False\nName: 10, dtype: object\ntitanic_df[\"embark_town\"].to_frame()\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\nSouthampton\n\n\n1\nCherbourg\n\n\n2\nSouthampton\n\n\n3\nSouthampton\n\n\n4\nSouthampton\n\n\n...\n...\n\n\n886\nSouthampton\n\n\n887\nSouthampton\n\n\n888\nSouthampton\n\n\n889\nCherbourg\n\n\n890\nQueenstown\n\n\n\n\n891 rows × 1 columns\n## Explicar que va una lista de elementos... no es un \"doble\" paréntesis.\ntitanic_df[[\"embark_town\", \"class\"]]\n\n\n\n\n\n\n\n\nembark_town\nclass\n\n\n\n\n0\nSouthampton\nThird\n\n\n1\nCherbourg\nFirst\n\n\n2\nSouthampton\nThird\n\n\n3\nSouthampton\nFirst\n\n\n4\nSouthampton\nThird\n\n\n...\n...\n...\n\n\n886\nSouthampton\nSecond\n\n\n887\nSouthampton\nFirst\n\n\n888\nSouthampton\nThird\n\n\n889\nCherbourg\nFirst\n\n\n890\nQueenstown\nThird\n\n\n\n\n891 rows × 2 columns\ntitanic_df.loc[[10, 15], [\"embark_town\", \"fare\", \"age\"]]\n\n\n\n\n\n\n\n\nembark_town\nfare\nage\n\n\n\n\n10\nSouthampton\n16.7\n4.0\n\n\n15\nSouthampton\n16.0\n55.0\ntitanic_df_shuffle = titanic_df.sample(frac=1)\ntitanic_df_shuffle\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n133\n1\n2\nfemale\n29.0\n1\n0\n26.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n748\n0\n1\nmale\n19.0\n1\n0\n53.1000\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n876\n0\n3\nmale\n20.0\n0\n0\n9.8458\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n226\n1\n2\nmale\n19.0\n0\n0\n10.5000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n342\n0\n2\nmale\n28.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283\n1\n3\nmale\n19.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n863\n0\n3\nfemale\nNaN\n8\n2\n69.5500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n124\n0\n1\nmale\n54.0\n0\n1\n77.2875\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n583\n0\n1\nmale\n36.0\n0\n0\n40.1250\nC\nFirst\nman\nTrue\nA\nCherbourg\nno\nTrue\n\n\n85\n1\n3\nfemale\n33.0\n3\n0\n15.8500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n\n\n891 rows × 15 columns\n# Esto es un error... si es que no se separa...\ntitanic_df_shuffle.iloc[3][[\"who\", \"adult_male\"]]\n\nwho            man\nadult_male    True\nName: 226, dtype: object\n## Algunos métodos importante...\ntitanic_df.describe(percentiles=[0.05, 0.25, 0.75, 0.95])\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n5%\n0.000000\n1.000000\n4.000000\n0.000000\n0.000000\n7.225000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\n95%\n1.000000\n3.000000\n56.000000\n3.000000\n2.000000\n112.079150\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\ntitanic_df.mean(numeric_only=True)\n\nsurvived       0.383838\npclass         2.308642\nage           29.699118\nsibsp          0.523008\nparch          0.381594\nfare          32.204208\nadult_male     0.602694\nalone          0.602694\ndtype: float64\ntitanic_df.median(numeric_only=True)\n\nsurvived       0.0000\npclass         3.0000\nage           28.0000\nsibsp          0.0000\nparch          0.0000\nfare          14.4542\nadult_male     1.0000\nalone          1.0000\ndtype: float64\ntitanic_df.mode()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n24.0\n0\n0\n8.05\nS\nThird\nman\nTrue\nC\nSouthampton\nno\nTrue"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html#agrupar",
    "href": "tics411/notebooks/pandas_basics.html#agrupar",
    "title": "Seleccionar Filas, y columnas…",
    "section": "Agrupar",
    "text": "Agrupar\n\ndf = pd.DataFrame(\n    dict(a=[1, 1, 1, 1, 2, 2, 2, 2], b=[1, 2, 3, 4, 5, 6, 7, 8])\n)\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1\n\n\n1\n1\n2\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n2\n5\n\n\n5\n2\n6\n\n\n6\n2\n7\n\n\n7\n2\n8\n\n\n\n\n\n\n\n\nfor i in [df.shape, df.columns, df.index, df.dtypes]:\n    print(i)\n\n(8, 2)\nIndex(['a', 'b'], dtype='object')\nRangeIndex(start=0, stop=8, step=1)\na    int64\nb    int64\ndtype: object\n\n\n\ngroups = df.groupby(\"a\")\nfor id, g in groups:\n    print(f\"Este es el grupo: {id}\")\n    display(g)\n\nEste es el grupo: 1\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1.0\n\n\n1\n1\n2.0\n\n\n2\n1\n3.0\n\n\n3\n1\n4.0\n\n\n\n\n\n\n\nEste es el grupo: 2\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n4\n2\n5.0\n\n\n5\n2\n6.0\n\n\n6\n2\n7.0\n\n\n7\n2\n8.0\n\n\n\n\n\n\n\n\ndf.groupby(\"a\")[\"b\"].mean()\n\na\n1    2.5\n2    6.5\nName: b, dtype: float64\n\n\n\ntitanic_df.groupby(\"sex\")[\"fare\"].mean()\n\nsex\nfemale    44.479818\nmale      25.523893\nName: fare, dtype: float64\n\n\n\ntitanic_df.groupby([\"sex\", \"pclass\"])[[\"age\", \"fare\"]].median()\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\nsex\npclass\n\n\n\n\n\n\nfemale\n1\n35.0\n82.66455\n\n\n2\n28.0\n22.00000\n\n\n3\n21.5\n12.47500\n\n\nmale\n1\n40.0\n41.26250\n\n\n2\n30.0\n13.00000\n\n\n3\n25.0\n7.92500"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html",
    "href": "tics411/notebooks/06-ex-DBSCAN.html",
    "title": "DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\ndf = sns.load_dataset(\"iris\")\nX = df.drop(columns=\"species\")\nX\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#función-para-visualizar",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#función-para-visualizar",
    "title": "DBSCAN",
    "section": "Función para Visualizar",
    "text": "Función para Visualizar\n\nimport matplotlib.pyplot as plt\n\n\n## Función ligeramente modificada para no requerir centroides en caso que no sea aplicable.\ndef pca_viz(pca_X, labels, pca_centroids=None, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    if pca_centroids is not None:\n        plt.scatter(\n            pca_centroids[:, 0],\n            pca_centroids[:, 1],\n            marker=\"*\",\n            c=\"red\",\n            s=150,\n        )\n    plt.title(title)\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=13, eps=0.6)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\n\n\nfrom sklearn.decomposition import PCA\n\n## Probar minPts = 4 y eps = 0.2\n## Probar minPts = 10 y eps = 0.5\n## Probar minPts = 13 y eps = 0.6\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X_sc)\n\npca_viz(\n    pca_X,\n    labels=labels,\n    title=\"Visualización de DBSCAN para Iris en 2D\",\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino más cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino más cercano\")\n    plt.title(f\"Búsqueda de EPS para DBSCAN con k={k}\")\n\n\n# k = 5 escogido ya que tenemos 4 dimensiones.\ndbscan_elbow_plot(X_sc, k=5)"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperparámetros-óptimos",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperparámetros-óptimos",
    "title": "DBSCAN",
    "section": "Modelo entrenado con Hiperparámetros Óptimos",
    "text": "Modelo entrenado con Hiperparámetros Óptimos\n\nMIN_PTS = 20\nEPS = 0.75\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=MIN_PTS, eps=EPS)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\npca_viz(\n    pca_X,\n    labels=labels,\n    title=f\"Visualización de DBSCAN para Iris en 2D con los mejores Hiperparámetros: MinPts: {MIN_PTS} y eps = {EPS}\",\n)"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html",
    "href": "tics411/notebooks/11-ex-knn.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\ndf.dtypes.value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Tipos de Variable presente en Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "href": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "title": "Clases UAI",
    "section": "Supongamos que utilizaremos las siguientes variables",
    "text": "Supongamos que utilizaremos las siguientes variables\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape\n\n((891, 5), (891,))"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#eda",
    "href": "tics411/notebooks/11-ex-knn.html#eda",
    "title": "Clases UAI",
    "section": "EDA",
    "text": "EDA\n\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nprint(f\"Variables Numéricas: {num_cols}\")\nprint(f\"Variables Categóricas: {cat_cols}\")\n\nVariables Numéricas: ['fare', 'age']\nVariables Categóricas: ['class', 'sex', 'embark_town']\n\n\n\nValores Faltantes (Nulos)\n\nX.isnull().mean().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Cantidad de Valores Nulos en el Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-numéricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-numéricas",
    "title": "Clases UAI",
    "section": "Variables Numéricas",
    "text": "Variables Numéricas\n\nX.hist(grid=False, edgecolor=\"k\")\nplt.suptitle(\"Distribución de Variables Numéricas\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-categóricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-categóricas",
    "title": "Clases UAI",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\n\ncolor = [\"red\", \"blue\", \"green\"]\nfor cat, color in zip(cat_cols, color):\n    df[cat].value_counts().plot(\n        kind=\"bar\",\n        edgecolor=\"k\",\n        color=color,\n        title=f\"Categorías para '{cat}'\",\n    )\n    plt.show()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "href": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "title": "Clases UAI",
    "section": "Preprocesamiento",
    "text": "Preprocesamiento\n\nfrom feature_engine.imputation import CategoricalImputer\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X)\nX_imp\n\n\n\n\n\n\n\n\nclass\nsex\nembark_town\nfare\nage\n\n\n\n\n0\nThird\nmale\nSouthampton\n7.2500\n22.0\n\n\n1\nFirst\nfemale\nCherbourg\n71.2833\n38.0\n\n\n2\nThird\nfemale\nSouthampton\n7.9250\n26.0\n\n\n3\nFirst\nfemale\nSouthampton\n53.1000\n35.0\n\n\n4\nThird\nmale\nSouthampton\n8.0500\n35.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nSecond\nmale\nSouthampton\n13.0000\n27.0\n\n\n887\nFirst\nfemale\nSouthampton\n30.0000\n19.0\n\n\n888\nThird\nfemale\nSouthampton\n23.4500\nNaN\n\n\n889\nFirst\nmale\nCherbourg\n30.0000\n26.0\n\n\n890\nThird\nmale\nQueenstown\n7.7500\n32.0\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nfrom feature_engine.imputation import MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X_imp)\nX_imp\n\n\n\n\n\n\n\n\nclass\nsex\nembark_town\nfare\nage\n\n\n\n\n0\nThird\nmale\nSouthampton\n7.2500\n22.000000\n\n\n1\nFirst\nfemale\nCherbourg\n71.2833\n38.000000\n\n\n2\nThird\nfemale\nSouthampton\n7.9250\n26.000000\n\n\n3\nFirst\nfemale\nSouthampton\n53.1000\n35.000000\n\n\n4\nThird\nmale\nSouthampton\n8.0500\n35.000000\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nSecond\nmale\nSouthampton\n13.0000\n27.000000\n\n\n887\nFirst\nfemale\nSouthampton\n30.0000\n19.000000\n\n\n888\nThird\nfemale\nSouthampton\n23.4500\n29.699118\n\n\n889\nFirst\nmale\nCherbourg\n30.0000\n26.000000\n\n\n890\nThird\nmale\nQueenstown\n7.7500\n32.000000\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nfrom feature_engine.encoding import OneHotEncoder\n\nohe = OneHotEncoder()\nX_ohe = ohe.fit_transform(X_imp)\nX_ohe\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n7.2500\n22.000000\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n71.2833\n38.000000\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n2\n7.9250\n26.000000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n3\n53.1000\n35.000000\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n4\n8.0500\n35.000000\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n13.0000\n27.000000\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n887\n30.0000\n19.000000\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n888\n23.4500\n29.699118\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n889\n30.0000\n26.000000\n0\n1\n0\n1\n0\n0\n1\n0\n\n\n890\n7.7500\n32.000000\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 10 columns\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_ohe)\nX_sc_all\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.502445\n-0.592481\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n1\n0.786845\n0.638789\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n-1.623803\n2.074505\n-0.307562\n\n\n2\n-0.488854\n-0.284663\n0.902587\n-0.565685\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n3\n0.420730\n0.407926\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n4\n-0.486337\n0.407926\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.386671\n-0.207709\n-1.107926\n-0.565685\n1.960202\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n887\n-0.044381\n-0.823344\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n888\n-0.176263\n0.000000\n0.902587\n-0.565685\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n889\n-0.044381\n-0.284663\n-1.107926\n1.767767\n-0.510152\n0.737695\n-0.737695\n-1.623803\n2.074505\n-0.307562\n\n\n890\n-0.492378\n0.177063\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n-1.623803\n-0.482043\n3.251373\n\n\n\n\n891 rows × 10 columns\n\n\n\n\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"fare\", \"age\"])\nX_sc = sc.fit_transform(X_ohe)\nX_sc\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.502445\n-0.592481\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n0.786845\n0.638789\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n2\n-0.488854\n-0.284663\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n3\n0.420730\n0.407926\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n4\n-0.486337\n0.407926\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.386671\n-0.207709\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n887\n-0.044381\n-0.823344\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n888\n-0.176263\n0.000000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n889\n-0.044381\n-0.284663\n0\n1\n0\n1\n0\n0\n1\n0\n\n\n890\n-0.492378\n0.177063\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 10 columns"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "href": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "title": "Clases UAI",
    "section": "Entrenamiento del Modelo",
    "text": "Entrenamiento del Modelo\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn_clf(X, y, k=5, prep=\"\"):\n    knn = KNeighborsClassifier(\n        n_neighbors=k, metric=\"euclidean\", n_jobs=-1\n    )\n    ## Notar que es posible utilizar Variables categóricas como Etiquetas...\n    knn.fit(X, y)\n    y_pred = knn.predict(X)\n    print(\n        f\"Score k = {k}, y Preprocesamiento: {prep}: {knn.score(X,y):.4f}\"\n    )\n    return y_pred\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    print(\n        \"=================================================================\"\n    )\n    y_pred_sc = knn_clf(X_sc, y, k=k, prep=\"StandardScaler Numérico\")\n    y_pred_sc_all = knn_clf(X_sc_all, y, k=k, prep=\"StandardScaler a todo\")\n    y_pred_ohe = knn_clf(X_ohe, y, k=k, prep=\"Sin Escalar\")\n\n=================================================================\nScore k = 3, y Preprocesamiento: StandardScaler Numérico: 0.8844\nScore k = 3, y Preprocesamiento: StandardScaler a todo: 0.8855\nScore k = 3, y Preprocesamiento: Sin Escalar: 0.8384\n=================================================================\nScore k = 5, y Preprocesamiento: StandardScaler Numérico: 0.8698\nScore k = 5, y Preprocesamiento: StandardScaler a todo: 0.8698\nScore k = 5, y Preprocesamiento: Sin Escalar: 0.8204\n=================================================================\nScore k = 7, y Preprocesamiento: StandardScaler Numérico: 0.8608\nScore k = 7, y Preprocesamiento: StandardScaler a todo: 0.8575\nScore k = 7, y Preprocesamiento: Sin Escalar: 0.7834\n=================================================================\nScore k = 9, y Preprocesamiento: StandardScaler Numérico: 0.8418\nScore k = 9, y Preprocesamiento: StandardScaler a todo: 0.8406\nScore k = 9, y Preprocesamiento: Sin Escalar: 0.7733\n=================================================================\nScore k = 11, y Preprocesamiento: StandardScaler Numérico: 0.8373\nScore k = 11, y Preprocesamiento: StandardScaler a todo: 0.8361\nScore k = 11, y Preprocesamiento: Sin Escalar: 0.7643\n=================================================================\nScore k = 13, y Preprocesamiento: StandardScaler Numérico: 0.8272\nScore k = 13, y Preprocesamiento: StandardScaler a todo: 0.8283\nScore k = 13, y Preprocesamiento: Sin Escalar: 0.7587\n=================================================================\nScore k = 15, y Preprocesamiento: StandardScaler Numérico: 0.8215\nScore k = 15, y Preprocesamiento: StandardScaler a todo: 0.8249\nScore k = 15, y Preprocesamiento: Sin Escalar: 0.7486\n\n\n\nConclusión: Los Preprocesamientos afectan de manera importante el entrenamiento de un modelo."
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "href": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "title": "Clases UAI",
    "section": "Uso de Pipelines",
    "text": "Uso de Pipelines\n\nfrom sklearn.pipeline import Pipeline\n\n\ndef model_pipeline(num_method, cat_method, sc_variables, k=5):\n\n    sc = SklearnTransformerWrapper(\n        StandardScaler(), variables=sc_variables\n    )\n\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", OneHotEncoder()),\n            # (\"sc\", StandardScaler()),\n            (\"sc\", sc),\n            (\"model\", KNeighborsClassifier(n_neighbors=k, n_jobs=-1)),\n        ]\n    )\n\n    return pipe\n\n\npipe = model_pipeline(\n    num_method=\"mean\", cat_method=\"frequent\", sc_variables=[\"age\", \"fare\"]\n)\npipe\n\nPipeline(steps=[('num_imp', MeanMedianImputer(imputation_method='mean')),\n                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n                ('ohe', OneHotEncoder()), ('sc', StandardScaler()),\n                ('model', KNeighborsClassifier(n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('num_imp', MeanMedianImputer(imputation_method='mean')),\n                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n                ('ohe', OneHotEncoder()), ('sc', StandardScaler()),\n                ('model', KNeighborsClassifier(n_jobs=-1))]) MeanMedianImputerMeanMedianImputer(imputation_method='mean') CategoricalImputerCategoricalImputer(imputation_method='frequent') OneHotEncoderOneHotEncoder()  StandardScaler?Documentation for StandardScalerStandardScaler()  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_jobs=-1) \n\n\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\npipe.score(X, y)\n\n0.8698092031425365\n\n\n\ny_pred\n\narray(['no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes',\n       'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no',\n       'yes', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no',\n       'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes',\n       'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no',\n       'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no',\n       'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no',\n       'yes', 'no', 'no', 'no'], dtype=object)"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html",
    "href": "tics411/notebooks/03-ex_kmeans.html",
    "title": "Ejemplo K-Means",
    "section": "",
    "text": "import seaborn as sns\n\n# Importamos el Dataset Iris\ndf = sns.load_dataset(\"iris\")\ndf\ndf[\"species\"].value_counts()\n# Definimos X como una Matriz sin la variable Species.\nX = df.drop(columns=\"species\")\nX"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "href": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "title": "Ejemplo K-Means",
    "section": "Ayuda Visual",
    "text": "Ayuda Visual\nVamos a utilizar PCA para poder reducir las dimensiones a un tamaño el cual podamos visualizar: 2D.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n## Esto es sólo una ayuda para poder visualizar datos\n# que están en más dimensiones de las que podemos ver.\npca = PCA(n_components=2, random_state=1)\npca_X = pca.fit_transform(X)\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(pca_X[:, 0], pca_X[:, 1])\nplt.title(\"Visualización de Iris en 2D.\")\nplt.tight_layout()\n\n\n## Esta es una función que nos permitirá visualizar nuestras etiquetas en un espacio reducido por PCA.\n## Además permite la visualización de los centroides de nuestro proceso...\n\n\ndef pca_viz(pca_X, pca_centroids, labels, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    plt.scatter(\n        pca_centroids[:, 0],\n        pca_centroids[:, 1],\n        marker=\"*\",\n        c=\"red\",\n        s=150,\n    )\n    plt.title(title)\n\n\nImplementación de K-Means\n\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\n\npca_viz(\n    pca_X,\n    pca_centroids,\n    labels=labels,\n    title=\"Visualización de K-Means en Iris 2D\",\n)\n\n\n\nEfecto del Escalamiento en K-Means\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_sc = pca.fit_transform(X_sc)\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nsc_labels = km.fit_predict(X_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\npca_viz(\n    pca_X_sc,\n    pca_centroids,\n    sc_labels,\n    title=\"K-Means de Iris en 2D luego de Estandarizar los datos. \",\n)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\nX_mm = mm.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_mm = pca.fit_transform(X_mm)\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nmm_labels = km.fit_predict(X_mm)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(\n    pca_X_mm,\n    pca_centroids,\n    mm_labels,\n    title=\"K-Means de Iris en 2D luego de Normalizar los datos.\",\n)"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ejemplo-más-avanzado-sin-entrenar-con-todos-los-datos",
    "href": "tics411/notebooks/03-ex_kmeans.html#ejemplo-más-avanzado-sin-entrenar-con-todos-los-datos",
    "title": "Ejemplo K-Means",
    "section": "Ejemplo más avanzado sin entrenar con todos los datos…",
    "text": "Ejemplo más avanzado sin entrenar con todos los datos…\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(X, test_size=0.25, random_state=1)\n\n\nEstamos dejando un 25% de los datos fuera para poder chequear cuál sería la predicción que se le dan a dichos datos.\n\n\npca = PCA(n_components=2)\nkm = KMeans(n_clusters=2, n_init=10)\nsc = StandardScaler()\n## Fit siempre se hace con datos de `Entrenamiento`.\n\n## Escalamos los datos...\nsc.fit(X_train)\nX_train_sc = sc.transform(X_train)\nX_test_sc = sc.transform(X_test)\n\n# Generamos las coordenadas del PCA para visualizar\npca.fit(X_train_sc)\npca_train = pca.transform(X_train_sc)\npca_test = pca.transform(X_test_sc)\n\ntrain_labels = km.fit_predict(X_train_sc)\ntest_labels = km.predict(X_test_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(pca_train, pca_centroids, train_labels)\npca_viz(pca_test, pca_centroids, test_labels, cmap=\"tab20b\")"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#cuál-es-el-k-óptimo",
    "href": "tics411/notebooks/03-ex_kmeans.html#cuál-es-el-k-óptimo",
    "title": "Ejemplo K-Means",
    "section": "Cuál es el K óptimo?",
    "text": "Cuál es el K óptimo?\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"Número de Clústers\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X_train,\n    k_max=15,\n    color=\"red\",\n    title=\"Curva del Codo para el Dataset Iris, sólo con Train Set.\",\n)\n\n\nwc"
  },
  {
    "objectID": "tics411/notebooks/preguntas-prueba-2.html",
    "href": "tics411/notebooks/preguntas-prueba-2.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\ndf = pd.DataFrame(\n    dict(\n        y=[0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n        y_pred=[1, 1, 0, 0, 0, 1, 1, 1, 0, 0],\n    )\n)\nprint(classification_report(df.y, df.y_pred, digits=2))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0       0.40      0.40      0.40         5\n           1       0.40      0.40      0.40         5\n\n    accuracy                           0.40        10\n   macro avg       0.40      0.40      0.40        10\nweighted avg       0.40      0.40      0.40        10\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n4\nMucho\nMedio\n1\n\n\n5\nMucho\nBajo\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\nX_train\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n4\nMucho\nMedio\n1\n\n\n5\nMucho\nBajo\n1\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(X=[7, 5, 3, 5, 2], Y=[4, 7, 5, 7, 3], Clase=[1, 1, 1, -1, -1])\n)\nprint(df.to_latex())\n\n\\begin{tabular}{lrrr}\n\\toprule\n & X & Y & Clase \\\\\n\\midrule\n0 & 7 & 4 & 1 \\\\\n1 & 5 & 7 & 1 \\\\\n2 & 3 & 5 & 1 \\\\\n3 & 5 & 7 & -1 \\\\\n4 & 2 & 3 & -1 \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\ndf = pd.DataFrame(\n    {\n        \"Humedad\": [\n            85,\n            90,\n            86,\n            96,\n            80,\n            65,\n            70,\n            70,\n            95,\n            80,\n            91,\n            70,\n            90,\n            75,\n        ],\n        \"Se juega?\": [\n            \"No\",\n            \"No\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n            \"No\",\n            \"No\",\n            \"Sí\",\n            \"No\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n        ],\n    }\n)\ndf.index = [*range(1, 15)]\nprint(df.to_latex())\n\n\\begin{tabular}{lrl}\n\\toprule\n & Humedad & Se juega? \\\\\n\\midrule\n1 & 85 & No \\\\\n2 & 90 & No \\\\\n3 & 86 & Sí \\\\\n4 & 96 & Sí \\\\\n5 & 80 & Sí \\\\\n6 & 65 & Sí \\\\\n7 & 70 & Sí \\\\\n8 & 70 & No \\\\\n9 & 95 & No \\\\\n10 & 80 & Sí \\\\\n11 & 91 & No \\\\\n12 & 70 & Sí \\\\\n13 & 90 & Sí \\\\\n14 & 75 & Sí \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\n\ndf = pd.DataFrame(\n    dict(\n        y=[1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2],\n        y_pred=[1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 2],\n    )\n)\n\nprint(classification_report(df.y, df.y_pred, digits=3))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0      0.500     0.333     0.400         3\n           1      0.800     0.800     0.800         5\n           2      0.500     0.667     0.571         3\n\n    accuracy                          0.636        11\n   macro avg      0.600     0.600     0.590        11\nweighted avg      0.636     0.636     0.629        11\n\n\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\nX_train\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\n\ny_train = X_train.c\nX_train = X_train.drop(columns=\"c\")\n\noe = OrdinalEncoder()\nX_train_oe = oe.fit_transform(X_train)\nprint(oe.categories_)\ndt = DecisionTreeClassifier()\ndt.fit(X_train_oe, y_train)\n\nplt.figure(figsize=(20, 6))\nplot_tree(dt, filled=True, feature_names=X_train.columns)\nplt.tight_layout()\n\n[array(['Mucho', 'Poco'], dtype=object), array(['Alto', 'Bajo', 'Medio'], dtype=object)]\n\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        Brillo=[40, 50, 60, 10, 70, 60, 25],\n        Saturacion=[20, 50, 90, 25, 70, 10, 80],\n    )\n)\n\ndisplay(df)\ndf = pd.DataFrame(distance_matrix(df, df, p=2))\ndf.index = [*range(1, 8)]\ndf.columns = [*range(1, 8)]\ndf\n\n\n\n\n\n\n\n\nBrillo\nSaturacion\n\n\n\n\n0\n40\n20\n\n\n1\n50\n50\n\n\n2\n60\n90\n\n\n3\n10\n25\n\n\n4\n70\n70\n\n\n5\n60\n10\n\n\n6\n25\n80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n1\n0.000000\n31.622777\n72.801099\n30.413813\n58.309519\n22.360680\n61.846584\n\n\n2\n31.622777\n0.000000\n41.231056\n47.169906\n28.284271\n41.231056\n39.051248\n\n\n3\n72.801099\n41.231056\n0.000000\n82.006097\n22.360680\n80.000000\n36.400549\n\n\n4\n30.413813\n47.169906\n82.006097\n0.000000\n75.000000\n52.201533\n57.008771\n\n\n5\n58.309519\n28.284271\n22.360680\n75.000000\n0.000000\n60.827625\n46.097722\n\n\n6\n22.360680\n41.231056\n80.000000\n52.201533\n60.827625\n0.000000\n78.262379\n\n\n7\n61.846584\n39.051248\n36.400549\n57.008771\n46.097722\n78.262379\n0.000000\n\n\n\n\n\n\n\n\nimport numpy as np\n\nnp.sqrt((40 - 60) ** 2 + (20 - 10) ** 2)\n\n22.360679774997898\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/distancia.html",
    "href": "tics411/notebooks/distancia.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(dict(x=[0, 2, 3, 5], y=[2, 0, 1, 1]))\ndf.index = [1, 2, 3, 4]\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n1\n0\n2\n\n\n2\n2\n0\n\n\n3\n3\n1\n\n\n4\n5\n1\n\n\n\n\n\n\n\n\nnp.zeros((4, 4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\ndef distancia_l1(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n\n    return np.abs(x1 - x2) + np.abs(y1 - y2)\n\n\ndef distancia_l2(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n\n\ndef distancia_linf(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    d_x = np.abs(x1 - x2)\n    d_y = np.abs(y1 - y2)\n    return np.max([d_x, d_y])\n\n\ndef calculate_matrix(distance, n_puntos):\n    m = np.zeros((n_puntos, n_puntos))\n    for i in range(n_puntos):\n        for j in range(n_puntos):\n            p = df.iloc[i]\n            q = df.iloc[j]\n            m[i, j] = distance(p, q)\n\n    return m\n\n\nm_m = calculate_matrix(distancia_l1, 4)\nm_e = calculate_matrix(distancia_l2, 4)\nm_c = calculate_matrix(distancia_linf, 4)\n\n\nm_m\n\narray([[0., 4., 4., 6.],\n       [4., 0., 2., 4.],\n       [4., 2., 0., 2.],\n       [6., 4., 2., 0.]])\n\n\n\nm_e\n\narray([[0.        , 2.82842712, 3.16227766, 5.09901951],\n       [2.82842712, 0.        , 1.41421356, 3.16227766],\n       [3.16227766, 1.41421356, 0.        , 2.        ],\n       [5.09901951, 3.16227766, 2.        , 0.        ]])\n\n\n\nm_c\n\narray([[0., 2., 3., 5.],\n       [2., 0., 1., 3.],\n       [3., 1., 0., 2.],\n       [5., 3., 2., 0.]])\n\n\n\n\na = pd.Series(\n    [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0]\n)\nb = pd.Series(\n    [1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0]\n)\n\na.var(ddof=0)  # Varianza Poblacional\n\n3.5\n\n\n\na.var(ddof=1)  # Varianza Muestral\n\n3.7916666666666665\n\n\n\nb.var(ddof=1)\n\n1.5916666666666668\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579.html",
    "href": "tics579.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentación del Curso\n\n\n\nAug 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase P1\n\n\nÁlgebra Tensorial\n\n\n\nAug 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase P2\n\n\nCálculo Tensorial\n\n\n\nAug 7, 2025\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  },
  {
    "objectID": "tics411.html",
    "href": "tics411.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentación del Curso\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 1\n\n\nCalidad de los Datos y Feature Engineering\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 2\n\n\nExploratory Data Analysis (EDA)\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase Bonus\n\n\nIntroducción a Scikit-Learn\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 3\n\n\nModelación Descriptiva y K-Means\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 4\n\n\nClustering Jerárquico\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 5\n\n\nDBSCAN\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 6\n\n\nEvaluación de Clusters\n\n\n\nApr 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 7\n\n\nAlgoritmo Apriori\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 8\n\n\nIntroducción al Aprendizaje Supervisado\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 9\n\n\nEvaluación de Modelos\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 10\n\n\nÁrboles de Decisión\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 11\n\n\nNaive Bayes\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 12\n\n\nRegresión Logística\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 13\n\n\nDetección de Anomalías\n\n\n\nJun 27, 2024\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  },
  {
    "objectID": "tics411-labs.html",
    "href": "tics411-labs.html",
    "title": "Prácticos",
    "section": "",
    "text": "Práctico\nColab\n\n\n\n\nPreprocesamiento\n\n\n\nEDA\n\n\n\nK-Means\n\n\n\nAnálisis de Centros\n\n\n\nAglomerativo\n\n\n\nDBSCAN\n\n\n\nEvaluación de Clusters\n\n\n\nEjemplos Hopkins\n\n\n\nProyecto Clustering\n\n\n\nApriori\n\n\n\nResolución Guía\n\n\n\nKNN\n\n\n\nCross Validation\n\n\n\nDecision Tree\n\n\n\nNaive Bayes\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "tics579/clase-0.html#quién-soy",
    "href": "tics579/clase-0.html#quién-soy",
    "title": "TICS-579-Deep Learning",
    "section": "¿Quién soy?",
    "text": "¿Quién soy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia\nEstudié Ingeniería Civil pero llevo 10 años trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nSoy Msc. en Data Science y estoy cursando el PhD. en la UAI especificamente en Deep Learning.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqué mi primer paper el año pasado sobre Hate Speech en Español.\nJuego Tenis de Mesa, hacía Agility con mi perrita Kira y escribo en mi Blog (poco). Estoy volviendo a tocar batería."
  },
  {
    "objectID": "tics579/clase-0.html#disclaimer",
    "href": "tics579/clase-0.html#disclaimer",
    "title": "TICS-579-Deep Learning",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\n\nMucho del contenido de este curso será una mezcla entre inglés y español. Esto debido a que el contenido del curso está en constante desarrollo y casi no existen libros o artículos en español al respecto.\n\n\n\n\n\n\n\n\n\n\n\nEste curso se considera altamente teórico y con una fuerte componente en programación. ¡Están advertidos!\n\n\n\n\n\n\n\n\n\n\n\n\nEstá completamente prohibido copiar y pegar código de algún modelo de IA. Gente sorprendida se va directamente a Código de Honor. Y además… no se aprende.\nSe prohibe el uso de código de librerías no vistas en clases. Esto también se considerará copia.\n\n\n\n\n\n\n\n\n\n\n\n\nVamos a sufrir harto al menos las primeras semanas (yo al menos he sufrido harto preparando las clases), pero les aseguro que va a valer la pena.\nBare with me!!\n\n\n\n\n\n\n\n\n\n\n\nVamos a aprender conceptos muy avanzados que no muchos cursos consideran. Lo siento, pero van a ser mis conejillos de indias.\n\n\n\n\n\n\n\n\n\n\n\nVamos a pasarla bien mal, estudiando harto, demorándonos harto en las tareas, pero vamos a aprender harto."
  },
  {
    "objectID": "tics579/clase-0.html#objetivos-del-curso",
    "href": "tics579/clase-0.html#objetivos-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\n\n\n\nSer el curso más completo y exhaustivo de Deep Learning del país.\n\n\n\n\n\nIdentificar elementos claves de las Redes Neuronales.\n\nInputs, Capas, Outputs, Parámetros, Optimizadores, Funciones de Activación, Funciones de Pérdida, etc.\n\nEntender conceptos básicos como el Training Loop, Gradient Propagation, Optimización, etc.\nIdentificar los distintos tipos de Redes Neuronales:\n\nFeed Fordward Networks (MLP),\nConvolutional Neural Networks,\nRecurrent Neural Networks,\nTransformers.\n\nEntender las Arquitecturas Estado del Arte principalmente en Computer Vision y Natural Language Processing.\nImplementar, entrenar y evaluar Deep Neural Networks utilizando Pytorch."
  },
  {
    "objectID": "tics579/clase-0.html#cómo-aprovechar-las-diapositivas-al-máximo-véanlo-después",
    "href": "tics579/clase-0.html#cómo-aprovechar-las-diapositivas-al-máximo-véanlo-después",
    "title": "TICS-579-Deep Learning",
    "section": "¿Cómo aprovechar las diapositivas al máximo? (Véanlo después)",
    "text": "¿Cómo aprovechar las diapositivas al máximo? (Véanlo después)\n\n\nDiapositivas Interactivas creadas en Quarto (links van a estar disponibles en Webcursos)\n\nContiene un índice de todas las slides.\nPermite copiar y pegar código directamente.\nImágenes se pueden ver en tamaño completo al clickearlas.\nSe puede buscar contenido específico de cualquier Slide utilizando la Search Bar.\nSe puede obtener una copia en PDF presionando la tecla E para luego guardarlas para tomar notas."
  },
  {
    "objectID": "tics579/clase-0.html#cómo-apruebo",
    "href": "tics579/clase-0.html#cómo-apruebo",
    "title": "TICS-579-Deep Learning",
    "section": "¿Cómo apruebo?",
    "text": "¿Cómo apruebo?\n\n\n\n\n\n\n\nNota Final\n\n\n\\[NF = 0.7 \\cdot NP + 0.3\\cdot \\overline{NT}\\]\n\n\\(\\overline{NT}\\): Promedio de tareas. Será un 1.0 si es que NP es menor a 5.0.\n\n\n\n\n\n\n\n\n\n\nPruebas\n\n\nMidterm: Prueba a mitad del Curso el cuál vale un 30% de la Nota de Presentación.\nFinal Exam: 70% de la Nota Final, considera todo el contenido del curso.\n\n\n\n\n\n\n\n\n\nTareas\n\n\n\nSerán principalmente desafíos de programación y posibles problemas teóricos.\nSe prohibe la copia de código.\nHágalas a conciencia, ya que será material preguntado durante las pruebas.\nFechas de tarea serán anunciadas cuando corresponda."
  },
  {
    "objectID": "tics579/clase-8.html#lenet-5-lecun-et-al.-1998",
    "href": "tics579/clase-8.html#lenet-5-lecun-et-al.-1998",
    "title": "TICS-579-Deep Learning",
    "section": "LeNet-5 (LeCun et al., 1998)",
    "text": "LeNet-5 (LeCun et al., 1998)\n\nProbablemente la primera arquitectura famosa en poder realizar tareas importantes de reconocimiento de imagen. Diseñada especialmente para reconocimiento de dígitos, introduce los bloques de convolución más pooling para luego conectarse con FFN.\n\n\n\n\n\n\n\n\n\nAdaptive Pooling\n\n\nLa mayoría de arquitecturas más modernas utiliza una capa llamada Adaptive Pooling antes del proceso de Flatten. El Adaptive Pooling es una especie de Pooling inverso, donde uno define el tamaño del output, y automáticamente se calcula el Kernel, Stride, Padding, etc. necesario para obtener ese tamaño.\nEso garantiza que cualquier tamaño de imagen puede pasar por la red sin romper las dimensiones necesarias para la transición al MLP."
  },
  {
    "objectID": "tics579/clase-8.html#alexnext-krizhevsky-sutskever-y-hinton-2012",
    "href": "tics579/clase-8.html#alexnext-krizhevsky-sutskever-y-hinton-2012",
    "title": "TICS-579-Deep Learning",
    "section": "AlexNext (Krizhevsky, Sutskever y Hinton, 2012)",
    "text": "AlexNext (Krizhevsky, Sutskever y Hinton, 2012)\n\nGanó el concurso Imagenet (ILSVRC) en 2012 por un largo margen (algo impensado para ese tiempo). Introdujo los conceptos de ReLU, Dropout y Aceleración por GPU. Esta arquitectura está disponible en torchvision.\n\n\n\n\n\n\nimport torchvision\ntorchvision.models.alexnet(weights = \"IMAGENET1K_V1\")\n\n\n\n\n\n\nLa arquitectura de Torchvision está inspirada en una versión alternativa de Alexnet. Esto probablemente no será corregido ya que no es una arquitectura que se utilice comunmente en la actualidad."
  },
  {
    "objectID": "tics579/clase-8.html#vggnet-simonyan-zisserman-2014",
    "href": "tics579/clase-8.html#vggnet-simonyan-zisserman-2014",
    "title": "TICS-579-Deep Learning",
    "section": "VGGNet (Simonyan, Zisserman, 2014)",
    "text": "VGGNet (Simonyan, Zisserman, 2014)\n\nPresentaron las primeras redes relativamente profundas con Kernels pequeños de \\(3 \\times 3\\). Su propuesta incluye Redes de hasta 19 capas.\n\n\n\n\n\n\n\n\n\nimport torchvision\ntorchvision.models.vgg16(weights = \"IMAGENET1K_V1\")\n## Versión con Batchnorm\ntorchvision.models.vgg16_bn(weights = \"IMAGENET1K_V1\")\n\n\n\n\n\n\ntorchvision incluye las arquitecturas de 11, 13, 16 y 19 capas, además de variantes que incluyen Batchnorm (que en eltiempo del paper no existían aún)."
  },
  {
    "objectID": "tics579/clase-8.html#googlenetinception-szegedy-et-al.-2014",
    "href": "tics579/clase-8.html#googlenetinception-szegedy-et-al.-2014",
    "title": "TICS-579-Deep Learning",
    "section": "GoogleNet/Inception (Szegedy et al., 2014)",
    "text": "GoogleNet/Inception (Szegedy et al., 2014)\n\n\n\nIntroduce las “Pointwise Convolutions” (Convoluciones de 1x1) que permiten reducir la complejidad de canales (mediante una combinación lineal) manteniendo las dimensiones de la imagen. Además introduce los Inception Modules, que combinan resultados de Kernels de distinto tamaño. Fue la Arquitectura ganadora de ILSVRC 2014.\n\nimport torchvision\ntorchvision.models.googlenet(weights = \"IMAGENET1K_V1\")\n\n1x1 Convolutions"
  },
  {
    "objectID": "tics579/clase-8.html#resnet-he-et-al.-2015",
    "href": "tics579/clase-8.html#resnet-he-et-al.-2015",
    "title": "TICS-579-Deep Learning",
    "section": "Resnet (He et al., 2015)",
    "text": "Resnet (He et al., 2015)\n\n\n\n\n\n\n\n\n\nIntroduce las conexiones residuales, lo cual permite evitar el problema del vanishing gradient para redes muy profundas. Es la Arquitectura ganadora de ILSVRC 2015.\n\n\n\n\n\n\n\nEsta arquitectura se puede encontrar tanto en torchvision como timm. Recomiendo timm, ya que hay muchas más variantes, mejor mantención y procesos de entrenamiento actualizados.\n\n\n\nimport timm\nmodel = timm.create_model(\"resnet50\", pretrained = True)\n\n## Listar todas las versiones de Resnet disponibles\ntimm.list_models(\"resnet*\")\n\nConexiones Residuales"
  },
  {
    "objectID": "tics579/clase-8.html#efficientnet-tan-le-2019",
    "href": "tics579/clase-8.html#efficientnet-tan-le-2019",
    "title": "TICS-579-Deep Learning",
    "section": "EfficientNet (Tan, Le, 2019)",
    "text": "EfficientNet (Tan, Le, 2019)\n\nIntroducen el concepto de Compound Scaling que permite cambiar la escala de profundidad (número de capas en la red), ancho (número de canales en cada capa) y resolución (dimensiones de la imagen) para poder mejorar la performance. Permite crear resultados al nivel del estado del arte con muchísimos menos parámetros.\n\n\n\n\n\n\nimport timm\nmodel = timm.create_model(\"efficientnet_b0\", pretrained = True)\n\n## Listar todas las versiones de Resnet disponibles\ntimm.list_models(\"efficientnet*\")"
  },
  {
    "objectID": "tics579/clase-8.html#pre-training",
    "href": "tics579/clase-8.html#pre-training",
    "title": "TICS-579-Deep Learning",
    "section": "Pre-training",
    "text": "Pre-training\n\nImagenet\n\nCorresponde a un dataset de cerca de 14M de imágenes con que fueron anotados a mano. Este dataset se utilizó para la competencia de ImageNet Large Scale Visual Recognition Challenge (ILSVRC) desde el año 2010 al 2017, el cuál generó innumerables avances en el estado del arte. Normalmente las imágenes tienen rangos entre \\(4288 \\times 2848\\) hasta \\(75 \\times 56\\). Las imágenes se encuentran normalizadas restando medias por canal de \\([0.485,0.456,0.406]\\) y divididas por SD de \\([0.229,0.224,0.225]\\).\n\n\n\n\n\n\n\n\n\nLas dos variantes más conocidas son el ImageNet-1K que tiene 1.281.167, 50.000 y 100.000 imágenes para train, validation y test set con 1000 categorías y el ImageNet-21K que tiene 14.197.122 imágenes con 21.841 clases.\n\n\n\n\n\n\n\n\n\n\nDebido a la importancia y complejidad de este dataset es que la mayoría de los backbones han sido pre-entrenados con este él. Por lo que las distintas arquitecturas “pueden ver” gracias a este dataset.\n\n\n\n\n\n\n\n\n\nDebido a que muchas arquitecturas pueden/saben ver en un dataset tan complejo como Imagenet. ¿Sería posible utilizar ese conocimiento en otro dataset?\n\n\n\n\nEntering Transfer Learning"
  },
  {
    "objectID": "tics579/clase-8.html#transfer-learning",
    "href": "tics579/clase-8.html#transfer-learning",
    "title": "TICS-579-Deep Learning",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\n\n\n\n\n\n\n\n\nDataset Público/alta complejidad\n\n\nNormalmente se utilizan datos públicos y de alta complejidad y se utiliza para pre-entrenar una arquitectura.\n\n\n\n\n\n\nPre-entrenamiento\n\n\nSe entrena una arquitectura para una tarea en específico con los detalles del dataset a utilizar.\n\n\n\n\n\n\nFine-Tuning\n\n\nSe carga la arquitectura pre-entrenada, con los pesos obtenidos en el pre-entrenamiento y se ajusta el prediction head para la nueva tarea y se vuelve a entrenar el modelo.\n\n\n\n\n\n\nFreezing Layers\n\n\nSe refiere a congelar los parámetros del backbone pre-entrenado, es decir, estos no se actualizan. Este paso es opcional, y en ocasiones puede funcionar de mejor manera que un Full-Fine-Tuning"
  },
  {
    "objectID": "tics579/clase-8.html#image-preprocessing-y-data-augmentation",
    "href": "tics579/clase-8.html#image-preprocessing-y-data-augmentation",
    "title": "TICS-579-Deep Learning",
    "section": "Image Preprocessing y Data Augmentation",
    "text": "Image Preprocessing y Data Augmentation\n\nEn general el proceso de Preprocesamiento de Imágenes es bastante más engorroso que el de datos tabulares. Afortunadamente Pytorch tiene algunos utilities que permiten hacer el proceso más sencillo:\n\n\nImageFolder\n\nPermite cargar imágenes de un Path en específico. Dentro de esa carpeta ImageFolder considerará cada carpeta como una clase y los elementos (imágenes) dentro de dicha clase como instancia de la clase en cuestión.\n\n\nfrom torchvision.dataset import ImageFolder\n\ntrain_data = ImageFolder(\"path/to/train/images\", transform = None)\nvalidation_data = ImageFolder(\"path/to/validation/images\", transform = None)\ntest_data = ImageFolder(\"path/to/test/images\", transform = None)\n\n\n\n\n\n\nAdemás ImageFolder posee un parámetro llamado transform en el cuál se pueden ingresar transformaciones a los datos para realizar procesos de Data Augmentation.\n\n\n\n\n\n\nOjo\n\n\nImage Folder entrega los datos como una Imagen PIL. Por lo tanto, es necesario aplicar procesamientos que permitan su transformación en Tensor."
  },
  {
    "objectID": "tics579/clase-8.html#data-augmentation",
    "href": "tics579/clase-8.html#data-augmentation",
    "title": "TICS-579-Deep Learning",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\nCorresponde a un proceso de generación de datos sintéticos. Este proceso se puede utilizar para:\n\nPermite la generación de datos adicionales debido a escasez por costo o disponibilidad de ellos. Ejemplo: Datos médicos.\nGenera variedad de datos, que entrega al modelo un mayor poder de generalización en datos no vistos.\nAl introducir mayor variabilidad en los datos entrega una mayor robustez ante el overfitting (Regularización).\nSimular condiciones adversas para el modelo en la cuál se quiera generar robustez.\n\nEj: Se tiene un modelo de reconocimiento de vehículos, pero que tiene que funcionar en condiciones de niebla.\n\n\n\n\n\n\n\n\n\n\n\n\nAlbumentations\n\n\nExisten diversas librerías que permiten generar Aumento de Datos. La librerías más famosas son Albumentations y Kornia. Albumentations, permite transformaciones extremadamente eficientes en CPU, mientras que Kornia hace lo mismo pero en GPU. Debido a las limitaciones de GPU que contamos, utilizaremos Albumentations, de manera tal de balancear procesamiento tanto en CPU como en GPU.\n\n\n\n\n\n\n\n\n\nNormalmente este tipo de transformaciones entrega mejores resultados cuando se generan de manera aleatoria y on-the-fly. Es decir, se genera el aumento de datos en la carga de datos durante el entrenamiento."
  },
  {
    "objectID": "tics579/clase-8.html#transformaciones-básicas",
    "href": "tics579/clase-8.html#transformaciones-básicas",
    "title": "TICS-579-Deep Learning",
    "section": "Transformaciones Básicas",
    "text": "Transformaciones Básicas\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\n\n\n\n\nAlbumentations espera que la imagen venga como Numpy Array. Además es una librería bastante quisquillosa, por lo que toma un rato acostumbrarse. Pero su eficiencia y utilidad hace que valga la pena.\n\n\n\n\nA.Compose()\n\nPermite generar Pipelines de Transformación. Es decir, irá aplicando transformaciones una a una.\n\nA.ToFloat()\n\nTransforma los datos en tipo Float. Esto a veces es necesario cuando hay incompatibilidad de data types en ciertos módulos.\n\nToTensorV2()\n\nTransforma a Tensor de Pytorch. Existe una versión ToTensor() pero está deprecada y no debería usarse.\n\nA.Normalize()\n\nPermite normalizar imágenes según su proceso de pre-entrenamiento. Normalmente estos provienen de pre-entrenamiento en Imagenet por lo que se debe normalizar con \\(mean=[0.485,0.456,0.406]\\) y \\(SD=[0.229,0.224,0.225]\\).\n\nA.Resize()\n\nSe utiliza para estandarizar el tamaño de las imágenes. Imágenes más grandes permiten mejores resultados pero son computacionalmente más costosas."
  },
  {
    "objectID": "tics579/clase-8.html#transformaciones-probabilísticas",
    "href": "tics579/clase-8.html#transformaciones-probabilísticas",
    "title": "TICS-579-Deep Learning",
    "section": "Transformaciones Probabilísticas",
    "text": "Transformaciones Probabilísticas\n\n\n\n\n\n\nComo su nombre lo indica, la transformación se aplicará con una cierta probabilidad, lo que permitirá que cada epoch haya mayor variabilidad.\n\n\n\n\nA.CenterCrop/A.RandomCrop\n\nGenera un Crop de la imagen o al centro o Random. Esto logrará que los elementos de la imagen cambien de posición.\n\nA.VerticalFlip\n\nGenera Flip Vertical.\n\nA.HorizontalFlip\n\nGenera Flip Horizontal.\n\nA.Rotate\n\nGenera rotaciones aleatorias entre un ángulo mínimo y máximo.\n\n\n\n\n\n\n\n\nExisten un sinnúmero de transformaciones que se pueden aplicar. La lista completa se puede encontrar acá. Y existen transformaciones que incluso permiten simular niebla, lluvia, nieve, sepia, Zoom, y variados otros efectos.\n\n\n\n\n\n\n\n\n\nAplicar estas transformaciones es de extremo cuidado ya que para tareas más complejas como Semantic Segmentation, Object Detection, Keypoint Detection, se debe aplicar dichas transformaciones también a las etiquetas."
  },
  {
    "objectID": "tics579/clase-P1.html#historia",
    "href": "tics579/clase-P1.html#historia",
    "title": "TICS-579-Deep Learning",
    "section": "Historia",
    "text": "Historia\nLa verdad podríamos estudiar historia e importancia de porqué el Deep Learning es importante, pero la verdad…\n\n\n\n\n\n\nNO TENEMOS TIEMPO PARA ESO.\n\n\n\n\n\nAlexnet (2012)\n\n\nTransformers (2017)\n\n\nGPT (2019)\n\n\nLLMs (2023) (ChatGPT/Llama)"
  },
  {
    "objectID": "tics579/clase-P1.html#por-qué-estudiar-deep-learning",
    "href": "tics579/clase-P1.html#por-qué-estudiar-deep-learning",
    "title": "TICS-579-Deep Learning",
    "section": "¿Por qué estudiar Deep Learning?",
    "text": "¿Por qué estudiar Deep Learning?\n\n\n\n\n\n\n\nImágen tomada de la Clase de Zico Colter"
  },
  {
    "objectID": "tics579/clase-P1.html#por-qué-estudiar-deep-learning-1",
    "href": "tics579/clase-P1.html#por-qué-estudiar-deep-learning-1",
    "title": "TICS-579-Deep Learning",
    "section": "¿Por qué estudiar Deep Learning?",
    "text": "¿Por qué estudiar Deep Learning?\n\n\n\n\n\n\n\nFacilidad y Autograd\n\n\n\nFrameworks como Tensorflow, Pytorch o Jax permiten realizar esto de manera mucho más sencilla.\n\nFrameworks permiten calcular gradientes de manera automática.\nAntigua mente trabajar en Torch, Caffe o Theano podía tomar cerca de 50K líneas de código.\n\n\n\n\n\n\n\n\n\n\n\nCómputo\n\n\n\nProliferación de las GPUs, TPUs, HPUs, IPUs, como sistemas masivos de Cómputos.\n\nHow many computers to identify a cat? 16,000\n\n\n\n\n\n\n\n\n\n\n\nEstado del Arte\n\n\n\nModelos de Deep Learning pueden generar sistemas que entiendan imágenes, textos, audios, videos, grafos, etc."
  },
  {
    "objectID": "tics579/clase-P1.html#tensores",
    "href": "tics579/clase-P1.html#tensores",
    "title": "TICS-579-Deep Learning",
    "section": "Tensores",
    "text": "Tensores\n\nCorresponde a una generalización de los vectores y matrices que permite representar datos de múltiples dimensiones.\n\n\n\nEscalares (Orden 0)\n\\[-1, 11.27, \\pi\\]\n\nVectores Filas (Orden 1)\n\\[\\begin{bmatrix}1.0 & -0.27 & -1.22\\end{bmatrix}\\]\nVectores Columnas (Orden 1)\n\\[\\begin{bmatrix}\n1 \\\\\n-0.27\\\\\n-1.22\n\\end{bmatrix}\\]\n\nMatrices (Orden 2)\n\\[\\begin{bmatrix}\n1.0 & -0.27 & 3\\\\\n3.15 & 2.02 & 1.2\\\\\n-1.22& 0.55 & 3.97 \\\\\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nNormalmente los tensores utilizan un mismo tipo de dato: Integers o Float es lo más común."
  },
  {
    "objectID": "tics579/clase-P1.html#tensores-1",
    "href": "tics579/clase-P1.html#tensores-1",
    "title": "TICS-579-Deep Learning",
    "section": "Tensores",
    "text": "Tensores\n\n\nTensores (Orden 3+)\n\\[\\begin{bmatrix}\n    \\begin{bmatrix}\n        0.2 & 0.1 & -0.25 \\\\\n        0.1 & -1.0 & 0.22\\\\\n    \\end{bmatrix} \\\\\n    \\begin{bmatrix}\n        0.24 & 0.1 & -0.25 \\\\\n        0.05 & -0.69 & 0.98\n    \\end{bmatrix} \\\\\n    \\begin{bmatrix}\n        0.66& -1.0 & 0.22\\\\\n        -0.07 & -0.59 & 0.99\n    \\end{bmatrix} \\\\\n    \\begin{bmatrix}\n        0.16& 1.0 & 3.22\\\\\n        9.17 & 7.19 & 9.99\n    \\end{bmatrix}\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nNomenclatura\n\n\n\n\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\): Minúsculas griegas denotan a Escalares.\nx, y, z: Minúsculas latinas denotan a Vectores.\nX, Y, Z: Mayúsculas latinas denotan a Matrices o Tensores.\n\n\n\n\n\n\n\n\n\n\nShape/Tamaño: Tamaño del tensor, tiene tantas dimensiones como su orden.\n\n\n\nEscalar: No tiene dimensiones.\nVector: Tamaño es equivalente al número de elementos del vector. (3,)\n\nA veces se usa la versión (1,3) para vectores filas y (3,1) para vectores columnas.\n\nMatrices: Tamaño es equivalente al número de filas y columnas. Ejemplo: (3,3)\nTensores: Tamaño es equivalente al número de matrices que lo componen y el número de filas y columnas de cada una de ellas. Ejemplo: (4,2,3)\n\n\n\n\n\n\n\nImportante\n\n\n\nLa primera dimensión del shape se conoce como Batch Size el cual denota la cantidad de elementos de orden inferior.\n(3, ) tenemos 3 escalares.\n(3,2) tenemos 3 vectores filas de 2 elementos cada uno.\n(4,2,3) tenemos 4 matrices de (2,3) cada una."
  },
  {
    "objectID": "tics579/clase-P1.html#vectores-suma",
    "href": "tics579/clase-P1.html#vectores-suma",
    "title": "TICS-579-Deep Learning",
    "section": "Vectores: Suma",
    "text": "Vectores: Suma\n\nCorresponde a un arreglo unidimensional de números reales. Se puede representar como fila o columna. Por convención denotaremos \\(\\bar{x}\\) como vector columna y \\(\\bar{x}^T\\) como vector fila.\n\n\n\n\n\n\n\n\n\nOperación Suma\n\n\n\nPermite sumar dos vectores de igual tamaño dimensión por dimensión.\n\nEj: \\([7,2]^T + [2,3]^T = [9,5]^T\\)\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nConmutatividad: \\(\\bar{x}+\\bar{y} = \\bar{y}+\\bar{x}\\)\nAsociatividad: \\((\\bar{x}+\\bar{y})+\\bar{z} = \\bar{x}+(\\bar{y}+\\bar{z})\\)\nElemento Neutro: \\(\\bar{x} + \\bar{0} = \\bar{x}\\)"
  },
  {
    "objectID": "tics579/clase-P1.html#vectores-ponderación",
    "href": "tics579/clase-P1.html#vectores-ponderación",
    "title": "TICS-579-Deep Learning",
    "section": "Vectores: Ponderación",
    "text": "Vectores: Ponderación\n\nCorresponde a un arreglo unidimensional de números reales. Se puede representar como fila o columna. Por convención denotaremos \\(\\bar{x}\\) como vector columna y \\(\\bar{x}^T\\) como vector fila.\n\n\n\n\n\n\n\n\n\nOperación Ponderación\n\n\n\nPermite multiplicar/ponderar cada dimensión del vector por un escalar.\nEj: \\(2 \\cdot [3,2]^T = [6,4]^T\\)\n\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nDistributividad Escalar: \\(a(\\bar{x}+\\bar{y}) = a\\bar{x} + a\\bar{y}\\)\nDistributividad Vectorial: \\((a+b)\\bar{x} = a\\bar{x} + b\\bar{x}\\)\nElemento Neutro: \\(1\\cdot \\bar{x} = \\bar{x}\\)\nCompatibilidad: \\(a(b\\bar{x}) = (ab)\\bar{x}\\)"
  },
  {
    "objectID": "tics579/clase-P1.html#vectores-norma",
    "href": "tics579/clase-P1.html#vectores-norma",
    "title": "TICS-579-Deep Learning",
    "section": "Vectores: Norma",
    "text": "Vectores: Norma\n\n\n\n\n\n\n\n\nNorma (Euclideana)\n\n\nPara un vector \\(\\bar{x}=[x_1, ..., x_n] \\in \\mathbb{R}^n\\) se define la norma como:\n\\[||\\bar{x}|| = \\sqrt{\\sum_{i=1}^n x_i^2}\\]\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nDesigualdad Triangular: \\(||\\bar{x}+\\bar{y}|| \\leq ||\\bar{x}|| + ||\\bar{y}||\\)\n\\(||\\alpha \\bar{x}||= |\\alpha| \\cdot ||\\bar{x}||\\)\n\\(||\\bar{x}|| = 0 \\Longleftrightarrow \\bar{0}\\)\n\n\n\n\n\n\n\n\n\n\nAplicación\n\n\nLa norma permite calcular la distancia entre dos vectores.\n\\[d_{x,y} = ||\\bar{x} - \\bar{y}||\\]\nTambién serviría para puntos. ¿Por qué?"
  },
  {
    "objectID": "tics579/clase-P1.html#vectores-producto-interno-producto-punto",
    "href": "tics579/clase-P1.html#vectores-producto-interno-producto-punto",
    "title": "TICS-579-Deep Learning",
    "section": "Vectores: Producto Interno (Producto Punto)",
    "text": "Vectores: Producto Interno (Producto Punto)\n\n\n\n\n\n\n\n\nInner Product or Dot Product\n\n\nEl producto interno entre dos vectores en \\(\\mathbb{R}^n\\) se define como:\n\\(\\bar{x} = [x_1, ..., x_n]\\) e \\(\\bar{y} = [y_1, ..., y_n]\\)\n\\[ \\bar{x} \\cdot \\bar{y} = ||\\bar{x}|| ||\\bar{y}|| Cos\\theta =  \\sum_{i=1}^n x_i y_i = x_1 y_1 + ... + x_n y_n\\]\nA veces el producto interno se denota como \\(\\bar{x}^T \\bar{y}\\) o \\(\\langle \\bar{x}, \\bar{y} \\rangle\\).\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nConmutatividad: \\(\\bar{x} \\cdot \\bar{y} = \\bar{y} \\cdot \\bar{x}\\)\nLinealidad: \\((\\alpha \\bar{x})\\cdot \\bar{y} = \\alpha(\\bar{x}\\cdot \\bar{y})\\)\nDistributividad: \\(\\bar{x} \\cdot (\\bar{y} + \\bar{z}) = (\\bar{x} \\cdot \\bar{y}) + (\\bar{x} \\cdot \\bar{z})\\)\n\\(||\\bar{x}||^2 = \\bar{x} \\cdot \\bar{x}\\)\n\n\n\n\n\n\n\n\n\n\nAplicaciones\n\n\n\nOrtogonalidad: Dos vectores son ortogonales si su producto interno es cero.\nSimilaridad: Se puede usar el Cosine Similarity para calcular qué tan parecidos son dos vectores.\n\n\n\n\n\n\n\n\nCosine Similarity\n\n\n\\[sim(\\bar{a}, \\bar{b}) = \\frac{\\bar{a} \\cdot \\bar{b}}{||\\bar{a}|| \\cdot ||\\bar{b}||}\\]\n\n1 implica misma dirección (idénticos)\n-1 implica direcciones opuestas (opuestos).\n0 implica totalmente distintos (ortogonales)."
  },
  {
    "objectID": "tics579/clase-P1.html#vectores-otras-propiedades",
    "href": "tics579/clase-P1.html#vectores-otras-propiedades",
    "title": "TICS-579-Deep Learning",
    "section": "Vectores: Otras Propiedades",
    "text": "Vectores: Otras Propiedades\n\n\n\n\n\n\nCombinación Lineal\n\n\n\nSe denomina una combinación lineal de vectores a la suma ponderada de estos.\n\nEj: \\(\\bar{w} = \\alpha \\cdot \\bar{x} + \\beta \\cdot \\bar{y} + \\gamma \\cdot \\bar{z}\\)\n\\(\\bar{w}\\) es una combinación lineal de los vectores \\(\\bar{x}, \\bar{y}, \\bar{z}\\).\n\n\n\n\n\n\n\n\n\nIndependencia Lineal\n\n\n\nUn conjunto de vectores es linealmente independiente si:\n\n\\(\\alpha_1 \\cdot \\bar{x}_1 + \\alpha_2 \\cdot \\bar{x}_2 + ... + \\alpha_n \\cdot \\bar{x}_n = 0\\) implica que \\(\\alpha_i = 0\\) para todo \\(i\\).\n\n\n\n\n\n\n\n\n\n👀 Ojito\n\n\nHay otras propiedades sumamente importantes de vectores, por lo que coloquen atención al curso de Algebra Lineal."
  },
  {
    "objectID": "tics579/clase-P1.html#matrices-definición",
    "href": "tics579/clase-P1.html#matrices-definición",
    "title": "TICS-579-Deep Learning",
    "section": "Matrices: Definición",
    "text": "Matrices: Definición\n\n\nCorresponde a un arreglo bidimensional de números reales. Se dice que una matriz es de \\(m\\times n\\) o que es \\(\\mathbb{R}^{m \\times n}\\) cuando tiene \\(m\\) filas y \\(n\\) columnas.\n\\[A = \\begin{bmatrix}\nA_{1,1} & \\dots & A_{1,n} \\\\\nA_{2,1} & \\dots & A_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{m, 1} & \\dots & A_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\n\n\n\n\n\n👀\n\n\n\nNormalmente se utiliza \\(m\\) para denotar el número de registros y \\(n\\) como el número de features de un dataset tabular (o también conocido como Dataframe).\nSi \\(m=n\\) nos referimos a una matriz cuadrada."
  },
  {
    "objectID": "tics579/clase-P1.html#matrices-notación",
    "href": "tics579/clase-P1.html#matrices-notación",
    "title": "TICS-579-Deep Learning",
    "section": "Matrices: Notación",
    "text": "Matrices: Notación\n\nSi \\(A\\) es una matriz entonces:\n\n\\(A_{i,j}\\) corresponde al elemento en la fila \\(i\\) y columna \\(j\\). Es decir, un escalar.\n\\(A_{i,:}\\) corresponde a la fila \\(i\\) completa. Es decir, un vector fila.\n\\(A_{:,j}\\) corresponde a la columna \\(j\\) completa. Es decir un vector columna.\n\n\n\n\\[A = \\begin{bmatrix}\n0.2 & 1 & -5.2 & 3.1 & -1.3 \\\\\n-0.5 & 10 & 0 & 3.1 & 3 \\\\\n2 & 25 & -5.2 & 0 & 0 \\\\\n100 & 3.4 & 4.1 & 0 & 42\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\nImportante: Recordar que los índices en Python son 0-based.\n\n\n\n\\(A_{2,4} = 3.1\\)\n\\(A_{:,3} = \\begin{bmatrix}-5.2 & 0 & -5.2 & 4.1\\end{bmatrix}^T\\)\n\\(A_{1,:} = \\begin{bmatrix} 0.2, 1, -5.2, 3.1, -1.3\\end{bmatrix}\\)"
  },
  {
    "objectID": "tics579/clase-P1.html#matrices-suma",
    "href": "tics579/clase-P1.html#matrices-suma",
    "title": "TICS-579-Deep Learning",
    "section": "Matrices: Suma",
    "text": "Matrices: Suma\n\n\n\n\n\n\n\n\nOperación Suma\n\n\n\nPermite sumar dos matrices elemento a elemento.\nEj: Sea \\(A\\) y \\(B\\) dos matrices:\n\n\\[A = \\begin{bmatrix}\nA_{1,1} & \\dots & A_{1,n} \\\\\nA_{2,1} & \\dots & A_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{m, 1} & \\dots & A_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\n\\[B = \\begin{bmatrix}\nB_{1,1} & \\dots & B_{1,n} \\\\\nB_{2,1} & \\dots & B_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nB_{m, 1} & \\dots & B_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\n\n\n\n\n\n\n\n\nResultado\n\n\n\\[A + B = \\begin{bmatrix}\nA_{1,1} + B_{1,1} & \\dots & A_{1,n} + B_{1,n} \\\\\nA_{2,1} + A_{2,1} & \\dots & A_{2,n} + B_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{m, 1} + B_{m,1} & \\dots & A_{m,n} + B_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\n\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nAsociatividad: \\((A + B) + C = A + (B + C)\\)\nConmutatividad: \\(A + B = B + A\\)\nElemento Neutro: \\(A + 0 = A\\)\nElemento Inverso: \\(A + (-A) = 0\\)"
  },
  {
    "objectID": "tics579/clase-P1.html#matrices-ponderación",
    "href": "tics579/clase-P1.html#matrices-ponderación",
    "title": "TICS-579-Deep Learning",
    "section": "Matrices: Ponderación",
    "text": "Matrices: Ponderación\n\n\n\n\n\n\n\n\nOperación Ponderación\n\n\n\nPermite multiplicar/ponderar cada elemento de la matriz por un escalar.\nEj: Sea \\(A\\) una matriz:\n\n\\[A = \\begin{bmatrix}\nA_{1,1} & \\dots & A_{1,n} \\\\\nA_{2,1} & \\dots & A_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{m, 1} & \\dots & A_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\ny \\(\\gamma\\) un escalar.\n\n\n\n\n\n\n\n\nResultado\n\n\n\\[\\gamma \\cdot A = \\begin{bmatrix}\n\\gamma \\cdot A_{1,1} & \\dots & \\gamma \\cdot A_{1,n} \\\\\n\\gamma \\cdot A_{2,1} & \\dots & \\gamma \\cdot A_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\gamma \\cdot A_{m, 1} & \\dots & \\gamma \\cdot A_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\n\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nDistibutividad Escalar: \\(\\gamma(A + B) = \\gamma A + \\gamma B\\)\nDistibutividad Matricial: \\((\\gamma + \\delta) A = \\gamma A + \\delta A\\)\nCompatibilidad: \\((\\gamma \\delta) A = \\gamma (\\delta A) = \\delta (\\gamma A)\\)"
  },
  {
    "objectID": "tics579/clase-P1.html#transpuesta-y-reshape",
    "href": "tics579/clase-P1.html#transpuesta-y-reshape",
    "title": "TICS-579-Deep Learning",
    "section": "Transpuesta y Reshape",
    "text": "Transpuesta y Reshape\n\n\n\n\n\n\n\n\nTranspuesta\n\n\nSea:\n\\[A = \\begin{bmatrix}\nA_{1,1} & \\dots & A_{1,n} \\\\\nA_{2,1} & \\dots & A_{2,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{m, 1} & \\dots & A_{m,n} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\]\nEntonces, \\(A^T\\) se define como:\n\\[A^T = \\begin{bmatrix}\nA_{1,1} & \\dots & A_{1,m} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{n,1} & \\dots & A_{n,m} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times m}\\]\nEs decir, intercambiamos filas por las columnas y viceversa.\n\n\n\n\n\n\n\n\n\n\nReshape\n\n\n\\[B = \\begin{bmatrix}\n1 & 3 & 5 \\\\\n1 & 7 & 9 \\\\\n4 & 6 & 7 \\\\\n3 & 3 & 5 \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}\\]\nPodemos hacer un reshape a (6,2)\n\\[B_{reshaped} = \\begin{bmatrix}\n1 & 3 \\\\\n5 & 1 \\\\\n7 & 9 \\\\\n4 & 6 \\\\\n7 & 3 \\\\\n3 & 5\n\\end{bmatrix} \\in \\mathbb{R}^{6 \\times 2}\\]"
  },
  {
    "objectID": "tics579/clase-P1.html#producto-matriz-vector-por-la-derecha",
    "href": "tics579/clase-P1.html#producto-matriz-vector-por-la-derecha",
    "title": "TICS-579-Deep Learning",
    "section": "Producto Matriz-Vector (Por la derecha)",
    "text": "Producto Matriz-Vector (Por la derecha)\nA diferencia de todas las otras operaciones, el producto entre una matriz y un vector no es conmutativo.\nPost-multiplicación (Multiplicación por la derecha)\n\n\n\n\n\n\n\n\nSea\n\n\n\\[\\bar{y} = A \\cdot \\bar{x}\\]\n\\[A = \\begin{bmatrix}\n2 & 3 & 0 \\\\\n1 & 0 & 7\n\\end{bmatrix}\\]\n\\[\\bar{x} = \\begin{bmatrix}\n4 \\\\\n2 \\\\\n1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\nAtención\n\n\nLa post-multiplicación se puede ver como la combinación lineal de las columnas de una matriz por cada elemento del vector. \\[\n\\begin{align}\n\\bar{y} = A \\cdot \\bar{x} &= \\begin{bmatrix}\n2 \\cdot 4 + 3 \\cdot 2 + 0 \\cdot 1 \\\\\n1 \\cdot 4 + 0 \\cdot 2 + 7 \\cdot 1\n\\end{bmatrix} \\\\\n&= 4 \\cdot \\begin{bmatrix}2 \\\\ 1\\end{bmatrix} + 2 \\cdot \\begin{bmatrix}3 \\\\ 0\\end{bmatrix} + 1 \\cdot \\begin{bmatrix}0 \\\\ 7\\end{bmatrix} \\\\\n&= \\begin{bmatrix}14 \\\\ 11\\end{bmatrix}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n👀\n\n\n\nLa multiplicación sólo es válida si la dimensión de las columnas de la matriz es igual a la dimensión del vector. El resultado siempre es un vector columna.\nLa multiplicación de una fila por una columna es equivalente al Producto Interno. Es decir, \\(\\bar{y}_{i,:} = A_{i,:} \\cdot \\bar{x}\\)"
  },
  {
    "objectID": "tics579/clase-P1.html#producto-matriz-vector-por-la-izquierda",
    "href": "tics579/clase-P1.html#producto-matriz-vector-por-la-izquierda",
    "title": "TICS-579-Deep Learning",
    "section": "Producto Matriz-Vector (Por la izquierda)",
    "text": "Producto Matriz-Vector (Por la izquierda)\nA diferencia de todas las otras operaciones, el producto entre una matriz y un vector no es conmutativo.\nPre-multiplicación (Multiplicación por la izquierda)\n\n\n\n\n\n\n\n\nSea\n\n\n\\[\\bar{y}^T = \\bar{x}^T \\cdot A\\]\n\\[A = \\begin{bmatrix}\n2 & 3 & 0 \\\\\n1 & 0 & 7\n\\end{bmatrix}\\]\n\\[\\bar{x} = \\begin{bmatrix}\n2 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\nAtención\n\n\nLa pre-multiplicación se puede ver como la combinación lineal de las filas de una matriz por cada elemento del vector. \\[\n\\begin{align}\n\\bar{y}^T = \\bar{x}^T \\cdot A &=\n\\begin{bmatrix}\n(2 \\cdot 2 + 1 \\cdot 1)  & (2 \\cdot 3 + 1 \\cdot 0) & (2 \\cdot 0 + 1 \\cdot 7) \\\\\n\\end{bmatrix} \\\\\n&= 2 \\cdot \\begin{bmatrix} 2 & 3 & 0\\end{bmatrix} + 1 \\cdot \\begin{bmatrix} 1 & 0 & 7\\end{bmatrix} \\\\\n&= \\begin{bmatrix}5 & 6 & 7\\end{bmatrix}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n👀\n\n\n\nLa multiplicación sólo es válida si la dimensión de las filas de la matriz es igual a la dimensión del vector. El resultado siempre es un vector fila"
  },
  {
    "objectID": "tics579/clase-P1.html#producto-matriz-matriz",
    "href": "tics579/clase-P1.html#producto-matriz-matriz",
    "title": "TICS-579-Deep Learning",
    "section": "Producto Matriz-Matriz",
    "text": "Producto Matriz-Matriz\nCorresponde a una operación que permite multiplicar 2 matrices si las columnas de la primera son iguales a las filas de la segunda. Una matriz de \\(n \\times p\\) multiplicada con una de \\(p \\times m\\) nos dará una matriz de \\(n \\times m\\). La manera de multiplicar es tomar cada fila de la primera y multiplicarla por cada columna de la segunda.\n\n\n\n\nOjito!!\n\n\n\nLa multiplicación matricial es equivalente a \\(m\\) post-multiplicaciones Matriz-Vector, stackeadas hacia el lado.\nTambién se puede ver como \\(n\\) pre-multiplicaciones Matriz-Vector, stackeadas hacia abajo.\n\n\\[\n\\begin{align}\nAB &= \\begin{bmatrix}\nA_{1,1} & \\dots & A_{1,p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{n, 1} & \\dots & A_{n,p} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{1,1} & \\dots & B_{1,m} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nB_{p, 1} & \\dots & B_{p,m} \\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n| & &  | \\\\\nA \\cdot B_{:,1}& \\dots &  A \\cdot B_{:,m} \\\\\n| & &  | \\\\\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n- & A_{1,:} \\cdot B & - \\\\\n& \\vdots &  \\\\\n- & A_{n,:} \\cdot B & - \\\\\n\\end{bmatrix}\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "tics579/clase-P1.html#producto-matriz-matriz-propiedades-útiles",
    "href": "tics579/clase-P1.html#producto-matriz-matriz-propiedades-útiles",
    "title": "TICS-579-Deep Learning",
    "section": "Producto Matriz-Matriz: Propiedades Útiles",
    "text": "Producto Matriz-Matriz: Propiedades Útiles\n\n\n\n\n\n\n\n\nSupongamos el siguiente caso:\n\n\n\\[A = \\begin{bmatrix}\n4 & 3 & 2 \\\\\n2 & 2 & 4 \\\\\n4 & 4 & 4\n\\end{bmatrix}\nB = \\begin{bmatrix}\n1 & 2 & 1 \\\\\n2 & 3 & 4 \\\\\n4 & 3 & 1 \\\\\n\\end{bmatrix}\n\\]\n\\[\nAB = \\begin{bmatrix}\n18 & 18 & 23 \\\\\n22 & 22 & 14 \\\\\n28 & 24 & 32 \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\nPermutación de Columnas en Post-multiplicación\n\n\nSi permuto columnas de \\[B^* = \\begin{bmatrix}\n1 & 1 & 2 \\\\\n2 & 4 & 3 \\\\\n4 & 1 & 3 \\\\\n\\end{bmatrix}\nAB^*= \\begin{bmatrix}\n18 & 23 & 18\\\\\n22 & 14 & 22\\\\\n28 & 32 & 24\\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRecordar que la multiplicación no es conmutativa. \\(AB \\neq BA\\)."
  },
  {
    "objectID": "tics579/clase-P1.html#otros-productos",
    "href": "tics579/clase-P1.html#otros-productos",
    "title": "TICS-579-Deep Learning",
    "section": "Otros Productos",
    "text": "Otros Productos\n\n\n\n\n\n\n\n\nHadamard Product\n\n\nCorresponde a otra operación que permite multiplicar 2 matrices si y sólo si tienen el mismo tamaño. La multiplicación se realiza elemento a elemento.\n\\[A = \\begin{bmatrix}\n2 & 3 & 0 \\\\\n1 & 0 & 7\n\\end{bmatrix}\\]\n\\[B = \\begin{bmatrix}\n2 & 5 & 1 \\\\\n2& 3 & 7\n\\end{bmatrix}\\]\n\\[A \\odot B = \\begin{bmatrix} 4 & 15 & 0 \\\\ 2 & 0 & 49\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\nOuter Product (Producto Externo)\n\n\nCorresponde a otra operación que permite multiplicar 2 vectores. El resultado es una matriz de tamaño \\(d1 \\times d2\\) donde \\(d1\\) es la dimensión del primer vector y \\(d2\\) es la dimensión del segundo vector.\n\\[\\bar{x} = \\begin{bmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{bmatrix} \\, \\bar{y} = \\begin{bmatrix}\n4 \\\\\n1 \\\\\n5 \\\\\n-2\n\\end{bmatrix}\\]\n\\[\n\\begin{align}\n\\bar{x} \\otimes \\bar{y} = \\bar{x} \\cdot \\bar{y}^T &= \\begin{bmatrix}\n2 \\cdot 4 & 2 \\cdot 1 & 2 \\cdot 5 & 2 \\cdot -2 \\\\\n-1 \\cdot 4 & -1 \\cdot 1 & -1 \\cdot 5 & -1 \\cdot -2 \\\\\n3 \\cdot 4 & 3 \\cdot 1 & 3 \\cdot 5 & 3 \\cdot -2\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n8 & 2 & 10 & -4 \\\\\n-4 & -1 & -5 & 2 \\\\\n12 & 3 & 15 & -6\n\\end{bmatrix}\n\\end{align}\n\\]"
  },
  {
    "objectID": "tics579/clase-P1.html#batch-product",
    "href": "tics579/clase-P1.html#batch-product",
    "title": "TICS-579-Deep Learning",
    "section": "Batch Product",
    "text": "Batch Product\nEste tipo de operación es bastante poco común en otras áreas, pero extremadamente común en Deep Learning.\n\n\n\n\n\n\nEjemplo\n\n\n¿Qué pasa si queremos calcular la multiplicación de un tensor de dimensiones (2, 3, 2) y otra de (2, 2, 4)?\n\nEl resultado es un tensor de dimensiones (2, 3, 4). Podemos interpretarlo como que se harán 2 multiplicaciones a matrices de (3,2) y (2,4) respectivamente (las cuales son compatibles).\n\n\n\n\n\n\n\n\\[A = \\begin{bmatrix}\n\\begin{bmatrix}\n2 & 2 \\\\\n2 & 3 \\\\\n1 & 2 \\\\\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n1 & 2 \\\\\n4 & 1 \\\\\n1 & 4 \\\\\n\\end{bmatrix} \\\\\n\\end{bmatrix}\nB = \\begin{bmatrix}\n\\begin{bmatrix}\n4 & 3 & 4 & 4 \\\\\n3 & 3 & 3 & 2 \\\\\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n4 & 2 & 4 & 4 \\\\\n4 & 1 & 1 & 4 \\\\\n\\end{bmatrix}\n\\end{bmatrix}\n\\]\n\\[\nAB = \\begin{bmatrix}\n\\begin{bmatrix}\n14 & 12 & 14 & 12 \\\\\n17 & 15 & 17 & 14 \\\\\n10 & 9 & 10 & 8 \\\\\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n12 & 4 & 6 & 12 \\\\\n20 & 9 & 17 & 20 \\\\\n20 & 6 & 8 & 20 \\\\\n\\end{bmatrix} \\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEs importante notar que para que esta multiplicación sea válida. Las dimensiones de las matrices internas deben ser compatibles.\nEl Batch Size tiene que ser idéntico."
  },
  {
    "objectID": "tics579/clase-9.html#datos-secuenciales",
    "href": "tics579/clase-9.html#datos-secuenciales",
    "title": "TICS-579-Deep Learning",
    "section": "Datos Secuenciales",
    "text": "Datos Secuenciales\n\n\n\n\n\n\nHasta ahora, asumimos que todos nuestros datos son i.i.d. Pero en la realidad existen datos que contienen una secuencia temporal y que debe ser considerada al momento de modelarlos.\n\n\n\n\n\nTime Series\n\n\n\n\n\nAudio\n\n\n\n\n\n\nText\n\n\n\n\n\nGenoma"
  },
  {
    "objectID": "tics579/clase-9.html#datos-secuenciales-1",
    "href": "tics579/clase-9.html#datos-secuenciales-1",
    "title": "TICS-579-Deep Learning",
    "section": "Datos Secuenciales",
    "text": "Datos Secuenciales\n\nTambién pudiesen existir datos “multimodales”, donde por ejemplo, se combinan secuencias con imágenes.\n\n\n\nImage Time Series\n\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "tics579/clase-9.html#modelamiento-de-una-secuencia",
    "href": "tics579/clase-9.html#modelamiento-de-una-secuencia",
    "title": "TICS-579-Deep Learning",
    "section": "Modelamiento de una secuencia",
    "text": "Modelamiento de una secuencia\n\n\n\n\n\n\n\n\n\n\n\nEn este caso consideramos que \\(x_t\\) corresponde a la instancia de un dato en el tiempo \\(t\\) asociado a un target \\(y_t\\). Además tenemos cierta dependencia entre los elementos en \\(t\\) y \\(t+1\\).\n\n\n\n\n\n\n\n\n\nEs importante notar que \\(x_t\\) no tiene por qué ser un escalar, sino que puede ser un vector de constituidos por varios features.\n\n\n\n\n\n\n\n\n\nOjo\n\n\nUn dato corresponde una secuencia de elementos, por lo tanto \\(x = [x_1,x_2,...x_L]\\), donde \\(L\\) es el largo de la secuencia."
  },
  {
    "objectID": "tics579/clase-9.html#modelamiento-de-una-secuencia-ejemplo",
    "href": "tics579/clase-9.html#modelamiento-de-una-secuencia-ejemplo",
    "title": "TICS-579-Deep Learning",
    "section": "Modelamiento de una secuencia: Ejemplo",
    "text": "Modelamiento de una secuencia: Ejemplo\nPart of Speech Tagging\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante recalcar que la correcta descripción de cada palabra depende del contexto en el que se está usando y no sólo la palabra en sí misma."
  },
  {
    "objectID": "tics579/clase-9.html#speech-recognition",
    "href": "tics579/clase-9.html#speech-recognition",
    "title": "TICS-579-Deep Learning",
    "section": "Speech Recognition",
    "text": "Speech Recognition\n\nEl contexto ayuda a interpretar cuál es la manera correcta de interpretar el sonido emitido."
  },
  {
    "objectID": "tics579/clase-9.html#recurrent-neural-networks",
    "href": "tics579/clase-9.html#recurrent-neural-networks",
    "title": "TICS-579-Deep Learning",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\n\nRNN\n\nCorresponden a un tipo de red neuronal diseñada para procesar secuencias de datos manteniendo en memoria los inputs previos. A diferencia de los otros tipos de redes que procesan datos de manera independiente, acá existen conexiones cíclicas que permiten retener información en el tiempo.\n\n\nEjemplo\n\n\n\n\n\n\n\n\n\n\n\nPros\n\n\n\nPueden tomar secuencias de distinto tamaño (Largo) como predictores de un problema.\nToman como antecedentes los puntos pasados como referencia para las predicciones futuras.\n\n\n\n\n\n\n\nCons\n\n\n\nSe van complicando a medida que las secuencias son cada vez más largas.\nVanishing/Exploding Gradients Problem."
  },
  {
    "objectID": "tics579/clase-9.html#rnns",
    "href": "tics579/clase-9.html#rnns",
    "title": "TICS-579-Deep Learning",
    "section": "RNNs",
    "text": "RNNs\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupongamos los siguientes parámetros para nuestro modelo:\n\n\\(W_1\\) = 1.8\n\\(W_2\\) = -0.5\n\\(W_3\\) = 1.1\n\\(sigma(\\cdot) = ReLU(\\cdot)\\)\nConsideraremos los bias como 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOjo\n\n\nEsta es solamente una secuencia de largo 9."
  },
  {
    "objectID": "tics579/clase-9.html#unrolled-rnn",
    "href": "tics579/clase-9.html#unrolled-rnn",
    "title": "TICS-579-Deep Learning",
    "section": "Unrolled RNN",
    "text": "Unrolled RNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl “Unrollment” o el desenrollar la Red, se refiere a generar copias de las redes que irán recibiendo las salidas de las capas en un \\(t\\) anterior. Estos valores, conocidos como “hidden state”, irán alimentando las siguientes copias considerando los casos anteriores. Sólo el último output es el que nos interesa para la predicción.\n\n\n\n\n\n\n\n\n\n\n\nTodos los parámetros de la red (Weights and Biases), son compartidos por cada una de las copias realizadas.\nSe realizarán tantas copias como el largo de la secuencia. Esto permite que pase toda la secuencia “de una sola vez”.\nSin importar cuantas copias se realicen, el número de parámetros NO aumenta."
  },
  {
    "objectID": "tics579/clase-9.html#rnn-formalmente",
    "href": "tics579/clase-9.html#rnn-formalmente",
    "title": "TICS-579-Deep Learning",
    "section": "RNN formalmente",
    "text": "RNN formalmente\n\n\n\n\n\n\n\ndonde:\n\\[h_t = f(W_{hh} \\cdot h_{t-1} + W_{hx} \\cdot x_t + b_h)\\] \\[y_t = g(W_{yh}\\cdot h_{t} + b_y)\\]\n\n\n\\(h_t \\in \\mathbb{R}^d\\)\n\\(x_t \\in \\mathbb{R}^n\\)\n\\(y_t \\in \\mathbb{R}^k\\)\n\n\n\n\\(W_{hh} \\in \\mathbb{R}^{d \\times d}\\)\n\\(W_{hx} \\in \\mathbb{R}^{d \\times n}\\)\n\\(W_{yh} \\in \\mathbb{R}^{k \\times d}\\)\n\\(b_h \\in \\mathbb{R}^d\\)\n\\(b_y \\in \\mathbb{R}^k\\)"
  },
  {
    "objectID": "tics579/clase-9.html#stacking-rnns",
    "href": "tics579/clase-9.html#stacking-rnns",
    "title": "TICS-579-Deep Learning",
    "section": "Stacking RNNs",
    "text": "Stacking RNNs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs posible juntar varias capas recurrentes, para que las salidas de una alimenten un siguiente Hidden State, y que luego de algunas capas efectivamente se llegue a las salidas de interés.\n\n\n\n\n\n\n\n\n\nA diferencia de otro tipos de Redes como las Convolucionales o FFN, la profundidad en este tipo de redes es de bastante menos impacto.\n\n\n\n\n\n\n\n\n\nOJO\n\n\nNo existen salidas intermedias, sino que los Hidden States de capas anteriores son utilizados directamente como inputs de los hidden states posteriores."
  },
  {
    "objectID": "tics579/clase-9.html#vanishingexploding-gradients",
    "href": "tics579/clase-9.html#vanishingexploding-gradients",
    "title": "TICS-579-Deep Learning",
    "section": "Vanishing/Exploding Gradients",
    "text": "Vanishing/Exploding Gradients\n\n\n\n\n\n\n\nEntre más larga se la secuencia (más unrolls se realicen), más difícil es entrenar la red.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Output = Input \\times W_2^{N_{Unroll}}\\]\n\nSi \\(W_2\\) corresponde a parámetros muy pequeños (menores a 1), entonces, el gradiente se desvanecerá (vanishing gradient).\nSi \\(W_2\\) corresponde a parámetros muy grandes (mayores a 1), entonces, el gradiente explotará (exploding gradient).\n\n\n\n\n\n\n\nEsto ocurre ya que si intentamos derivar la función de pérdida con respecto a alguno de los parámetros, eventualmente \\(W_2^{N_unroll}\\) aparecerá en la ecuación, provocando dicho efecto en el gradiente.\n\n\n\n\n\n\n\n\n\nEsta es quizás la razón más importante del por qué Vanilla RNNs son usadas rara vez en la práctica. Lo importante histórica de este tipo de redes es que abrieron las puertas a sistemas más modernos que hoy en día sí son usados (LSTMs, y Transformers)."
  },
  {
    "objectID": "tics579/clase-9.html#lstm-formalmente",
    "href": "tics579/clase-9.html#lstm-formalmente",
    "title": "TICS-579-Deep Learning",
    "section": "LSTM: Formalmente",
    "text": "LSTM: Formalmente\n\nLSTMs\n\nEs un tipo de Red Neuronal Recurrente que está diseñada para capturar dependencias de largo rango en datos secuenciales abordando algunas de las limitaciones de las RNNs tradicionales, tales como el vanishing gradient problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorresponde a la misma forma de una RNN, sólo que el “hidden state” se divide en dos partes: \\(h_t\\) y \\(C_t\\), llamados “hidden state” y “cell state” respectivamente.\n\n\n\n\n\n\n\n\n\nLa manera de calcular el “Hidden State” y el “Cell State” es muchísimo más engorrosa.\n\n\n\n\n\n\n\n\n\n\nSpoiler: El Hidden y Cell State está compuesto por multiples set de parámetros a los cuales se les dan los nombres de forget gate, input gate, cell gate y output gate. Su interpretabilidad nunca ha logrado ser completamente explicada."
  },
  {
    "objectID": "tics579/clase-9.html#long-short-term-memory-rnns-1997",
    "href": "tics579/clase-9.html#long-short-term-memory-rnns-1997",
    "title": "TICS-579-Deep Learning",
    "section": "Long Short-Term Memory RNNs (1997)",
    "text": "Long Short-Term Memory RNNs (1997)\n\n\n\n\n\n\n\n\nLa LSTM está regida por las siguientes ecuaciones:\n\\[i_t = \\sigma(W_{ii}x_t) + b_{ii} + W_{hi}h_{t-1} + b_{hi}\\]\n\\[f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf})\\]\n\\[g_t = tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})\\]\n\\[o_t = \\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})\\]\n\\[c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\\]\n\\[h_t = o_t \\odot tanh(c_t)\\]\n\n\n\n\n\n\n\n\nTodas estos elementos \\(i_t,f_t, g_t,o_t, c_t,h_t \\in \\mathbb{R}^d\\), donde \\(d\\) corresponde a la “hidden dimension”."
  },
  {
    "objectID": "tics579/clase-9.html#lstm-forget-gate",
    "href": "tics579/clase-9.html#lstm-forget-gate",
    "title": "TICS-579-Deep Learning",
    "section": "LSTM: Forget Gate",
    "text": "LSTM: Forget Gate\n\n\n\n\n\n\n\n\n\nForget Gate\n\n\n\nCorresponde a una red neuronal que indica qué informacion debe ser descartada del Cell State.\nBásicamente combina la secuencia en el tiempo t y el hidden state anterior.\nLuego se le aplica una Sigmoide que indicará el porcentaje a olvidar.\n\n\\[f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf})\\]"
  },
  {
    "objectID": "tics579/clase-9.html#lstm-input-y-cell-gate",
    "href": "tics579/clase-9.html#lstm-input-y-cell-gate",
    "title": "TICS-579-Deep Learning",
    "section": "LSTM: Input y Cell Gate",
    "text": "LSTM: Input y Cell Gate\n\n\n\n\n\n\n\n\n\nInput Gate\n\n\n\nControla Cuánta información debe ingresar al Cell State.\n\n\\[i_t = \\sigma(W_{ii}x_t) + b_{ii} + W_{hi}h_{t-1} + b_{hi}\\]\n\n\n\n\n\n\n\n\nCell Gate\n\n\n\nRepresenta los potenciales nuevos candidatos a entrar al Cell State.\n\n\\[g_t = tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})\\]"
  },
  {
    "objectID": "tics579/clase-9.html#output-gate-y-hidden-state",
    "href": "tics579/clase-9.html#output-gate-y-hidden-state",
    "title": "TICS-579-Deep Learning",
    "section": "Output Gate y Hidden State",
    "text": "Output Gate y Hidden State\n\n\n\n\n\n\n\n\n\nOutput Gate\n\n\n\nDetermina qué “porcentaje” de información del “Cell State” debe salir como “Hidden State” para el tiempo \\(t\\) actual.\n\\[o_t = \\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})\\]\n\n\n\n\n\n\n\n\n\nHidden State\n\n\n\nCorresponde a las dependencias del tiempo anterior que se van traspasando en cada time step.\nAdicionalmente el Hidden State corresponde a la salida de la red para el tiempo \\(t\\).\n\n\\[h_t = o_t \\odot tanh(c_t)\\]"
  },
  {
    "objectID": "tics579/clase-9.html#cell-state",
    "href": "tics579/clase-9.html#cell-state",
    "title": "TICS-579-Deep Learning",
    "section": "Cell State",
    "text": "Cell State\n\n\n\n\n\n\n\n\n\nCell State\n\n\nRepresenta la principal innovación de este tipo de redes ya que permite recordar dependencias de largo plazo (es decir time steps anteriores en secuencias largas). Esto ya que el Cell State puede avanzar casi sin interacciones lineales (no hay parámetros que influyen en ella, por lo que no es afectada por problemas de gradientes).\n\\[c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\\]"
  },
  {
    "objectID": "tics579/clase-9.html#gated-recurrent-unit-2014",
    "href": "tics579/clase-9.html#gated-recurrent-unit-2014",
    "title": "TICS-579-Deep Learning",
    "section": "Gated Recurrent Unit (2014)",
    "text": "Gated Recurrent Unit (2014)\n\nGRU\n\nCorresponde a otro tipo de Arquitectura Recurrente, similar a la LSTM, pero con una estructura más simplificada en la cuál se mantiene sólo un “Hidden State” y se tienen menos gates.\n\n\n\n\n\n\n\n\n\n\n\n\nHidden State\n\n\nRepresenta la potencial actualización del “Hidden State”.\n\\[h_t = (1-z_t) \\odot n_t + z_t \\odot h_{t-1}\\]\n\n\n\n\n\n\n\nUpdate Gate\n\n\nControla qué porcentaje del “hidden state” previo se lleva al siguiente paso.\n\\[z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz}h_{t-1} + b_{hz})\\]\n\n\n\n\n\n\nReset Gate\n\n\nControla cuánta información del pasado se debe olvidar.\n\\[r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr}h_{t-1} + b_{hr})\\]\n\n\n\n\n\n\nCandidate Hidden State\n\n\nRepresenta la potencial actualización del “Hidden State”.\n\\[n_t = tanh(W_{in} x_t + b_{in} + r_t \\odot (W_{hn} h_{t-1} + b_{hn}))\\]"
  },
  {
    "objectID": "tics579/clase-9.html#bidirectional-rnns",
    "href": "tics579/clase-9.html#bidirectional-rnns",
    "title": "TICS-579-Deep Learning",
    "section": "Bidirectional RNNs",
    "text": "Bidirectional RNNs\nExisten ocasiones en las que se requiere no sólo el contexto de los tiempos anteriores, sino también de los posteriores. Por ejemplo, problemas de traducción.\n\nPara ello existen las redes bidireccionales, en la cual se agrega una segunda capa pero que mueve los hidden state en el otro sentido.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso la capa amarilla será la encargada de detectar dependencias del pasado.\nMientras que la capa verde será la encargada de traer dependencias desde el futuro.\n\n\n\n\n\n\n\n\n\n\nLos hidden states pueden ser capas Vanilla RNN, LSTM o GRUs."
  },
  {
    "objectID": "tics579/clase-9.html#pytorch-layers",
    "href": "tics579/clase-9.html#pytorch-layers",
    "title": "TICS-579-Deep Learning",
    "section": "Pytorch Layers",
    "text": "Pytorch Layers\nnn.RNN(input_size, hidden_size, num_layers=1, batch_first=False, \n        dropout=0, bidirectional=False, nonlinearity=\"tanh\")\nnn.LSTM(input_size, hidden_size, num_layers=1, batch_first=False, \n        dropout=0, bidirectional=False) \nnn.GRU(input_size, hidden_size, num_layers=1, batch_first=False, \n        dropout=0, bidirectional=False) \n\n\ninput_size: Corresponde al número de features de la secuencia.\nhidden_size: Corresponde al número de dimensiones del hidden state.\nnum_layers: Corresponderá al número de capas recurrentes a apilar, por defecto 1.\nbatch_first: Este siempre deben fijarlo como True, de esa manera se espera que los tensores a recibir siempre tengan el batch como primera dimensión. Por defecto False.\n\nLuego RNNs esperan tensores de tamaño \\((N,L,H_{in})\\). Donde \\(N\\) es el batch_size, \\(L\\) es el largo de secuencia y \\(H_in\\) es el input_size.\n\ndropout: Cantidad de dropout a aplicar a la salida de cada capa, excepto la última. Por defecto 0.\nbidirectional: Indica si se hace la red Bidireccional o no. Por defecto False.\nnonlinearity: Función de activación a utilizar para activar cada matriz de peso. Puede ser “tanh” o “relu”. Sólo para Vanilla RNN."
  },
  {
    "objectID": "tics579/clase-6.html#entrenamiento-de-un-modelo",
    "href": "tics579/clase-6.html#entrenamiento-de-un-modelo",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de un Modelo",
    "text": "Entrenamiento de un Modelo\n\nEl entrenamiento de un modelo tiene demasiadas variables que pueden influir en el éxito del modelo. Algunos aspectos relevantes a los que hay que poner énfasis al momento de entrenar:\n\n\nOverfitting\nConvergencia/Tiempo de Convergencia\nGeneralización\nOptimización de Recursos Computacionales/Hardware.\nPrevenir problemas de Vanishing Gradient y Exploding Gradients.\n\n\n\n\n\n\n\nMuchas de las técnicas que veremos acá permiten abordar mejoras en nuestros modelos para uno o más aspectos de los mencionados anteriormente."
  },
  {
    "objectID": "tics579/clase-6.html#normalización",
    "href": "tics579/clase-6.html#normalización",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización",
    "text": "Normalización\n\nEn general el término Normalización está muy trillado y en la práctica se utiliza para referirse a muchos temas distintos. Algunas definiciones conocidas:\n\n\n\nNormalización\n\\[x_{i\\_norm} = \\frac{x_i-x_{min}}{x_{max} - x_{min}}\\] Esta operación se puede hacer mediante MinMaxScaler de Scikit-Learn.\n\nEstandarización\n\\[ x_{i\\_est} = \\frac{x_i - E[x]}{\\sqrt(Var[x])}\\]\nEsta operación se puede hacer mediante StandardScaler de Scikit-Learn."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm",
    "href": "tics579/clase-6.html#normalización-batch-norm",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)\nPaper 2015: Batch Normalization\n\n\n\n\n\n\n\n¿Por qué?\n\n\n\nAcelera el entrenamiento\nDisminuye la importancia de los Parámetros iniciales.\nRegulariza el modelo (un poquito)\nResuelve el problema de Internal Covariate Shift.\n\n\n\n\n\nEjemplo: Supongamos que dado la altura y la edad queremos predecir si será deportista de alto rendimiento."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-1",
    "href": "tics579/clase-6.html#normalización-batch-norm-1",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)\n\n\n\n\n\n\n\n\n\nCambios en Altura son mucho más pequeños que en Edad debido al rango.\nToma más tiempo optimizar (requiere parámetros más pequeños)\nSi el learning rate es alto puede diverger.\nSi el learning rate es bajo implica que demora mucho más en converger.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPros\n\n\n\nSin importar el punto inicial, el mínimo se encuentra casi a la misma distancia.\n\nEs posible utilizar un learning rate más grande sin miedo a diverger.\n\n\n\n\n\n\n\n\n\n\nCons\n\n\n\nMás cálculos y parámetros involucrados"
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-2",
    "href": "tics579/clase-6.html#normalización-batch-norm-2",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)"
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-3",
    "href": "tics579/clase-6.html#normalización-batch-norm-3",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)\n\n\n\n\n\n\n\n\nCálculo de Estadísticos\n\n\n\\[ \\mu_B = \\frac{1}{B} \\sum_{i=1}^B z^{(i)} = \\frac{1}{3}(4 + 7 + 5) = 5.33\\] \\[ \\sigma_B^2 = \\frac{1}{B} \\sum_{i=1}^B (z^{(i)} - \\mu_B)^2 = 1.555\\]\n\n\n\n\n\n\n\n\n\nNormalización\n\n\n\\[\\widehat{z^{(i)}} = \\frac{z^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\n\n\n\n\n\n\n\n\n\n\nScale and Shift: \\(\\gamma\\) y \\(\\beta\\) son parámetros.\n\n\n\\[BN_{\\gamma,\\beta}(z_i)= \\gamma \\widehat{z_i} + \\beta \\]\n\n\n\n\n\n\n\n\n\n\nZ2 norm\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nDonde \\(\\gamma\\) y \\(\\beta\\) son parámetros aprendidos durante el entrenamiento."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-test-time",
    "href": "tics579/clase-6.html#normalización-batch-norm-test-time",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm): Test Time",
    "text": "Normalización (Batch Norm): Test Time\n\n\n\n\n\n\nProblema\n\n\nLa predicción de una instancia \\(i\\) específica, ahora depende de otros elementos dentro del Batch. ¿Cómo funciona entonces el modelo en Test Time?\n\n\n\n\n\n\n\n\n\nSe estiman valores de \\(\\mu_B\\) y \\(\\sigma_B\\) para usar en inferencia basados en los valores obtenidos en entrenamiento.\n\n\n\n\n\n\n\n\n\n\n\nEstimación de Estadísticos\n\n\n\n\\(\\mu_B^{inf} = E[\\mu_B^{j}]\\), \\(j = 1,...,B\\)\n\\(\\sigma_B^{inf} = \\frac{m}{m-1}E[\\mu_B^{j}]\\), \\(j = 1,...,B\\)\n\n\n\n\n\n\n\n\n\n\n\nNormalización\n\n\n\\[\\widehat{z^{(i)}} = \\frac{z^{(i)} - \\mu_B^{inf}}{\\sqrt{(\\sigma_B^{inf})^2 + \\epsilon}}\\]\n\n\n\n\n\n\n\n\n\n\nScale and Shift: \\(\\gamma\\) y \\(\\beta\\) son parámetros.\n\n\n\\[BN_{\\gamma,\\beta}(z_i)= \\gamma \\widehat{z_i} + \\beta \\]\n\n\n\n\n\n\n\n\n\nLos parámetros \\(\\gamma\\) y \\(\\beta\\) son los aprendidos durante el proceso de entrenamiento."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-consejos",
    "href": "tics579/clase-6.html#normalización-batch-norm-consejos",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm): Consejos",
    "text": "Normalización (Batch Norm): Consejos\n\nAndrew Ng propone utilizar BatchNorm justo antes de la función de Activacion.\nEl paper original también propone su uso justo antes de la activación.\nFrancoise Chollet, creador de Keras dice que los autores del paper en realidad lo utilizaron después de la función de activación.\nAdicionalmente existen benchmarks que muestran mejoras usando BatchNorm después de las funciones de activación.\n\n\n\n\n\n\n\nEntonces, la posición del BatchNorm termina siendo parte de la Arquitectura, y se debe comprobar donde tiene un mejor efecto.\n\n\n\n\n\n\n\n\n\nBatchnorm tiene efectos distintos al momento de entrenar o de evaluar/predecir en un modelo. Por lo tanto, de usar Batchnorm es imperativo utilizar los modos model.train() y model.eval() de manera apropiada."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-layer-norm",
    "href": "tics579/clase-6.html#normalización-layer-norm",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización: Layer Norm",
    "text": "Normalización: Layer Norm\nPaper 2016: Layer Normalization\n\n\n\n\n\n\nBatch Norm tiene algunos problemas:\n\n\n\nMuy difícil de calcular en datos secuenciales (lo veremos más adelante).\nInestable cuando el Batch Size es muy pequeño.\nDifícil de Paralelizar.\n\n\n\n\n\n\n\n\n\n\nBeneficios de Layer Norm\n\n\n\nPuede trabajar con secuencias.\nNo tiene problemas para trabajar con cualquier tipo de Batch Size.\nSe puede paralelizar, lo cuál es útil en redes como las RNN.\n\n\n\n\n\n\n\n\n\n\n\nEn este caso se realiza la normalización por capa o por Data Point (instancia).\nAdemás son el elementos cruciales en las Arquitecturas de Transformers."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-layer-norm-1",
    "href": "tics579/clase-6.html#normalización-layer-norm-1",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización: Layer Norm",
    "text": "Normalización: Layer Norm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\mu_{norm} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} z_j = \\frac{1}{4}(4 + 9 + 6 + 7) = 6.5\\] \\[ \\sigma_{norm}^2 = \\frac{1}{n_i} \\sum_{j=1}^{n_i} (z_j - \\mu_B)^2 = 3.25\\]\n\n\n\n\n\n\nNormalización\n\n\n\\[\\widehat{z_j} = \\frac{z_j - \\mu_{norm}}{\\sqrt{\\sigma_{norm}^2 + \\epsilon}}\\]"
  },
  {
    "objectID": "tics579/clase-6.html#regularización-l2-aka-weight-decay",
    "href": "tics579/clase-6.html#regularización-l2-aka-weight-decay",
    "title": "TICS-579-Deep Learning",
    "section": "Regularización L2 aka Weight Decay",
    "text": "Regularización L2 aka Weight Decay\nPaper 1991: Weight Decay\n\n\nEn general el gran problema de las Redes Neuronales es el Overfitting. Esto porque las redes neuronales normalmente se denominan como Overparametrized Models. ¿Qué significa esto?\n\n\nWeight Decay\n\n\nCorresponde a una penalización que se da a los modelos para limitar su complejidad y asegurar que pueda generalizar correctamente en datos no vistos.\n\n\n\n\\[ \\underset{W_{i:L}}{minimize} \\frac{1}{m} \\sum_{i=1}^m l(h_\\theta(x^{(i)}),y^{(i)}) + \\frac{\\lambda}{2} \\sum_{i=1}^L ||W_i||_f^2\\]\nEso implica una transformación a nuestro Update Rule:\n\\[W_i := W_i - \\alpha \\nabla \\frac{1}{m} \\sum_{i=1}^m l(h_\\theta(x^{(i)}),y^{(i)}) - \\alpha \\lambda W_i = (1-\\alpha\\lambda)W_i - \\alpha \\nabla l(h_\\theta(x^{(i)}),y^{(i)})\\]\n\n\n\n\n\n\n\n\n\nSe puede ver que los pesos (weights) se contraen (decaen) antes de actualizarse en la dirección del gradiente.\n\n\n\n\n\n\n\n\n\n\nPor alguna razón Pytorch decidió implementarlo como una propiedad de los Optimizers cuando en realidad debió ser de la Loss Function."
  },
  {
    "objectID": "tics579/clase-6.html#dropout",
    "href": "tics579/clase-6.html#dropout",
    "title": "TICS-579-Deep Learning",
    "section": "Dropout",
    "text": "Dropout\nPaper 2014: Dropout\n\n\nA diferencia de la estrategia anterior, este tipo de regularización se aplica a las activaciones de la red (resultados de la Transformación Affine, previo a la transformación no lineal).\n\nDefiniremos el Dropout como:\n\\[Z_{i+1} = \\sigma(W_i^T Z_i + b_i)\\] \\[\\widehat{Z_{i+1}} = D(Z_{i+1})\\]\ndonde \\(D\\) implica la aplicación de Dropout a la capa \\(i+1\\). El elemento \\(j\\) de la capa \\(\\widehat{Z_i}\\) se calcula como:\n\\[(\\widehat{Z_{i+1}})_j = \\begin{cases}\n\\frac{(Z_{i+1})_j}{1-p}  & \\text{with prob 1-p} \\\\\n0, & \\text{with prob p}\n\\end{cases}\\]\n\\(p\\) se conoce como el Dropout Rate.\n\n\n\n\n\n\nEl factor \\(\\frac{1}{1-p}\\) se aplica para mantener la varianza estable luego de haber eliminado activaciones con probabilidad \\(p\\).\n\n\n\n\n\n\n\n\n\n\nDropout se aplica normalmente al momento de entrenar el modelo. Por lo tanto, de usar Dropout es imperativo cambiar al modo model.eval() al momento de predecir."
  },
  {
    "objectID": "tics579/clase-6.html#weights-initialization",
    "href": "tics579/clase-6.html#weights-initialization",
    "title": "TICS-579-Deep Learning",
    "section": "Weights Initialization",
    "text": "Weights Initialization\nPaper 2010: Xavier Initialization Paper 2015: Kaiming Initialization\nHemos hablado que los métodos basados en SGD normalmente utilizan valores aleatorios para partir su entrenamiento, lo cual deja un poco al azar el éxito de un proceso de entrenamiento.\nExisten diversos estudios de cómo inicializar los parámetros para una convergencia óptima. Algunas de las inicializaciones son:\n\n\n\n\n\n\nActivaciones Triviales\n\n\n\nConstante\nSólo unos\nSólo Zeros"
  },
  {
    "objectID": "tics579/clase-6.html#weights-initialization-1",
    "href": "tics579/clase-6.html#weights-initialization-1",
    "title": "TICS-579-Deep Learning",
    "section": "Weights Initialization",
    "text": "Weights Initialization\n\n\nXavier o Glorot Uniforme\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{U}(-a,a)\\)\n\\[ a = gain \\cdot \\sqrt{\\frac{6}{fan_{in} + fan_{out}}}\\]\n\nXavier o Glorot Normal\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{N}(0,std^2)\\)\n\\[ std = gain \\cdot \\sqrt{\\frac{2}{fan_{in} + fan_{out}}}\\]\n\n\n\n\n\n\n\n\n\\(fan_{in}\\) corresponde al número de conexiones que entran a una neurona. Mientras que \\(fan_{out}\\) corresponde al número de neuronas que salen de dicha neurona.\n\\(fan\\_mode\\) corresponde a la elección de \\(fan_{in}\\) o \\(fan_{out}\\).\n\n\n\n\n\n\nKaiming (aka He) Uniforme\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{U}(-bound,bound)\\)\n\\[ bound = gain \\cdot \\sqrt{\\frac{3}{fan\\_mode}}\\]\n\nKaiming (aka He) Normal\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{N}(0,std^2)\\)\n\\[std =\\sqrt{\\frac{gain}{fan\\_mode}}\\]"
  },
  {
    "objectID": "tics579/clase-6.html#training-control",
    "href": "tics579/clase-6.html#training-control",
    "title": "TICS-579-Deep Learning",
    "section": "Training Control",
    "text": "Training Control\nEl entrenamiento de una red neuronal puede tomar mucho tiempo. Es por eso que algunas buenas prácticas serían:\n\nDisponer de resultados preliminares aunque el entrenamiento no haya terminado.\nGuardar los pesos del mejor modelo obtenido en el proceso de entrenamiento.\nEvitar entrenar pasado el punto de Overfitting.\n\nAunque hay nuevas ideas de lo que se llama el grokking.\n\n\n\n\n\n\n\n\n\n\nEarly Stopping\n\n\n\nSe refiere al proceso de detener el entrenamiento luego de patience epochs sin mejorar el validation loss u otro criterio.\n\n\n\n\n\n\n\n\n\n\n\nCheckpointing\n\n\n\nCorresponde al proceso de guardar los parámetros obtenidos en un epoch en específico. Normalmente se guarda la mejor epoch y la última, pero se puede generar algún criterio."
  },
  {
    "objectID": "tics579/clase-6.html#categorical-variables",
    "href": "tics579/clase-6.html#categorical-variables",
    "title": "TICS-579-Deep Learning",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nEs importante mencionar que normalmente no se utilizan redes neuronales para poder entrenar datos tabulares. Pero de hacerlo, es muy probable que nos encontremos con variables categóricas. Para ello existen dos técnicas que son las más comunes en redes neuronales.\n\n\nOne Hot Encoder\n\nCorresponde a la representación mediante dummy variables. Normalmente se considera una representación Sparse de los datos.\n\n\n\n\n\n\n\n\nEn Pytorch se puede implementar como F.one_hot(), pero mi recomendación es utilizar las herramientas de Scikit-Learn para evitar Data Leakage."
  },
  {
    "objectID": "tics579/clase-6.html#categorical-variables-1",
    "href": "tics579/clase-6.html#categorical-variables-1",
    "title": "TICS-579-Deep Learning",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nEmbeddings\n\nEs una representación de Densa de los Datos. Corresponde a una representación a en un espacio dimensional definido que es aprendido por la misma red. La representación aprendida considera aspectos como la similaridad la cual se refleja como una medida de distancia.\n\n\nEn Pytorch esto se puede realizar mediante: nn.Embedding().\n\nnn.Embedding(num_embeddings, embedding_dim)\n\n\nnum_embeddings: Corresponde al número de categórías.\nembedding_dim: El número de dimensiones en el cual se quiere representar.\n\n\n\n\n\n\n\n\nEste proceso tiene parámetros entrenables asociados."
  },
  {
    "objectID": "tics579/clase-5.html#entrenamiento-de-la-red",
    "href": "tics579/clase-5.html#entrenamiento-de-la-red",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de la Red",
    "text": "Entrenamiento de la Red\n\nA diferencia de un Modelo de Machine Learning, las Redes Neuronales se entrenan de manera progresiva (se espera una mejora en cada Epoch). Si nuestra Arquitectura es apropiada nosotros deberíamos esperar que el Loss de nuestra red siempre disminuya. ¿Por qué?\n\n\n\n\n\n\n\n\n\n¿Siempre buscamos la Red que tenga el mejor Loss de Entrenamiento?\n\n\n\n\n\n\n\n\n\n\n\n\n\nAl igual que en los modelos de Machine Learning debemos evitar a toda costa el Overfitting. ¿Qué es el overfitting?"
  },
  {
    "objectID": "tics579/clase-5.html#entrenamiento-de-la-red-1",
    "href": "tics579/clase-5.html#entrenamiento-de-la-red-1",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de la Red",
    "text": "Entrenamiento de la Red\n\nBias-Variance Tradeoff (Dilema Sesgo-Varianza)\n\n\nProbablemente el concepto más importante para determinar si un modelo tiene potencial o no. Corresponden a dos tipos de errores que pueden sufrir los modelos de ML.\n\n\n\n\n\n\n\nBias\n\nCorresponde al sesgo, y tiene que ver con la diferencia entre el valor real y el valor predicho. Bajo sesgo implica una mejor predicción.\n\n\n\n\nVariance\n\nCorresponde a la varianza y tiene que ver con la dispersión dada por los valores predichos. Baja Varianza implica un modelo más estable pero menos flexible.\n\n\n\n\n\n\n\n\n\n\nEn general hay que buscar el equilibrio entre ambos tipos de errores:\n\nAlto Sesgo y baja Varianza: Underfitting.\nBajo Sesgo y Alta Varianza: Overfitting."
  },
  {
    "objectID": "tics579/clase-5.html#model-validation",
    "href": "tics579/clase-5.html#model-validation",
    "title": "TICS-579-Deep Learning",
    "section": "Model Validation",
    "text": "Model Validation\n\nValidación Cruzada\n\n\nSe refiere al proceso de entrenar un modelo en una cierta porción de los datos, pero validar sus rendimiento y capacidad de generalización en un set de datos no vistos por el modelo al momento de entrenar.\n\n\n\n\n\n\n\n\n\n\n¿Qué es la Generalización?\n\n\n\n\n\n\n\n\n\n\nLos dos métodos más populares que se usan en Machine Learning son Holdout y K-Fold. Más métodos se pueden encontrar en los docs de Scikit-Learn.\n\n\n\n\n\n\n\n\n\nDebido a los volúmenes de datos utilizados, el esquema de validación más utilizado es el Holdout."
  },
  {
    "objectID": "tics579/clase-5.html#model-validation-holdout",
    "href": "tics579/clase-5.html#model-validation-holdout",
    "title": "TICS-579-Deep Learning",
    "section": "Model Validation: Holdout",
    "text": "Model Validation: Holdout\n\n\n\n\n\n\n\n\n\n\n\nTrain\n\n\nSe utiliza para entrenar.\n\n\n\n\n\n\nValidation\n\n\nSe utiliza para medir el nivel de generalización del modelo.\n\n\n\n\n\n\nTest\n\n\nSe utiliza para evaluar reportando una métrica de diseño del Modelo.\n\n\n\n\n\n\n\n\n\nOJO\n\n\nLoss no es lo mismo que métrica. ¿Cuál es la diferencia?\n\n\n\n\n\n\n\n\n\nA diferencia de un modelo de Machine Learning el proceso de validación del modelo se realiza en paralelo con el entrenamiento. Es decir, se entrena y valida el modelo Epoch a Epoch."
  },
  {
    "objectID": "tics579/clase-5.html#model-validation-k-fold",
    "href": "tics579/clase-5.html#model-validation-k-fold",
    "title": "TICS-579-Deep Learning",
    "section": "Model Validation: K-Fold",
    "text": "Model Validation: K-Fold\n\n\n\n\n\n\n\n\n\n\n\nCorresponde al proceso de Holdout pero repetido \\(K\\) veces."
  },
  {
    "objectID": "tics579/clase-5.html#model-evaluation",
    "href": "tics579/clase-5.html#model-evaluation",
    "title": "TICS-579-Deep Learning",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nLa Evaluación del Modelo se hará en torno a una métrica definida a priori por el modelador. ¿Entonces es un Hiperparámetro?\n\n\n\n\n\n\n\nLa métrica a utilizar está íntimamente ligada al tipo de modelo.\n\n\n\n\n\nClasificación\n\n\\(Accuracy = \\frac{1}{m} \\sum_{i = 1}^m 1\\{y_i = \\hat{y_i}\\}\\)\n\\(Precision = \\frac{TP}{TP + FP}\\)\n\\(Recall = \\frac{TP}{TP + FN}\\)\n\\(F1-Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\n\n\nRegresión\n\n\\(RMSE = \\frac{1}{m} \\sum_{i=1}^m (y_i-\\hat{y_i})^2\\)\n\\(MAE = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y_i}|\\)\n\\(MAPE = 100 \\cdot \\frac{1}{m} \\sum_{i=1}^m \\frac{|y_i-\\hat{y_i}|}{max(\\epsilon,y_i)}\\)\n\\(SMAPE = \\frac{2}{m} \\sum_{i=1}^2 \\frac{|y_i - \\hat{y_i}  |}{max(|y_i + \\hat{y_i}|,\\epsilon)}\\)\n\n\n\n\n\n\n\n\n\nLas métricas acá explicadas son métricas básicas de cualquier modelo general de Clasificación y Regresión. Existen muchas otras métricas que son específicas para campos específicos. IoU por ejemplo es una Métrica de Segmentación Semántica, Map@k es una métrica para modelos de Recomendación, Bleu o Rouge son métricas para NLP, etc. Para ver millones de métricas pueden ver las docs de Torchmetrics.\n\n\n\n\n\n\n\n\n\n\nEs posible utilizar métricas para ir monitoreando el progreso del modelo Epoch a Epoch."
  },
  {
    "objectID": "tics579/clase-5.html#training-validation-loop",
    "href": "tics579/clase-5.html#training-validation-loop",
    "title": "TICS-579-Deep Learning",
    "section": "Training-Validation Loop",
    "text": "Training-Validation Loop\n\nCorresponde a la modificación del Training Loop con el Objetivo de Entrenar y Validar de manera simultánea.\n\n\n\n\n\n\n\n\n\n\n\n\nSe realiza un Forward Pass con datos de Train y se calcula el Loss asociado. Internamente, Pytorch comienza a acumular Gradientes."
  },
  {
    "objectID": "tics579/clase-5.html#training-validation-loop-1",
    "href": "tics579/clase-5.html#training-validation-loop-1",
    "title": "TICS-579-Deep Learning",
    "section": "Training-Validation Loop",
    "text": "Training-Validation Loop\n\n\n\n\n\n\n\n\n\n\n\nSe realiza un Backward Pass, se aplican los gradientes y se aplica el Update Rule."
  },
  {
    "objectID": "tics579/clase-5.html#training-validation-loop-2",
    "href": "tics579/clase-5.html#training-validation-loop-2",
    "title": "TICS-579-Deep Learning",
    "section": "Training-Validation Loop",
    "text": "Training-Validation Loop\n\n\n\n\n\n\n\n\n\n\n\nSe realiza un nuevo Forward Pass, pero esta vez con los datos de Validación. En este caso Pytorch internamente sigue acumulando gradientes, lo cual no es correcto. Para ello se debe utilizar un with torch.no_grad(). Se calcula un Validation Loss."
  },
  {
    "objectID": "tics579/clase-5.html#monitoreo-de-un-modelo-validation-curve",
    "href": "tics579/clase-5.html#monitoreo-de-un-modelo-validation-curve",
    "title": "TICS-579-Deep Learning",
    "section": "Monitoreo de un Modelo: Validation Curve",
    "text": "Monitoreo de un Modelo: Validation Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante ser capaz de identificar el momento exacto en el cual el momento comienza su overfitting. Para ello se utiliza el “Checkpointing”.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\nCorresponde a un snapshot del modelo a un cierto punto. En la práctica se almacenan los parámetros del mejor modelo y del último Epoch.\n\n\n\n\n\n\n\n\n\n\nEarlyStopping\n\n\n\nTeoricamente, una vez que la red Neuronal alcanza el punto de Overfitting ya no tiene sentido seguir el entrenamiento. Por lo tanto es posible detener el entrenamiento bajo una cierta condición."
  },
  {
    "objectID": "tics579/clase-2.html#clase-anterior",
    "href": "tics579/clase-2.html#clase-anterior",
    "title": "TICS-579-Deep Learning",
    "section": "Clase anterior",
    "text": "Clase anterior\n\nLa Regresión Softmax es capaz de generar separaciones lineales para más de dos clases para cualquier punto \\(x \\in \\mathbb{R}^{1 \\times n}\\):\n\n\n\n\n\n\\(h_\\theta(x) = \\theta^T x\\), tal que \\(\\theta \\in \\mathbb{R}^{n \\times k}\\).\n\n\n\n\n\n\n\n\n\nEsta hipótesis es bastante limitada, y existen muchos problemas que no podrán solucionarse con este tipo de solución."
  },
  {
    "objectID": "tics579/clase-2.html#limitaciones-de-una-hipótesis-lineal",
    "href": "tics579/clase-2.html#limitaciones-de-una-hipótesis-lineal",
    "title": "TICS-579-Deep Learning",
    "section": "Limitaciones de una Hipótesis Lineal",
    "text": "Limitaciones de una Hipótesis Lineal\n\nEs claro que un problema como el que se muestra acá no podrá ser resuelto mediante un clasificador lineal (hipótesis lineal).\n\n\n\n\n\n\n\n\n\n\n¿Cómo se resuelve este tipo de problemas?\n\n\n\nCreando nuevas features que permitan predecir problemas no-lineales.\n\n\n\n\n\n\n\\[h_\\theta(x) = \\theta^T \\phi(x)\\]\n\ntal que \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(\\phi(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^d\\) con \\(d &gt; n\\).\n\n\n\n\n\n\n\n\nBásicamente \\(\\phi(.)\\) es la manera matemática de denotar la creación de más features que permiten resolver el problema.\n\n\n\n\n\n\n\n\n\n\n\n\nSVM es un algoritmo que hace esto de manera automática utilizando el famoso Kernel Trick, donde \\(\\phi(.)\\) es conocido como el Kernel."
  },
  {
    "objectID": "tics579/clase-2.html#diferencias-entre-ml-y-dl",
    "href": "tics579/clase-2.html#diferencias-entre-ml-y-dl",
    "title": "TICS-579-Deep Learning",
    "section": "Diferencias entre ML y DL",
    "text": "Diferencias entre ML y DL\n\n\n\n\n\n\nLa diferencia principal entre el Machine Learning y el Deep Learning es la manera en la que se crean las features.\n\n\n\n\n\n\n\n\n\nNormalmente el Machine Learning está enfocado en que manualmente se generen features.\nDeep Learning busca que el Algoritmo busque esas features. El énfasis está en buscar la Arquitectura adecuada."
  },
  {
    "objectID": "tics579/clase-2.html#cómo-creamos-features-de-manera-automática",
    "href": "tics579/clase-2.html#cómo-creamos-features-de-manera-automática",
    "title": "TICS-579-Deep Learning",
    "section": "¿Cómo creamos features de manera automática?",
    "text": "¿Cómo creamos features de manera automática?\n\n\nUna primera idea sería crearlas de manera lineal:\n\\[\\phi(x) = W^T x\\]\ndonde \\(W \\in \\mathbb{R}^{n \\times d}\\).\n\n\n\n\n\n\n\n\n\nEn este caso nuestra hipótesis queda como: \\[ h_\\theta(x) = \\theta^T \\phi(x) = \\theta^T W^T x = \\tilde{\\theta}^T x\\]\n\n\n\n\n\n\n\nLamentablemente este approach no funciona, ya que \\(\\tilde{\\theta}^T\\) es sólo otra matriz que genera dos transformaciones simultáneas, pero que en este caso llevará de \\(n\\) a \\(k\\) de manera directa.\n\n\n\n\n\n\n\n\n\n\n\nOjo con las dimensiones.\n\n\n\n\\(W^t\\) tiene dimensión \\(d \\times n\\).\nSabemos que \\(h_\\theta(x)\\) tiene que devolver \\(k\\) outputs. Por lo tanto, \\(\\theta^T\\) tiene que tener dimensiones \\(k \\times d\\).\n\\(x\\) es un vector con \\(n\\) features por lo tanto es de dimensión \\(n \\times 1\\).\nEso hará que \\(h_\\theta(x)\\) sea de tamaño \\(k \\times 1\\)."
  },
  {
    "objectID": "tics579/clase-2.html#entonces-cómo",
    "href": "tics579/clase-2.html#entonces-cómo",
    "title": "TICS-579-Deep Learning",
    "section": "¿Entonces cómo?",
    "text": "¿Entonces cómo?\n\n\nVamos a utilizar funciones no lineales. Cualquiera sirve tal que:\n\\[\\phi(x) = \\sigma(W^Tx)\\]\ndonde \\(W \\in \\mathbb{R}^{n \\times d}\\) y \\(\\sigma: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\), es decir, \\(\\sigma\\) es una función escalar.\n\n\n\n\n\n\n\n\nDe este modo nuestra hipótesis quedaría como:\n\\[h_\\theta(x) = \\theta^T \\sigma(W^T x) \\neq \\tilde{\\theta}^T x\\]\n\n\n\n\n\n\n\n\nEstamos aplicando una transformación no-lineal a la transformación lineal de \\(x\\) con \\(W\\).\n\n\n\n\n\n\n\n\n\n\n\nNormalmente escogeremos funciones no-lineales que sean diferenciables para poder actualizar \\(\\theta\\) y \\(W\\).\nEsto es lo que llamaremos el entrenamiento de una red neuronal."
  },
  {
    "objectID": "tics579/clase-2.html#activation-functions",
    "href": "tics579/clase-2.html#activation-functions",
    "title": "TICS-579-Deep Learning",
    "section": "Activation Functions",
    "text": "Activation Functions\n\n\n\nDefiniremos las funciones de activación como funciones no-lineales que se aplican a la salida de cada capa para evitar la composición de dos trasnformaciones lineales consecutivas.\n\n\n\n\n\n\n\nEsta es la única manera de transformar hipótesis lineales en hipótesis no lineales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunciones Clásicas\n\n\n\nSigmoide\nReLU\nTanh\nSoftmax\n\n\n\n\n\n\n\n\n\n\n\nFunciones más modernas\n\n\n\nSwish\nGELU\nELU"
  },
  {
    "objectID": "tics579/clase-2.html#layer-non-linear-softmax-regression",
    "href": "tics579/clase-2.html#layer-non-linear-softmax-regression",
    "title": "TICS-579-Deep Learning",
    "section": "2-Layer non-linear Softmax Regression",
    "text": "2-Layer non-linear Softmax Regression\n\n\n\\[h_\\theta(x) = W_2^T \\phi(x) = W_2^T \\sigma(W_1^T x)\\]\ndonde \\(\\theta=\\{W_1 \\in \\mathbb{R}^{n \\times d}, W_2 \\in \\mathbb{R}^{d \\times k}\\}\\)\n\n\n\n\n\n\n\nPodemos pensar que \\(W_1 \\in \\mathbb{R}^{n \\times d}\\) es aquella matriz que lleva a un vector \\(x\\) de \\(n\\) a \\(d\\) dimensiones.\nDe la misma forma, \\(W_2 \\in \\mathbb{R}^{d \\times k}\\) es aquella matriz que lleva a un vector \\(x\\) de \\(d\\) a \\(k\\) dimensiones/salidas.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Batch Form\n\\[h_\\theta(X) = \\sigma(XW_1)W_2\\]\n\nUpdate Rule\n\\[W_1 := W_1 - \\frac{\\alpha}{m} \\nabla_{W_1} l_{ce}(h_\\theta(X),y)\\] \\[W_2 := W_2 - \\frac{\\alpha}{m} \\nabla_{W_2} l_{ce}(h_\\theta(X),y)\\]"
  },
  {
    "objectID": "tics579/clase-2.html#cálculo-de-gradientes",
    "href": "tics579/clase-2.html#cálculo-de-gradientes",
    "title": "TICS-579-Deep Learning",
    "section": "Cálculo de Gradientes",
    "text": "Cálculo de Gradientes\n\n\n\n\n\n\n\n\nGradiente de \\(W_1\\)\n\n\n\\[\\begin{align} \\nabla_{W_1} &= \\frac{\\partial l_{ce}(h_\\theta(X),y)}{\\partial \\sigma(XW_1)} \\cdot \\frac{\\partial h_\\theta(X)}{\\partial \\sigma(XW_1)} \\cdot \\frac{\\partial \\sigma(XW_1)}{\\partial XW_1} \\cdot \\frac{\\partial XW_1}{\\partial W_1} \\\\\n&= (Z-I_y)_{m \\times k} \\cdot (W_{2})_{d \\times k}  \\cdot \\sigma'(XW_1)_{m \\times d} \\cdot X_{m \\times n}\n\\end{align}\\]\nLuego, corrigiendo por dimensiones obtenemos que \\[\\nabla_{W_1} \\in \\mathbb{R}^{n \\times d} = X^T_{n \\times m} \\left[\\sigma'(XW_1) \\odot (Z-I_y)W_2^T \\right]_{m \\times d}\\]\n\n\n\n\n\n\n\n\n\n\nGradiente de \\(W_2\\)\n\n\n\\[\\begin{align} \\nabla_{W_2} &= \\frac{\\partial l_{ce}(h_\\theta(X),y)}{\\partial h_\\theta(X)} \\cdot \\frac{\\partial h_\\theta(X)}{\\partial W_2}\\\\\n&= (Z-I_y)_{m\\times k} \\cdot \\sigma(XW_1)_{m \\times d}\n\\end{align}\\]\nLuego, corrigiendo por dimensiones obtenemos que \\[\\nabla_{W_2} \\in \\mathbb{R}^{d \\times k} = \\sigma(XW_1)^T_{d \\times m}(Z - I_y)_{m \\times k}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(\\odot\\) representa el producto Hadamard entre dos matrices. Esto es, multiplicación elemento a elemento.\n\\(\\sigma'(.)\\) representa la derivada de la función de activación \\(\\sigma(.)\\)"
  },
  {
    "objectID": "tics579/clase-2.html#definiciones",
    "href": "tics579/clase-2.html#definiciones",
    "title": "TICS-579-Deep Learning",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\\[Z_1 = X\\]\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Outputs\n\n\n\\[Z_{i+1} = \\sigma_i(Z_iW_i), i=1,...,L\\] \\[Z_i \\in \\mathbb{R}^{m \\times n_i}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nOutput (Head)\n\n\n\\[h_\\theta(X) = Z_{L+1}\\]\n\n\n\n\n\n\n\n\n\nParámetros\n\n\n\\[\\theta = \\left[W_1,..., W_L\\right]\\] \\[ W_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nLas salidas intermedias (intermediate outputs) son las mal llamadas hidden layers. Esta red cuenta con \\(L\\) hidden layers \\(W\\)."
  },
  {
    "objectID": "tics579/clase-2.html#definiciones-1",
    "href": "tics579/clase-2.html#definiciones-1",
    "title": "TICS-579-Deep Learning",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\n\n\n\nRed Neuronal\n\n\nVamos a definir como Red Neuronal un tipo particular de hipótesis que consiste en:\n\nMultiples capas que permiten cambiar de dimensión.\nFunciones de activación no-lineales y diferenciables que permiten desacoplar transformaciones lineales.\nUn set de parámetros optimizables, que permiten reducir una Loss Function.\n\n\n\n\n\n\n\n\n\n\nSi bien estas redes toman inspiración de la biólogía, poco o nada tienen que ver con neuronas reales.\n\n\n\n\n\n\n\n\n\nTérminos como Neural Network, Deep Networks, Deep Learning, son ampliamente usados y algunas veces usados para diferenciar el tamaño de distintas arquitecturas.\nNosotros los vamos a usar prácticamente como sinónimos.\n\n\n\nUpdate Rule\n\\[W_i := W_i - \\frac{\\alpha}{m} \\nabla_{W_i} l(h_\\theta(X),y)\\]"
  },
  {
    "objectID": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal",
    "href": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal",
    "title": "TICS-579-Deep Learning",
    "section": "Cálculo de Gradientes de una Red Neuronal",
    "text": "Cálculo de Gradientes de una Red Neuronal\n\\[\\nabla_{W_i} l(Z_{L+1},y) = \\underbrace{\\frac{\\partial l(Z_{L+1},i)}{\\partial Z_{L+1}} \\cdot \\frac{\\partial Z_{{L+1}}}{\\partial Z_L} \\cdot \\frac{\\partial Z_L}{\\partial Z_{L-1}}...\\cdot \\frac{\\partial Z_{i+2}}{\\partial Z_{i+1}}}_{G_{i+1} = \\frac{\\partial l(Z_{L+1},y)}{\\partial Z_{i+1}}}\\cdot \\frac{\\partial Z_{i+1}}{\\partial W_i}\\]\n\n\n\n\n\n\nGradiente Entrante (Incoming Backward Gradient)\n\n\n\nVamos a definir el Gradiente Entrante hasta antes de la capa \\(i\\) (desde la salida en dirección a la entrada) como: \\[\\begin{align}G_i &= G_{i+1} \\cdot \\frac{\\partial Z_{i + 1}}{\\partial Z_i} \\\\\n&= G_{i+1} \\cdot \\frac{\\partial \\sigma_i(Z_i W_i)}{\\partial Z_i W_i} \\cdot \\frac{\\partial Z_i W_i}{\\partial Z_i}_{} \\\\\n&= (G_{i+1})_{m \\times n_{i+1}} \\cdot \\sigma'(Z_i W_i)_{m \\times n_{i + 1}} \\cdot (W_i)_{n_i \\times n_{i+1}}\n\\end{align}\\]\n\n\n\n\nLuego, \\[ G_i \\in \\mathbb{R}^{m \\times n_i} = \\left[ G_{i+1} \\odot \\sigma_i'(Z_i W_i)\\right] W_i^T\\]"
  },
  {
    "objectID": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal-1",
    "href": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal-1",
    "title": "TICS-579-Deep Learning",
    "section": "Cálculo de Gradientes de una Red Neuronal",
    "text": "Cálculo de Gradientes de una Red Neuronal\n\\[\\begin{align}\\nabla_{W_i} l(Z_{L+1},y) &= G_{i+1} \\cdot \\frac{\\partial Z_{i+1}}{\\partial W_i} \\\\\n&= G_{i+1} \\cdot \\frac{\\partial \\sigma_i'(Z_i W_i)}{\\partial Z_i W_i} \\cdot \\frac{\\partial Z_i W_i}{\\partial W_i} \\\\\n&= (G_{i+1})_{m \\times n_{i+1}} \\cdot \\sigma'(Z_i W_i)_{m \\times n_{i+1}} \\cdot (Z_i)_{m \\times n_i}\n\\end{align}\\]\n\n\n\n\n\n\n\n\nLuego el Gradiente de cualquier Loss Function con respecto a un set de parámetros \\(W_i\\) se escribe como:\n\\[\\nabla_{W_i}l(Z_{L+1}, y) = Z_i^T \\left[G_{i+1} \\odot \\sigma'(Z_i W_i)\\right]\\]"
  },
  {
    "objectID": "tics579/clase-2.html#forward-y-backward-passes",
    "href": "tics579/clase-2.html#forward-y-backward-passes",
    "title": "TICS-579-Deep Learning",
    "section": "Forward y Backward Passes",
    "text": "Forward y Backward Passes\n\nBackpropagation\n\nCorresponde al Algoritmo con el cuál calcularemos los Gradientes de una Red Neuronal. Es un nombre muy fancy para calcular la Regla de la Cadena de manera eficiente aplicando caching de los resultados intermedios.\n\n\n\nForward Pass\n\nInicializar \\(Z_1 = X\\).\nIterar calculando: \\(Z_i = \\sigma_i(Z_i W_i), i=1,...,L\\).\n\nBackward Pass\n\nInicializar \\(G_{L+1} = \\nabla_{Z_{L+1}}l(Z_{L+1},y) = S-I_y\\) (Este ejemplo es sólo el caso de Cross Entropy como Loss Function).\nIterar calculando: \\(G_i = \\left[G_{i+1} \\odot \\sigma_i'(Z_i W_i)\\right]W_i^T, i=L,...,1\\)\n\nUpdate Rule\n\nCalcular Gradientes para poder aplicar el Update Rule.\n\n\\[W_i := W_i - \\frac{\\alpha}{m}\\nabla_{W_i}l(Z_{L+1},y) = W_i - \\frac{\\alpha}{m} Z_i^T\\left[G_{i+1} \\odot \\sigma'(Z_i W_i)\\right]\\]"
  },
  {
    "objectID": "tics579/clase-2.html#conceptos-clásicos-del-entrenamiento-de-una-nn",
    "href": "tics579/clase-2.html#conceptos-clásicos-del-entrenamiento-de-una-nn",
    "title": "TICS-579-Deep Learning",
    "section": "Conceptos Clásicos del Entrenamiento de una NN",
    "text": "Conceptos Clásicos del Entrenamiento de una NN\n\n\n\n\n\n\n\n\n\n\n\n\nDefiniremos una Epoch como el número de veces que repetiremos el Algoritmo de Backpropagation con todos los datos de Entrenamiento. El número de epochs de entrenamiento será un hiperparámetro de un modelo.\nDefiniremos el learning rate como un hiperparámetro que controlará el aprendizaje del modelo.\nDefiniremos este tipo de redes neuronales como Feed Forward Networks o FFN aunque en la práctica tienen una pequeña modificación que veremos en la siguiente clase.\n\n\n\n\n\n\n\n\n\n\nEste tipo de redes es muy utilizada y recibe diversos nombres:\n\nFully Connected Layers\nDense Layers: Proviene de la nomenclatura utilizada por Tensorflow.\nLinear Layers: Proviene de la nomenclatura utilizada por Pytorch, pero no es del todo correcto.\nMLP o Multilayer Perceptron."
  },
  {
    "objectID": "tics579/notebooks/Intro_pytorch.html",
    "href": "tics579/notebooks/Intro_pytorch.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nnp.random.seed(1)\n\n\nX_numpy = np.random.randn(1000, 10)\ny_numpy = np.random.randint(0, 2, 1000)\n\nX = torch.from_numpy(X_numpy).float()\ny = torch.from_numpy(y_numpy).float()\ny.shape\n\n((750, 10), (250, 10), (750,), (250,))\n\n\n\ntorch.__version__\n\n'2.4.0'\n\n\n\nw1 = nn.Linear(in_features=10, out_features=32)\nrelu_1 = nn.ReLU()\nw2 = nn.Linear(in_features=32, out_features=64)\nrelu_2 = nn.ReLU()\nw3 = nn.Linear(64, 1)\n\n\nout_w1 = w1(X)\nout_relu_1 = relu_1(out_w1)\nout_w2 = w2(out_relu_1)\nout_relu_2 = relu_2(out_w2)\nout_w3 = w3(out_relu_2)\nout_w3.shape\n\ntorch.Size([1000, 1])\n\n\n\nclass MyFFN(nn.Module):\n    def __init__(self, n_features, hidden_dim_1, hidden_dim_2, out_dim):\n        super().__init__()\n        self.w1 = nn.Linear(\n            in_features=n_features, out_features=hidden_dim_1\n        )\n        self.relu_1 = nn.ReLU()\n        self.w2 = nn.Linear(\n            in_features=hidden_dim_1, out_features=hidden_dim_2\n        )\n        self.relu_2 = nn.ReLU()\n        self.w3 = nn.Linear(hidden_dim_2, out_dim)\n\n    def forward(self, x):\n        x = self.w1(x)\n        x = self.relu_1(x)\n        x = self.w2(x)\n        x = self.relu_2(x)\n        x = self.w3(x)\n        return x\n\n\nmodel = MyFFN(n_features=10, hidden_dim_1=32, hidden_dim_2=64, out_dim=1)\nmodel\n\nMyFFN(\n  (w1): Linear(in_features=10, out_features=32, bias=True)\n  (relu_1): ReLU()\n  (w2): Linear(in_features=32, out_features=64, bias=True)\n  (relu_2): ReLU()\n  (w3): Linear(in_features=64, out_features=1, bias=True)\n)\n\n\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n\nimport matplotlib.pyplot as plt\n\n## Entrenamiento... (Training Loop)\nEPOCHS = 2000\n\nloss_list = []\nfor e in range(EPOCHS):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(X)\n    ## predicciones primero, y luego el target\n    loss = criterion(preds, y.unsqueeze(1))\n    loss.backward()\n    optimizer.step()\n    loss_list.append(loss.item())\n\nplt.plot(range(EPOCHS), loss_list)\n\n\n\n\n\n\n\n\n\nmodel.w1\n\nLinear(in_features=10, out_features=32, bias=True)\n\n\n\nmodel\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/notebooks/pretrained-models.html",
    "href": "tics579/notebooks/pretrained-models.html",
    "title": "Torchvision",
    "section": "",
    "text": "import torchvision\nfrom torchvision.models import list_models\n\nlist_models(module=torchvision.models)\n\n['alexnet',\n 'convnext_base',\n 'convnext_large',\n 'convnext_small',\n 'convnext_tiny',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'efficientnet_b0',\n 'efficientnet_b1',\n 'efficientnet_b2',\n 'efficientnet_b3',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_v2_l',\n 'efficientnet_v2_m',\n 'efficientnet_v2_s',\n 'googlenet',\n 'inception_v3',\n 'maxvit_t',\n 'mnasnet0_5',\n 'mnasnet0_75',\n 'mnasnet1_0',\n 'mnasnet1_3',\n 'mobilenet_v2',\n 'mobilenet_v3_large',\n 'mobilenet_v3_small',\n 'regnet_x_16gf',\n 'regnet_x_1_6gf',\n 'regnet_x_32gf',\n 'regnet_x_3_2gf',\n 'regnet_x_400mf',\n 'regnet_x_800mf',\n 'regnet_x_8gf',\n 'regnet_y_128gf',\n 'regnet_y_16gf',\n 'regnet_y_1_6gf',\n 'regnet_y_32gf',\n 'regnet_y_3_2gf',\n 'regnet_y_400mf',\n 'regnet_y_800mf',\n 'regnet_y_8gf',\n 'resnet101',\n 'resnet152',\n 'resnet18',\n 'resnet34',\n 'resnet50',\n 'resnext101_32x8d',\n 'resnext101_64x4d',\n 'resnext50_32x4d',\n 'shufflenet_v2_x0_5',\n 'shufflenet_v2_x1_0',\n 'shufflenet_v2_x1_5',\n 'shufflenet_v2_x2_0',\n 'squeezenet1_0',\n 'squeezenet1_1',\n 'swin_b',\n 'swin_s',\n 'swin_t',\n 'swin_v2_b',\n 'swin_v2_s',\n 'swin_v2_t',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'vit_b_16',\n 'vit_b_32',\n 'vit_h_14',\n 'vit_l_16',\n 'vit_l_32',\n 'wide_resnet101_2',\n 'wide_resnet50_2']\nmodel = torchvision.models.alexnet(weights=\"IMAGENET1K_V1\")\nmodel\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\nfrom torchinfo import summary\n\nsummary(model, input_size=(1, 3, 224, 224))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 1000]                 --\n├─Sequential: 1-1                        [1, 256, 6, 6]            --\n│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n├─Sequential: 1-3                        [1, 1000]                 --\n│    └─Dropout: 2-14                     [1, 9216]                 --\n│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n│    └─ReLU: 2-16                        [1, 4096]                 --\n│    └─Dropout: 2-17                     [1, 4096]                 --\n│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n│    └─ReLU: 2-19                        [1, 4096]                 --\n│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n==========================================================================================\nTotal params: 61,100,840\nTrainable params: 61,100,840\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 714.68\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 3.95\nParams size (MB): 244.40\nEstimated Total Size (MB): 248.96\n=========================================================================================="
  },
  {
    "objectID": "tics579/notebooks/pretrained-models.html#uso-de-timm",
    "href": "tics579/notebooks/pretrained-models.html#uso-de-timm",
    "title": "Torchvision",
    "section": "Uso de Timm",
    "text": "Uso de Timm\n\nimport timm\n\nlen(timm.list_models())\n\ntimm.list_models(\"efficientnet*\")\n\n['efficientnet_b0',\n 'efficientnet_b0_g8_gn',\n 'efficientnet_b0_g16_evos',\n 'efficientnet_b0_gn',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b3',\n 'efficientnet_b3_g8_gn',\n 'efficientnet_b3_gn',\n 'efficientnet_b3_pruned',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_b8',\n 'efficientnet_blur_b0',\n 'efficientnet_cc_b0_4e',\n 'efficientnet_cc_b0_8e',\n 'efficientnet_cc_b1_8e',\n 'efficientnet_el',\n 'efficientnet_el_pruned',\n 'efficientnet_em',\n 'efficientnet_es',\n 'efficientnet_es_pruned',\n 'efficientnet_h_b5',\n 'efficientnet_l2',\n 'efficientnet_lite0',\n 'efficientnet_lite1',\n 'efficientnet_lite2',\n 'efficientnet_lite3',\n 'efficientnet_lite4',\n 'efficientnet_x_b3',\n 'efficientnet_x_b5',\n 'efficientnetv2_l',\n 'efficientnetv2_m',\n 'efficientnetv2_rw_m',\n 'efficientnetv2_rw_s',\n 'efficientnetv2_rw_t',\n 'efficientnetv2_s',\n 'efficientnetv2_xl']\n\n\n\nresnet = timm.create_model(\"resnet18\", pretrained=True, num_classes=0)\nresnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Identity()\n)"
  },
  {
    "objectID": "tics579/notebooks/pretrained-models.html#get-activations-from-a-specific-layer",
    "href": "tics579/notebooks/pretrained-models.html#get-activations-from-a-specific-layer",
    "title": "Torchvision",
    "section": "Get Activations from a specific Layer",
    "text": "Get Activations from a specific Layer\n\nmodel = torchvision.models.alexnet(weights=\"IMAGENET1K_V1\")\nmodel\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\n\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\n\n\ndef hook_fn(model, input, output):\n    print(output.shape)\n\n\nmodel = torchvision.models.alexnet(weights=\"IMAGENET1K_V1\")\nmodel.classifier[4].register_forward_hook(hook_fn)\n\na = torch.zeros(1, 3, 224, 256)\nmodel.eval()\nwith torch.no_grad():\n    model(a)\n\ntorch.Size([1, 4096])\n\n\n\nresnet.layer1[0].conv1.register_forward_hook(hook_fn)\na = torch.zeros(1, 3, 224, 256)\nresnet.eval()\nwith torch.no_grad():\n    resnet(a)\n\ntorch.Size([1, 64, 56, 64])"
  },
  {
    "objectID": "tics579/notebooks/pretrained-models.html#transfer-learning",
    "href": "tics579/notebooks/pretrained-models.html#transfer-learning",
    "title": "Torchvision",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\nfrom torchvision.datasets import ImageFolder\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchmetrics\nimport time\nfrom torchinfo import summary\n\n\nclass Transforms:\n    def __init__(self, transform: A.Compose):\n        self.transform = transform\n\n    def __call__(self, image):\n        image = np.array(image)\n        return self.transform(image=image)[\"image\"]\n\n\ntrain_transforms = A.Compose(\n    [A.Resize(256, 256), A.ToFloat(), ToTensorV2()]\n)\nvalidation_transforms = A.Compose(\n    [A.Resize(256, 256), A.ToFloat(), ToTensorV2()]\n)\n\n\ntrain_data = ImageFolder(\n    \"CATS_DOGS/train\",\n    transform=Transforms(train_transforms),\n)\nvalidation_data = ImageFolder(\n    \"CATS_DOGS/validation\",\n    transform=Transforms(validation_transforms),\n)\n\nprint(f\"Elementos en Entrenamiento: {len(train_data)}\")\nprint(f\"Elementos en Validación: {len(validation_data)}\")\n\nElementos en Entrenamiento: 18743\nElementos en Validación: 6251\n\n\n\ntrain_data.class_to_idx\n\n{'CAT': 0, 'DOG': 1}\n\n\n\nidx_to_class_dict = {v: k for k, v in train_data.class_to_idx.items()}\nidx_to_class_dict\n\n{0: 'CAT', 1: 'DOG'}\n\n\n\ntrain_data[0]\n\n(tensor([[[0.7961, 0.8000, 0.8118,  ..., 0.9569, 0.9412, 0.9373],\n          [0.7961, 0.8000, 0.8118,  ..., 0.9569, 0.9451, 0.9373],\n          [0.7961, 0.8000, 0.8118,  ..., 0.9529, 0.9451, 0.9373],\n          ...,\n          [0.6000, 0.6039, 0.6078,  ..., 0.0078, 0.0078, 0.0078],\n          [0.6000, 0.6000, 0.6039,  ..., 0.0078, 0.0078, 0.0078],\n          [0.5922, 0.5961, 0.5961,  ..., 0.0039, 0.0039, 0.0039]],\n \n         [[0.6431, 0.6471, 0.6588,  ..., 0.7961, 0.7882, 0.7804],\n          [0.6431, 0.6471, 0.6588,  ..., 0.7961, 0.7922, 0.7843],\n          [0.6431, 0.6471, 0.6588,  ..., 0.8039, 0.7922, 0.7843],\n          ...,\n          [0.4784, 0.4824, 0.4863,  ..., 0.0078, 0.0078, 0.0078],\n          [0.4784, 0.4784, 0.4824,  ..., 0.0078, 0.0078, 0.0078],\n          [0.4706, 0.4745, 0.4745,  ..., 0.0039, 0.0039, 0.0039]],\n \n         [[0.3412, 0.3451, 0.3569,  ..., 0.4745, 0.4784, 0.4706],\n          [0.3412, 0.3451, 0.3569,  ..., 0.4745, 0.4824, 0.4745],\n          [0.3412, 0.3451, 0.3569,  ..., 0.4784, 0.4824, 0.4784],\n          ...,\n          [0.2157, 0.2196, 0.2235,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2157, 0.2157, 0.2196,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2078, 0.2118, 0.2118,  ..., 0.0000, 0.0000, 0.0000]]]),\n 0)\n\n\n\nidx = np.random.randint(0, len(train_data))\n\n\n## Es importante cambiar el orden de los canales para poder mostrar la imagen...\ndef plot_images(idx, data):\n    plt.imshow(data[idx][0].permute(1, 2, 0))\n    class_label = data[idx][1]\n    plt.title(data.classes[class_label])\n    plt.axis(\"off\")\n\n\nplot_images(idx, train_data)\n\n\n\n\n\n\n\n\n\ntorchvision.models.alexnet(weights=\"IMAGENET1K_V1\")\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n\n\n\nclass CNN(nn.Module):\n    def __init__(self, in_channels=3, n_outputs=1, ks=3):\n        super().__init__()\n        self.conv1 = self.CNN_block(in_channels, 32, k=ks)\n        self.conv2 = self.CNN_block(32, 16, k=ks)\n        self.conv3 = self.CNN_block(16, 8, k=ks)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.LazyLinear(4096)\n        # self.fc1 = nn.Linear(32 * 5 * 5, 16)  # filtros x tamaño\n        self.fc2 = nn.Linear(4096, n_outputs)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\n    @staticmethod\n    def CNN_block(c_in, c_out, k=3, p=0, s=1, pk=2, ps=2):\n        return nn.Sequential(\n            nn.Conv2d(\n                in_channels=c_in,\n                out_channels=c_out,\n                kernel_size=k,\n                padding=p,\n                stride=s,\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=pk, stride=ps),\n        )\n\n\nclass Alexnet(nn.Module):\n    def __init__(self, backbone, output_dim, frozen=False):\n        super().__init__()\n        self.backbone = backbone\n        # Modificar la Prediction Head para que permite Clasificación Binaria\n        self.backbone.classifier[6] = nn.LazyLinear(output_dim)\n\n        if frozen:\n            self.backbone.features.requires_grad_(False)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return x\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cpu\")\n\n\ndef train_model(\n    train_data,\n    val_data,\n    model,\n    training_params,\n    criterion=nn.CrossEntropyLoss(),\n):\n    print(f\"Making {torch.cuda.get_device_name(0)} go brrruuummmmm....\")\n    model.to(device)\n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=training_params[\"learning_rate\"],\n    )\n\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=training_params[\"batch_size\"],\n        shuffle=True,\n        num_workers=12,\n        pin_memory=True,\n    )\n    val_dataloader = DataLoader(\n        val_data,\n        batch_size=training_params[\"batch_size\"],\n        shuffle=False,\n        num_workers=12,\n        pin_memory=True,\n    )\n\n    train_metric = torchmetrics.Precision(task=\"binary\").to(device)\n    val_metric = torchmetrics.Precision(task=\"binary\").to(device)\n\n    train_loss = []\n    val_loss = []\n    for e in range(training_params[\"num_epochs\"]):\n        start_time = time.time()\n        train_batch_loss = []\n        val_batch_loss = []\n        model.train()\n        for batch in train_dataloader:\n            X, y = batch\n            X, y = X.to(device), y.float().unsqueeze(1).to(device)\n\n            optimizer.zero_grad()\n            y_hat = model(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n            tr_acc = train_metric(y_hat, y)\n            train_batch_loss.append(loss.item())\n\n        tr_acc = train_metric.compute()\n        train_epoch_loss = np.mean(train_batch_loss)\n\n        model.eval()\n        with torch.no_grad():\n            for batch in val_dataloader:\n                X, y = batch\n                X, y = X.to(device), y.float().unsqueeze(1).to(device)\n\n                y_hat = model(X)\n                loss = criterion(y_hat, y)\n                val_acc = val_metric(y_hat, y)\n                val_batch_loss.append(loss.item())\n\n        val_acc = val_metric.compute()\n        val_epoch_loss = np.mean(val_batch_loss)\n\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        print(\n            f\"Epoch: {e+1}: Time: {elapsed_time:.2f} - Train Loss: {train_epoch_loss:.4f} - Validation Loss: {val_epoch_loss:.4f} - Train Precision: {tr_acc:.4f} - Validation Precision: {val_acc:.4f}\"\n        )\n        train_loss.append(train_epoch_loss)\n        val_loss.append(val_epoch_loss)\n\n    return model, train_loss, val_loss\n\n\ntraining_params = dict(\n    learning_rate=3e-4,\n    batch_size=64,\n    num_epochs=3,\n)\nvanilla_model = CNN(in_channels=3, n_outputs=1).to(device)\n\nsummary(vanilla_model, input_size=(1, 3, 256, 256))\nvanilla_model, train_loss, val_loss = train_model(\n    train_data,\n    validation_data,\n    model=vanilla_model,\n    training_params=training_params,\n    criterion=nn.BCEWithLogitsLoss(),\n)\n\nMaking NVIDIA GeForce RTX 2070 with Max-Q Design go brrruuummmmm....\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 1: Time: 28.16 - Train Loss: 0.6229 - Validation Loss: 0.5685 - Train Precision: 0.6580 - Validation Precision: 0.7332\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 2: Time: 27.84 - Train Loss: 0.5356 - Validation Loss: 0.5340 - Train Precision: 0.7012 - Validation Precision: 0.7305\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 3: Time: 27.93 - Train Loss: 0.5076 - Validation Loss: 0.5361 - Train Precision: 0.7216 - Validation Precision: 0.7446\n\n\n\nsummary(vanilla_model, input_size=(1, 3, 256, 256))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 1]                    --\n├─Sequential: 1-1                        [1, 32, 127, 127]         --\n│    └─Conv2d: 2-1                       [1, 32, 254, 254]         896\n│    └─ReLU: 2-2                         [1, 32, 254, 254]         --\n│    └─MaxPool2d: 2-3                    [1, 32, 127, 127]         --\n├─Sequential: 1-2                        [1, 16, 62, 62]           --\n│    └─Conv2d: 2-4                       [1, 16, 125, 125]         4,624\n│    └─ReLU: 2-5                         [1, 16, 125, 125]         --\n│    └─MaxPool2d: 2-6                    [1, 16, 62, 62]           --\n├─Sequential: 1-3                        [1, 8, 30, 30]            --\n│    └─Conv2d: 2-7                       [1, 8, 60, 60]            1,160\n│    └─ReLU: 2-8                         [1, 8, 60, 60]            --\n│    └─MaxPool2d: 2-9                    [1, 8, 30, 30]            --\n├─Flatten: 1-4                           [1, 7200]                 --\n├─Linear: 1-5                            [1, 4096]                 29,495,296\n├─Linear: 1-6                            [1, 1]                    4,097\n==========================================================================================\nTotal params: 29,506,073\nTrainable params: 29,506,073\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 163.73\n==========================================================================================\nInput size (MB): 0.79\nForward/backward pass size (MB): 18.78\nParams size (MB): 118.02\nEstimated Total Size (MB): 137.59\n==========================================================================================\n\n\n\ntraining_params = dict(\n    learning_rate=3e-4,\n    batch_size=256,\n    num_epochs=3,\n)\nfrozen_alexnet = Alexnet(\n    backbone=torchvision.models.alexnet(weights=\"IMAGENET1K_V1\"),\n    output_dim=1,\n    frozen=True,\n).to(device)\n\nsummary(frozen_alexnet, input_size=(1, 3, 256, 256))\nfrozen_alexnet, train_loss, val_loss = train_model(\n    train_data,\n    validation_data,\n    model=frozen_alexnet,\n    training_params=training_params,\n    criterion=nn.BCEWithLogitsLoss(),\n)\n\nMaking NVIDIA GeForce RTX 2070 with Max-Q Design go brrruuummmmm....\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 1: Time: 18.64 - Train Loss: 0.2144 - Validation Loss: 0.1692 - Train Precision: 0.9140 - Validation Precision: 0.9594\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 2: Time: 19.05 - Train Loss: 0.1446 - Validation Loss: 0.1470 - Train Precision: 0.9288 - Validation Precision: 0.9493\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 3: Time: 18.95 - Train Loss: 0.1293 - Validation Loss: 0.1491 - Train Precision: 0.9355 - Validation Precision: 0.9525\n\n\n\ntraining_params = dict(\n    learning_rate=3e-4,\n    batch_size=256,\n    num_epochs=3,\n)\nft_alexnet = Alexnet(\n    backbone=torchvision.models.alexnet(weights=\"IMAGENET1K_V1\"),\n    output_dim=1,\n    frozen=False,\n).to(device)\n\nsummary(ft_alexnet, input_size=(1, 3, 256, 256))\nft_alexnet, train_loss, val_loss = train_model(\n    train_data,\n    validation_data,\n    model=ft_alexnet,\n    training_params=training_params,\n    criterion=nn.BCEWithLogitsLoss(),\n)\n\nMaking NVIDIA GeForce RTX 2070 with Max-Q Design go brrruuummmmm....\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 1: Time: 25.42 - Train Loss: 0.2199 - Validation Loss: 0.1031 - Train Precision: 0.9122 - Validation Precision: 0.9493\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 2: Time: 25.65 - Train Loss: 0.0879 - Validation Loss: 0.1166 - Train Precision: 0.9396 - Validation Precision: 0.9666\n\n\n/home/datacuber/miniconda3/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n\n\nEpoch: 3: Time: 25.53 - Train Loss: 0.0468 - Validation Loss: 0.0963 - Train Precision: 0.9542 - Validation Precision: 0.9601\n\n\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom pathlib import Path\n\n\ndef predict(model, image, transform):\n    image = np.array(Image.open(image))\n    pic = transform(image=image)[\"image\"]\n\n    model.eval()\n    with torch.no_grad():\n        new_prediction = model(pic.view(1, 3, 256, 256).to(device))\n        prob = new_prediction.sigmoid().item()\n\n    new_prediction = torch.where(new_prediction &gt; 0, 1, 0).item()\n\n    return idx_to_class_dict[new_prediction], prob\n\n\ndef prediction_grid(model, image_list, transform, grid_size=(20, 40)):\n\n    fig = plt.figure(1, grid_size)\n    grid = ImageGrid(\n        fig,\n        111,\n        nrows_ncols=(3, 4),\n        axes_pad=0.6,\n    )\n\n    for img, axes in zip(image_list, grid):\n        pred, proba = predict(model, img, transform)\n        image_name = Path(img).name\n        axes.set_title(\n            f\"{image_name} predicted as {pred}, \\n with a probability of {proba:.2f}\",\n            fontdict=None,\n            loc=\"center\",\n            color=\"k\",\n            fontsize=20,\n        )\n        axes.axis(\"off\")\n        axes.imshow(Image.open(img))\n\n    return plt.show()\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nimport glob\n\nimages = glob.glob(\"CATS_DOGS/test/*\")\n\ntest_transform = A.Compose([A.Resize(256, 256), A.ToFloat(), ToTensorV2()])\nprediction_grid(\n    vanilla_model,\n    reversed(images),\n    transform=test_transform,\n    grid_size=(30, 70),\n)\n\n\n\n\n\n\n\n\n\nprediction_grid(\n    frozen_alexnet,\n    reversed(images),\n    transform=test_transform,\n    grid_size=(30, 70),\n)\n\n\n\n\n\n\n\n\n\nprediction_grid(\n    ft_alexnet,\n    reversed(images),\n    transform=test_transform,\n    grid_size=(30, 70),\n)"
  },
  {
    "objectID": "tics579/notebooks/Intro_pytorch_2.html",
    "href": "tics579/notebooks/Intro_pytorch_2.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\n\nSEED = 1\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_numpy = np.random.randn(1000, 10)\ny_numpy = np.random.randint(0, 2, 1000)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_numpy, y_numpy, test_size=0.25, random_state=42\n)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape\n\n((750, 10), (250, 10), (750,), (250,))\n\n\n\nclass MyData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return dict(\n            features=self.X[idx].float(), target=self.y[idx].float()\n        )\n\n\ndataset = MyData(X_numpy, y_numpy)\ndataset[0][\"target\"]\n\ntensor(1.)\n\n\n\nclass MyFFN(nn.Module):\n    def __init__(self, n_features, hidden_dim_1, hidden_dim_2, out_dim):\n        super().__init__()\n        self.w1 = nn.Linear(\n            in_features=n_features, out_features=hidden_dim_1\n        )\n        self.relu_1 = nn.ReLU()\n        self.w2 = nn.Linear(\n            in_features=hidden_dim_1, out_features=hidden_dim_2\n        )\n        self.relu_2 = nn.ReLU()\n        self.w3 = nn.Linear(hidden_dim_2, out_dim)\n\n    def forward(self, x):\n        x = self.w1(x)\n        x = self.relu_1(x)\n        x = self.w2(x)\n        x = self.relu_2(x)\n        x = self.w3(x)\n        return x\n\n\nmodel = MyFFN(n_features=10, hidden_dim_1=32, hidden_dim_2=32, out_dim=1)\n\n\nfrom tqdm.notebook import tqdm\n\n\ndef training_loop(\n    model, X_train, y_train, X_val, y_val, batch_size=32, epochs=50\n):\n    EPOCHS = epochs\n    model.to(device)\n\n    print(f\"Modelo entrenándose en {next(model.parameters()).device}\")\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n    train_data = MyData(X_train, y_train)\n    val_data = MyData(X_val, y_val)\n\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=10,\n        shuffle=True,\n        drop_last=True,  ## Opcional... se supone que genera más estabilidad en los Gradientes.\n    )\n    val_dataloader = DataLoader(\n        val_data,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=10,\n        shuffle=False,\n        drop_last=False,  # Debe ser falso para validar, sino no estamos calculando un Loss para todos los puntos.\n    )\n    train_loss = []\n    val_loss = []\n\n    for e in tqdm(range(1, EPOCHS + 1)):\n        train_batch_loss = []\n        val_batch_loss = []\n        model.train()\n        for batch in train_dataloader:\n            features, target = batch[\"features\"].to(device), batch[\n                \"target\"\n            ].to(device)\n\n            optimizer.zero_grad()\n            output = model(features)\n            ## Por qué no va acá una sigmoide a la salida?\n            loss = criterion(output, target.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            train_batch_loss.append(loss.item())\n        train_epoch_loss = np.mean(train_batch_loss)\n        if e % 50 == 0:\n            print(f\"Training Loss for Epoch {e}: {train_epoch_loss}\")\n        train_loss.append(train_epoch_loss)\n\n        model.eval()\n        with torch.no_grad():\n            for batch in val_dataloader:\n                features, target = batch[\"features\"].to(device), batch[\n                    \"target\"\n                ].to(device)\n\n                output = model(features)\n                loss = criterion(output, target.unsqueeze(1))\n                val_batch_loss.append(loss.item())\n        val_epoch_loss = np.mean(val_batch_loss)\n        if e % 50 == 0:\n            print(f\"Validation Loss for Epoch {e}: {val_epoch_loss}\")\n        val_loss.append(val_epoch_loss)\n\n    return model, train_loss, val_loss\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_validation_curve(train_loss, val_loss, epoch=50):\n    plt.plot(range(epoch), train_loss, label=\"Train Loss\")\n    plt.plot(range(epoch), val_loss, label=\"Validation Loss\")\n    plt.title(\"Validation Curve\")\n    plt.legend()\n    plt.show()\n\n\nEPOCHS = 50\nmodel = MyFFN(n_features=10, hidden_dim_1=64, hidden_dim_2=64, out_dim=1)\nmodel, train_loss, val_loss = training_loop(\n    model, X_train, y_train, X_val, y_val, batch_size=32, epochs=EPOCHS\n)\nplot_validation_curve(train_loss, val_loss)\nprint(f\"Min Loss: {min(val_loss)}\")\n\nModelo entrenándose en cuda:0\n\n\n\n\n\nTraining Loss for Epoch 50: 0.616158355837283\nValidation Loss for Epoch 50: 0.7319772690534592\n\n\n\n\n\n\n\n\n\nMin Loss: 0.6952822953462601\n\n\n\nEPOCHS = 100\nmodel = MyFFN(n_features=10, hidden_dim_1=64, hidden_dim_2=64, out_dim=1)\nmodel, train_loss, val_loss = training_loop(\n    model, X_train, y_train, batch_size=128, epochs=EPOCHS\n)\nplot_validation_curve(train_loss, val_loss, epoch=EPOCHS)\nprint(f\"Min Loss: {min(val_loss)}\")\n\n\n\n\nTraining Loss for Epoch 50: 0.6657268881797791\nValidation Loss for Epoch 50: 0.7142947316169739\nTraining Loss for Epoch 100: 0.6180922746658325\nValidation Loss for Epoch 100: 0.7341216802597046\n\n\n\n\n\n\n\n\n\nMin Loss: 0.6952072083950043\n\n\n\nfrom torchinfo import summary\n\nsummary(model)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nMyFFN                                    --\n├─Linear: 1-1                            704\n├─ReLU: 1-2                              --\n├─Linear: 1-3                            4,160\n├─ReLU: 1-4                              --\n├─Linear: 1-5                            65\n=================================================================\nTotal params: 4,929\nTrainable params: 4,929\nNon-trainable params: 0\n=================================================================\n\n\n\nEPOCHS = 20\nmodel = MyFFN(n_features=10, hidden_dim_1=32, hidden_dim_2=32, out_dim=1)\nmodel, train_loss, val_loss = training_loop(\n    model, X_train, y_train, batch_size=64, epochs=EPOCHS\n)\nplot_validation_curve(train_loss, val_loss, epoch=EPOCHS)\nprint(f\"Min Loss: {min(val_loss)}\")\n\n\n\n\n\n\n\n\n\n\n\nMin Loss: 0.6932080686092377\n\n\n\nsummary(model)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nMyFFN                                    --\n├─Linear: 1-1                            352\n├─ReLU: 1-2                              --\n├─Linear: 1-3                            1,056\n├─ReLU: 1-4                              --\n├─Linear: 1-5                            33\n=================================================================\nTotal params: 1,441\nTrainable params: 1,441\nNon-trainable params: 0\n=================================================================\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/clase-11.html#transformers-attention-is-all-you-need-2017",
    "href": "tics579/clase-11.html#transformers-attention-is-all-you-need-2017",
    "title": "TICS-579-Deep Learning",
    "section": "Transformers (Attention is all you need, 2017)",
    "text": "Transformers (Attention is all you need, 2017)\n\n\n\nTransformers\n\nCorresponden a la arquitectura más moderna diseñada al día de hoy. Está basado en mecanismos de atención y posee hasta 4 tipos de atención distintos.\n\n\n\n\n\nVentajas\n\n\n\nNo tiene problemas de “memoria” para modelar dependencias de largo plazo.\nPermite procesamiento en paralelo.\nSu bajísimo inductive bias le permite adaptarse a distintos dominios.\nNo es necesario utilizar el transformer completo para un problema en específico.\n\n\n\n\n\n\n\nDesventajas\n\n\n\nApto sólo para datos secuenciales.\nLas secuencias deben de ser del mismo largo.\nAlta demanda de recursos computacionales GPU y/o TPUs para entrenamiento distribuido.\nData hungry.\nLimitaciones de secuencias muy largas por restricciones de memoria computacional."
  },
  {
    "objectID": "tics579/clase-11.html#encoder",
    "href": "tics579/clase-11.html#encoder",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder",
    "text": "Encoder\n\n\n\n\n\n\n\n\n\n\n\nObjetivo\n\n\nCodificar y comprimir información en Logits que puedan ser usados para clasificar o para ser utilizados por un Decoder.\n\n\n\n\n\n\nForward Pass en el Encoder\n\n\n\nEl embedding asociado a una secuencia se bifurca en 4 ramas:\n\nResidual Connection\nQuery\nKey\nValue\n\nQuery, Key y Value ingresan al Multihead Attention.\nLa salida del Multihead Attention + el Residual Connection pasan por un LayerNorm.\nNueva bifurcación.\n\nUna parte entra a un MLP\nOtra va como skip connection.\n\nLa salida del MLP + la Residual Connection pasan por un segundo LayerNorm para generar la salida del Encoder."
  },
  {
    "objectID": "tics579/clase-11.html#decoder",
    "href": "tics579/clase-11.html#decoder",
    "title": "TICS-579-Deep Learning",
    "section": "Decoder",
    "text": "Decoder\n\n\n\n\n\n\n\n\n\n\n\nObjetivo\n\n\nTomar información de entrada y generar una salida fijándose sólo en tokens pasados.\n\n\n\n\n\n\nForward Pass en el Decoder\n\n\n\nEl embedding asociado a una secuencia se bifurca en 4 ramas:\n\nResidual Connection\nQuery\nKey\nValue\n\nQuery, Key y Value ingresan al Masked (Causal) Multihead Attention.\nLa salida del Masked Multihead Attention + el Residual Connection pasan por LayerNorm.\nSe pasa por un Cross Attention (esto podría ser opcional).\n\nKey y Value provienen del Encoder como contexto.\nLa salida del Causal Multihead Attention se utiliza como Query.\n\nNueva bifurcación.\n\nUna parte entra a un MLP\nOtra va como skip connection.\n\nLa salida del MLP + la Residual Connection pasan por LayerNorm para generar el Output."
  },
  {
    "objectID": "tics579/clase-11.html#ejemplo",
    "href": "tics579/clase-11.html#ejemplo",
    "title": "TICS-579-Deep Learning",
    "section": "Ejemplo",
    "text": "Ejemplo\nSupongamos que tenemos la siguiente frase:\n\nMe gusta la pizza de Pepperoni\n\n\nTokenización\n\n\nAplicaremos un proceso de Tokenización simple, donde cada palabra es un Token.\n\n\n\n\n\n\n\n\n\nYa sabemos que esto no tiene por qué ser así. De hecho cada modelo tiene su propio tipo de tokenización, e incluso se pueden entrenar Tokenizaciones nuevas. Más información al respecto pueden encontrarla acá.\n\n\n\n\n\n\n\n\n\nLa documentación de Tokenizers de HuggingFace la traduje yo, así que si encuentran algo me dicen para corregir.\n\n\n\nLuego la secuencia tokenizada de largo \\(L=6\\) será:\n\n[105,6587,5475,301,708,358]\n\n\n\n\n\n\n\nRecordar que dependiendo del modelo se pueden agregar al inicio o al final tokens especiales. Hablaremos de eso más adelante."
  },
  {
    "objectID": "tics579/clase-11.html#embedding",
    "href": "tics579/clase-11.html#embedding",
    "title": "TICS-579-Deep Learning",
    "section": "Embedding",
    "text": "Embedding\n\n\n\nEmbedding\n\nNo es más que una Lookup Table. Es una tabla de parámetros entrenables que permitirá transformar índices enteros en vectores de una dimensión determinada.\n\n\nnn.Embedding(num_embeddings, embedding_dim)\n\n\n\n\n\n\nEsta clase permite el ingreso de tensores de cualquier tamaño \\((*)\\) y devuelve tensores de tamaño \\((*,H)\\). Donde \\(H\\) es el embedding_dim.\n\n\n\n\n\n\n\n\n\nLa sección 3.1 del paper se refiere al tamaño del embedding como \\(d_{model}=512\\).\n\n\n\n\n\n\n\n\n\nEn la sección 3.4 del paper se menciona que los parámetros de los embeddings son multiplicados por \\(\\sqrt{d_{model}}\\)."
  },
  {
    "objectID": "tics579/clase-11.html#embedding-1",
    "href": "tics579/clase-11.html#embedding-1",
    "title": "TICS-579-Deep Learning",
    "section": "Embedding",
    "text": "Embedding\n\n\n\n\n\n\n\n\n\n\n\nCada token está representado por un Embedding de \\(d_{model}\\) dimensiones."
  },
  {
    "objectID": "tics579/clase-11.html#positional-encoder",
    "href": "tics579/clase-11.html#positional-encoder",
    "title": "TICS-579-Deep Learning",
    "section": "Positional Encoder",
    "text": "Positional Encoder\n\n\n\n\n\n\n\n\nUn potencial problema que puede tener un transformer es reconocer el orden de las frases.\n\n\n\n\n\n\n\n\n\nNo es lo mismo decir “El perro del papá mordió al niño” que “El perro del niño mordió al papá”. Las palabras usadas en ambas frases son exactamente las mismas, pero en un orden distinto implican desenlaces distintos. ¿Cómo podemos entender el concepto de orden si no tenemos recurrencia?\n\n\n\n\n\n\n\n\n\nIncluso algunos órdenes no tienen tanto sentido lógico: “El niño del perro mordió al papá”.\n\n\n\n\nPositional Encoder\n\n\nCorresponden a una manera en la que se pueden generar un vector único que representa el orden en el que aparece cada token."
  },
  {
    "objectID": "tics579/clase-11.html#positional-encoder-1",
    "href": "tics579/clase-11.html#positional-encoder-1",
    "title": "TICS-579-Deep Learning",
    "section": "Positional Encoder",
    "text": "Positional Encoder\n\n\n\\[PE_{(pos,2i)} = sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\] \\[PE_{(pos,2i+1)} = cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\]\n\n\n\n\n\n\\(pos\\) corresponde a la posición del Token en la secuencia, y \\(2i\\) y \\(2i+1\\) corresponden a las posiciones pares e impares respectivamente del embedding dimension de cada token, en este caso llamado \\(d_{model}\\) (\\(i\\) comienza en 0).\n\n\n\n\n\n\n\n\n\n\nUna forma más clara de ver esto es que la posición está definida por sinusoidales de periodo \\(2\\pi \\cdot 10000^{i/d_{model}/2}\\).\n\n\n\n\n\n\n\n\n\n\nEl positional encoder debe tener el mismo tamaño que el Embedding para que se puedan sumar.\n\n\n\n\n\n\nEstabilidad numérica\n\n\nPor temas de estabilidad el argumento del \\(sin(\\cdot)\\) y \\(cos(\\cdot)\\) se suele implementar como: \\[pos \\cdot exp\\left[-\\frac{2i}{d_{model}} log(10000)\\right]\\]\n\n\n\n\n\n\nRegularización\n\n\nLa sección 5.4 menciona que se aplica Dropout posterior a sumar los Embeddings con el Positional Encoding. Se utilizo un \\(P_{drop}=0.1\\)."
  },
  {
    "objectID": "tics579/clase-11.html#positional-encoder-ejemplo",
    "href": "tics579/clase-11.html#positional-encoder-ejemplo",
    "title": "TICS-579-Deep Learning",
    "section": "Positional Encoder: Ejemplo",
    "text": "Positional Encoder: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementación Eficiente\n\n\nEl Positional Encoding se implementa como una matriz de tamaño \\((L,d_{model})\\) en la que cada fila es un embedding de \\(d_{model}\\) dimensiones asociado a cada token.\n\n\n\n\n\n\nImportante\n\n\nLa suma del Embedding con el Positional Encoder codifica la información del token y su posición relativa dentro de la secuencia."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-self-attention",
    "href": "tics579/clase-11.html#encoder-self-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Self-Attention",
    "text": "Encoder: Self-Attention\n\n\n\n\n\n\n\n\\[Attention(Q,K,V) = Softmax\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right) V\\]\n\n\n\nEjemplo\n\n\n“La sopa se cocinó en la olla y estaba rica”. Rica podría estar refiriéndose a olla o a sopa. Sabemos que se refiere a la sopa.\n\n\n\n\n\n\n\n\n\n\nEl Scaled Dot-Product, más conocido como Self-Attention, es el mecanismo clave en las redes neuronales modernas. Permite determinar la atención/relación que existe entre palabras de una misma secuencia.\n\n\n\n\n\n\n\n\n\n\nEstá compuesto por 3 proyecciones lineales las cuales reciben los nombres de Query (Q), Key (K) y Value (V).\nEstás 3 proyecciones se combinan para poder determinar la atención/relación que cada Token tiene con los otros tokens de una misma secuencia.\nVarios procesos de Self-Attention dan pie al Multihead Attention.\n\n\n\n\n\n\n\n\n\n\n\nEl Self-Attention tiene la capacidad de acceder a toda la secuencia, por ende modelar relaciones a larga distancia.\nEl Causal Self-Attention, una variante que se utiliza en el Decoder sólo puede ver la relación con tokens pasados.\n\n\n\n\n\n\n\n\n\n\n\nSu característica más importante es que el Multihead Attention es paralelizable y no secuencial como las RNN.\nTiene capacidad de escalabilidad para secuencias largas."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-self-attention-1",
    "href": "tics579/clase-11.html#encoder-self-attention-1",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Self-Attention",
    "text": "Encoder: Self-Attention\n\n\n\n\n\n\n\nSupongamos que tenemos la secuencia “Me gusta la Pizza de Pepperoni”.\nUtilizaremos \\(d_{model} = 512\\) y \\(h=8\\).\nEl paper utiliza \\(d_k=d_v=d_{model}/h=64\\) para el cálculo de los Attention Weights.\n\n\n\n\n\n\n\\[\n\\begin{array}{c c}\nX = \\left[\n\\begin{array}{c c c}\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n\\end{array}\n\\right]\n\\begin{array}{c c c}Me\\\\gusta\\\\la\\\\pizza\\\\de\\\\pepperoni \\end{array} &\n\\end{array}\n\\]\n\n\n\n\n👀 Ojito\n\n\nEsto se debe aplicar a cada secuencia. Por lo tanto se debe agregar una dimensión (como unsqueeze(0)) que contabilice el número de secuencias para \\(Q\\), \\(K\\), y \\(V\\).\n\n\n\n\n\n\n\n\nMatrices de Proyección\n\n\nDefiniremos 3 matrices de Proyección. Una matriz de proyección permite llevar transportar un vector \\(X\\) a otro espacio (es decir, son entrenables). En este caso crearemos matrices que puedan multiplicarse con \\(X\\). Por lo tanto irán desde \\(d_{model}\\) hasta \\(d_q=d_k\\) y \\(d_v\\) respectivamente.\n\n\\(W_q = (d_{model}, d_k)\\)\n\\(W_k = (d_{model}, d_k)\\)\n\\(W_v = (d_{model}, d_v)\\)\n\n\n\n\n\n\n\nDimensiones de Q,K y V\n\n\n\n\\(Q = (L, d_k)\\)\n\\(K = (L, d_k)\\)\n\\(V = (L, d_v)\\)\n\\(L\\) corresponde a largo de la secuencia (es decir, el número de Tokens)\n\n\n\n\n\n:::"
  },
  {
    "objectID": "tics579/clase-11.html#encoder-self-attention-2",
    "href": "tics579/clase-11.html#encoder-self-attention-2",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Self-Attention",
    "text": "Encoder: Self-Attention\n\nSiguiendo nuestro ejemplo: \\(d_k = d_v = 64\\)\n\n\n\n\n\n\nQuery (6,64)\n\n\n\\[\n\\begin{array}{c c}\nQ = \\left[\n\\begin{array}{c c c}\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n\n\n\n\n\nKey (6,64)\n\n\n\\[\n\\begin{array}{c c}\nK = \\left[\n\\begin{array}{c c c}\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n\n\n\n\n\nValue (6,64)\n\n\n\\[\n\\begin{array}{c c}\nV = \\left[\n\\begin{array}{c c c}\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n\n\n\n\n\n\n👀 Ojito\n\n\nComo esto se aplica a una sola secuencia, la dimensión real de estos tensores debería ser \\((1,6,64)\\). Ese 1 cambiará si tenemos más secuencias. Pero, todas las secuencias deben ser del mismo largo."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-self-attention-scale-dot-product",
    "href": "tics579/clase-11.html#encoder-self-attention-scale-dot-product",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Self-Attention (Scale Dot Product)",
    "text": "Encoder: Self-Attention (Scale Dot Product)\n\n\n\\[\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\]\n\n\n\nSimilaridad\n\n\n\n\\(Q \\cdot K^T\\) representa el producto punto entre \\(Q\\) (un token de referencia que está consultando la atención contra otros tokens) y \\(K\\) (otro token que se compara contra la “query”).\n\n\n\n\n\n\n\nControl de Gradientes\n\n\n\\(\\sqrt{d_k}\\) es un factor que reduce la escala de los valores para el control de los gradientes. Recordar que esta matriz es de parámetros entrenables.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention\n\n\nDado que el rango de estos valores van de \\(-\\infty\\) a \\(\\infty\\), es más común aplicar una softmax para poder garantizar que la suma de las atenciones para cada palabra “query” sume 1."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-self-attention-scale-dot-product-1",
    "href": "tics579/clase-11.html#encoder-self-attention-scale-dot-product-1",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Self-Attention (Scale Dot Product)",
    "text": "Encoder: Self-Attention (Scale Dot Product)\n\n\n\n\n\n\n\n\nAttention Weights\n\n\n\nEsta matriz indica cuánta atención (en términos porcentuales) entrega cada palabra “query” a cada palabra “key”.\nEsta matriz permitirá crear embeddings contextualizados, que incluyen la información de la palabra y su contexto de atención."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-self-attention-scale-dot-product-2",
    "href": "tics579/clase-11.html#encoder-self-attention-scale-dot-product-2",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Self-Attention (Scale Dot Product)",
    "text": "Encoder: Self-Attention (Scale Dot Product)\n\\[Attention(Q,K,V) = Softmax\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right) V\\]\n\n\n\n\n\n\n\n\nOJO\n\n\n\nA modo de ejemplo, el elemento en Rojo representa una suma ponderada de la primera dimensión de cada proyección de tokens.\nEl resultado de cada dimensión es una combinación lineal de las dimensiones de cada token.\nCada fila corresponde a un embedding contextualizado que tiene información sobre el token y su contexto combinado.\n\n\n\n\n\n\n\n¿Y, estamos seguros que las atenciones/relaciones obtenidas por este algoritmos son (las) únicas/más correctas?"
  },
  {
    "objectID": "tics579/clase-11.html#encoder-multihead-attention",
    "href": "tics579/clase-11.html#encoder-multihead-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Multihead Attention",
    "text": "Encoder: Multihead Attention\n\n\n\n\n\n\n\n\nMultihead Attention\n\n\nEs una extensión del Self-Attention. En lugar de calcular sólo “una atención” sobre el input, genera distintas “atenciones” en múltiples “cabezas” independientes. Cada Attention Head se encarga de aprender relaciones diferentes, lo que mejora la capacidad del modelo de captar patrones cada vez más complejos.\n\n\n\n\n\n\n\n\n\nNormalmente se calculan entre \\(h=8\\) y \\(h=12\\) attention heads, las cuales se concatenan para luego pasar por una proyección lineal."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-multihead-attention-1",
    "href": "tics579/clase-11.html#encoder-multihead-attention-1",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Multihead Attention",
    "text": "Encoder: Multihead Attention\n\n\n\nSi queremos calcular \\(h=8\\) attention heads. Necesitamos 8 \\(Q\\), 8 \\(K\\) y 8 \\(V\\). Por lo tanto, necesitamos 8 matrices de proyección. ¿Cómo lo paralelizamos?\n\n\n\n\n\n\n\n\nImplementación en paralelo\n\n\nPodemos definir en realidad todas las matrices de manera análoga rescribiendo las matrices de proyección para \\(Q\\), \\(K\\) y \\(V\\) como una subdivisión de cada embedding en \\(h\\) cabezas.\n\nPor lo tanto si \\(d_k=d_v=64\\) y \\(h=8\\) tendríamos una dimensión total de 512.\n\\(dim(Q) = dim(K) = (N,h \\cdot d_k) = (6, 512)\\)\n\\(dim(V) = (N, h \\cdot d_v) = (6, 512)\\)\n\\(dim(W_q) = (d_{model}, h, d_k)\\)\n\\(dim(W_k) = (d_{model}, h, d_k)\\)\n\\(dim(W_v) = (d_{model}, h, d_v)\\)\n\n\n\n\n\n\n\n\nDimensiones de Q, K y V\n\n\n\n\\(Q = X \\cdot W_q = (6,512) \\cdot (512,\\overbrace{8,64}^{512}) = (6,8,512)\\)\n\\(K = X \\cdot W_k = (6,512) \\cdot (512,\\overbrace{8,64}^{512}) = (6,8,512)\\)\n\\(V = X \\cdot W_v = (6,512) \\cdot (512,\\overbrace{8,64}^{512}) = (6,8,512)\\)"
  },
  {
    "objectID": "tics579/clase-11.html#encoder-multihead-attention-independencia",
    "href": "tics579/clase-11.html#encoder-multihead-attention-independencia",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Multihead Attention (Independencia)",
    "text": "Encoder: Multihead Attention (Independencia)\n\n\n\n\n\nIndependent Heads\n\n\nEs importante mencionar que cada cabeza debe ser independiente una de otra para que se pueda paralelizar. Para ello basta con transponer las dos primeras dimensiones.\n\n\n\n\n\n\nQuery/Key/Value (6, 8, 64) (Previo a Transponer)\n\n\n\\[\n\\begin{array}{c c}\nQ/K/V = \\left[\n\\begin{array}{c c c}\n\\overbrace{[1,...,64]}^{Head 1}, \\overbrace{[65,...,128]}^{Head 2}, ..., \\overbrace{[449,...,512]}^{Head 8}\\\\\n[1,...,64],[65,..., 128],...[449,...,512]\\\\\n[1,...,64],[65,..., 128],...[449,...,512]\\\\\n[1,...,64],[65,..., 128],...[449,...,512]\\\\\n[1,...,64],[65,..., 128],...[449,...,512]\\\\\n[1,...,64],[65,..., 128],...[449,...,512]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n\n\n\n\nOJO\n\n\nEsto permite calcular cada Head en paralelo. Este procedimiento se aplica a cada secuencia. Por lo tanto, un Multihead Attention recibe Tensores de dimensión \\((N,L,h \\cdot d_i)\\) con \\(i=k,v\\).\n\n\n\n\n\n\n\nQuery/Key/Value (8,6, 64) (Luego de Transponer)\n\n\n\\[\n\\begin{array}{c c}\n\\left[\n\\begin{array}{c c c}\n\\text{Head1}\\left\\{\n\\begin{array}{c c c}\n\\begin{bmatrix}\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n[1,...,64]\\\\\n\\end{bmatrix}\n\\end{array}\n\\right. \\\\\n\\vdots \\\\\n\\vdots \\\\\n\\text{Head8}\\left\\{\n\\begin{array}{c c c}\n\\begin{bmatrix}\n[449,...,512]\\\\\n[449,...,512]\\\\\n[449,...,512]\\\\\n[449,...,512]\\\\\n[449,...,512]\\\\\n[449,...,512]\\\\\n\\end{bmatrix}\n\\end{array}\n\\right. \\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]"
  },
  {
    "objectID": "tics579/clase-11.html#encoder-multihead-attention-concatenación",
    "href": "tics579/clase-11.html#encoder-multihead-attention-concatenación",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Multihead Attention (Concatenación)",
    "text": "Encoder: Multihead Attention (Concatenación)\n\n\n\n\n\nSelf-Attentions (aka Multihead Attention) (6,8,64)\n\n\n\\[\n\\begin{array}{c c}\n\\left[\n\\begin{array}{c c c}\n\\text{Head1}\\left\\{\n\\begin{array}{c c c}\n\\begin{bmatrix}\n[SA_1,...,SA_{64}]\\\\\n[SA_1,...,SA_{64}]\\\\\n[SA_1,...,SA_{64}]\\\\\n[SA_1,...,SA_{64}]\\\\\n[SA_1,...,SA_{64}]\\\\\n[SA_1,...,SA_{64}]\\\\\n\\end{bmatrix}\n\\end{array}\n\\right. \\\\\n\\vdots \\\\\n\\vdots \\\\\n\\text{Head8}\\left\\{\n\\begin{array}{c c c}\n\\begin{bmatrix}\n[SA_{449},...,SA_{512}]\\\\\n[SA_{449},...,SA_{512}]\\\\\n[SA_{449},...,SA_{512}]\\\\\n[SA_{449},...,SA_{512}]\\\\\n[SA_{449},...,SA_{512}]\\\\\n[SA_{449},...,SA_{512}]\\\\\n\\end{bmatrix}\n\\end{array}\n\\right. \\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n\n\n\n\n\nSelf-Attention Transpuesto (6,8,64)\n\n\n\\[\n\\begin{array}{c c}\n\\left[\n\\begin{array}{c c c}\n\\overbrace{[SA_1,...,SA_{64}]}^{Head 1}, \\overbrace{[SA_{65},...,SA_{128}}^{Head 2}, ..., \\overbrace{[SA_{449},...,SA_{512}]}^{Head 8}\\\\\n[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\\\\n[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\\\\n[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\\\\n[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\\\\n[SA_1,...,SA_{128}],[SA_{65},..., SA_{128}],...[SA_{449},...,SA_{512}]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n\n\n\n\nSelf-Attention Concatenado (6,512)\n\n\n\\[\n\\begin{array}{c c}\n\\left[\n\\begin{array}{c c c}\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]"
  },
  {
    "objectID": "tics579/clase-11.html#encoder-multihead-attention-output",
    "href": "tics579/clase-11.html#encoder-multihead-attention-output",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Multihead Attention (Output)",
    "text": "Encoder: Multihead Attention (Output)\n\n\n\nHead Mixing\n\n\nLos outputs de cada cabeza ahora están uno al lado del otro. Por lo tanto, si aplicamos una capa lineal \\(W^O \\in \\mathbb{R}^{d_v \\cdot h \\times d_{model}}\\), estos parámetros entrenables se encargarán de aprender una combinación lineal que mezcla la información aprendida por cada Attention Head de manera óptima.\n\n\n\n\n\n\nMultihead Attention Output (6,512)\n\n\n\\[\nMultihead(Q,K,V) = \\begin{array}{c c}\n\\left[\n\\begin{array}{c c c}\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n[SA_1,.....,SA_{512}]\\\\\n\\end{array}\n\\right]\n\\end{array}\n\\cdot W^O\n\\]\n\n\n\n\n\n\n\n\n\n\\(W^O\\) se encarga de retornar a la dimensión del Input original para poder realizar la Residual Connection (similar al downsample de la Resnet)."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-add-layernorm",
    "href": "tics579/clase-11.html#encoder-add-layernorm",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Add + LayerNorm",
    "text": "Encoder: Add + LayerNorm\n\n\n\n\n\n\n\n\n\n\n\nResidual Connection (Add&Norm)\n\n\nCorresponde a una conexión residual. Combina la información de entrada al Multihead y su salida para luego aplicar LayerNorm.\n\\[Add\\&Norm = LayerNorm(X + Multihead(Q,K,V))\\]\n\n\n\n\n\n\nLayerNorm\n\n\nEl LayerNorm calcula el promedio y la varianza por token normalizando las dimensiones del embedding de cada token.\n\\[X_{norm} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\cdot \\gamma + \\beta\\]\n\n\n\n\n\n\n\n\n\n\n\\(\\gamma\\) y \\(\\beta\\) son parámetros entrenables.\n\n\n\n\n\n\n\nRegularización\n\n\nLa sección 5.4 menciona que se aplica Dropout posterior a cada sublayer del Encoder (y el Decoder) con \\(P_{drop}=0.1\\)."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-feed-forward-mlp",
    "href": "tics579/clase-11.html#encoder-feed-forward-mlp",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Feed Forward (MLP)",
    "text": "Encoder: Feed Forward (MLP)\n\n\n\n\n\n\n\n\nLa sección 3.3 del paper define el bloque Feed Forward de la siguiente manera:\n\\[FFN(x) = max(0,x \\cdot W_1+b_1)W_2 + b_2\\]\nDonde \\(W_1 \\in \\mathbb{R}^{d_{model} \\times {d_{ff}}}\\) y \\(W_2 \\in \\mathbb{R}^{d_{ff} \\times {d_{model}}}\\).\n\n\n\nArquitectura\n\n\n\n2 capas Feed Forward con bias.\nUna RelU como activación intermedia.\nDe acuerdo a la sección 5.4, a la salida incluiría un Dropout con \\(P_{drop}=0.1\\).\n\n\n\n\n\n\n\nResidual Connection (Add&Norm)\n\n\nAl igual que en el Multihead Attention, se la salida de esta capa se une con una conexión residual y se pasa por un LayerNorm."
  },
  {
    "objectID": "tics579/clase-11.html#encoder-output-final",
    "href": "tics579/clase-11.html#encoder-output-final",
    "title": "TICS-579-Deep Learning",
    "section": "Encoder: Output Final",
    "text": "Encoder: Output Final\n\n\n\n\n\n\n\n\n\n\n\nEncoder Layers\n\n\n\nLa combinación de todos los pasos anteriores constituyen un (1) Encoder. En el caso del paper el Transformer está compuesto de \\(N=6\\) Encoder Layers uno después del otro.\n\n\n\n\n\n\n\n👀 Ojito\n\n\n\nSólo antes de la primera Encoder Layer se aplica el Input Embedding y el Positional Encoding.\n\n\n\n\n\n\n\nArquitectura Encoder-Decoder\n\n\nEn el caso de estas arquitecturas, entonces el output del Encoder sirve como Keys y Values para el proceso de Cross Attention."
  },
  {
    "objectID": "tics579/clase-11.html#decoder-causal-self-attention",
    "href": "tics579/clase-11.html#decoder-causal-self-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Decoder: Causal Self-Attention",
    "text": "Decoder: Causal Self-Attention\n\n\n\n\n\n\n\n\n\\[Attention(Q,K,V) = Softmax\\left(\\frac{Q \\cdot K^T + Mask}{\\sqrt{d_k}}\\right) V\\]\n\n\n\n\n\n\n\nCorresponde a una variante del Self-Attention en el cuál sólo se presta atención a Tokens pasados, esto para preservar las propiedades auto-regresivas."
  },
  {
    "objectID": "tics579/clase-11.html#decoder-cross-attention",
    "href": "tics579/clase-11.html#decoder-cross-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Decoder: Cross Attention",
    "text": "Decoder: Cross Attention\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpcionalmente podría utilizar una Máscara en caso de querer evitar el Look Ahead.\n\n\n\n\n\n\n\nCross Attention\n\n\nEste mecanismo permite generar relaciones/atenciones entre dos secuencias de datos distintos. En este caso se relaciona una secuencia “query” con elementos “key” y “values” de otra secuencia. Además limita la generación del Decoder.\n\n\n\n\n\n\nDimensiones de Q, K y V\n\n\n\n\\(Q = X_{decoder} \\cdot W_q = (F,512) \\cdot (512,\\overbrace{8,64}^{512}) = (6,8,512)\\)\n\\(K = X_{encoder} \\cdot W_k = (6,512) \\cdot (512,\\overbrace{8,64}^{512}) = (6,8,512)\\)\n\\(V = X_{encoder} \\cdot W_v = (6,512) \\cdot (512,\\overbrace{8,64}^{512}) = (6,8,512)\\)\n\n\n\n\n\n\\[Attention(Q_{decoder},K_{encoder},V_{encoder}) = Softmax\\left(\\frac{Q_{decoder} \\cdot K_{encoder}^T + Mask}{\\sqrt{d_k}}\\right) V_{encoder}\\]"
  },
  {
    "objectID": "tics579/clase-11.html#prediction-head",
    "href": "tics579/clase-11.html#prediction-head",
    "title": "TICS-579-Deep Learning",
    "section": "Prediction Head",
    "text": "Prediction Head\n\n\n\n\n\n\n\n\n\n\n\nArquitectura\n\n\nCorresponde a una capa Feed Forward que proyecta desde \\(d_{model}\\) hasta \\(vocab\\_size\\) seguida de una Softmax.\n\n\n\n\n\n\n¿Por qué es necesaria?\n\n\nPor que la salida del Decoder tiene dimensiones \\((N, L, d_{model})\\). Es decir, tenemos \\(N\\) secuencias de largo \\(L\\), donde cada token está representado como un embedding de \\(d_{model}\\) dimensiones, lo cuál no es interpretable por humanos.\nEsta capa tiene el objetivo de estimar la probabilidad de que ocurra el siguiente token, de este modo predecir de manera autoregresiva."
  },
  {
    "objectID": "tics579/notebooks/control-5.html",
    "href": "tics579/notebooks/control-5.html",
    "title": "Control 5",
    "section": "",
    "text": "¿Qué significa desenrrollar (unrollment) la RNN?\n¿Qué es el problema de Vanishing/Exploding Gradients?\n¿Cuáles son las 4 Gates de una LSTM?\n¿Cuál es la diferencia entre Cell State y Hidden State?\n¿Qué es una RNN bidireccional?\n¿Qué es la nn.Embedding en Pytorch y por qué es necesarios en tareas de Texto?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/notebooks/transformers.html",
    "href": "tics579/notebooks/transformers.html",
    "title": "Input Example",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport math\n\nSEED = 10\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\nx = torch.randint(0, 100, (1, 6))\nx\n\ntensor([[37,  5, 32, 67, 32,  5]])"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#embeddings",
    "href": "tics579/notebooks/transformers.html#embeddings",
    "title": "Input Example",
    "section": "Embeddings",
    "text": "Embeddings\n\nclass InputEmbeddings(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n\n\nembedding_encoder = InputEmbeddings(d_model=4, vocab_size=100)\noutput = embedding_encoder(x)\noutput\n\ntensor([[[-1.8799, -0.8493,  3.3999,  1.4201],\n         [-0.1888,  0.1051,  0.4773, -3.1130],\n         [ 1.2626,  1.2161, -2.1373, -4.4780],\n         [-1.1958,  3.4485, -0.8264, -0.4976],\n         [ 1.2626,  1.2161, -2.1373, -4.4780],\n         [-0.1888,  0.1051,  0.4773, -3.1130]]], grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#positional-encoding",
    "href": "tics579/notebooks/transformers.html#positional-encoding",
    "title": "Input Example",
    "section": "Positional Encoding",
    "text": "Positional Encoding\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, seq_len, dropout):\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(p=dropout)\n\n        ## (L,d_model)\n        pe = torch.zeros(seq_len, d_model)\n        ## (L, 1)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2, dtype=torch.float)\n            * (-math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # (N, L, d_model)\n        pe = pe.unsqueeze(0)\n\n        ## Register Buffer\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        ## x = x + self.pe[:, : x.shape[1], :].requires_grad_(False)\n        x = x + self.pe\n        return self.dropout(x)\n\n\npe_encoder = PositionalEncoding(d_model=4, seq_len=6, dropout=0.1)\noutput_pe = pe_encoder(output)\noutput_pe\n\ntensor([[[-2.0888,  0.1674,  3.7777,  0.0000],\n         [ 0.7252,  0.7172,  0.5414, -2.3479],\n         [ 0.0000,  0.8888, -0.0000, -3.8646],\n         [-1.1719,  2.7317, -0.8849,  0.5577],\n         [ 0.5620,  0.6249, -2.3304, -3.8653],\n         [-1.2753,  0.4320,  0.5858, -2.3492]]], grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#multihead-attention",
    "href": "tics579/notebooks/transformers.html#multihead-attention",
    "title": "Input Example",
    "section": "Multihead Attention",
    "text": "Multihead Attention\n\nclass MultiHeadAttentionBlock(nn.Module):\n    def __init__(self, d_model, h, dropout):\n        super().__init__()\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n        self.d_k = d_model // h\n        self.h = h\n        self.d_model = d_model\n        ## Tensores empaquetados\n        self.W_q = nn.Linear(d_model, self.d_k * h)\n        self.W_k = nn.Linear(d_model, self.d_k * h)\n        self.W_v = nn.Linear(d_model, self.d_k * h)\n\n        self.w_o = nn.Linear(self.d_k * h, d_model)\n        self.dropout = nn.Dropout(p=dropout)\n\n    @staticmethod\n    def scale_dot_prod(Q, K, V, mask=None, dropout=None):\n        d_k = Q.shape[-1]\n\n        # (N, h, L, L)\n        attention_scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, -1e9)\n\n        attention_scores = attention_scores.softmax(dim=-1)\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n\n        ## (N, h, L, d_v)\n        return attention_scores @ V, attention_scores\n\n    def forward(self, q, k, v, mask=None):\n        ## (N, L, d_k*h)\n        Q = self.W_q(q)\n        K = self.W_k(k)\n        ## (N, L, d_v*h)\n        V = self.W_v(v)\n\n        # (N, L, h, d_k) --&gt; (N,h,L,d_k)\n        Q = Q.view(q.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n        # (N, L, h, d_k) --&gt; (N,h,L,d_k)\n        K = K.view(k.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n        # (N, L, h, d_k) --&gt; (N,h,L,d_k)\n        V = V.view(v.shape[0], -1, self.h, self.d_k).transpose(1, 2)\n\n        x, self.attention_scores = self.scale_dot_prod(\n            Q, K, V, mask, self.dropout\n        )\n        x = x.transpose(1, 2).reshape(q.shape[0], -1, self.h * self.d_k)\n        return self.w_o(x)\n\n\nmh_attention = MultiHeadAttentionBlock(d_model=4, h=2, dropout=0.1)\noutput_mh = mh_attention(output, output, output)\noutput_mh\n\ntensor([[[-0.0834, -0.5228,  0.0203, -0.0140],\n         [ 0.0389, -0.4723, -0.1963, -0.0514],\n         [ 0.1011, -0.5052, -0.5318, -0.1051],\n         [-0.0349, -0.4584, -0.4303, -0.1006],\n         [ 0.1187, -0.5545, -0.5845, -0.1033],\n         [ 0.0778, -0.5032, -0.2475, -0.0361]]], grad_fn=&lt;ViewBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#addnorm",
    "href": "tics579/notebooks/transformers.html#addnorm",
    "title": "Input Example",
    "section": "Add&Norm",
    "text": "Add&Norm\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        ## Multiplicative\n        self.alpha = nn.Parameter(torch.ones(1))\n        ## Additive\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, unbiased=False, keepdim=True)\n        return (x - mean) / torch.sqrt(\n            var + self.eps\n        ) * self.alpha + self.bias\n\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.norm = LayerNormalization()\n\n    def forward(self, x, sublayer):\n        output = sublayer(x)\n        output = self.dropout(output)\n        return self.norm(x + output)\n\n\nresidual_mh = ResidualConnection(dropout=0.1)\nmh_attention = MultiHeadAttentionBlock(d_model=4, h=2, dropout=0.1)\noutput = residual_mh(output_pe, lambda x: mh_attention(x, x, x))\noutput\n\ntensor([[[-0.9388, -0.2235,  1.6751, -0.5128],\n         [ 0.5153,  0.6049,  0.6106, -1.7308],\n         [ 0.5091,  0.6443,  0.5766, -1.7301],\n         [-1.0167,  1.4599, -0.8285,  0.3853],\n         [ 1.0495,  0.7769, -0.3532, -1.4732],\n         [-0.1209,  0.5691,  1.1103, -1.5585]]], grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#feed-forward",
    "href": "tics579/notebooks/transformers.html#feed-forward",
    "title": "Input Example",
    "section": "Feed Forward",
    "text": "Feed Forward\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.w1(x)\n        x = self.relu(x)\n        x = self.w2(x)\n        return x\n\n\nresidual_ffn = ResidualConnection(dropout=0.1)\nffn = FeedForward(d_model=4, d_ff=8)\noutput = residual_ffn(output, ffn)\noutput\n\ntensor([[[-1.2459,  0.4804,  1.3635, -0.5981],\n         [ 0.1829,  0.9943,  0.4803, -1.6575],\n         [ 0.1813,  1.0141,  0.4568, -1.6523],\n         [-1.3174,  1.4358, -0.3721,  0.2538],\n         [ 0.8782,  0.8883, -0.2200, -1.5464],\n         [-0.4689,  1.0912,  0.7833, -1.4056]]], grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#encoder",
    "href": "tics579/notebooks/transformers.html#encoder",
    "title": "Input Example",
    "section": "Encoder",
    "text": "Encoder\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model, d_ff, h, dropout):\n        super().__init__()\n        self.mh_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n\n        self.ffn = FeedForward(d_model, d_ff)\n        self.residuals = nn.ModuleDict(\n            dict(\n                mh=ResidualConnection(dropout),\n                ffn=ResidualConnection(dropout),\n            )\n        )\n\n    def forward(self, x):\n        x = self.residuals[\"mh\"](x, lambda x: self.mh_attention(x, x, x))\n        x = self.residuals[\"ffn\"](x, self.ffn)\n        return x\n\n\nencoder = EncoderBlock(d_model=4, d_ff=8, h=2, dropout=0.1)\nencoder(output_pe)\n\ntensor([[[-1.4794, -0.3560,  0.9296,  0.9059],\n         [ 0.9185,  0.6873,  0.0322, -1.6380],\n         [ 0.9489,  0.8417, -0.2676, -1.5230],\n         [-0.6131,  1.1944, -1.2978,  0.7165],\n         [ 1.2573,  0.6877, -0.8072, -1.1378],\n         [-0.3268,  1.6067, -0.1436, -1.1363]]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, N, d_model, d_ff, h, dropout):\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.h = h\n        self.dropout = dropout\n        self.encoders = nn.ModuleList(\n            [\n                EncoderBlock(self.d_model, self.d_ff, self.h, self.dropout)\n                for _ in range(N)\n            ]\n        )\n\n    def forward(self, x):\n        for encoder in self.encoders:\n            x = encoder(x)\n        return x\n\n\ntransformer_encoder = TransformerEncoder(\n    N=6, d_model=4, d_ff=8, h=2, dropout=0.1\n)\nencoder_output = transformer_encoder(output_pe)\nencoder_output\n\ntensor([[[-1.1471, -0.7726,  1.3095,  0.6102],\n         [ 0.8043, -1.5599,  0.9410, -0.1854],\n         [ 0.7972, -1.3047,  1.1294, -0.6219],\n         [-1.2670,  0.3383, -0.4971,  1.4258],\n         [ 1.5233,  0.1592, -1.1868, -0.4956],\n         [-0.3347, -1.1785,  1.5795, -0.0662]]], grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#decoder",
    "href": "tics579/notebooks/transformers.html#decoder",
    "title": "Input Example",
    "section": "Decoder",
    "text": "Decoder\n\nx_decoder = torch.randint(0, 200, (2, 6))\nembedding_decoder = InputEmbeddings(d_model=4, vocab_size=200)\noutput_decoder = embedding_decoder(x_decoder)\n\npe_decoder = PositionalEncoding(d_model=4, seq_len=6, dropout=0.1)\noutput_pe_decoder = pe_decoder(output_decoder)\noutput_pe_decoder\n\ntensor([[[ 1.4902, -0.1060, -0.2673,  0.9572],\n         [ 0.0000, -1.3015,  3.5659, -0.9811],\n         [ 1.9236, -4.7471,  1.0052, -0.0000],\n         [-1.0813,  1.5492, -0.6965,  2.6818],\n         [ 0.9349,  2.8193,  0.6835,  5.1822],\n         [-0.1522, -3.9696,  1.0385, -1.7455]],\n\n        [[-0.0000, -1.7481, -0.6793,  2.1605],\n         [-1.4601, -2.3530, -0.7971,  0.4837],\n         [-1.2516,  0.9073, -2.5760, -1.4898],\n         [ 3.9898, -2.8120, -0.0000, -0.0000],\n         [-2.0904,  3.8685,  0.0390,  1.1420],\n         [-2.1899, -2.4715,  0.2579,  1.6673]]], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, d_model, d_ff, h, dropout):\n        super().__init__()\n        self.causal_mh_attention = MultiHeadAttentionBlock(\n            d_model, h, dropout\n        )\n        self.cross_attention = MultiHeadAttentionBlock(d_model, h, dropout)\n        self.ffn = FeedForward(d_model, d_ff)\n        self.residuals = nn.ModuleDict(\n            dict(\n                causal=ResidualConnection(dropout),\n                cross=ResidualConnection(dropout),\n                ffn=ResidualConnection(dropout),\n            )\n        )\n\n    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n        x = self.residuals[\"causal\"](\n            x, lambda x: self.causal_mh_attention(x, x, x, tgt_mask)\n        )\n        x = self.residuals[\"cross\"](\n            x,\n            lambda x: self.cross_attention(\n                x, encoder_output, encoder_output, src_mask\n            ),\n        )\n        x = self.residuals[\"ffn\"](x, self.ffn)\n        return x\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, N, d_model, d_ff, h, dropout):\n        super().__init__()\n        self.layers = N\n        self.decoders = nn.ModuleList(\n            [DecoderBlock(d_model, d_ff, h, dropout) for _ in range(N)]\n        )\n\n    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n        for decoder in self.decoders:\n            x = decoder(x, encoder_output, src_mask, tgt_mask)\n\n        return x\n\n\ntransformer_decoder = TransformerDecoder(\n    N=6, d_model=4, d_ff=8, h=2, dropout=0.1\n)\ndecoder_output = transformer_decoder(output_pe_decoder, encoder_output)\ndecoder_output\n\ntensor([[[ 0.2385,  1.3848, -0.2166, -1.4066],\n         [-0.2838,  0.8749,  0.9249, -1.5160],\n         [ 1.2895, -0.1632,  0.3523, -1.4786],\n         [-0.9994,  1.6398, -0.5515, -0.0889],\n         [-0.1921,  1.5629, -0.1464, -1.2244],\n         [ 1.0752, -0.8200,  0.9065, -1.1617]],\n\n        [[ 0.1356,  0.0096, -1.4816,  1.3365],\n         [-1.2769,  0.2926,  1.4407, -0.4565],\n         [-0.6868,  1.6583, -0.0942, -0.8773],\n         [ 0.7595,  1.1732, -0.6672, -1.2655],\n         [-0.8131,  1.6748, -0.1457, -0.7160],\n         [-1.2560,  0.6754,  1.2384, -0.6578]]], grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "tics579/notebooks/transformers.html#projection-layer",
    "href": "tics579/notebooks/transformers.html#projection-layer",
    "title": "Input Example",
    "section": "Projection Layer",
    "text": "Projection Layer\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.linear_proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        x = self.linear_proj(x)\n        ## En general esto no se hace ya que el Loss puede incluir el Softmax\n        ## (batch, vocab_size)\n        return x.softmax(dim=-1)\n\n\nproj_layer = ProjectionLayer(d_model=4, vocab_size=200)\nlogits = proj_layer(decoder_output)\nlogits.shape\n\ntorch.Size([2, 6, 200])\n\n\n\n## Secuencias Predichas\ntorch.argmax(logits, dim=-1)\n\ntensor([[ 21, 188,  33, 180,  21,  27],\n        [  9,  80, 188,  33, 188,  80]])"
  },
  {
    "objectID": "tics579/notebooks/CNN-pytorch.html",
    "href": "tics579/notebooks/CNN-pytorch.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom torchinfo import summary\n\nSEED = 10\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n\nfrom sklearn.datasets import fetch_openml\n\ndf, target = fetch_openml(\"mnist_784\", return_X_y=True)\nprint(f\"Shape X: {df.shape}\")\nprint(f\"Shape y: {target.shape}\")\n\nShape X: (70000, 784)\nShape y: (70000,)\n\n\n\ndf\n\n\n\n\n\n\n\n\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\npixel9\npixel10\n...\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\npixel784\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69995\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n69996\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n69997\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n69998\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n69999\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n70000 rows × 784 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df, target, test_size=0.25, random_state=SEED\n)\n\n\ntorch.randn(100).view(10, 10).shape\n\ntorch.Size([10, 10])\n\n\n\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass MNIST(Dataset):\n\n    def __init__(self, X, y):\n\n        self.X = X.to_numpy()\n        self.y = y.values\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return dict(\n            X=torch.tensor(self.X[idx], dtype=torch.float32)\n            .view(28, 28)\n            .unsqueeze(0),\n            y=torch.tensor(int(self.y[idx]), dtype=torch.long),\n        )\n\n\ntrain_set = MNIST(X_train, y_train)\ntest_set = MNIST(X_test, y_test)\n\n\ntrain_set[0][\"X\"].shape\n\ntorch.Size([1, 28, 28])\n\n\n\ndef plot_number(X, y, tensor=True):\n\n    if tensor:\n        X = X.numpy().squeeze(0)\n        y = y.item()\n\n    plt.imshow(X, cmap=\"gray\")\n    plt.title(f\"Label: {y:.0f}\")\n    plt.show()\n\n\nidx = torch.randint(0, len(train_set), (1,)).item()\nplot_number(train_set[idx][\"X\"], train_set[idx][\"y\"])\n\n\n\n\n\n\n\n\n\ntrain_set[0][\"X\"].shape, train_set[0][\"y\"].shape\n\n(torch.Size([1, 28, 28]), torch.Size([]))\n\n\n\nclass CNN(nn.Module):\n    def __init__(self, in_channels=1, n_outputs=10, ks=3):\n        super().__init__()\n        self.conv1 = self.CNN_block(in_channels, 64, k=ks)\n        self.conv2 = self.CNN_block(64, 32, k=ks)\n        self.flatten = nn.Flatten()\n        # self.fc1 = nn.LazyLinear(16)\n        self.fc1 = nn.Linear(32 * 5 * 5, 16)  # filtros x tamaño\n        self.fc2 = nn.Linear(16, n_outputs)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\n    @staticmethod\n    def CNN_block(c_in, c_out, k=3, p=0, s=1, pk=2, ps=2):\n        return nn.Sequential(\n            nn.Conv2d(\n                in_channels=c_in,\n                out_channels=c_out,\n                kernel_size=k,\n                padding=p,\n                stride=s,\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=pk, stride=ps),\n        )\n\n\nmodel = CNN(in_channels=1, n_outputs=10)\nsummary(model, input_size=(1, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Sequential: 1-1                        [1, 64, 13, 13]           --\n│    └─Conv2d: 2-1                       [1, 64, 26, 26]           640\n│    └─ReLU: 2-2                         [1, 64, 26, 26]           --\n│    └─MaxPool2d: 2-3                    [1, 64, 13, 13]           --\n├─Sequential: 1-2                        [1, 32, 5, 5]             --\n│    └─Conv2d: 2-4                       [1, 32, 11, 11]           18,464\n│    └─ReLU: 2-5                         [1, 32, 11, 11]           --\n│    └─MaxPool2d: 2-6                    [1, 32, 5, 5]             --\n├─Flatten: 1-3                           [1, 800]                  --\n├─Linear: 1-4                            [1, 16]                   12,816\n├─Linear: 1-5                            [1, 10]                   170\n==========================================================================================\nTotal params: 32,090\nTrainable params: 32,090\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 2.68\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.38\nParams size (MB): 0.13\nEstimated Total Size (MB): 0.51\n==========================================================================================\n\n\n\nimport torchmetrics\nimport numpy as np\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cpu\")\nprint(f\"Training in {device}\")\nmodel = CNN(in_channels=1, n_outputs=10).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\nEPOCHS = 100\n\ntrain_dataloader = DataLoader(\n    train_set,\n    batch_size=1024,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=10,\n    drop_last=True,\n)\ntest_dataloader = DataLoader(\n    test_set, batch_size=32, shuffle=False, pin_memory=True, num_workers=10\n)\n\ntrain_metric = torchmetrics.Recall(task=\"multiclass\", num_classes=10).to(\n    device\n)\ntest_metric = torchmetrics.Recall(task=\"multiclass\", num_classes=10).to(\n    device\n)\ntrain_losses = []\ntest_losses = []\n\nfor e in range(EPOCHS):\n    start_time = time.time()\n    train_batch_losses = []\n    test_batch_losses = []\n    for batch in train_dataloader:\n        X, y = batch[\"X\"].to(device), batch[\"y\"].to(device)\n\n        optimizer.zero_grad()\n        y_pred = model(X)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        tm = train_metric(y_pred, y)\n        train_batch_losses.append(loss.item())\n\n    tm = train_metric.compute()\n    train_epoch_loss = np.mean(train_batch_losses)\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            X, y = batch[\"X\"].to(device), batch[\"y\"].to(device)\n            y_pred = model(X)\n            loss = criterion(y_pred, y)\n            tst_m = test_metric(y_pred, y)\n            test_batch_losses.append(loss.item())\n    tst_m = test_metric.compute()\n    test_epoch_loss = np.mean(test_batch_losses)\n    end_time = time.time()\n\n    train_losses.append(train_epoch_loss)\n    test_losses.append(test_epoch_loss)\n\n    epoch_time = end_time - start_time\n    ## Logging\n    print(\n        f\"Epoch: {e+1}- time: {epoch_time:.2f} - Train Loss: {train_epoch_loss:.4f} - Test Loss: {test_epoch_loss:.4f}- Train Recall: {tm:.4f} - Test Recall: {tst_m:.4f}\"\n    )\n\nTraining in cuda\nEpoch: 1- time: 2.36 - Train Loss: 2.5542 - Test Loss: 0.5054- Train Recall: 0.6231 - Test Recall: 0.8648\nEpoch: 2- time: 2.33 - Train Loss: 0.3456 - Test Loss: 0.2863- Train Recall: 0.7620 - Test Recall: 0.8933\nEpoch: 3- time: 2.29 - Train Loss: 0.2156 - Test Loss: 0.2080- Train Recall: 0.8205 - Test Recall: 0.9091\nEpoch: 4- time: 2.33 - Train Loss: 0.1617 - Test Loss: 0.1729- Train Recall: 0.8534 - Test Recall: 0.9195\nEpoch: 5- time: 2.36 - Train Loss: 0.1331 - Test Loss: 0.1467- Train Recall: 0.8748 - Test Recall: 0.9270\nEpoch: 6- time: 2.39 - Train Loss: 0.1108 - Test Loss: 0.1281- Train Recall: 0.8900 - Test Recall: 0.9328\nEpoch: 7- time: 2.36 - Train Loss: 0.0968 - Test Loss: 0.1171- Train Recall: 0.9014 - Test Recall: 0.9375\nEpoch: 8- time: 2.33 - Train Loss: 0.0863 - Test Loss: 0.1056- Train Recall: 0.9104 - Test Recall: 0.9413\nEpoch: 9- time: 2.29 - Train Loss: 0.0770 - Test Loss: 0.0987- Train Recall: 0.9177 - Test Recall: 0.9446\nEpoch: 10- time: 2.32 - Train Loss: 0.0699 - Test Loss: 0.0916- Train Recall: 0.9237 - Test Recall: 0.9474\nEpoch: 11- time: 2.32 - Train Loss: 0.0625 - Test Loss: 0.0880- Train Recall: 0.9289 - Test Recall: 0.9499\nEpoch: 12- time: 2.32 - Train Loss: 0.0581 - Test Loss: 0.0834- Train Recall: 0.9333 - Test Recall: 0.9520\nEpoch: 13- time: 2.37 - Train Loss: 0.0543 - Test Loss: 0.0846- Train Recall: 0.9371 - Test Recall: 0.9537\nEpoch: 14- time: 2.32 - Train Loss: 0.0506 - Test Loss: 0.0759- Train Recall: 0.9405 - Test Recall: 0.9554\nEpoch: 15- time: 2.35 - Train Loss: 0.0452 - Test Loss: 0.0761- Train Recall: 0.9435 - Test Recall: 0.9569\nEpoch: 16- time: 2.35 - Train Loss: 0.0427 - Test Loss: 0.0731- Train Recall: 0.9462 - Test Recall: 0.9583\nEpoch: 17- time: 2.34 - Train Loss: 0.0406 - Test Loss: 0.0706- Train Recall: 0.9486 - Test Recall: 0.9595\nEpoch: 18- time: 2.36 - Train Loss: 0.0364 - Test Loss: 0.0691- Train Recall: 0.9509 - Test Recall: 0.9606\nEpoch: 19- time: 2.35 - Train Loss: 0.0348 - Test Loss: 0.0700- Train Recall: 0.9529 - Test Recall: 0.9616\nEpoch: 20- time: 2.33 - Train Loss: 0.0323 - Test Loss: 0.0684- Train Recall: 0.9547 - Test Recall: 0.9625\nEpoch: 21- time: 2.34 - Train Loss: 0.0314 - Test Loss: 0.0656- Train Recall: 0.9564 - Test Recall: 0.9634\nEpoch: 22- time: 2.33 - Train Loss: 0.0285 - Test Loss: 0.0667- Train Recall: 0.9580 - Test Recall: 0.9642\nEpoch: 23- time: 2.35 - Train Loss: 0.0270 - Test Loss: 0.0634- Train Recall: 0.9595 - Test Recall: 0.9649\nEpoch: 24- time: 2.28 - Train Loss: 0.0250 - Test Loss: 0.0655- Train Recall: 0.9608 - Test Recall: 0.9656\nEpoch: 25- time: 2.41 - Train Loss: 0.0240 - Test Loss: 0.0635- Train Recall: 0.9621 - Test Recall: 0.9662\nEpoch: 26- time: 2.28 - Train Loss: 0.0232 - Test Loss: 0.0619- Train Recall: 0.9633 - Test Recall: 0.9668\nEpoch: 27- time: 2.19 - Train Loss: 0.0203 - Test Loss: 0.0618- Train Recall: 0.9644 - Test Recall: 0.9674\nEpoch: 28- time: 2.30 - Train Loss: 0.0190 - Test Loss: 0.0618- Train Recall: 0.9655 - Test Recall: 0.9679\nEpoch: 29- time: 2.27 - Train Loss: 0.0181 - Test Loss: 0.0681- Train Recall: 0.9665 - Test Recall: 0.9684\nEpoch: 30- time: 2.42 - Train Loss: 0.0179 - Test Loss: 0.0610- Train Recall: 0.9674 - Test Recall: 0.9689\nEpoch: 31- time: 2.46 - Train Loss: 0.0165 - Test Loss: 0.0624- Train Recall: 0.9683 - Test Recall: 0.9693\nEpoch: 32- time: 2.51 - Train Loss: 0.0152 - Test Loss: 0.0614- Train Recall: 0.9692 - Test Recall: 0.9698\nEpoch: 33- time: 2.40 - Train Loss: 0.0149 - Test Loss: 0.0608- Train Recall: 0.9700 - Test Recall: 0.9702\nEpoch: 34- time: 2.38 - Train Loss: 0.0137 - Test Loss: 0.0610- Train Recall: 0.9708 - Test Recall: 0.9705\nEpoch: 35- time: 2.36 - Train Loss: 0.0132 - Test Loss: 0.0605- Train Recall: 0.9715 - Test Recall: 0.9709\nEpoch: 36- time: 2.38 - Train Loss: 0.0122 - Test Loss: 0.0603- Train Recall: 0.9722 - Test Recall: 0.9713\nEpoch: 37- time: 2.42 - Train Loss: 0.0107 - Test Loss: 0.0601- Train Recall: 0.9729 - Test Recall: 0.9716\nEpoch: 38- time: 2.47 - Train Loss: 0.0110 - Test Loss: 0.0645- Train Recall: 0.9735 - Test Recall: 0.9719\nEpoch: 39- time: 2.23 - Train Loss: 0.0107 - Test Loss: 0.0616- Train Recall: 0.9741 - Test Recall: 0.9722\nEpoch: 40- time: 2.27 - Train Loss: 0.0103 - Test Loss: 0.0620- Train Recall: 0.9747 - Test Recall: 0.9725\nEpoch: 41- time: 2.35 - Train Loss: 0.0089 - Test Loss: 0.0630- Train Recall: 0.9753 - Test Recall: 0.9728\nEpoch: 42- time: 2.54 - Train Loss: 0.0078 - Test Loss: 0.0634- Train Recall: 0.9758 - Test Recall: 0.9730\nEpoch: 43- time: 2.36 - Train Loss: 0.0075 - Test Loss: 0.0620- Train Recall: 0.9763 - Test Recall: 0.9733\nEpoch: 44- time: 2.39 - Train Loss: 0.0066 - Test Loss: 0.0614- Train Recall: 0.9769 - Test Recall: 0.9735\nEpoch: 45- time: 2.28 - Train Loss: 0.0070 - Test Loss: 0.0630- Train Recall: 0.9773 - Test Recall: 0.9737\nEpoch: 46- time: 2.33 - Train Loss: 0.0067 - Test Loss: 0.0636- Train Recall: 0.9778 - Test Recall: 0.9739\nEpoch: 47- time: 2.42 - Train Loss: 0.0056 - Test Loss: 0.0651- Train Recall: 0.9783 - Test Recall: 0.9742\nEpoch: 48- time: 2.26 - Train Loss: 0.0062 - Test Loss: 0.0672- Train Recall: 0.9787 - Test Recall: 0.9743\nEpoch: 49- time: 2.27 - Train Loss: 0.0056 - Test Loss: 0.0630- Train Recall: 0.9791 - Test Recall: 0.9746\nEpoch: 50- time: 2.36 - Train Loss: 0.0046 - Test Loss: 0.0646- Train Recall: 0.9795 - Test Recall: 0.9748\nEpoch: 51- time: 2.35 - Train Loss: 0.0043 - Test Loss: 0.0625- Train Recall: 0.9799 - Test Recall: 0.9749\nEpoch: 52- time: 2.41 - Train Loss: 0.0040 - Test Loss: 0.0659- Train Recall: 0.9803 - Test Recall: 0.9751\nEpoch: 53- time: 2.29 - Train Loss: 0.0037 - Test Loss: 0.0637- Train Recall: 0.9806 - Test Recall: 0.9753\nEpoch: 54- time: 2.30 - Train Loss: 0.0034 - Test Loss: 0.0648- Train Recall: 0.9810 - Test Recall: 0.9755\nEpoch: 55- time: 2.26 - Train Loss: 0.0032 - Test Loss: 0.0660- Train Recall: 0.9813 - Test Recall: 0.9756\nEpoch: 56- time: 2.22 - Train Loss: 0.0029 - Test Loss: 0.0654- Train Recall: 0.9817 - Test Recall: 0.9758\nEpoch: 57- time: 2.21 - Train Loss: 0.0028 - Test Loss: 0.0672- Train Recall: 0.9820 - Test Recall: 0.9759\nEpoch: 58- time: 2.37 - Train Loss: 0.0027 - Test Loss: 0.0662- Train Recall: 0.9823 - Test Recall: 0.9761\nEpoch: 59- time: 2.34 - Train Loss: 0.0025 - Test Loss: 0.0650- Train Recall: 0.9826 - Test Recall: 0.9762\nEpoch: 60- time: 2.32 - Train Loss: 0.0024 - Test Loss: 0.0657- Train Recall: 0.9829 - Test Recall: 0.9764\nEpoch: 61- time: 2.28 - Train Loss: 0.0023 - Test Loss: 0.0658- Train Recall: 0.9832 - Test Recall: 0.9765\nEpoch: 62- time: 2.29 - Train Loss: 0.0021 - Test Loss: 0.0676- Train Recall: 0.9834 - Test Recall: 0.9767\nEpoch: 63- time: 2.44 - Train Loss: 0.0019 - Test Loss: 0.0675- Train Recall: 0.9837 - Test Recall: 0.9768\nEpoch: 64- time: 2.47 - Train Loss: 0.0019 - Test Loss: 0.0686- Train Recall: 0.9839 - Test Recall: 0.9769\nEpoch: 65- time: 2.42 - Train Loss: 0.0016 - Test Loss: 0.0705- Train Recall: 0.9842 - Test Recall: 0.9770\nEpoch: 66- time: 2.38 - Train Loss: 0.0016 - Test Loss: 0.0685- Train Recall: 0.9844 - Test Recall: 0.9772\nEpoch: 67- time: 2.29 - Train Loss: 0.0014 - Test Loss: 0.0684- Train Recall: 0.9847 - Test Recall: 0.9773\nEpoch: 68- time: 2.21 - Train Loss: 0.0014 - Test Loss: 0.0685- Train Recall: 0.9849 - Test Recall: 0.9774\nEpoch: 69- time: 2.29 - Train Loss: 0.0013 - Test Loss: 0.0688- Train Recall: 0.9851 - Test Recall: 0.9775\nEpoch: 70- time: 2.21 - Train Loss: 0.0012 - Test Loss: 0.0709- Train Recall: 0.9853 - Test Recall: 0.9776\nEpoch: 71- time: 2.34 - Train Loss: 0.0011 - Test Loss: 0.0694- Train Recall: 0.9855 - Test Recall: 0.9777\nEpoch: 72- time: 2.35 - Train Loss: 0.0012 - Test Loss: 0.0703- Train Recall: 0.9857 - Test Recall: 0.9778\nEpoch: 73- time: 2.24 - Train Loss: 0.0011 - Test Loss: 0.0724- Train Recall: 0.9859 - Test Recall: 0.9779\nEpoch: 74- time: 2.23 - Train Loss: 0.0011 - Test Loss: 0.0711- Train Recall: 0.9861 - Test Recall: 0.9780\nEpoch: 75- time: 2.20 - Train Loss: 0.0009 - Test Loss: 0.0719- Train Recall: 0.9863 - Test Recall: 0.9781\nEpoch: 76- time: 2.18 - Train Loss: 0.0008 - Test Loss: 0.0720- Train Recall: 0.9865 - Test Recall: 0.9782\nEpoch: 77- time: 2.20 - Train Loss: 0.0008 - Test Loss: 0.0730- Train Recall: 0.9866 - Test Recall: 0.9783\nEpoch: 78- time: 2.19 - Train Loss: 0.0008 - Test Loss: 0.0741- Train Recall: 0.9868 - Test Recall: 0.9784\nEpoch: 79- time: 2.26 - Train Loss: 0.0007 - Test Loss: 0.0733- Train Recall: 0.9870 - Test Recall: 0.9785\nEpoch: 80- time: 2.30 - Train Loss: 0.0007 - Test Loss: 0.0741- Train Recall: 0.9871 - Test Recall: 0.9786\nEpoch: 81- time: 2.25 - Train Loss: 0.0007 - Test Loss: 0.0739- Train Recall: 0.9873 - Test Recall: 0.9786\nEpoch: 82- time: 2.22 - Train Loss: 0.0006 - Test Loss: 0.0738- Train Recall: 0.9875 - Test Recall: 0.9787\nEpoch: 83- time: 2.27 - Train Loss: 0.0006 - Test Loss: 0.0744- Train Recall: 0.9876 - Test Recall: 0.9788\nEpoch: 84- time: 2.41 - Train Loss: 0.0006 - Test Loss: 0.0742- Train Recall: 0.9878 - Test Recall: 0.9789\nEpoch: 85- time: 2.42 - Train Loss: 0.0005 - Test Loss: 0.0753- Train Recall: 0.9879 - Test Recall: 0.9790\nEpoch: 86- time: 2.31 - Train Loss: 0.0005 - Test Loss: 0.0749- Train Recall: 0.9880 - Test Recall: 0.9790\nEpoch: 87- time: 2.26 - Train Loss: 0.0005 - Test Loss: 0.0760- Train Recall: 0.9882 - Test Recall: 0.9791\nEpoch: 88- time: 2.43 - Train Loss: 0.0005 - Test Loss: 0.0760- Train Recall: 0.9883 - Test Recall: 0.9792\nEpoch: 89- time: 2.29 - Train Loss: 0.0005 - Test Loss: 0.0763- Train Recall: 0.9884 - Test Recall: 0.9792\nEpoch: 90- time: 2.35 - Train Loss: 0.0004 - Test Loss: 0.0764- Train Recall: 0.9886 - Test Recall: 0.9793\nEpoch: 91- time: 2.21 - Train Loss: 0.0004 - Test Loss: 0.0772- Train Recall: 0.9887 - Test Recall: 0.9794\nEpoch: 92- time: 2.25 - Train Loss: 0.0004 - Test Loss: 0.0772- Train Recall: 0.9888 - Test Recall: 0.9795\nEpoch: 93- time: 2.25 - Train Loss: 0.0004 - Test Loss: 0.0773- Train Recall: 0.9889 - Test Recall: 0.9795\nEpoch: 94- time: 2.31 - Train Loss: 0.0004 - Test Loss: 0.0786- Train Recall: 0.9891 - Test Recall: 0.9796\nEpoch: 95- time: 2.38 - Train Loss: 0.0004 - Test Loss: 0.0775- Train Recall: 0.9892 - Test Recall: 0.9796\nEpoch: 96- time: 2.27 - Train Loss: 0.0004 - Test Loss: 0.0788- Train Recall: 0.9893 - Test Recall: 0.9797\nEpoch: 97- time: 2.37 - Train Loss: 0.0003 - Test Loss: 0.0788- Train Recall: 0.9894 - Test Recall: 0.9798\nEpoch: 98- time: 2.22 - Train Loss: 0.0003 - Test Loss: 0.0789- Train Recall: 0.9895 - Test Recall: 0.9798\nEpoch: 99- time: 2.22 - Train Loss: 0.0003 - Test Loss: 0.0794- Train Recall: 0.9896 - Test Recall: 0.9799\nEpoch: 100- time: 2.30 - Train Loss: 0.0003 - Test Loss: 0.0796- Train Recall: 0.9897 - Test Recall: 0.9799\n\n\n\ndef plot_training_curves(train_loss, validation_loss, n_epochs, title=\"\"):\n    plt.plot(\n        range(1, n_epochs + 1),\n        train_loss,\n        label=\"Train Loss\",\n    )\n    plt.plot(\n        range(1, n_epochs + 1),\n        validation_loss,\n        label=\"Validation Loss\",\n    )\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n\nplot_training_curves(train_losses, test_losses, EPOCHS)\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/clase-7.html#limitaciones-de-las-ffn",
    "href": "tics579/clase-7.html#limitaciones-de-las-ffn",
    "title": "TICS-579-Deep Learning",
    "section": "Limitaciones de las FFN",
    "text": "Limitaciones de las FFN\n\n\n\n\n\n\n\nNúmero de Parametros:\n\n\\(W_1 = 784 \\cdot 256 + 256 = 200960\\)\n\\(W_2 = 256 \\cdot 128 + 128 = 32896\\)\n\\(W_3 = 128 \\cdot 10 + 10 = 1290\\)\nTotal = 235146.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si tengo una imágen de \\(512 \\times 512\\)? 67,143,306 de parámetros."
  },
  {
    "objectID": "tics579/clase-7.html#limitaciones-de-las-ffn-1",
    "href": "tics579/clase-7.html#limitaciones-de-las-ffn-1",
    "title": "TICS-579-Deep Learning",
    "section": "Limitaciones de las FFN",
    "text": "Limitaciones de las FFN\n\n\n\n\n\n\n\n\n\nTranslation Invariance\n\nSe refiere a la capacidad de poder detectar un patrón/objeto en diferentes posiciones de la imágen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLas FFN no son capaces de ver patrones globales sino que se enfocan en el valor preciso de una feature. Si el objeto cambia de posición las features cambian a valores completamente distintos haciendo que la red tenga mayor tendencia al error."
  },
  {
    "objectID": "tics579/clase-7.html#imágenes",
    "href": "tics579/clase-7.html#imágenes",
    "title": "TICS-579-Deep Learning",
    "section": "Imágenes",
    "text": "Imágenes\n\nImagen\n\nDefiniremos una imágen como un Tensor de Tres dimensiones, en el cual se representa H, W y C (Altura, Ancho y Canales).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nLa convención más común es utilizar imágenes de 24-bits 3 canales de \\(2^8\\) valores (es decir 8-bits) que representan la intensidad de los colores Rojo (R), Verde (G) y Azul (B). Las dimensiones de \\(H\\) y \\(W\\) definen la resolución en píxeles de la imágen."
  },
  {
    "objectID": "tics579/clase-7.html#imágenes-1",
    "href": "tics579/clase-7.html#imágenes-1",
    "title": "TICS-579-Deep Learning",
    "section": "Imágenes",
    "text": "Imágenes\n\nPara importar imágenes en Pytorch existen distintas librerías como PIL u OpenCV. Ambas usan la convención de \\((H,W,C)\\), la diferencia está en el orden de los canels. PIL utiliza la convención RGB, mientras que OpenCV utiliza BGR por lo que se necesitan algunas convenciones adicionales.\n\n\n\n\n\n\n\nLamentablemente la convención que escogió Pytorch es que una imágen tiene (C, H, W), es decir, primero canales, y luego alto y ancho. Normalmente las librerías no usan esta convención por lo que una permutación o transposición de dimensiones va a ser necesario casi la mayoría de las veces.\n\n\n\n\n\nimage = torch.tensor(...)\n\n\n\n\n\n\nimage[0,:,:]\nimagen[0] # alternativa\n\n\n\n\n\n\nimage[1,:,:]\nimagen[1] # alternativa\n\n\n\n\n\n\nimage[2,:,:]\nimagen[2] # alternativa"
  },
  {
    "objectID": "tics579/clase-7.html#imágenes-2",
    "href": "tics579/clase-7.html#imágenes-2",
    "title": "TICS-579-Deep Learning",
    "section": "Imágenes",
    "text": "Imágenes\n\n\n\n\n\n\n\n\nCuando tenemos un batch de imágenes entonces tendremos un Tensor de 4 dimensiones de dimensiones (N,C,H,W), donde \\(N\\) representa el Batch Size, \\(C\\) el número de Canales, \\(H\\) el alto y \\(W\\) el ancho.\n\n\n\n\n\n\n\n\n\nLuego un Tensor de Dimensiones (32,3,224,512) implica que tenemos 32 imágenes RGB de dimensiones \\(224\\times512\\).\n\n\n\n\n\n\n\n\n\n\nIndexing\n\n\n\n\n\n\n\nAl tener tantas dimensiones elegir elementos se vuelve un poco complicado. En general se utilizarán sólo los primeros dos índices, el primero para escoger la imágen y el segundo para escoger el canal. En general Pytorch permite usar sólo el primer index y obviar el resto. Pero no permite hacer lo mismo con los siguientes."
  },
  {
    "objectID": "tics579/clase-7.html#redes-convolucionales-definición-e-inspiración",
    "href": "tics579/clase-7.html#redes-convolucionales-definición-e-inspiración",
    "title": "TICS-579-Deep Learning",
    "section": "Redes Convolucionales: Definición e Inspiración",
    "text": "Redes Convolucionales: Definición e Inspiración\n\nRedes Convolucionales (CNN)\n\nSon un tipo distinto de Redes Neuronales donde sus parámetros aprenden “feature maps” de los datos. Principalemente se aplican en imágenes, pero pueden aplicarse para secuencias unidimensionales o secuencias de imágenes (videos).\n\n\n\n\n\n\n\n\nExiste el mito de que las Redes Convolucionales se inspiraron en el funcionamiento del Cortex Visual humano. No sé si es tan así.\n\n\n\n\n\n\n\n\n\n¿Por qué necesitamos Redes Convolucionales? Evitar la sobreparametrización. ¿Por qué esto es un problema?\n\n\n\n\n\nTimeline\n\n1990: Yann LeCun et al. propone uno de los primeros intentos de CNN, el cual va agregando features más simples en features más complejas progresivamente.\n1998: Yann LeCun, propone LeNet-5 con 2 redes convolucionales y 3 FFN.\n2012: Alex Krizhevsky et al. propone AlexNet (5 capas convolucionales y 3 FFN), el cual obtiene SOTA performance en ImageNet."
  },
  {
    "objectID": "tics579/clase-7.html#partes-de-una-cnn",
    "href": "tics579/clase-7.html#partes-de-una-cnn",
    "title": "TICS-579-Deep Learning",
    "section": "Partes de una CNN",
    "text": "Partes de una CNN\n\nConvolución\n\nCorresponde a una operación para extraer features maps en la cual un filtro o kernel se va desplazando en cada sección de los datos (secuencia, imagen o video).\n\n\n\n\n\nOjo\n\n\nEsto es nuevamente un término marketero, porque no es una Convolucional real, sino una operación llamada Cross Correlation.\n\n\n\n\nFeature Map\n\nCorresponde a la salida de una convolución y es un nuevo tensor que captura ciertas características del dato (secuencia, imagen o video). Cuando se trata de imágenes normalmente es capaz de detectar bordes, cambios de textura, color, formas, o elementos más pequeños.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que los features maps son de una dimensionalidad menor a la entrada debido a la operación de Convolución.\n\n\n\n\n\n\n\n\n\nSe obtendrán tantos feature maps como filtros se apliquen."
  },
  {
    "objectID": "tics579/clase-7.html#entendiendo-el-kernel",
    "href": "tics579/clase-7.html#entendiendo-el-kernel",
    "title": "TICS-579-Deep Learning",
    "section": "Entendiendo el Kernel",
    "text": "Entendiendo el Kernel\n\n\nGaussian Blur\n\n\n\n\n\nLíneas Horizontales\n\n\n\n\n\nBordes\n\n\n\n\n\n\n\n\n\n\n\n\nKernel\n\n\nEl Kernel va a ser el set de parámetros que la red convolucional va a aprender. En palabras sencillas, la misma red aprende cuáles son los aspectos más relevantes de la imagen que le permitirán entender cómo clasificar o detectar elementos en ella.\n\n\n\n\n\n\n\n\n\nEl Kernel se aplica a todos los canales a la vez, lo cuál inicialmente lo hace ver como una operación bastante costosa computacionalmente.\n\n\n\n\n\n\n\n\n\nEl Kernel introduce el primer hiperparámetro de las CNN que es el Kernel Size. En general son cuadrados, y de dimensión impar."
  },
  {
    "objectID": "tics579/clase-7.html#hiperparámetros-de-la-convolución",
    "href": "tics579/clase-7.html#hiperparámetros-de-la-convolución",
    "title": "TICS-579-Deep Learning",
    "section": "Hiperparámetros de la Convolución",
    "text": "Hiperparámetros de la Convolución\n\n\n\n\n\n\n\n\n\n\nStride\n\n\nCorresponde a la cantidad de pasos en que se mueve el Kernel. Un stride más grande implica feature maps más pequeños y menos detalles. Strides más pequeños retiene más detalles, pero implica un mayor número de operaciones.\n\n\n\n\n\n\n\n\n\n\n\n\nPadding\n\n\nCorresponde a un relleno para dar mayor movimiento del kernel. Permite evitar la reducción de dimensionalidad por parte de la convolución además de considerar la información de los bordes de la imagen. Se llama “valid” a no usar padding, y “same” a agregar suficientes píxeles para evitar la reducción de dimensión.\n\n\n\n\n\n\n\n\n\n\n\n\nDilation\n\n\nEn este caso se tienen gaps al momento de aplicar el Kernel. Normalmente aplicar dilation aumenta el campo receptivo de la convolución capturando más contexto sin la necesidad de aumentar el kernel size. 1 implica sin dilation."
  },
  {
    "objectID": "tics579/clase-7.html#convolución-en-pytorch",
    "href": "tics579/clase-7.html#convolución-en-pytorch",
    "title": "TICS-579-Deep Learning",
    "section": "Convolución en Pytorch",
    "text": "Convolución en Pytorch\n\nnn.Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0,dilation=1)\n\n\n\n\n\nInput\n\n\nEste tipo de redes no requiere que se le den las dimensiones de la imagen, pero sí espera recibir tensores de dimensión \\((N,C_{in}, H_{in},W_{in})\\).\n\n\n\n\n\n\nOutput\n\n\nLa Red convolucional devuelve un Tensor de Dimensiones \\((N,C_{out}, H_{out}, W_{out})\\). Donde:\n\\[H_{out} = \\left\\lfloor \\frac{H_{in} + 2 \\cdot padding[0] - dilation[0]\\cdot (kernel\\_size[0] - 1) - 1}{stride[0]} + 1 \\right\\rfloor\\] \\[W_{out} = \\left\\lfloor \\frac{W_{in} + 2 \\cdot padding[1] - dilation[1]\\cdot (kernel\\_size[1] - 1) - 1}{stride[1]} + 1 \\right\\rfloor\\]\n\n\n\n\n\n\n\n\n\n\nEs importante tener noción del tamaño de la imagen para poder escoger un kernel_size que recorra la imagen completa y que no deje partes sin convolucionar."
  },
  {
    "objectID": "tics579/clase-7.html#partes-de-una-cnn-pooling",
    "href": "tics579/clase-7.html#partes-de-una-cnn-pooling",
    "title": "TICS-579-Deep Learning",
    "section": "Partes de una CNN: Pooling",
    "text": "Partes de una CNN: Pooling\n\nPooling\n\nEl Pooling es una operación de agregación que permite ir disminuyendo la dimensionalidad. De esta manera la red puede comenzar a especializarse en aspectos cada vez más finos.\n\n\n\n\n\n\n\n\nEl Pooling también se aplica de manera móvil como una convolución. Pero a diferencia de esta normalmente no genera traslape.\n\n\n\n\n\n\n\n\n\nAcá se introduce otro hiperparámetro que es el Pooling Size. En general es cuadrado pero de dimensión par."
  },
  {
    "objectID": "tics579/clase-7.html#pooling-in-pytorch",
    "href": "tics579/clase-7.html#pooling-in-pytorch",
    "title": "TICS-579-Deep Learning",
    "section": "Pooling in Pytorch",
    "text": "Pooling in Pytorch\n\nnn.AvgPool2d(kernel_size, stride=None,padding=0)\nnn.MaxPool2d(kernel_size, stride=None,padding=0, dilation=1)\n\n\n\n\n\n\n\nOjo\n\n\nstride=None implica stride = kernel_size.\n\n\n\n\n\n\n\n\n\nImportante mencionar que Average Pooling no permite Dilation.\n\n\n\n\\[H_{out} = \\left\\lfloor \\frac{H_{in} + 2 \\cdot padding[0] - dilation[0]\\cdot (kernel\\_size[0] - 1) - 1}{stride[0]} + 1 \\right\\rfloor\\] \\[W_{out} = \\left\\lfloor \\frac{W_{in} + 2 \\cdot padding[1] - dilation[1]\\cdot (kernel\\_size[1] - 1) - 1}{stride[1]} + 1 \\right\\rfloor\\]"
  },
  {
    "objectID": "tics579/clase-7.html#arquitectura-de-una-cnn",
    "href": "tics579/clase-7.html#arquitectura-de-una-cnn",
    "title": "TICS-579-Deep Learning",
    "section": "Arquitectura de una CNN",
    "text": "Arquitectura de una CNN\n\n\n\n\n\n\n\n\n\n\n\nFeature Extractor - Encoder - Backbone\n\n\nCorresponde al bloque de que generalmente contiene CNNs que se encargará de extraer features.\n\n\n\n\n\n\nFlatten\n\n\nCorresponde a una Operación Intermedia que dejará todos los píxeles de la imagen como un vector fila que puede ser tomado por la FFN.\n\n\n\n\n\n\nPrediction Head - Head - MLP\n\n\nCorresponde a una FFN que tomará las features aprendidas por la CNN y generará una predicción."
  },
  {
    "objectID": "tics579/clase-7.html#mnist-con-cnn",
    "href": "tics579/clase-7.html#mnist-con-cnn",
    "title": "TICS-579-Deep Learning",
    "section": "MNIST con CNN",
    "text": "MNIST con CNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl número de Parámetros para una Red con muchas más capas bajó considerablemente, de 67M a 373K de Parámetros."
  },
  {
    "objectID": "tics579/clase-7.html#variante-en-1d",
    "href": "tics579/clase-7.html#variante-en-1d",
    "title": "TICS-579-Deep Learning",
    "section": "Variante en 1d",
    "text": "Variante en 1d\n\n\n\nConv1d\n\nCorresponde a la variante de una dimensión, en la cual la entrada corresponden a secuencias de elementos como podrían ser series de tiempo, audio o hasta cadenas de texto.\n\n\n\n\n\nEn este caso la implementación en Pytorch es similar a la 2D sólo que esperando tensores de dimensiones \\((N,C_{in}, L_{in})\\), donde \\(C_{in}\\) corresponde al número de canales, que en el caso de series de tiempo equivale a features, y \\(L_{in}\\) corresponde al largo de la secuencia.\n\n\n\n\n\n\nLa salida de la Conv1d tendrá dimensiones \\((N,C_{out},L_{out})\\) con:\n\\[L_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\cdot padding - dilation \\cdot (kernel\\_size - 1) - 1}{stride} + 1 \\right\\rfloor\\]"
  },
  {
    "objectID": "tics579/clase-7.html#variante-en-3d",
    "href": "tics579/clase-7.html#variante-en-3d",
    "title": "TICS-579-Deep Learning",
    "section": "Variante en 3d",
    "text": "Variante en 3d\n\n\n\nConv3d\n\nCorresponde a la variante de tres dimensiones, en la cual la entrada corresponde a secuencias de imágenes, es decir, videos.\n\n\n\n\n\nEste caso también es similar sólo que se esperan tensores de dimensiones \\((N, C_{in}, D_{in}, H_{in}, W_{in})\\) donde \\(C_in\\) corresponde al número de canales, \\(D\\) en el caso de un video corresponde al número de frames de tamaño \\(H_{in} \\times W_{in}\\).\n\n\n\n\n\n\nLa salida de la Conv1d tendrá dimensiones \\((N,C_{out},D_{out},H_{out},W_{out})\\) con:\n\\[D_{out} = \\left\\lfloor \\frac{D_{in} + 2 \\cdot padding[0] - dilation[0] \\cdot (kernel\\_size[0] - 1) - 1}{stride[0]} + 1 \\right\\rfloor\\] \\[H_{out} = \\left\\lfloor \\frac{H_{in} + 2 \\cdot padding[1] - dilation[1]\\cdot (kernel\\_size[1] - 1) - 1}{stride[1]} + 1 \\right\\rfloor\\] \\[W_{out} = \\left\\lfloor \\frac{W_{in} + 2 \\cdot padding[2] - dilation[2]\\cdot (kernel\\_size[2] - 1) - 1}{stride[2]} + 1 \\right\\rfloor\\]"
  },
  {
    "objectID": "tics579/clase-7.html#ejemplos-de-arquitecturas",
    "href": "tics579/clase-7.html#ejemplos-de-arquitecturas",
    "title": "TICS-579-Deep Learning",
    "section": "Ejemplos de Arquitecturas",
    "text": "Ejemplos de Arquitecturas"
  },
  {
    "objectID": "tics579/clase-4.html#pytorch",
    "href": "tics579/clase-4.html#pytorch",
    "title": "TICS-579-Deep Learning",
    "section": "Pytorch",
    "text": "Pytorch\n\nEs una librería de manipulación de Tensores especializada en Deep Learning. Provee principalmente, manipulación de tensores (igual que Numpy, pero en GPU), además de Autograd (calcula derivadas de manera automática).\n\nPara poder comenzar a utilizarlo se requieren normalmente 3 imports:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\n\n\n\ntorch es donde se encuentran la mayoría de funciones básicas para manipular tensores.\ntorch.nn es donde se encuentran los módulos necesarios para poder crear redes neuronales (neural networks). Cada módulo es una clase en Python.\ntorch.nn.functional es donde se encontrarán las versiones funcionales de elementos de torch.nn."
  },
  {
    "objectID": "tics579/clase-4.html#gpu",
    "href": "tics579/clase-4.html#gpu",
    "title": "TICS-579-Deep Learning",
    "section": "GPU",
    "text": "GPU\n\n\n\n\n\n\n\nSu principal ventaja es que puede ejecutarse en GPU, lo cual entrega una ventaja comparativa enorme (Muchos más núcleos).\n\n\n\n\n\n\n\n\n\n\n\nLas GPUs están programadas en CUDA, una variante de C++ que es muy complicado de entender. Por lo que los mensajes de error son sumamente crípticos. Se recomienda desarrollar en CPU, y cambiar a GPU sólo cuando sea necesario ejecutar libre de errores.\n\n\n\n\n## Permite automáticamente reconocer si es que existe GPU en el sistema y de existir lo asigna.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n\n\n\nEl código de arriba es particularmente útil para Google Colab o plataformas que permitan activar o desactivar GPUs.\nTambién es posible definirlo de manera manual en caso de querer debuggear algo en particular."
  },
  {
    "objectID": "tics579/clase-4.html#mapeando-lo-aprendido-con-pytorch",
    "href": "tics579/clase-4.html#mapeando-lo-aprendido-con-pytorch",
    "title": "TICS-579-Deep Learning",
    "section": "Mapeando lo aprendido con Pytorch",
    "text": "Mapeando lo aprendido con Pytorch\n\n\n\n\n\n\n\nSupongamos el caso particular en el cual queremos resolver un problema de clasificación binaria. ¿Cuánto valdría \\(k\\) y cuál sería la Loss Function a utilizar?\nSupongamos que queremos transformar una Matriz \\(X\\) de 1000 registros y 10 variables. Además tenemos un vector \\(y\\) el cuál queremos predecir.\nSupongamos que queremos llevar a 32 variables, luego a 64 para luego generar nuestra predicción.\nSupongamos además que queremos usar como función de activación la función ReLU en ambas capas de transformación.\n\n\n\n\n\n\n\n\n\n¿Cómo definimos los 3 elementos principales de una red?\n\n\n\n\n\n\n\n\n\n\n\n(Hipótesis, Loss Function y Optimizador)"
  },
  {
    "objectID": "tics579/clase-4.html#nn.module",
    "href": "tics579/clase-4.html#nn.module",
    "title": "TICS-579-Deep Learning",
    "section": "nn.Module",
    "text": "nn.Module\n\n\nEn Pytorch, cada parte de una red es una clase.\n\n\n\n\n\n\n\n\nUna clase tiene la ventaja de que es un objeto mutable que puede almacenar estados en su interior. En el caso particular de una red neuronal, ¿qué estado será importante que guarde?\n\n\n\n\n\n\n\n\n\n\nUna vez que un módulo es instanciado, acepta tensores de entrada y devuelve tensores de salida.\n\n\n\n\nnn.Linear()\n\nCorresponde a la Red más básica de Pytorch y permite realizar Transformaciones Affine.\n\n\n\nfc = nn.Linear(in_features, out_features, bias=True)\n\n\nin_features es la dimensión inicial (\\(n_i\\)).\nout_features la dimensión a la que se quiere llevar (\\(n_{i+1}\\)).\n\n\n\n\n\n\n\nDe manera análoga, nn.ReLU() será el módulo que representará una función de activación ReLU.\n\n\n\n\n\n\n\n\n\n\n\nPero, ¿Cómo combinamos distintos módulos para crear una sóla arquitectura que represente nuestra Hipótesis?"
  },
  {
    "objectID": "tics579/clase-4.html#hipótesis",
    "href": "tics579/clase-4.html#hipótesis",
    "title": "TICS-579-Deep Learning",
    "section": "Hipótesis",
    "text": "Hipótesis\n\nPara poder crear una Hipótesis en Pytorch podemos combinar cada Módulo entra clase que herede desde nn.Module.\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self,):\n        pass\n    def forward(self,x):\n        pass\n\n\n\nLa red neuronal siempre debe heredar nn.Module. Esto permitirá que transformar la clase en Módulos que pueden combinarse para crear Arquitecturas cada vez más complejas.\n__init__() corresponde al constructor. Acá se deben definir todos los parámetros de entrada (similar a una función), con la que se instanciará la clase.\nforward() corresponde a la definición del *forward pass de la red en cuestión."
  },
  {
    "objectID": "tics579/clase-4.html#hipótesis-__init__",
    "href": "tics579/clase-4.html#hipótesis-__init__",
    "title": "TICS-579-Deep Learning",
    "section": "Hipótesis: __init__()",
    "text": "Hipótesis: __init__()\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self,*):\n        super().__init__()\n        self.w1 = nn.Linear(10,32)\n        self.w2 = nn.Linear(32,64)\n        self.w3 = nn.Linear(64,1)\n        self.relu_1= nn.ReLU()\n        self.relu_2= nn.ReLU()\n\n\n\nSiempre el primer elemento de una red neuronal la inicialización del nn.Module mediante el super().__init__().\nEs importante notar que todos los elementos dentro de la clase deben tener el prefijo self. Esto permite que estos elementos puedan estar disponibles en cualquier método de la clase.\nEs posible inicializar elementos mediante parámetros (representado por *) para que la red sea flexible y reutilizable. La convención es que todos los métodos tienen que tener como primer parámetro la palabra self y luego pueden tener otros parámetros."
  },
  {
    "objectID": "tics579/clase-4.html#hipótesis-forward",
    "href": "tics579/clase-4.html#hipótesis-forward",
    "title": "TICS-579-Deep Learning",
    "section": "Hipótesis: forward()",
    "text": "Hipótesis: forward()\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self,*):\n        super().__init__()\n        self.w1 = nn.Linear(10,32)\n        self.w2 = nn.Linear(32,64)\n        self.w3 = nn.Linear(64,1)\n        self.relu_1= nn.ReLU()\n        self.relu_2= nn.ReLU()\n    def forward(self,x):\n        x = self.w1(x)\n        x = self.relu_1(x)\n        x = self.w2(x)\n        x = self.relu_2(x)\n        x = self.w3(x)\n        return x\n\n\nLa método forward representa el *forward pass de la red e indica cómo están conectadas las distintas etapas de la red.\nEn este caso \\(x\\) representa una instancia/registro que va pasando por la red."
  },
  {
    "objectID": "tics579/clase-4.html#loss-function-y-optimizer",
    "href": "tics579/clase-4.html#loss-function-y-optimizer",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function y Optimizer",
    "text": "Loss Function y Optimizer\n\n\nLoss Function\n\n\nLa nomenclatura utilizada en Pytorch para referirse a la definición de la función de Pérdida es el criterion. Es decir, el criterio con el que se mide la pérdida. Más Loss Functions se pueden encontrar acá.\n\n\nOptimizador\n\n\nLa nomenclatura utilizada en Pytorch para referirse al optimizador a utilizar es optimizer. Éste se importa desde torch.optim y debe recibir como argumentos model.parameters() y al menos el learning_rate. Todos los optimizers pueden encontrarse acá.\n\n\n\n\nmodel = MyNeuralNetwork()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)"
  },
  {
    "objectID": "tics579/clase-4.html#training-loop",
    "href": "tics579/clase-4.html#training-loop",
    "title": "TICS-579-Deep Learning",
    "section": "Training Loop",
    "text": "Training Loop\n\nDefiniremos como Training Loop al proceso en el cual entrenaremos el modelo.\n\nfor e in range(EPOCHS):\n    ## Fijar el modelo en Modo Entrenamiento\n    model.train()\n\n    ## Fijar Gradientes en 0\n    optimizer.zero_grad()\n\n    ## Forward Pass\n    preds = model(X)\n\n    ## Cálculo del Loss (Ojo, primero va la predicción y luego el target). Ver Docs.\n    loss = criterion(preds, y)\n\n    ## Cálculo de Gradientes\n    loss.backward()\n\n    ## Update Rule\n    optimizer.step()\n\n\n\n\n\n\n\n\n.zero_grad() fijan los gradientes a cero, ya que Pytorch acumula gradientes siempre. Es importante que en cada epoch todos los gradientes acumulados vuelvan a cero para una siguiente optimización.\nEn el caso de querer dejar en zero los gradientes de un tensor, y no del optimizador, se puede usar .zero_()."
  },
  {
    "objectID": "tics579/clase-4.html#inferencia",
    "href": "tics579/clase-4.html#inferencia",
    "title": "TICS-579-Deep Learning",
    "section": "Inferencia",
    "text": "Inferencia\n\n\nPara generar predicciones basta con generar un Forward Pass con el modelo ya entrenado. Dependiendo del modelo, es posible que sea necesario aplicar un post-procesamiento.\n\n\n\n## Fijar el Modelo en Evaluación.\nmodel.eval()\n\n## Evita que Pytorch calcule Gradientes ya que no es necesario.\nwith torch.no_grad():\n    ## Cálculo de la salida del modelo (h)\n    h = model(X)\n\n## Cálculo de Probabilidades (si es que fuera necesario)\ny_proba = torch.sigmoid(h)\n\n## Clasificación propiamente tal\ny_preds = torch.where(y_proba&gt;=0.5, 1,0)"
  },
  {
    "objectID": "tics579/clase-4.html#mini-batching",
    "href": "tics579/clase-4.html#mini-batching",
    "title": "TICS-579-Deep Learning",
    "section": "Mini-Batching",
    "text": "Mini-Batching\n\n\n\n\n\n\n\n\nRara vez los datos vienen en formato de Tensor de Pytorch. Por lo tanto, el dataset (tablas, imágenes, videos, texto, audio, etc) debe ser llevado a formato Tensor, lo cual puede ser un proceso bastante costoso y que consume muchos recursos.\n\n\n\n\n\n\n\n\n\n\n\nAdemás, la cantidad de datos necesaria para poder entrenar un modelo de Deep Learning normalmente es alta. Lo cual limita el cierto Hardware al no contar con la capacidad necesaria.\n\n\n\n\n\nMini-Batching\n\nSe refiere a aplicar un proceso de Optimización Estocástica, con sólo una muestra de los datos. Se basa en que el gradiente de la suma de las muestras es equivalente al gradiente total.\n\n\nPara ello Pytorch introduce los conceptos de Dataset y DataLoader para implementar conversión y carga de datos on-the-fly.\n\n\nfrom torch.utils.data import Dataset, DataLoader"
  },
  {
    "objectID": "tics579/clase-4.html#mini-batching-dataset",
    "href": "tics579/clase-4.html#mini-batching-dataset",
    "title": "TICS-579-Deep Learning",
    "section": "Mini-Batching: Dataset",
    "text": "Mini-Batching: Dataset\n\nPytorch necesita crear una clase que herede de Dataset y que permita tomar elementos uno a uno y transformarlos en Tensores. Este clase debe tener al menos 3 métodos: __init__, __len__ y __getitem__.\n\n\n\n\n\n\n\nSupongamos que nuestros datos iniciales estaban en Numpy.\n\n\n\nclass MyDataSet(Dataset):\n    def __init__(self, X,y):\n        self.X = X\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self,idx):\n        features = torch.from_numpy(self.X[idx])\n        target = torch.from_numpy(self.y[idx])\n        return features, target"
  },
  {
    "objectID": "tics579/clase-4.html#model-registry",
    "href": "tics579/clase-4.html#model-registry",
    "title": "TICS-579-Deep Learning",
    "section": "Model Registry",
    "text": "Model Registry\n\nCada vez que nosotros llamamos un objeto modelo (que herede de nn.Module) este modelo mostrará el model registry. El registry permitirá ver todos los elementos que son parte del modelo. Para que un elemento sea parte del registro, debe haber sido definido como self.----.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi es que se define un elemento como self.--- debe definirse como un nn.Module y no como un F.---\nAdemás se puede acceder a cualquier elemento/atributo mediante el comando model.atributo."
  },
  {
    "objectID": "tics579/clase-4.html#model-registry-1",
    "href": "tics579/clase-4.html#model-registry-1",
    "title": "TICS-579-Deep Learning",
    "section": "Model Registry",
    "text": "Model Registry\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs posible acceder a los datos de Parámetros y Bias de una capa linear utilizando:\n\n\n\n\nmodel.w1.weights.data\nmodel.w1.bias.data\n\n\n\n\n\n\nIdea:\n\n\n\nPodría utilizarse esto para poder definir valores iniciales de capas de parámetros y de bias.\n\n\n\n\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self, *):\n        self.w1 = ...\n        self.relu = ...\n        self.model.w1.weights.data = tensor([...])\n        self.model.w1.bias.data = tensor([...])\n    \n    def forward(self,x):\n        ..."
  },
  {
    "objectID": "tics579/clase-4.html#mini-batching-dataloader",
    "href": "tics579/clase-4.html#mini-batching-dataloader",
    "title": "TICS-579-Deep Learning",
    "section": "Mini-Batching: Dataloader",
    "text": "Mini-Batching: Dataloader\n\n\nEl Dataloader permitirá ir cargando los datos en memoria en un cierto batch_size. La idea es no generar cuellos de botella por falta de memoria disponible.\n\n\ndata = MyDataset(X,y)\ntrain_loader = DataLoader(data, batch_size=32, pin_memory=True,num_workers=12, shuffle=True)\n\n\n\n\n\n\n\nEsto implica que nuestro Training Loop deberá sufrir ciertas modificaciones para ir actualizandose por Batch y no sólo por Epoch.\n\n\n\n\nfor e in range(EPOCHS):\n    train_loss = []\n\n    model.train()\n    for batch in train_loader:\n        X, y = batch\n        optimizer.zero_grad()\n        preds = model(X)\n        loss = criterion(preds, y)\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n    print(f\"Loss para Epoch {e}: {np.mean(train_loss)}\")"
  },
  {
    "objectID": "tics579/clase-3.html#feed-forward-networks-1",
    "href": "tics579/clase-3.html#feed-forward-networks-1",
    "title": "TICS-579-Deep Learning",
    "section": "Feed Forward Networks",
    "text": "Feed Forward Networks\n\nTeorema de aproximación Universal\n\n\nSe dice que una Red Neuronal puede aproximar cualquier función en una región cerrada.\n\n\n\nEs decir,\n\\[\\underset{x \\in \\mathbb{D}}{max}|f(x) - \\hat{f(x)}| \\le \\epsilon\\]\ncon \\(D \\subset \\mathbb{R}\\) y \\(\\epsilon &gt;0\\).\n\n\n\n\n\n\n\nExisten muchas otras funciones como Splines, KNN y otras que también tienen esta propiedad.\nSi se escogen puntos suficientemente cercanos, cumplir esta propiedad es trivial."
  },
  {
    "objectID": "tics579/clase-3.html#feed-forward-networks-ffn",
    "href": "tics579/clase-3.html#feed-forward-networks-ffn",
    "title": "TICS-579-Deep Learning",
    "section": "Feed Forward Networks (FFN)",
    "text": "Feed Forward Networks (FFN)\n\nEs un tipo de Arquitectura caracterizada por Nodos en un nivel que se conectan con todos los nodos del siguiente nivel. Este es probablemente el tipo de Arquitectura de Red Neuronal más común.\n\n\n\n\n\n\n\nEste tipo de Redes tiene distintos nombres que son usados de manera intercambiable:\n\nCapas Lineales: Probablemente por su denominación en Pytorch.\nCapas/Redes Densas: Probablemente por su denominación en Tensorflow.\nMultilayer Perceptron: O también conocido como MLP, debido a que es la generalización del Perceptrón, la primera propuesta de Redes Neuronales de Rosenblatt en 1958.\n\n\n\n\n\n\n\n\n\n\n\n\nComunmente\n\nCada suma-producto de parámetros e inputs corresponde a una capa. Nosotros llamamos capas de parámetros a cada grupo de conexiones.\nCada nodo corresponde a una Neurona. Nosotros consideramos que eso es la nueva dimensión a la que se mueve nuestro vector de entrada.\n\nSPOILER: El número de nodos/neuronas no es importante, nos importan más los parámetros necesarios.\n\nLa capa de salida corresponde a nuestra Hipótesis."
  },
  {
    "objectID": "tics579/clase-3.html#ffn-formalmente",
    "href": "tics579/clase-3.html#ffn-formalmente",
    "title": "TICS-579-Deep Learning",
    "section": "FFN: Formalmente",
    "text": "FFN: Formalmente\n\n\n\\[ Z_1 = X\\] \\[Z_{i +1} = \\sigma_i(Z_i W_i + b_i)\\] \\[h_\\theta(X) = Z_{L + 1}\\]\ncon \\(\\theta = \\{W_{1:L}, b_{1:L}\\}\\)\npara \\(i=1,...,L\\)\n\n\n\n\n\n\n\n\\(Z_{i}\\) corresponde a la salida de la capa \\(i\\), \\(W_i\\) corresponde al conjunto de parámetros de la capa \\(i\\), y \\(b_i\\) corresponde al bias de la capa \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\\(b_i\\) es otro set de parámetros, llamado bias (el cual se traduce como sesgo, pero lo vamos a mantener en inglés para evitar confusiones semánticas).\nAl agregar este componente, ya no se tiene una Transformación Lineal, sino que una Transformación Affine."
  },
  {
    "objectID": "tics579/clase-3.html#ffn---broadcasting",
    "href": "tics579/clase-3.html#ffn---broadcasting",
    "title": "TICS-579-Deep Learning",
    "section": "FFN - Broadcasting",
    "text": "FFN - Broadcasting\n\\[Z_{i + 1} = \\sigma_i(Z_i W_i + b_i^T)\\]\nSi chequeamos las dimensiones:\n\n\n\n\\(Z_i \\in \\mathbb{R}^{m \\times n_i}\\)\n\n\n\n\\(W_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}\\)\n\n\n\nPero, \\(b_i \\in \\mathbb{R}^{n_i+1}\\)\n\n\n\n\n\n\n\n\n\nTenemos un problema y es que esto hace que las dimensiones no calcen. Esto sería una operación no válida en términos matriciales. Sin embargo es posible realizarla aplicando Broadcasting.\n\n\n\n\n\n\n\n\n\n\n\nBroadcasting\n\n\nCorresponde a una replica de una dimensión de manera de permitir alguna operación que requiera que ciertas dimensiones calcen.\n\n\n\n\n\n\n\n\n\n\n\nBroadcasting Rules\n\n\n\nCada tensor debe tener al menos una dimensión.\nMoviéndose de derecha a izquierda por cada dimensión una vez alineadas a la derecha, las dimensiones deben:\n\nSer iguales,\niguales a 1,\no no debe existir."
  },
  {
    "objectID": "tics579/clase-3.html#ffn---broadcasting-1",
    "href": "tics579/clase-3.html#ffn---broadcasting-1",
    "title": "TICS-579-Deep Learning",
    "section": "FFN - Broadcasting",
    "text": "FFN - Broadcasting\n\nMatemáticamente el Broadcasting en este caso corresponde a:\n\n\\[ b_i^T = 1 b_i^T\\]\ndonde 1, es un vector de unos de \\(m \\times 1\\) y \\(b_i\\) es de dimensión \\(n_{i+1} \\times 1\\), al cuál se está aplicando el producto externo.\n\n\n\n\n\n\nEl Broadcasting permitirá que \\(b_i\\) tenga ahora dimensiones \\(m \\times n_{i+1}\\), lo cuál permitirá que la operación de suma se pueda realizar.\n\n\n\n\n\n\n\n\n\nEl Broadcasting evita que se tenga que almacenar información repetida, lo cual permite que las implementaciones sean más eficientes en términos de memoria. Siempre que se pueda se debe utilizar Broadcasting para simplificar un cálculo.\n\n\n\nMás info ver: Numpy Docs"
  },
  {
    "objectID": "tics579/clase-3.html#hiperparámetros-de-una-red-neuronal",
    "href": "tics579/clase-3.html#hiperparámetros-de-una-red-neuronal",
    "title": "TICS-579-Deep Learning",
    "section": "Hiperparámetros de una Red Neuronal",
    "text": "Hiperparámetros de una Red Neuronal\n\nHiperparámetros\n\n\nValores necesarios para el cómputo de una red neuronal que deben ser determinados por el modelador. Estos valores NO pueden ser aprendidos de manera autónoma por la red neuronal.\n\n\n\n\n\n\n\n\n\n\n\nLearning Rate (Karpathy Constant: 3e-4)\n¿Cuáles son las dimensiones de los Pesos (Weights) y de los sesgos (Biases)? (De qué tamaño es cada capa)\n¿Qué funciones de Activación se utilizarán?\n¿Qué funciones de perdida se utilizarán?\n¿Qué optimizadores se utilizarán?\n¿Cómo se inicializarán los parámetros de la Red Neuronal?\n¿Cuánto tiempo entrenanaremos nuestro modelo? ¿Cómo sabemos si es que convergió o no?"
  },
  {
    "objectID": "tics579/clase-3.html#tipos-de-hipótesis",
    "href": "tics579/clase-3.html#tipos-de-hipótesis",
    "title": "TICS-579-Deep Learning",
    "section": "Tipos de Hipótesis",
    "text": "Tipos de Hipótesis\n\nEn el aprendizaje supervisado contamos principalmente con la resolución de dos tipos de Problemas: Clasificación y Regresión. Dependiendo del tipo de Problema armaremos nuestra hipótesis.\n\n\\[h_\\theta(X) = Z_{L+1} \\in \\mathbb{R}^{m \\times k}\\]\n\n\n\nClasificación\n\nClasificación Binaria: Se requiere un \\(k=1\\). Se usa una Función Sigmoide para transformar el Output en la probabilidad de que ocurra la clase positiva.\nClasificación Multiclase: Se requiere un \\(k=C\\), donde C es el número de clases a clasificar. Se usa una función Softmax para transformar el output en una distribución de probabilidades.\nClasificación Multilabel: Se requiere un \\(k=C\\) donde C es el número de clases a clasificar. Se usa una función Sigmoide para transformar cada clase en probabilidades.\n\n\nRegresión\n\nRegresión Simple: Se requiere un \\(k=1\\). No requiere de funciones adicionales.\nRegresión Multiple: Se requiere un \\(k=V\\) con V el número de valores a predecir.\n\n\n\n\n\n\n\nAdicionalmente se pueden utilizar funciones como la sigmoide o ReLU para forzar salidas entre 0 y 1 o entre 0 e \\(\\infty\\) respectivamente.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente las funciones necesarias en la capa de salida van embebidas en la Loss Function. Normalmente estas funciones sí deben aplicarse al momento de la Predicción del modelo."
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación",
    "href": "tics579/clase-3.html#funciones-de-activación",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\nActivation Functions\n\nCorresponden a las funciones que agregarán características no lineales a nuestra hipótesis, impidiendo la composición de transformaciones lineales (o Affine).\n\n\n\n\n\n\n\n\nComo convención, las funciones de activación sólo se aplicarán a las Hidden Layers. Es decir \\(\\sigma_{L+1}(x) = x\\).\n\n\n\n\n\n\n\n\n\nOtras convenciones utilizan funciones de activación para la capa de salida. Esto bajo el abánico de Pytorch no es correcto ya que la Activación de la última capa esta embebida en la Loss Function (Recordar como Softmax es parte del Cross Entropy).\nAhora, sí es posible utilizar funciones de Activación a la salida de una predicción, pero dichas funciones tienen otro propósito y no son del todo estrictamente necesarias.\n\n\n\n\n\n\n\n\n\n¿Puedo aplicar distintas Funciones de Activación a cada Neurona?\n\n\n\n\n\n\n\n\n\nPara ver más Activation Functions y detalles de su funcionamiento, ir directamente a la Documentación de Pytorch."
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-1",
    "href": "tics579/clase-3.html#funciones-de-activación-1",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\n\n\n\n\nDerivadas:\n\n\n\nSigmoide: \\(g'(z) = g(z)(1 - g(z))\\)\n\n\n\nTanh: \\(g'(z) = 1 - g^2(z)\\)\n\n\n\nReLU: \\(g'(z) =\n\\begin{cases}\n0,  & \\text{if $z \\le$ 0} \\\\[2ex]\n1, & \\text{if $z &gt; 0$}\n\\end{cases}\\)"
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-modernas",
    "href": "tics579/clase-3.html#funciones-de-activación-modernas",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación Modernas",
    "text": "Funciones de Activación Modernas\n\n\nLeaky ReLU\n\n\n\n\n\n\\[g(z) = max(0.1z, z)\\]\n\nParametrized ReLU (PReLU)\n\n\n\n\n\n\\[g(z) = max(az, z)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-2",
    "href": "tics579/clase-3.html#funciones-de-activación-2",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\n\nELU\n \\(g(z) =\n\\begin{cases}\nz,  & \\text{if $z \\ge$ 0} \\\\[2ex]\n\\alpha(e^{z}-1), & \\text{if $z &lt; 0$}\n\\end{cases}\\)\n\nGELU\n \\[\\begin{align} g(z) &= z \\cdot \\Phi(z) \\\\\ng(z)&= 0.5 \\cdot z \\cdot \\left(1 + Tanh\\left(\\sqrt{2/\\pi}\\right) \\cdot \\left(z + 0.044715 \\cdot z^3\\right)\\right)\\end{align}\\]"
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-3",
    "href": "tics579/clase-3.html#funciones-de-activación-3",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\n\nSELU\n \\[ g(z) = scale \\cdot (max(0,z) + min(0,\\alpha(e^z - 1)))\\]\ncon \\(\\alpha=1.6732632423543772848170429916717\\) y \\(scale = 1.0507009873554804934193349852946\\)\n\nSwish\n \\[g(z) = z \\cdot sigmoid(z)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#loss-functions",
    "href": "tics579/clase-3.html#loss-functions",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nAl igual que el caso de la Hipótesis, la Loss Function dependerá del tipo de problema a resolver. Existen muchas Loss Functions, pero los más comunes para problemas generales son las siguientes:\n\nClasificación Binaria: Binary Cross Entropy\n\\[BCE_i = - \\left[y_i \\cdot log(h(x_i)) + (1-y_i) log(1-h(x_i))\\right]\\]\ndonde \\(h(x)\\) corresponde a un valor de probabilidad de la clase positiva (debe ir entre 0 y 1).\n\n\n\n\n\n\nEn Pytorch se suele utilizar BCEWithLogitsLoss ya que aplica una función Sigmoide a la capa de salida además de ser una clase numericamente más estable. Esto garantiza que la salida de la Red tiene valores entre 0 y 1 como se necesita.\n\n\n\nClasificación Multiclase: CrossEntropy\n\\[CE_i = -log \\left(\\frac{exp(h_{(i=y)}(x_i))}{\\sum_{j=1}^k exp(h_j(x_i))}\\right)\\]\n\n\n\n\n\n\nEn Pytorch se suele utilizar CrossEntropyLoss ya que combina aplica una función Softmax a la capa de salida además de ser una clase numericamente más estable."
  },
  {
    "objectID": "tics579/clase-3.html#loss-functions-1",
    "href": "tics579/clase-3.html#loss-functions-1",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Functions",
    "text": "Loss Functions\nClasificación Multilabel: CrossEntropy\nPara este tipo de problema se debería aplicar un Negative LogLoss combinado con la salidas de una red que van entre 0 y 1 (es decir, que se aplica una Sigmoide)\n\n\n\n\n\n\nEn Pytorch se suele utilizar BCEWithLogitsLoss ya que combina aplica una función Softmax a la capa de salida y permite resultados de más de una dimensión.\n\n\n\nRegresión\n\n\n\nMean Absolute Error o L1Loss\n\n\\[L1_i = |y_i - h(x_i)|\\]\n\n\nMean Squared Error Loss o L2Loss\n\n\\[L2_i = (y_i - h(x_i))^2\\]\n\n\n\n\n\n\n\nEs importante recordar que en general se debe calcular un valor agregado de la Loss Function. En Pytorch a esto se le llama reduction. Donde el más utilizado es reduction=\"mean\". Es decir,\n\\[l = \\frac{1}{m}\\sum_{i=1}^m L_i\\]"
  },
  {
    "objectID": "tics579/clase-3.html#optimizers",
    "href": "tics579/clase-3.html#optimizers",
    "title": "TICS-579-Deep Learning",
    "section": "Optimizers",
    "text": "Optimizers\n\nGradient Descent corresponde al algoritmo de Optimización más popular, pero no necesariamente el más eficiente. Distintas variantes han ido apareciendo para ir mejorando eventuales deficiencias de la proposición inicial.\n\nNormal Gradient Descent\n\\[\\theta := \\theta - \\frac{\\alpha}{m}\\nabla_\\theta l(h_\\theta(X), y), \\text{donde $X \\in \\mathbb{R}^{m \\times n}$ e $y \\in \\mathbb{R}^{m \\times 1}$}\\]\n\n\n\n\n\n\nLa dirección del Gradiente utilizando menos puntos debería ser más o menos similar. Sin duda, más ruidoso, pero a la larga debería dirigir en casi la misma dirección. Por lo que podríamos hacer actualizaciones de parámetros utilizando B datos con B &lt;&lt; m.\n\n\n\n\n\n\n\n\n\nEsto entrega como beneficio, menos requerimientos de memoria, ya que operarían matrices más pequeñas, por lo tanto, requiere de menos RAM tanto en CPU como en GPU.\n\n\n\nStochastic Gradient Descent (MiniBatch)\n\n\n\\[\\theta := \\theta - \\frac{\\alpha}{B}\\nabla_\\theta l(h_\\theta(X), y), \\text{donde $X \\in \\mathbb{R}^{B \\times n}$ e $y \\in \\mathbb{R}^{B \\times 1}$}\\]\n\n\n\n\n\n\n\nSe van tomando \\(B\\) muestras de manera incremental hasta utilizar la totalidad de datos de entrenamiento"
  },
  {
    "objectID": "tics579/clase-3.html#sgd-with-momentum",
    "href": "tics579/clase-3.html#sgd-with-momentum",
    "title": "TICS-579-Deep Learning",
    "section": "SGD with Momentum",
    "text": "SGD with Momentum\nUpdate Rule\n\\[u_{t + 1} = \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t)\\] \\[\\theta_{t+1} = \\theta_t - \\alpha u_{t + 1}\\]\ndonde \\(0&lt;\\beta&lt;1\\), pero normalmente \\(\\beta=0.9\\).\n\n\n\n\n\n\nEste cálculo se denomina un Exponential Moving Average de los Gradientes.\n\n\n\n\\[\\begin{align} u_{t+1}&=(1-\\beta)\\nabla_\\theta f(\\theta_{t}) + \\beta u_t \\\\\nu_{t+1}&=(1-\\beta)\\nabla_\\theta f(\\theta_{t}) + \\beta \\left[(1-\\beta) \\nabla_\\theta f(\\theta_{t-1}) + \\beta u_{t-1}\\right] \\\\\nu_{t+1}&=(1-\\beta)\\nabla_\\theta f(\\theta_{t}) + \\beta (1-\\beta) \\nabla_\\theta f(\\theta_{t-1}) + \\beta^2 (1-\\beta) \\nabla_\\theta f(\\theta_{t-2})... \\\\\n\\end{align}\\]\n\n\n\n\n\n\nLa componente de momento, está tomando en consideración todos los otros Gradientes en pasos anteriores para escoger correctamente la dirección del Gradiente actual."
  },
  {
    "objectID": "tics579/clase-3.html#sgd-with-nesterov-momentum",
    "href": "tics579/clase-3.html#sgd-with-nesterov-momentum",
    "title": "TICS-579-Deep Learning",
    "section": "SGD with Nesterov Momentum",
    "text": "SGD with Nesterov Momentum\n\\[u_{t + 1} = \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t - \\alpha u_t)\\] \\[\\theta_{t+1} = \\theta_t - \\alpha u_{t + 1}\\]\n\n\n\n\n\n\nNotar que la lógica es casi la misma, sólo que el Gradiente se evalúa en un punto futuro. Es decir, \\(\\theta_t-\\alpha u_t\\) corresponde al punto siguiente utilizando SGD con Momentum.\n\n\n\n\n\nMomentum\n\n\n\n\n\n\nNesterov"
  },
  {
    "objectID": "tics579/clase-3.html#métodos-adaptativos-adagrad",
    "href": "tics579/clase-3.html#métodos-adaptativos-adagrad",
    "title": "TICS-579-Deep Learning",
    "section": "Métodos Adaptativos: Adagrad",
    "text": "Métodos Adaptativos: Adagrad\n\n¿Qué tal, si la tasa de aprendizaje se va adaptando en el tiempo y deja de ser estática?\n\n\n\n\n\n\n\nIdea\n\n\n\nNormalizar por la historia de los gradientes al cuadrado.\n\n\n\n\n\\[r_{t+1} = r_t + \\nabla_\\theta f(\\theta_t)^2\\] \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{r_{t+1}}}\\nabla_\\theta f(\\theta_t)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#métodos-adaptativos-rmsprop",
    "href": "tics579/clase-3.html#métodos-adaptativos-rmsprop",
    "title": "TICS-579-Deep Learning",
    "section": "Métodos Adaptativos: RMSProp",
    "text": "Métodos Adaptativos: RMSProp\n\n\n\n\n\n\nIdea\n\n\n\nNormalizar por el Exponential Moving Average de los Gradientes al cuadrado.\n\n\n\n\n\\[s_{t+1} = \\beta r_t + (1-\\beta) \\nabla_\\theta f(\\theta_t)^2\\] \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{s_{t+1}}}\\nabla_\\theta f(\\theta_t)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#métodos-adaptativos-adam",
    "href": "tics579/clase-3.html#métodos-adaptativos-adam",
    "title": "TICS-579-Deep Learning",
    "section": "Métodos Adaptativos: Adam",
    "text": "Métodos Adaptativos: Adam\n\n\n\n\n\n\nIdea\n\n\n\nCombinar Descenso con Momentum y RMSProp.\n\n\n\n\n\n\n\\[v_{t+1} = \\beta_1 v_t + (1-\\beta_1) \\nabla_\\theta f(\\theta_t)\\] \\[s_{t+1} = \\beta_2 s_t + (1-\\beta_2) \\nabla_\\theta f(\\theta_t)^2\\] \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{s'_{t+1}}} v'_{t+1}\\]\n\nCorrecciones Iniciales\n\\[v'_{t+1} = \\frac{v_{t+1}}{1-\\beta_1^{t+1}}\\] \\[s'_{t+1} = \\frac{v_{t+1}}{1-\\beta_2^{t+1}}\\]\n\n\n\n\n\n\n\nPytorch utiliza 0.9 y 0.999 como valores de \\(\\beta_1\\) y \\(\\beta_2\\) respectivamente."
  },
  {
    "objectID": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales",
    "href": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales",
    "title": "TICS-579-Deep Learning",
    "section": "El nacimiento de las Redes Neuronales",
    "text": "El nacimiento de las Redes Neuronales\n\nLas redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad más básica es una Neurona."
  },
  {
    "objectID": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales-1",
    "href": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales-1",
    "title": "TICS-579-Deep Learning",
    "section": "El nacimiento de las Redes Neuronales",
    "text": "El nacimiento de las Redes Neuronales\n\nLas redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad más básica es una Neurona.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste tipo de nomenclatura está sumamente pasada de moda.\n\n\n\n\n\nEste cálculo se puede representar como:\n\n\\[ y = \\phi(w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_5 \\cdot x_5)\\] \\[ y = \\phi(w^T \\cdot x)\\]\ndonde \\(w = [w_1, w_2, w_3, w_4, w_5]\\) y \\(x = [x_1, x_2, x_3, x_4, x_5]\\).\n\n\n\n\n\n\n\n\n¿Qué pasa si \\(\\phi(.)\\) vale la función identidad?\nTenemos una Regresión Lineal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué pasa si \\(\\phi(.)\\) vale la función sigmoide?\nTenemos una Regresión Logística."
  },
  {
    "objectID": "tics579/clase-1.html#arquitectura-de-una-red",
    "href": "tics579/clase-1.html#arquitectura-de-una-red",
    "title": "TICS-579-Deep Learning",
    "section": "Arquitectura de una Red",
    "text": "Arquitectura de una Red\n\n\n\n\n\n\n\n\nEstructura más común\n(Probablemente tampoco seguiremos esta nomenclatura)\n\nNodos o Neuronas\nEdges o Conexiones\nCapas\n\n\n\n\n\n\n\n\n¿Cuántas capas tiene esta red?\n\n\n\n\n\n\n\n\n\n\n\nDepende\n\n\n\n\n\n\nNormalmente todas las neuronas de una capa anterior se conectan con las de una capa posterior (Hay excepciones).\nDependiendo de la forma en la que se conecten, cada Arquitectura recibe un nombre."
  },
  {
    "objectID": "tics579/clase-1.html#los-ingredientes-de-un-algoritmo-de-aprendizaje",
    "href": "tics579/clase-1.html#los-ingredientes-de-un-algoritmo-de-aprendizaje",
    "title": "TICS-579-Deep Learning",
    "section": "Los Ingredientes de un Algoritmo de Aprendizaje",
    "text": "Los Ingredientes de un Algoritmo de Aprendizaje\n\nHipótesis\n\n\nUna función que describe como mapear inputs (features) con outputs (labels) por medio de parámetros.\n\n\nLoss Function\n\n\nUna función que especifica cuanta información se pierde. Mayor pérdida implica más error de estimación.\n\n\nMétodo de Optimización\n\n\nEs el responsable de combinar la hipótesis y la loss function. Corresponde a un procedimiento para determinar los parámetros de la hipótesis, minimizando la suma de las pérdidas en un set de entrenamiento."
  },
  {
    "objectID": "tics579/clase-1.html#ejemplo-softmax-regression",
    "href": "tics579/clase-1.html#ejemplo-softmax-regression",
    "title": "TICS-579-Deep Learning",
    "section": "Ejemplo: Softmax Regression",
    "text": "Ejemplo: Softmax Regression\n\nSoftmax Regression\n\n\nCorresponde la versión multiclase de una Regresión Logística. También se le llama una Shallow Network.\n\n\n\n\n\n\n\n\n\n\n\nConsideremos un problema de clasificación multiclase de \\(k\\) clases tal que:\n\n\n\nDatos de Entrenamiento: \\(x^{(i)}, y^{(i)} \\in {1,...,k}\\) para \\(i=1,...,m\\).\n\n\\(n\\): Es el número de Features.\n\\(m\\): Es el número de puntos en el training set.\n\\(k\\): Es el número de clases del problema.\n\n\n\n\n\n\n\n\n\n\n\nVamos a tener en total \\(n \\times k\\) parámetros o pesos que actualizar."
  },
  {
    "objectID": "tics579/clase-1.html#softmax-regression-hipótesis",
    "href": "tics579/clase-1.html#softmax-regression-hipótesis",
    "title": "TICS-579-Deep Learning",
    "section": "Softmax Regression: Hipótesis",
    "text": "Softmax Regression: Hipótesis\n\nVamos a definir una función que mapea valores de \\(x \\in \\mathbb{R}\\) a vectores de \\(k\\) dimensiones.\n\n\\[ h: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\\] \\[ x \\rightarrow h_\\theta(x) = \\theta^T x\\]\n\ndonde \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(x \\in \\mathbb{R}^{n\\times 1}\\)\n\n\n\n\n\n\n\nEn este caso usamos una hipótesis lineal, ya que se usa una multiplicación matricial (o producto punto) para relacionar \\(\\theta\\) y \\(x\\).\n\n\n\n\n\n\n\n\n\nEn este caso el output de \\(h_i(x)\\) devolverá la probabilidad de pertenecer a una cierta clase \\(i\\).\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el tamaño/dimensión de \\(h_\\theta(x)\\)?"
  },
  {
    "objectID": "tics579/clase-1.html#notación-matricial",
    "href": "tics579/clase-1.html#notación-matricial",
    "title": "TICS-579-Deep Learning",
    "section": "Notación Matricial",
    "text": "Notación Matricial\n\nUna manera más conveniente de escribir estas operaciones es utilizar (Matrix Batch Form).\n\n\n\nDesign Matrix\n\\[X \\in \\mathbb{R}^{m \\times n} = \\begin{bmatrix}\n&-x^{(1)T}-\\\\\n& \\vdots & \\\\\n&-x^{(m)T}- &\\\\\n\\end{bmatrix}\\]\n\nLabels Vector\n\\[y \\in {1,...,k} = \\begin{bmatrix}\n&-y^{(1)}-\\\\\n& \\vdots & \\\\\n&-y^{(m)}- &\\\\\n\\end{bmatrix}\\]\n\nLa hipótesis también se puede reescribir de manera matricial como:\n\n\n\\[h_\\theta(X) = \\begin{bmatrix}\n&-h_\\theta(x^{(1)})^T-\\\\\n& \\vdots & \\\\\n&-h_\\theta(x^{(m)})^T-\\\\\n\\end{bmatrix}\\]\n\n\\[h_\\theta(X)= \\begin{bmatrix}\n&-x^{(1)T} \\theta-\\\\\n& \\vdots & \\\\\n&-x^{(m)T} \\theta-\\\\\n\\end{bmatrix} = X  \\theta\\]\n\n\n\n\n\n\n\n\nNormalmente este tipo de operaciones son las que utilizaremos para hacer nuestro código."
  },
  {
    "objectID": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss",
    "href": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function: Softmax/Cross-Entropy Loss",
    "text": "Loss Function: Softmax/Cross-Entropy Loss\n\n\n\n\n\n\n\nLa salida de nuestra Shallow Network retornará valores reales.\n\n\n\n\n\n\n\n\n\n\n\nPara poder tener una mejor interpretación del significado de cada una aplicaremos la función Softmax lo cual permitirá normalizar los resultados y llevará los resultados a una “distribución de probabilidad” (valores positivos que sumen 1).\n\n\n\n\n\n\n\n\n\n\n\n\nFormalmente definiremos la función Softmax como:\n\\[s_i = p(label = i) = \\frac{exp(h_i(x))}{\\sum_{j=1}^k exp(h_j(x))}\\]\n\\[s = \\begin{bmatrix}\n&s_1&\\\\\n& \\vdots & \\\\\n&s_k&\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss-1",
    "href": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss-1",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function: Softmax/Cross-Entropy Loss",
    "text": "Loss Function: Softmax/Cross-Entropy Loss\nPara medir el error/pérdida de información utilizaremos el Negative Log Loss o Cross Entropy Loss.\n\\[l_{ce}(h(x), y) = -log\\left(p(label = y)\\right)\\]\n\n\n\n\n\n\n\nPara garantizar el éxito de nuestro modelo, básicamente queremos maximizar la probabilidad de encontrar la etiqueta correcta, es decir, que \\(p(label = y)\\) sea lo más alto posible.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente en los problemas de optimización no se suele maximizar sino minimizar. Minimizar el valor negativo es equivalente a maximizar. Esto sería equivalente a minimizar el error del modelo.\n\n\n\n\n\n\n\n\n\n\n\nFinalmente por razones de estabilidad numérica, minimizamos el logaritmo de la probabilidad que es una técnica bien conocida en Estadística.\n\n\n\n\n\n\\[\\begin{align}\nl_{ce}(h(x), y) = -log\\left(p(label = y)\\right) &= -log \\left(\\frac{exp(h_{(i = y)}(x))}{\\sum_{j=1}^k exp(h_j(x))}\\right) \\\\\n&= - h_{(i=y)}(x) + log\\left(\\sum_{j = 1}^k exp(h_j(x))\\right)\\end{align}\\]"
  },
  {
    "objectID": "tics579/clase-1.html#método-de-optimización",
    "href": "tics579/clase-1.html#método-de-optimización",
    "title": "TICS-579-Deep Learning",
    "section": "Método de Optimización",
    "text": "Método de Optimización\n\nEl último ingrediente de un algoritmo de aprendizaje es el método de optimización. Es necesario minimizar la pérdida promedio asociada a todos los puntos de un cierto set de entrenamiento. Para ello definimos esto formalmente como:\n\n\\[\\underset{\\theta}{minimize} = \\frac{1}{m} \\sum_{i=1}^m l_{ce}(h_\\theta(x^{(i)}), y^{(i)})\\]\n\n\n\n\n\n\n¿Cómo encontramos los parámetros \\(\\theta\\) que minimizan la pérdida de información/error de estimación?\n\n\n\n\nGradient Descent\n\n\nEs un método numérico que permite minimizar funciones moviéndose en dirección contraria al Gradiente. Es computacionalmente muy eficiente y fácil de implementar en código."
  },
  {
    "objectID": "tics579/clase-1.html#gradient-descent",
    "href": "tics579/clase-1.html#gradient-descent",
    "title": "TICS-579-Deep Learning",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\nSe define el gradiente como la matriz que contiene las derivadas parciales de una función \\(f\\). Se denota como:\n\\[\\nabla_\\theta f(\\theta) \\in \\mathbb{R}^{n \\times k} =  \\begin{bmatrix}\n\\frac{\\partial f(\\theta)}{\\partial \\theta_{11}} & \\cdots & \\frac{\\partial f(\\theta)}{\\partial \\theta_{1k}} \\\\\n\\cdots & \\ddots & \\cdots \\\\\n\\frac{\\partial f(\\theta)}{\\partial \\theta_{n1}} & \\cdots & \\frac{\\partial f(\\theta)}{\\partial \\theta_{nk}}\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\\(\\theta_{ij}\\) corresponde al parámetro que une el nodo/feature \\(i\\) con el nodo/predicción \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl gradiente apunta a la dirección de máximo crecimiento de la función \\(f\\)."
  },
  {
    "objectID": "tics579/clase-1.html#gradient-descent-regla-de-actualización",
    "href": "tics579/clase-1.html#gradient-descent-regla-de-actualización",
    "title": "TICS-579-Deep Learning",
    "section": "Gradient Descent: Regla de Actualización",
    "text": "Gradient Descent: Regla de Actualización\nPara minimizar la función, la idea es descender iterativamente por el trayecto en contra del gradiente. La regla de actualización se define como:\n\\[\\theta := \\theta - \\alpha \\nabla_\\theta f(\\theta) = \\theta - \\frac{\\alpha}{m}\\nabla_\\theta l_{ce}(X\\theta,y)\\]\ncon \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(\\alpha &gt; 0\\) corresponde al step size o learning rate.\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso \\(f\\) corresponderá a nuestro \\(l_{ce}\\) calculado anteriormente. El problema es, ¿cuánto vale el gradiente del Cross Entropy Loss?"
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\nSimplifiquemos el problema a calcular para un sólo vector \\(x\\).\n\\[\\theta := \\theta - \\alpha \\nabla_\\theta l_{ce}(\\theta^Tx,y) \\]\n\n\n\n\n\n\n\n\n¿Cuánto vale el Gradiente?\n\nNo es tan sencillo, ya que derivamos respecto a \\(\\theta\\) que es una matriz.\nPero derivamos a \\(\\theta^T x\\) que es un vector.\nPara ello, lo correcto es utilizar Calculo Diferencial Matricial, Jacobianos y Productos de Kroenecker (que probablemente no han visto en ningún curso).\n\nSPOILER: Yo tampoco lo he visto en ningún curso.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsaremos un truco (sumamente hacky 😱) que jamás deben revelar y que avergonzaría a cualquier profesor de Cálculo.\n\nPretenderemos que todos los valores son escalares y corregiremos las dimensiones al final."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-1",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-1",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\nSimplifiquemos el problema pensando que calcularemos el Gradiente para un sólo vector \\(x\\).\n\n\nEs decir, \\(x \\in \\mathbb{R}^{n\\times1}\\).\n\nAdemás sabemos que \\(\\nabla_\\theta l_{ce}(\\theta^Tx, y)\\) debe tener dimensiones \\(n \\times k\\).\n\n\n\n\n\n\n\n¿Por qué?\n\n\n\n\n\n\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = \\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^Tx}{\\partial \\theta}\\]\n\n\\[\\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} = \\frac{\\partial l_{ce}(h_\\theta(x), y)}{\\partial h_\\theta(x)} = \\begin{bmatrix}\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_1} \\\\\n\\vdots\\\\\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_k} \\\\\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLuego el gradiente de \\(l_{ce}\\) respecto a \\(h\\) tiene dimensiones \\(k \\times 1\\)."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-2",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-2",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\\[\\begin{align}\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_i} &= \\frac{\\partial }{\\partial h_i}\\left(-h_{(i = y)} + log \\sum_{j = 1}^k exp(h_j)\\right) \\\\\n&= -\\frac{\\partial h_{(i = y)}}{\\partial h_i}+ \\frac{1}{\\sum_{j = 1}^k exp(h_j)} \\cdot \\frac{\\partial}{\\partial h_i}\\left(\\sum_{j=1}^k exp(h_j)\\right) \\\\\n&= -\\frac{\\partial h_{(i = y)}}{\\partial h_i}+ \\frac{exp(h_i)}{\\sum_{j = 1}^k exp(h_j)} \\\\\n&= - 1\\{i=y\\} + s_i = s_i - 1\\{i=y\\}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\\[1\\{i = y\\} = \\begin{cases}\n1,  & \\text{i = y} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\nFinalmente en forma vectorial quedaría como:\n\n\n\\[\\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} = s - e_y\\]\n\n\n\n\n\n\n\nDonde \\(z\\), es el vector de Softmax y \\(e_y\\) es un vector con un 1 en la posición \\(y\\) y 0 en el resto."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-3",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-3",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\n\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = \\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^Tx}{\\partial \\theta}\\] \\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = (s-e_y)\\cdot x \\]\n\n\n\n\n\n\n\nOjo con las dimensiones\n\n\n\n\\(s-e_y \\in \\mathbb{R}^{k \\times 1}\\)\n\\(x \\in \\mathbb{R}^{n \\times 1}\\)\n\n\n\n\n\n\nLuego:\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = x (s-e_y)^T\\]\n\n\n\n\n\n\n\n\n¿Cuál es el tamaño de \\(\\nabla_\\theta l_{ce}(\\theta^T x,y)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\\(n \\times k\\)\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué?"
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-matrix-batch-form",
    "href": "tics579/clase-1.html#calculando-el-gradiente-matrix-batch-form",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente Matrix Batch Form",
    "text": "Calculando el Gradiente Matrix Batch Form\nEsto sería equivalente a tomar en consideración todos los puntos del Training Set\n\n\n\\[\\begin{align}\\nabla_\\theta l_{ce}(X\\theta,y) &= \\frac{\\partial l_{ce}(X\\theta,y)}{\\partial X\\theta} \\cdot \\frac{\\partial X\\theta}{\\partial \\theta}\\\\\n&= (S - I_y) \\cdot X \\\\\n&= X^T \\cdot (S - I_y)\n\\end{align}\\]\n\n\n\n\n\n\n\n\\(S\\) corresponde al Softmax de \\(X\\theta\\) aplicado por filas.\n\\(I_y\\) corresponde al One Hot Encoder de las etiquetas. Filas con 1 en la etiqueta correcta y 0 en el resto.\n\n\n\n\n\n\n\n\n\n\n\n\nOjo con las dimensiones\n\n\n\n\\(S - I_y \\in \\mathbb{R}^{m \\times k}\\)\n\\(X \\in \\mathbb{R}^{m \\times n}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el tamaño de \\(\\nabla_\\theta l_{ce}(X\\theta,y)\\)?\n\n\n\n\n\n\nFinalmente la Regla de Actualización de parámetros usando Gradient Descent queda como:\n\\[\\theta := \\theta - \\frac{\\alpha}{m} X^T (S - I_y)\\]"
  },
  {
    "objectID": "tics579/clase-1.html#conclusiones",
    "href": "tics579/clase-1.html#conclusiones",
    "title": "TICS-579-Deep Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\n\n\n\n\n\n\n\n\n\nAcabamos de entrenar una Shallow Network, sin definir ningún concepto Fancy que es propio del área.\nNo hemos hablado ni de:\n\nForward Pass\nEpochs\nBackpropagation\nAdam\nActivation Functions\netc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAplicando esta simple regla se puede obtener cerca de un 8% de error clasificando dígitos en MNIST.\nSe puede programar en pocas líneas en Python.\n\n\n\n\n\n\n\n\n\n\n\nPero, ¿qué pasa con arquitecturas más complejas?"
  },
  {
    "objectID": "tics579/clase-10.html#datos-de-texto",
    "href": "tics579/clase-10.html#datos-de-texto",
    "title": "TICS-579-Deep Learning",
    "section": "Datos de Texto",
    "text": "Datos de Texto\nLos datos de texto corresponden a un caso particular de datos secuenciales. En este caso, dependiendo del idioma, las dependencias pueden venir tanto del pasado como del futuro. Consideramos texto libre como una secuencia de Strings, el cuál es inteligible para seres humanos pero no necesariamente para un computador.\n\n\n\n\n\n\nEs probablemente el tipo de dato más abundante, aunque también uno de los más sensible al ruido (variabilidad, idioma, tono, formalidad, etc.).\n\n\n\n\n\n\n\n\nEl problema\n\n\nLos computadores, y por ende los modelos, no pueden entender strings. El computador sólo puede entender datos numéricos.\n\n\n\n\n\n\n\n\n\nPara poder manipular texto dentro de un modelo será necesario un pre-procesamiento que permita transformar el texto en datos numéricos que un modelo pueda entender.\n\n\n\n\n\n\n\n\n\nLa disciplina encargada de desarrollar modelos asociado a lenguaje/texto es conocida como Procesamiento de Lenguaje Natural (NLP en inglés)."
  },
  {
    "objectID": "tics579/clase-10.html#tareas-asociadas-a-nlp",
    "href": "tics579/clase-10.html#tareas-asociadas-a-nlp",
    "title": "TICS-579-Deep Learning",
    "section": "Tareas asociadas a NLP",
    "text": "Tareas asociadas a NLP"
  },
  {
    "objectID": "tics579/clase-10.html#proceso-de-tokenización-y-embedding",
    "href": "tics579/clase-10.html#proceso-de-tokenización-y-embedding",
    "title": "TICS-579-Deep Learning",
    "section": "Proceso de Tokenización y Embedding",
    "text": "Proceso de Tokenización y Embedding\n\n\n\n\n\nTokenización\n\n\nEl proceso de Tokenización permite transformar texto en datos numéricos. Cada dato numérico se mapea con un “trozo de texto”. Normalmente los modelos van asociados a la tokenización con la que fueron entrenados. Cambiar la tokenización puede generar gran degradación.\n\n\n\n\n\n\nEmbedding\n\n\nCorresponde el proceso en el que los Tokens se transforman en vectores densos en las cuales la distancia entre ellos representa una noción de similaridad.\n\n\n\n\n\n\n\nEn este caso la frase “Frog on a log”  es separada en Tokens (en este caso cada token es una palabra).\nLuego cada Token es mapeado a un Token id proveniente de un vocabulario. ¿Qué es un vocabulario?\nLos embeddings en este caso representan una secuencia de largo 7 con 3 dimensiones."
  },
  {
    "objectID": "tics579/clase-10.html#embeddings",
    "href": "tics579/clase-10.html#embeddings",
    "title": "TICS-579-Deep Learning",
    "section": "Embeddings",
    "text": "Embeddings\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué es tan importante el uso de Embeddings?\n\n\n\nPrimero porque son entrenables. Es decir la red puede aprender cuál es la mejor manera de representar palabras.\nExisten embeddings pre-entrenados, es decir, se puede hacer transfer learning de embeddings.\nLa red puede aprender relaciones semánticas entre palabras, algo imposible utilizando otras representaciones."
  },
  {
    "objectID": "tics579/clase-10.html#problema-de-las-rnn",
    "href": "tics579/clase-10.html#problema-de-las-rnn",
    "title": "TICS-579-Deep Learning",
    "section": "Problema de las RNN",
    "text": "Problema de las RNN\n\n\n\n\n\n\nA pesar de las habilidades de las RNN, estas no son suficientes para distintas tareas de NLP.\n\n\n\n\n\n\n\n\nLas RNN inicialmente toman cada elemento de una secuencia y generan un output para cada entrada. Esto potencialmente genera ciertas limitantes. Una de ellas es el proceso llamado Machine Translation.\n\n\n\n\n\n\nEste es un ejemplo de Modelamiento seq2seq en el que se utiliza una secuencia de entrada pero se espera también una secuencia de salida."
  },
  {
    "objectID": "tics579/clase-10.html#machine-translation-ejemplo-del-inglés",
    "href": "tics579/clase-10.html#machine-translation-ejemplo-del-inglés",
    "title": "TICS-579-Deep Learning",
    "section": "Machine Translation: Ejemplo del Inglés",
    "text": "Machine Translation: Ejemplo del Inglés\nSupongamos que necesitamos hacer la siguiente traducción:\n\nInglés\n\n\nHi, my name is Alfonso\n\n\n\n\n\nEspañol\n\n\nHola, mi nombre es Alfonso\n\n\n\n\n\n\n\n\n\nEste tipo de traducción es uno a uno. Cada input puede tener asociado una salida de manera directa puede realizarse de manera directa con una RNN."
  },
  {
    "objectID": "tics579/clase-10.html#machine-translation-ejemplo-del-inglés-1",
    "href": "tics579/clase-10.html#machine-translation-ejemplo-del-inglés-1",
    "title": "TICS-579-Deep Learning",
    "section": "Machine Translation: Ejemplo del Inglés",
    "text": "Machine Translation: Ejemplo del Inglés\n\nInglés\n\n\nWould you help me prepare something for tomorrow?\n\n\n\n\n\nEspañol\n\n\n¿Me ayudarías a preparar algo para mañana?\n\n\n\n\n\n\n\n\n\nProblemas\n\n\n\nLa traducción no es uno. De hecho en inglés se utilizan 8 palabras y 1 signo de puntuación. En español se traduce en 7 palabras y 2 signos de puntuación.\n“Would” no tiene equivalente en español.\n“a” no tiene equivalente en el inglés.\n“Me” se traduce como “me” en inglés pero en vez de ir al inicio, va al final de “help”.\n“¿” no existe en inglés.\n\n\n\n\n\n\n\n\n\n\n\nOtros idiomas como el Alemán o el Ruso, tienen fusión de palabras o declinaciones que hacen la traducción mucho más difícil.\nEs por ello que se requiere una cierta libertad entre los tokens de entradas y los tokens de salida."
  },
  {
    "objectID": "tics579/clase-10.html#soluciones-redes-convolucionales",
    "href": "tics579/clase-10.html#soluciones-redes-convolucionales",
    "title": "TICS-579-Deep Learning",
    "section": "Soluciones: Redes Convolucionales",
    "text": "Soluciones: Redes Convolucionales\nUna potencial solución se puede dar por medio de Redes Convolucionales de 1D. En este caso las redes convolucionales tienen la ventaja de poder mirar tanto al pasado como al futuro de manera móvil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nPueden tomar contexto desde el inicio y desde el final.\n\n\n\n\n\n\n\n\n\n\nDesventajas\n\n\n\nSu campo receptivo es mucho más acotado y depende del número de capas y el largo del Kernel lo cual repercute directamente en el número de parámetros del modelo.\nNo tienen estado latente (o memoria) que almacena contexto.\nNo es útil para modelos de generación (ya que ve contexto desde el futuro)."
  },
  {
    "objectID": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder",
    "href": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder",
    "title": "TICS-579-Deep Learning",
    "section": "Soluciones: Arquitecturas Encoder-Decoder",
    "text": "Soluciones: Arquitecturas Encoder-Decoder\n\nEncoder\n\nCorresponde a una arquitectura que permitirá tomar datos de entrada y codificarlos en una representación numérica (normalmente como hidden states o como embeddings).\n\nDecoder\n\nCorresponde a una arquitectura que toma una representación codificada de datos (normalmente generado por un encoder) y la transforma nuevamente en una salida con un formato comprensible y no solamente una “simple etiqueta”.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste tipo de arquitecturas son quizás las más populares hoy en día y tienen aplicaciones en distintos dominios."
  },
  {
    "objectID": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder-1",
    "href": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder-1",
    "title": "TICS-579-Deep Learning",
    "section": "Soluciones: Arquitecturas Encoder-Decoder",
    "text": "Soluciones: Arquitecturas Encoder-Decoder\n\n\n\n\n\n\nUna arquitectura Encoder-Decoder convolucional permite devolver una imagen como salida. Este ejemplo se conoce como Segmentación Semántica."
  },
  {
    "objectID": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder-2",
    "href": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder-2",
    "title": "TICS-579-Deep Learning",
    "section": "Soluciones: Arquitecturas Encoder-Decoder",
    "text": "Soluciones: Arquitecturas Encoder-Decoder\n\n\n\n\n\n\nUna arquitectura recurrente permite devolver una secuencia como salida. La cual puede utilizarse para generación o traducción de texto.\n\n\n\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nPermite “desligarse” de la predicción uno a uno.\nLa salida de este tipo de modelos depende principalmente del contexto almacenado en el Hidden State/Bottleneck.\n\n\n\n\n\n\n\nDesventajas\n\n\n\nDado los problemas de Vanishing/Exploding Gradients es ingenuo pensar que todo el contexto de una frase vive en el último hidden state."
  },
  {
    "objectID": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder-3",
    "href": "tics579/clase-10.html#soluciones-arquitecturas-encoder-decoder-3",
    "title": "TICS-579-Deep Learning",
    "section": "Soluciones: Arquitecturas Encoder-Decoder",
    "text": "Soluciones: Arquitecturas Encoder-Decoder\n\n\n\n\n\n\n\n\nOjo\n\n\nEl último Hidden State del Encoder se utilizará como Hidden State inicial del Decoder."
  },
  {
    "objectID": "tics579/clase-10.html#entrenamiento-de-una-arquitectura-encoder-decoder",
    "href": "tics579/clase-10.html#entrenamiento-de-una-arquitectura-encoder-decoder",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de una Arquitectura Encoder-Decoder",
    "text": "Entrenamiento de una Arquitectura Encoder-Decoder\n\n\n\n\n\n\n\n\n\n\n\nArquitectura\n\n\n\nRNN Bidireccional\nCon un Encoder (colores pastel) y un Decoder independientes (colores más oscuros).\n\n\n\n\n\n\n\n\nTenemos dos frases una en inglés (input) y una en español (output). Ambas frases son claramente de tamaños distintos.\nTenemos el token especial &lt;eos&gt; (end of sentence) que separa el input del output.\n\n\n\n\n\n\n\n\nLos últimos Hidden States del Encoder son los Hidden States iniciales del Decoder. Estos también se conocen como Context Vectors.\n\n\n\n\n\n\n\nEntrenamiento\n\n\nAl momento de entrenarse, las salidas \\(y1\\) e \\(y2\\) pueden ser distintas al valor esperado y deben ir ajustándose epoch a epoch."
  },
  {
    "objectID": "tics579/clase-10.html#inferencia-de-una-arquitectura-encoder-decoder",
    "href": "tics579/clase-10.html#inferencia-de-una-arquitectura-encoder-decoder",
    "title": "TICS-579-Deep Learning",
    "section": "Inferencia de una Arquitectura Encoder-Decoder",
    "text": "Inferencia de una Arquitectura Encoder-Decoder\n\n\n\n\n\n\n\n\n\n\n\nPredicción\n\n\n\nLa predicción se va realizando de manera autoregresiva. Es decir, la predicción del primer step corresponde a la entrada del segundo step y así sucesivamente.\n\n\n\n\n\n\n\n\nLa primera entrada siempre será el token especial &lt;eos&gt; (otros modelos pueden utilizar otros tokens especiales).\n\n\n\n\n\n\n\n\nEl modelo irá prediciendo de manera autoregresiva hasta predecir el token &lt;eos&gt;.\n\n\n\n\n\n\n\n\n\n\nProblema\n\n\n\nPensar que todo el contexto se puede almacenar en el último hidden state es un poco ingenuo.\nEl último hidden state tiene más influencia de las palabras más cercanas y menos de las palabras iniciales (debido al problema de vanishing/exploding gradients)."
  },
  {
    "objectID": "tics579/clase-10.html#mecanismo-de-atención-bahdanau-et-al-2015",
    "href": "tics579/clase-10.html#mecanismo-de-atención-bahdanau-et-al-2015",
    "title": "TICS-579-Deep Learning",
    "section": "Mecanismo de Atención (Bahdanau et al, (2015))",
    "text": "Mecanismo de Atención (Bahdanau et al, (2015))\n\n\n\n\n\n\n\n\n\nAtención\n\nSe refiere a cualquier mecanismo en el que los hidden states se ponderan y combinan para poder utilizarlos como contexto. En otras palabras, el mecanismo busca a qué inputs iniciales debe poner más “atención” para poder generar la predicción.\n\n\n\\[c_i = \\sum_{t=1}^T a_{i,t} \\cdot h_t\\]\nDonde \\(c_i\\) corresponde al contexto para la predicción del output \\(i\\) y \\(a_{i,t}\\) (que van entre 0 y 1) corresponden a cuánta atención le presta el output \\(i\\) al token \\(t\\).\n\n\n\n\n\n\nOjo\n\n\nEn el paper original, se interpreta \\(a_{i,t}\\) como cuánto se “alínea” o se parece el estado \\(h_t\\) con \\(S_{t-1}\\)."
  },
  {
    "objectID": "tics579/clase-10.html#atención-de-bahdanau",
    "href": "tics579/clase-10.html#atención-de-bahdanau",
    "title": "TICS-579-Deep Learning",
    "section": "Atención de Bahdanau",
    "text": "Atención de Bahdanau\n\\[a_{i,t} = align(h_t, S_{i-1})\\] \n\\[[a_{i,1},...,a_{i,T}] = Softmax([\\tilde{a_{i,1}},...\\tilde{a_{i,T}}])\\]"
  },
  {
    "objectID": "tics579/clase-10.html#otras-formas-de-atención",
    "href": "tics579/clase-10.html#otras-formas-de-atención",
    "title": "TICS-579-Deep Learning",
    "section": "Otras formas de Atención",
    "text": "Otras formas de Atención\n\n\n\n1. Proyecciones Lineales\n\n\n\\[k_t = W_k \\cdot h_t\\] \\[q_{i-1} = W_q \\cdot S_{i-1}\\]\n\n\n\n\n\n\n2. Similaridad: Producto Punto es equivalente al Cosine Similarity\n\n\n\\[\\tilde{a}_{i,t} = k_t^T \\cdot q_{i-1}\\]\n\n\n\n\n\n\n3. Normalización:\n\n\n\\[[a_{i,1},...,a_{i,T}] = Softmax([\\tilde{a}_{i,1},...\\tilde{a}_{i,T}])\\]"
  },
  {
    "objectID": "tics579/clase-10.html#transformers-arquitectura",
    "href": "tics579/clase-10.html#transformers-arquitectura",
    "title": "TICS-579-Deep Learning",
    "section": "Transformers: Arquitectura",
    "text": "Transformers: Arquitectura\n\nTransformer\n\nCorresponde a la Arquitectura más avanzada que tenemos hoy en día. Está basada en distintos mecanismos de atención.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetalles de la Arquitectura\n\n\n\nCorresponde a un Encoder + un Decoder.\nCada uno contiene una capa de Embeddings.\nAdemás posee un Positional Encoding para entender el orden de la secuencia.\nEl decoder funciona de manera autoregresiva.\nPosee 4 tipos de atención."
  },
  {
    "objectID": "tics579/clase-10.html#transformers-self-y-multihead-attention",
    "href": "tics579/clase-10.html#transformers-self-y-multihead-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Transformers: Self y Multihead Attention",
    "text": "Transformers: Self y Multihead Attention\n\n\n\n\n\n\n\n\n\n\n\nDetalles\n\n\n\nEl self-attention pone atención (aprendiendo la relación) existente entre datos de una misma secuencia. La gran ventaja es que permite la paralelización de cálculos.\nEl Multihead Attention corresponde a la concatenación de varios Self-Attention. Esto permite no “sesgarse” con sólo una forma de poner atención, permitiendo aprender relaciones en distintas direcciones."
  },
  {
    "objectID": "tics579/clase-10.html#transformers-causal-self-attention",
    "href": "tics579/clase-10.html#transformers-causal-self-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Transformers: Causal Self Attention",
    "text": "Transformers: Causal Self Attention\n\n\n\n\n\n\n\n\n\n\n\nDetalles\n\n\n\nCorresponde a un tipo particular de Self Attention, en el cuál sólo se puede poner atención a valores de secuencia previa (no puede ver al futuro). Esto es particularmente necesario para tareas de generación autoregresiva."
  },
  {
    "objectID": "tics579/clase-10.html#transformers-cross-attention",
    "href": "tics579/clase-10.html#transformers-cross-attention",
    "title": "TICS-579-Deep Learning",
    "section": "Transformers: Cross Attention",
    "text": "Transformers: Cross Attention\n\n\n\n\n\n\n\n\n\n\n\nDetalles\n\n\n\nCorresponde a la atención que relaciona información proveniente tanto del Encoder como del Decoder. Muy similar al concepto original de Atención."
  },
  {
    "objectID": "tics579/clase-P2.html#derivadas",
    "href": "tics579/clase-P2.html#derivadas",
    "title": "TICS-579-Deep Learning",
    "section": "Derivadas",
    "text": "Derivadas\n\nLa derivada corresponde a la razón de cambio de una función con respecto a una variable de entrada \\(x\\). Es decir, cuánto cambia el valor de la función \\(f(x)\\) cuando cambiamos el valor de \\(x\\) en una cantidad infinitesimal \\(h\\).\n\n\n\n\n\n\n\n\n\nLa definición formal de la derivada\n\n\nPara una función \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\), la derivada se define como: \\[\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\]\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\n\\(f(x + h) \\approx f(x) + f'(x) \\cdot h\\), cuando \\(h \\to 0\\).\nSi la derivada existe en \\(x\\), decimos que la función es derivable o diferenciable en \\(x\\).\nSi las derivadas laterales no existen o no son iguales, entonces \\(f\\) no es derivable en \\(x\\).\nSi \\(f\\) es derivable, entonces \\(f\\) es continua.\n\n\n\n\n\n\n\n\n\n\n\nOtra interpretación\n\n\nEsto se puede interpretar como la pendiente de la recta tangente a la curva de \\(f(x)\\) en el punto \\(x\\)."
  },
  {
    "objectID": "tics579/clase-P2.html#derivadas-ejemplos",
    "href": "tics579/clase-P2.html#derivadas-ejemplos",
    "title": "TICS-579-Deep Learning",
    "section": "Derivadas: Ejemplos",
    "text": "Derivadas: Ejemplos\n\n\n\n\\((c)'= 0\\)\n\\((cx)' = c\\)\n\\((x^n)' = n x^{n-1}\\)\n\n\n\n\\((\\sqrt{x})' = \\frac{1}{2\\sqrt{x}}\\)\n\\((\\frac{1}{x})' = -\\frac{1}{x^2}\\)\n\n\n\n\\((e^x)' = e^x\\)\n\\((ln(x))' = \\frac{1}{x}\\)\n\\((log_a(x))' = \\frac{1}{x ln(a)}\\)\n\n\nReglas de Cálculo\n\n\\((f+g)' = f' + g'\\)\n\\((fg)' = f'g + fg'\\)\n\\((\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}\\)\n\\((\\alpha f)' = \\alpha f'\\)\n\\(f(x) = h(g(x)) \\Rightarrow f'(x) = h'(g(x)) g'(x)\\), conocida como la regla de la cadena."
  },
  {
    "objectID": "tics579/clase-P2.html#caso-multivariado",
    "href": "tics579/clase-P2.html#caso-multivariado",
    "title": "TICS-579-Deep Learning",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\nOjo\n\n\nSi tenemos una función \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\). ¿Cómo se define la derivada?\n\nSe requiere una dirección para derivar. Dado por un vector \\(\\bar{v} \\in \\mathbb{R}^n\\) y la recta que define.\n\n\n\n\n\n\n\n\n\n\nLa definición formal de la derivada direccional\n\n\nPara una función \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\), la derivada en torno a \\(\\bar{x}\\) en dirección \\(\\bar{v}\\) (\\(\\bar{v}\\) es un vector unitario) se define como: \\[\\nabla_{\\bar{v}}f(x) = \\lim_{h \\to 0} \\frac{f(\\bar{x}+h\\bar{v}) - f(\\bar{x})}{h}\\]\n\n\n\n\n\n\n\n\n\nProblema\n\n\nEste cálculo en general es poco práctico debido a que existen infinitas direcciones posibles. Por lo tanto, ¿cuál debería tomar?"
  },
  {
    "objectID": "tics579/clase-P2.html#derivadas-parciales",
    "href": "tics579/clase-P2.html#derivadas-parciales",
    "title": "TICS-579-Deep Learning",
    "section": "Derivadas Parciales",
    "text": "Derivadas Parciales\nSi \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), se define la derivada parcial de \\(f\\) en torno a \\(\\bar{x}=(x_1,...,x_n)\\) con respecto a la variable \\(x_i\\) como como la derivada direccional en la dirección del vector unitario \\(\\bar{e_i}\\).\n\\[\\frac{\\partial f(x)}{\\partial_{x_i}} = \\lim_{h \\to 0} \\frac{f(x_1, ..., x_i + h, ..., x_n) - f(x_1, ..., x_i, ...x_n)}{h}\\]\n\n\n\n\n\n\n\nGradiente: Función Escalar\n\n\nDefinimos el gradiente de \\(f\\) como:\n\\[\n\\begin{align}\n\\nabla f: \\mathbb{R}^n &\\rightarrow \\mathbb{R}^{1 \\times n} \\\\\n\\bar{x}  &\\rightarrow \\nabla f(\\bar{x}) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} & \\dots & \\frac{\\partial f}{\\partial x_n}\\end{bmatrix}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nEl gradiente podríamos considerarlo como un vector fila o columna según conveniencia. Por simplicidad lo dejaremos como un vector fila.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivada Direccional en función del Gradiente. ¿Cuál es la dirección en la que la Derivada Direccional es máxima?\n\n\n\\[\\nabla_{\\bar{v}}f(x) = \\nabla f(x) \\cdot \\bar{v}\\]"
  },
  {
    "objectID": "tics579/clase-P2.html#gradiente-ejemplo",
    "href": "tics579/clase-P2.html#gradiente-ejemplo",
    "title": "TICS-579-Deep Learning",
    "section": "Gradiente: Ejemplo",
    "text": "Gradiente: Ejemplo\n\\[f(x,y) = 3x^2 + 2xy + y^2 + 5x + 4\\]\nConsideremos entonces que \\(\\bar{x} = (x,y)\\). Es decir, va de \\(\\mathbb{R}^2\\) a \\(\\mathbb{R}\\).\n\n\n\n\n\n\n\nGradiente de \\(f\\)\n\n\n\\[\\nabla f(\\bar{x}) = \\nabla f(x,y) = \\begin{bmatrix}\\partial f_x & \\partial f_y\\end{bmatrix} = \\begin{bmatrix} 6x + 2y + 5 & 2x + 2y\\end{bmatrix}\\]"
  },
  {
    "objectID": "tics579/clase-P2.html#jacobiano-función-vectorial",
    "href": "tics579/clase-P2.html#jacobiano-función-vectorial",
    "title": "TICS-579-Deep Learning",
    "section": "Jacobiano: Función Vectorial",
    "text": "Jacobiano: Función Vectorial\nSea \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), el Jacobiano de \\(f\\) es la generalización del Gradiente y corresponderá a la matriz que contiene las derivadas parciales de una \\(f\\) multivariada respecto a cada una de sus variables de entrada. Se denota como:\nEs decir \\(f=(f1(\\bar{x}),...,f_m(\\bar{x}))^T\\)\nLuego,\n\\[\nJ = \\begin{bmatrix}\n- \\nabla f_1(\\bar{x}) -\\\\\n\\vdots \\\\\n- \\nabla f_m(\\bar{x}) -\\\\\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nImportante\n\n\nPodemos pensar el Jacobiano como el “vector de Gradientes” stackeados hacia abajo para cada componente de la función multivariada. Como cada componente es un vector de \\(1 \\times n\\), el Jacobiano tendrá dimensiones \\(m \\times n\\)."
  },
  {
    "objectID": "tics579/clase-P2.html#matrices-como-transformaciones-lineales",
    "href": "tics579/clase-P2.html#matrices-como-transformaciones-lineales",
    "title": "TICS-579-Deep Learning",
    "section": "Matrices como Transformaciones Lineales",
    "text": "Matrices como Transformaciones Lineales\nSupongamos:\n\\[f(x) = A \\cdot \\bar{x}\\]\n\n\n\n\n\n\n\nCaution\n\n\n\nSi \\(\\bar{x} \\in \\mathbb{R}^n\\) y \\(A \\in \\mathbb{R}^{m \\times n}\\), entonces \\(f(x) \\in \\mathbb{R}^m\\). Es decir, \\(f\\) es una tranformación que lleva al vector \\(\\bar{x}\\) de \\(\\mathbb{R}^n\\) a un vector de \\(\\mathbb{R}^m\\).\nEntenderemos una Transformación Lineal como una función que toma un vector de entrada (cada componente \\(x_j\\) es lineal) y lo transforma en otro vector de salida, manteniendo la estructura lineal .\n\n\n\n\n\n\n\n\n\n\n\n\nGradiente de una Transformación Lineal\n\n\nSi: \\[f_i(\\bar{x}) = A_{i,:} \\cdot \\bar{x} = \\sum_{j=1}^n A_{i,j}\\cdot x_j \\implies \\frac{\\partial f_i}{\\partial x_j} = A_{i,j}\\]\n\n\\(f_i(\\bar{x})\\) corresponde a la componente \\(i\\) de la función vectorial \\(f\\) evaluada en \\(\\bar{x}\\).\n\\(\\frac{\\partial f_i}{\\partial x_j}\\) corresponde a la derivada parcial de \\(f_i\\) respecto a la componente \\(x_j\\).\nTodas las derivadas se guardan en el Jacobiano de \\(f\\)."
  },
  {
    "objectID": "tics579/clase-P2.html#matrices-como-transformaciones-lineales-1",
    "href": "tics579/clase-P2.html#matrices-como-transformaciones-lineales-1",
    "title": "TICS-579-Deep Learning",
    "section": "Matrices como Transformaciones Lineales",
    "text": "Matrices como Transformaciones Lineales\nEl Jacobiano de \\(f\\) es entonces:\n\\[\nJ = \\begin{bmatrix}\nA_{1,1} & A_{1,2} & \\cdots & A_{1,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{m,1} & A_{m,2} & \\cdots & A_{m,n}\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nPropiedad\n\n\nDe acá podemos deducir que \\(\\frac{\\partial A \\cdot \\bar{x}}{\\partial \\bar{x}} = A\\). Es decir el calculo matricial se comporta como las reglas de derivación escalar (al menos lo que usaremos nosotros).\n\n\n\n\n\n\n\n\n\n\n\nRecomendación\n\n\nVer los siguientes videos para entender el concepto de Transformación Lineal de una Matriz:\n\nTransformaciones Lineales y Matrices\nMatrices no cuadradas como Transformaciones"
  },
  {
    "objectID": "tics579/clase-P2.html#hessiano",
    "href": "tics579/clase-P2.html#hessiano",
    "title": "TICS-579-Deep Learning",
    "section": "Hessiano",
    "text": "Hessiano\n\n\n\n\n\n\n\nHessiano\n\n\nSi la función a derivar es el Gradiente, entonces el Jacobiano pasa a llamarse Hessiano. Es decir, el Hessiano es el Jacobiano del Gradiente y es equivalente a la segunda derivada de una función vectorial/multivariada.\n\n\n\n\n\n\n\n\n\n\n\nPropiedades\n\n\n\nEl Hessiano es una matriz cuadrada de dimensiones \\(n \\times n\\).\nSiempre es simétrica.\nSi el Hessiano es PSD (Positive Semi-Definite), entonces la función es convexa. Demostrando que \\(\\bar{x}^T H_f(\\bar{x}) \\bar{x} \\geq 0\\) para todo \\(\\bar{x}\\).\n\n\n\n\n\n\\(\\nabla f_i(\\bar{x})\\) corresponde a la componente \\(i\\) del Gradiente de \\(f\\) evaluada en \\(\\bar{x}\\).\n\\[\nH_f(\\bar{x}) = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix} = \\begin{bmatrix}\n- \\nabla (\\nabla f_1(\\bar{x})) -\\\\\n- \\nabla (\\nabla f_2(\\bar{x})) -\\\\\n\\vdots \\\\\n- \\nabla (\\nabla f_n(\\bar{x})) -\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "tics579/clase-P2.html#hessiano-ejemplo",
    "href": "tics579/clase-P2.html#hessiano-ejemplo",
    "title": "TICS-579-Deep Learning",
    "section": "Hessiano: Ejemplo",
    "text": "Hessiano: Ejemplo\n\\[f(x,y) = 3x^2 + 2xy + y^2 + 5x + 4\\]\nConsideremos entonces que \\(\\bar{x} = (x,y)\\). Es decir, va de \\(\\mathbb{R}^2\\) a \\(\\mathbb{R}\\).\n\n\n\n\n\n\n\nGradiente de \\(f\\)\n\n\n\\[\\nabla f(\\bar{x}) = \\nabla f(x,y) = \\begin{bmatrix}\\partial f_x & \\partial f_y\\end{bmatrix} = \\begin{bmatrix} 6x + 2y + 5 & 2x + 2y\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\nHessiano\n\n\n\\[ H_f(\\bar{x}) = \\begin{bmatrix}\n- \\nabla (\\nabla f_1(\\bar{x})) - \\\\\n- \\nabla (\\nabla f_2(\\bar{x})) - \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n6 & 2 \\\\\n2 & 2\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "tics579/clase-P2.html#automatic-differentiation",
    "href": "tics579/clase-P2.html#automatic-differentiation",
    "title": "TICS-579-Deep Learning",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nSupongamos que tenemos que calcular la derivada de \\(f(1)\\) de la siguiente función:\n\\[f(x) = \\sqrt{x^2 + exp(x^2)} + cos((x^2 + exp(x^2))\\]\n\nSu derivada analítica, luego de bastante esfuerzo es:\n\n\\[f'(x) = 2x \\left(\\frac{1}{2\\sqrt{x^2+exp(x^2)}} - sen(x^2 + exp(x^2))\\right)\\left(1+exp(x^2)\\right)\\] \\[f'(1) = 5.983\\]\n\n\n\n\n\n\n\nAtención\n\n\nCalcular la derivada de manera analítica es muy engorroso y propenso a errores. Además, si la función tiene muchas variables, el cálculo se vuelve inviable y difícil de programar. Por ello se utiliza la diferenciación automática, un proceso algorítmico que permite calcular derivadas de funciones complejas de manera eficiente y precisa."
  },
  {
    "objectID": "tics579/clase-P2.html#automatic-differentiation-ejemplo",
    "href": "tics579/clase-P2.html#automatic-differentiation-ejemplo",
    "title": "TICS-579-Deep Learning",
    "section": "Automatic Differentiation: Ejemplo",
    "text": "Automatic Differentiation: Ejemplo\nPodemos reescribir la función \\(f\\) como una secuencia de operaciones elementales y asignar variables intermedias:\n\n\n\n\n\n\n\n\n\n\\(a = x^2\\)\n\\(b = exp(a)\\)\n\\(c = a + b\\)\n\\(d = \\sqrt{c}\\)\n\\(e = Cos(c)\\)\n\\(f = d + e\\)\n\n\n\n\n\n\n\n\n\nProcedimiento\n\n\n\nLa idea es que el camino entre dos nodos rojos son una derivada entre ambos nodos. Es decir, el camino entre \\(f\\) y \\(d\\) es \\(\\frac{\\partial f}{\\partial d}\\).\nLas derivadas se van acumulando y deben considerar todos los caminos. Por ejemplo, para calcular \\(\\frac{\\partial f}{\\partial c}\\), debemos considerar los caminos \\(f \\to d \\to c\\) y \\(f \\to e \\to c\\).\nTodas las variables definidas permiten calcular sus derivadas respecto a su input.\nSi comenzamos a derivar de atrás hacia adelante podemos reutilizar cálculos anteriores."
  },
  {
    "objectID": "tics579/clase-P2.html#automatic-differentiation-ejemplo-1",
    "href": "tics579/clase-P2.html#automatic-differentiation-ejemplo-1",
    "title": "TICS-579-Deep Learning",
    "section": "Automatic Differentiation: Ejemplo",
    "text": "Automatic Differentiation: Ejemplo\n\n\n\\[\\frac{\\partial f}{\\partial d} = \\frac{\\partial f}{\\partial e} = 1\\] \\[\\frac{\\partial f}{\\partial c} = \\frac{\\partial f}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} + \\frac{\\partial f}{\\partial e} \\cdot \\frac{\\partial e}{\\partial c} = 1 \\cdot 0.259 + 1 \\cdot 0.545 = 0.8045\\] \\[\n\\begin{align}\n\\frac{\\partial f}{\\partial b} &= \\frac{\\partial f}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} + \\frac{\\partial f}{\\partial e} \\cdot \\frac{\\partial e}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} \\\\\n&= \\frac{\\partial f}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} = 0.8045 \\cdot 1 = 0.8045\n\\end{align}\n\\]\n\n\n\n\n\n\nNotar que \\(\\frac{\\partial f}{\\partial c}\\) ya la habíamos calculado.\n\n\n\n\n\n\n\\[\\frac{\\partial f}{\\partial a} = \\frac{\\partial f}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a} + \\frac{\\partial f}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a} = 0.8045 \\cdot 2.7183 + 0.8045 \\cdot 1 = 2.9913\\]\n\\[\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} = 2.9913 \\cdot 2 = 5.983\\]\n\n\nSi \\(x = 1\\), entonces:\n\\(a = x^2 = 1\\)\n\\(b = exp(a) = 2.7183\\)\n\\(c = a + b = 3.7183\\)\n\\(d = \\sqrt{c} = 1.9283\\)\n\\(e = Cos(c) = -0.8383\\)\n\\(f = d + e = 1.09\\)\n\\(\\frac{\\partial d}{\\partial c} = \\frac{1}{2\\sqrt{c}} = 0.259\\)\n\\(\\frac{\\partial e}{\\partial c} = -sen(c) = 0.545\\)\n\\(\\frac{\\partial c}{\\partial b} = 1\\)\n\\(\\frac{\\partial b}{\\partial a} = exp(a) = 2.7183\\)\n\\(\\frac{\\partial a}{\\partial x} = 2x = 2\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     datacuber.cl\n  \n\n  \n  \nSoy Alfonso y he trabajado como Científico de Datos por los últimos 10 años. Además me gusta el Machine Learning Competitivo y hasta el momento he ganado 2 competencias.\nActualmente me encuentro cursando mi PhD. en Data Science. Mis intereses de investigación tienen que ver con Machine Learning y Deep Learning enfocándome principalmente en la aplicación de Transformers.\nEn mi tiempo libre practico Tenis de Mesa y escribo sobre Machine Learning en mi blog: datacuber.cl.\n\n\nUniversidad Adolfo Ibañez, Viña del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Ibañez, Viña del Mar | Msc. in Data Science | 2022 - 2024\nUniversidad Técnica Federico Santa María | Ingeniería Civil | 2005 - 2013\nPuedes ver más detalles de mi carrera acá.\n\n\n\n\nHate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "index.html#educación",
    "href": "index.html#educación",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "Universidad Adolfo Ibañez, Viña del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Ibañez, Viña del Mar | Msc. in Data Science | 2022 - 2024\nUniversidad Técnica Federico Santa María | Ingeniería Civil | 2005 - 2013\nPuedes ver más detalles de mi carrera acá."
  },
  {
    "objectID": "index.html#publicaciones",
    "href": "index.html#publicaciones",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "Hate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "tics579-labs.html",
    "href": "tics579-labs.html",
    "title": "Prácticos",
    "section": "",
    "text": "Práctico\nColab\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "tics411/clase-11.html#naive-bayes-preliminares",
    "href": "tics411/clase-11.html#naive-bayes-preliminares",
    "title": "TICS-411 Minería de Datos",
    "section": "Naive Bayes: Preliminares",
    "text": "Naive Bayes: Preliminares\n\nTambién conocido como Clasificador Inexperto de Bayes, es uno de los clasificadores más conocidos y sencillos.\n\n\n\n\n\n\n\nSe hizo particularmente conocido como uno de los primeros algoritmos en funcionar como Clasificador de Spam de manera efectiva.\n\n\n\n\nEs un modelo netamente probabilístico basado en el Teorema de Bayes.\n\nAprende una distribucional de Probabilidad Condicional.\nDado un punto \\(x_i\\), el modelo retorna la “probabilidad” de que \\(x_i\\) pertenezca a una clase específica."
  },
  {
    "objectID": "tics411/clase-11.html#definiciones",
    "href": "tics411/clase-11.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\nProbabilidad Condicional\n\n\\[P(X|C) = \\frac{P(X \\cap C)}{P(C)}\\]\n\nTeorema de Bayes\n\n\\[P(C|X) = \\frac{P(X|C)P(C)}{P(X)}\\]\n\nIndependencia Condicional\n\n\\[P(X_1, X_2, ..., X_k|C) = \\prod_{i=1}^k P(X_i|C)\\]\n\n\n\n\n\n\n\n\n\n\nSe lee como la Probabilidad de que Ocurra \\(X\\) dado que tenemos \\(C\\).\n\n\n\n\n\n\n\n\n\n\nLa probabilidad a posteriori (LHS), depende de el Likelihood, la probabilidad a priori y la evidencia (RHS).\n\n\n\n\n\n\n\n\n\n\nSi asumimos independencia, entonces la probabilidad conjunta de \\(k\\) eventos condicionados, se calcula como la productoria de las probabilidades condicionales independientes."
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo-básico",
    "href": "tics411/clase-11.html#ejemplo-básico",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo básico",
    "text": "Ejemplo básico\n\n\n\nSupongamos que:\n\nSabemos que la Meningitis produce Tortícolis el 50% de las veces.\nLa probabilidad de tener meningitis es: \\(1/50000\\).\nLa probabilidad de tener Tortícolis: \\(1/20\\).\n\n\n\n\nSi su paciente tiene tortícolis, ¿Cuál es la probabilidad de que tenga Meningitis?\n\\[P(M|T) = \\frac{P(T|M)P(M)}{P(T)}=\\frac{0.5 \\cdot 1/50000}{1/20} = 0.0002\\]"
  },
  {
    "objectID": "tics411/clase-11.html#modelo-naive-bayes-aprendizaje",
    "href": "tics411/clase-11.html#modelo-naive-bayes-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Modelo Naive Bayes: Aprendizaje",
    "text": "Modelo Naive Bayes: Aprendizaje\n\\[P(y = C_j|X_1, X_2, ..., X_k) = \\frac{P(X_1,X_2,..., X_k|y=C_j)P(y=C_j)}{P(X_1, X_2, ..., X_k)}\\]\n\n\n\n\\(P(y=C_j|X)\\) sería la probabilidad de que la predicción del modelo sea \\(C_j\\) dado que lo alimentamos con las variables \\(X\\).\nLuego \\(P(y=C_j)\\) es la probabilidad a priori de que la clase sea \\(C_j\\).\n\\(P(X|y=C_j)\\) es el likelihood (verosimilitud). Corresponde a la distribución de probabilidad de las variables X cuando la clase es \\(C_j\\).\n\\(P(X)\\) es la evidencia, y normalmente es muy complejo de calcular.\n\n\n\n\n\n\n\n\nPor simplicidad reduciremos \\(X_1, X_2, ..., X_k\\) a \\(X\\).\n\n\n\n\n\n\n\n\n\n\\(P(X)\\) tiene como única función la de normalizar la probabilidad para que vaya en un rango entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-11.html#modelo-naive-bayes-predicción",
    "href": "tics411/clase-11.html#modelo-naive-bayes-predicción",
    "title": "TICS-411 Minería de Datos",
    "section": "Modelo Naive Bayes: Predicción",
    "text": "Modelo Naive Bayes: Predicción\n\\[\\hat{y_i} = \\underset{C_j}{argmax} \\: P(y=C_j|X) \\]\ndonde, \\[P(y = C_j|X) \\propto \\prod_{i=1}^k P(X|y=C_j)P(y=C_j)\\]\n\n\n\n\n\n\nLa predicción de Naive Bayes corresponde a la clase que entrega [un estimado de] la Probabilidad a Posteriori más grande."
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo",
    "href": "tics411/clase-11.html#ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo clasificamos el siguiente punto?\n\\[ X = [C=Soleado,T=Media,H=Alta,V=Débil]\\]\n\n\n\n\n\n\nProbabilidad de Sí\n\n\n\\[P(y = Sí|X) = P(X|y=Sí)P(y=Sí)\\]\n\n\n\n\n\n\nProbabilidad de No\n\n\n\\[ P(y = No|X) = P(X|y=No)P(y=No)\\]"
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo-1",
    "href": "tics411/clase-11.html#ejemplo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\\[ P(y = Sí|X) = P(C=Soleado|y=Sí)P(T=Media|y=Sí)P(H=Alta|y=Sí)P(V=Débil|y=Sí)P(y=Sí)\\]\n\n\n\n\n\n\n\\[ P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=Débil|y=No)P(y=No)\\]\n\n\n\n\n\nProbabilidad Condicional para clase Sí\n\\[\\small P(C = Soleado|y = Sí) = 2/9\\] \\[\\small P(T = Media|y = Sí) = 4/9\\]\n\\[\\small P(H = Alta|y = Sí) = 3/9\\] \\[\\small P(V=Débil|y = Sí) = 6/9\\]\n\nProbabilidad Condicional para clase No\n\\[\\small P(C = Soleado|y = No) = 3/5\\] \\[\\small P(T = Media|y = No) = 2/5\\]\n\\[\\small P(H = Alta|y = No) = 4/5\\] \\[\\small P(V=Débil|y = No) = 2/5\\]\n\nProbabilidad a priori\n\\[P(y = Sí) = \\frac{9}{14} = 0.642\\] \\[P(y = No) = \\frac{5}{14} = 0.357\\]"
  },
  {
    "objectID": "tics411/clase-11.html#predicción",
    "href": "tics411/clase-11.html#predicción",
    "title": "TICS-411 Minería de Datos",
    "section": "Predicción",
    "text": "Predicción\n\n\n\n\\[\\scriptsize P(y = Sí|X) = P(C=Soleado|y=Sí)P(T=Media|y=Sí)P(H=Alta|y=Sí)P(V=Débil|y=Sí)P(y=Sí)\\] \\[\\small P(y = Sí|X) = \\frac{2}{9} \\cdot \\frac{4}{9} \\cdot \\frac{3}{9} \\cdot \\frac{6}{9} \\cdot \\frac{9}{14} = 0.0141\\]\n\n\n\n\n\n\n\\[\\scriptsize P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=Débil|y=No)P(y=No)\\] \\[\\small P(y = No|X) = \\frac{3}{5} \\cdot \\frac{2}{5} \\cdot \\frac{4}{5} \\cdot \\frac{2}{5} \\cdot \\frac{5}{14} = 0.0274\\]\n\n\n\n\n\n\n\n\n\n\\[\\hat{y} = argmax \\{0.0141, 0.0274\\} = No\\]"
  },
  {
    "objectID": "tics411/clase-11.html#smoothing",
    "href": "tics411/clase-11.html#smoothing",
    "title": "TICS-411 Minería de Datos",
    "section": "Smoothing",
    "text": "Smoothing\nSupongamos otro dataset más pequeño:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDado que Naive Bayes se calcula como una Productoria, al tener probabilidades 0 inmediatamente la Probabilidad a Posteriori es 0.\n\n\n\n\\[ P(Clima = Soleado|y = Sí) = \\frac{0}{6}\\] \\[ P(Clima = Soleado|y = No) = \\frac{5}{8}\\]\n\n\\[P(X_j|C = i) = \\frac{N_{yj} + \\alpha}{N_y + M\\alpha}\\]\n\n\\(\\alpha\\): Es un Hiperparámetro. Si \\(\\alpha = 1\\) se le llama Laplace Smoothing, si \\(\\alpha &lt;1\\) entonces se le llama Lidstone Smoothing.\nM: Corresponde al número de posibles valores que puede tomar \\(X_j\\)\n\\(N_{yj}\\): Corresponde a la cantidad de registros que toman el valor de la variable \\(X_j\\) solicitado en la clase \\(y\\).\n\\(N_{y}\\): Corresponde a la cantidad de registros totales que tienen la clase \\(y\\)."
  },
  {
    "objectID": "tics411/clase-11.html#laplace-smoothing",
    "href": "tics411/clase-11.html#laplace-smoothing",
    "title": "TICS-411 Minería de Datos",
    "section": "Laplace Smoothing",
    "text": "Laplace Smoothing\n\n\nSin Laplace\n\n\n\n\n\n\nCon Laplace\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso \\(\\alpha = 1\\) y \\(M=3\\) ya que Clima puede tomar 3 valores: Soleado, Cubierto y Lluvia."
  },
  {
    "objectID": "tics411/clase-11.html#variables-continuas",
    "href": "tics411/clase-11.html#variables-continuas",
    "title": "TICS-411 Minería de Datos",
    "section": "Variables Continuas",
    "text": "Variables Continuas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos calcular el Likelihood como una PDF (Probability Density Function). La más común: Distribución Normal (Gaussian Naive Bayes).\n\n\n\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]"
  },
  {
    "objectID": "tics411/clase-11.html#variables-continuas-predicción",
    "href": "tics411/clase-11.html#variables-continuas-predicción",
    "title": "TICS-411 Minería de Datos",
    "section": "Variables Continuas: Predicción",
    "text": "Variables Continuas: Predicción\n\n\n\n\\[P(humedad=74|y = Sí) = \\frac{1}{\\sqrt{2\\pi \\cdot 10.2^2}}e^{-\\frac{(74-79.1)^2}{2\\cdot 10.2^2}} = 0.0345 \\]\n\n\n\n\n\n\n\\[P(humedad=74|y = No) = \\frac{1}{\\sqrt{2\\pi \\cdot 9.7^2}}e^{-\\frac{(74-86.2)^2}{2\\cdot 9.7^2}} = 0.01865 \\]\n\n\n\n\n\n\n\n\n\nLuego la predicción es Sí."
  },
  {
    "objectID": "tics411/clase-11.html#detalles-técnicos",
    "href": "tics411/clase-11.html#detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Detalles Técnicos",
    "text": "Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nFácil de Implementar\nA menudo tiene un rendimiento decente a pesar de que las variables pueden no ser independientes.\nPuede aprender de forma incremental.\nValores faltantes son ignorados en el proceso de Aprendizaje.\nModelo robusto frente a datos atípicos y/o irrelevantes.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAsumir clases condicionadas produce probabilidades sesgadas.\nDependencias entre las variables no pueden ser modeladas."
  },
  {
    "objectID": "tics411/clase-11.html#implementación-en-scikit-learn",
    "href": "tics411/clase-11.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nMultinomial Naive Bayes (Normal)\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB(alpha = 1)\nnb.fit(X_train, y_train)\n\ny_pred = nb.predict(X_test)\ny_proba = nb.predict_proba(X_test)\nGaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngb = GaussianNB()\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\ny_proba = gb.predict_proba(X_test)"
  },
  {
    "objectID": "tics411/notebooks/knn_desarrollo.html",
    "href": "tics411/notebooks/knn_desarrollo.html",
    "title": "Preprocesamiento",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf.dtypes.value_counts().plot(kind=\"bar\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nX = df[[\"sex\", \"age\", \"class\", \"embark_town\", \"fare\"]]\ny = df.alive\ny\n\n0       no\n1      yes\n2      yes\n3      yes\n4       no\n      ... \n886     no\n887    yes\n888     no\n889    yes\n890     no\nName: alive, Length: 891, dtype: object\n\n\n\ny.value_counts(normalize=True).plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nnum_vars = X.select_dtypes(np.number).columns.tolist()\ncat_vars = [col for col in X.columns if col not in num_vars]\ncat_vars\n\n['sex', 'class', 'embark_town']\n\n\n\nX[num_vars].hist(figsize=(20, 6))\n\narray([[&lt;Axes: title={'center': 'age'}&gt;,\n        &lt;Axes: title={'center': 'fare'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\nX.groupby(\"age\").fare.mean()\n\nage\n0.42       8.5167\n0.67      14.5000\n0.75      19.2583\n0.83      23.8750\n0.92     151.5500\n           ...   \n70.00     40.7500\n70.50      7.7500\n71.00     42.0792\n74.00      7.7750\n80.00     30.0000\nName: fare, Length: 88, dtype: float64\n\n\n\ndf[\"rango_edad\"] = pd.cut(X[\"age\"], 5)\ndf.groupby(\"rango_edad\").fare.median()\n\nrango_edad\n(0.34, 16.336]      26.00000\n(16.336, 32.252]    10.50000\n(32.252, 48.168]    24.86875\n(48.168, 64.084]    29.70000\n(64.084, 80.0]      26.55000\nName: fare, dtype: float64\n\n\n\ndf.groupby(\"rango_edad\").fare.mean()\n\nrango_edad\n(0.34, 16.336]      31.588877\n(16.336, 32.252]    28.260499\n(32.252, 48.168]    42.788940\n(48.168, 64.084]    50.327235\n(64.084, 80.0]      28.905691\nName: fare, dtype: float64\n\n\n\ndf.fare.plot(kind=\"box\")\n\n\n\n\n\n\n\n\n\nfor cat in cat_vars:\n    X[cat].value_counts().plot(\n        kind=\"bar\", title=f\"Gráfico de variable {cat}\"\n    )\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX.groupby(\"class\").fare.mean()\n\nclass\nFirst     84.154687\nSecond    20.662183\nThird     13.675550\nName: fare, dtype: float64\n\n\n\nX\n\n\n\n\n\n\n\n\nsex\nage\nclass\nembark_town\nfare\n\n\n\n\n0\nmale\n22.0\nThird\nSouthampton\n7.2500\n\n\n1\nfemale\n38.0\nFirst\nCherbourg\n71.2833\n\n\n2\nfemale\n26.0\nThird\nSouthampton\n7.9250\n\n\n3\nfemale\n35.0\nFirst\nSouthampton\n53.1000\n\n\n4\nmale\n35.0\nThird\nSouthampton\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nmale\n27.0\nSecond\nSouthampton\n13.0000\n\n\n887\nfemale\n19.0\nFirst\nSouthampton\n30.0000\n\n\n888\nfemale\nNaN\nThird\nSouthampton\n23.4500\n\n\n889\nmale\n26.0\nFirst\nCherbourg\n30.0000\n\n\n890\nmale\n32.0\nThird\nQueenstown\n7.7500\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nX.isnull().mean().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X)\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X_imp)\nX_imp\n\n\n\n\n\n\n\n\nsex\nage\nclass\nembark_town\nfare\n\n\n\n\n0\nmale\n22.000000\nThird\nSouthampton\n7.2500\n\n\n1\nfemale\n38.000000\nFirst\nCherbourg\n71.2833\n\n\n2\nfemale\n26.000000\nThird\nSouthampton\n7.9250\n\n\n3\nfemale\n35.000000\nFirst\nSouthampton\n53.1000\n\n\n4\nmale\n35.000000\nThird\nSouthampton\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nmale\n27.000000\nSecond\nSouthampton\n13.0000\n\n\n887\nfemale\n19.000000\nFirst\nSouthampton\n30.0000\n\n\n888\nfemale\n29.699118\nThird\nSouthampton\n23.4500\n\n\n889\nmale\n26.000000\nFirst\nCherbourg\n30.0000\n\n\n890\nmale\n32.000000\nThird\nQueenstown\n7.7500\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nohe = OneHotEncoder(variables=[\"sex\", \"embark_town\"])\nX_enc = ohe.fit_transform(X_imp)\nod = OrdinalEncoder(encoding_method=\"arbitrary\")\nX_enc = od.fit_transform(X_enc)\nX_enc\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n22.000000\n0\n7.2500\n1\n0\n1\n0\n0\n\n\n1\n38.000000\n1\n71.2833\n0\n1\n0\n1\n0\n\n\n2\n26.000000\n0\n7.9250\n0\n1\n1\n0\n0\n\n\n3\n35.000000\n1\n53.1000\n0\n1\n1\n0\n0\n\n\n4\n35.000000\n0\n8.0500\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n27.000000\n2\n13.0000\n1\n0\n1\n0\n0\n\n\n887\n19.000000\n1\n30.0000\n0\n1\n1\n0\n0\n\n\n888\n29.699118\n0\n23.4500\n0\n1\n1\n0\n0\n\n\n889\n26.000000\n1\n30.0000\n1\n0\n0\n1\n0\n\n\n890\n32.000000\n0\n7.7500\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 8 columns\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"age\", \"fare\"])\nX_sc = sc.fit_transform(X_enc)\n\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_enc)\nX_sc_all\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.592481\n-0.820037\n-0.502445\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n1\n0.638789\n0.431081\n0.786845\n-1.355574\n1.355574\n-1.623803\n2.074505\n-0.307562\n\n\n2\n-0.284663\n-0.820037\n-0.488854\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n3\n0.407926\n0.431081\n0.420730\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n4\n0.407926\n-0.820037\n-0.486337\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.207709\n1.682199\n-0.386671\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n887\n-0.823344\n0.431081\n-0.044381\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n888\n0.000000\n-0.820037\n-0.176263\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n889\n-0.284663\n0.431081\n-0.044381\n0.737695\n-0.737695\n-1.623803\n2.074505\n-0.307562\n\n\n890\n0.177063\n-0.820037\n-0.492378\n0.737695\n-0.737695\n-1.623803\n-0.482043\n3.251373\n\n\n\n\n891 rows × 8 columns\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn(X, y, k=3):\n    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n    knn.fit(X, y)\n    print(f\"Puntaje para k = {k}: {knn.score(X, y)}\")\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_sc, y, k=k)\n\nPuntaje para k = 3: 0.8843995510662177\nPuntaje para k = 5: 0.8686868686868687\nPuntaje para k = 7: 0.8608305274971941\nPuntaje para k = 9: 0.8428731762065096\nPuntaje para k = 11: 0.835016835016835\nPuntaje para k = 13: 0.8249158249158249\nPuntaje para k = 15: 0.819304152637486\n\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_sc_all, y, k=k)\n\nPuntaje para k = 3: 0.8866442199775533\nPuntaje para k = 5: 0.8698092031425365\nPuntaje para k = 7: 0.8552188552188552\nPuntaje para k = 9: 0.8383838383838383\nPuntaje para k = 11: 0.835016835016835\nPuntaje para k = 13: 0.8282828282828283\nPuntaje para k = 15: 0.8237934904601572\n\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_enc, y, k=k)\n\nPuntaje para k = 3: 0.8372615039281706\nPuntaje para k = 5: 0.8204264870931538\nPuntaje para k = 7: 0.7867564534231201\nPuntaje para k = 9: 0.7721661054994389\nPuntaje para k = 11: 0.7676767676767676\nPuntaje para k = 13: 0.7575757575757576\nPuntaje para k = 15: 0.7508417508417509\n\n\n\nX_enc\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n22.000000\n0\n7.2500\n1\n0\n1\n0\n0\n\n\n1\n38.000000\n1\n71.2833\n0\n1\n0\n1\n0\n\n\n2\n26.000000\n0\n7.9250\n0\n1\n1\n0\n0\n\n\n3\n35.000000\n1\n53.1000\n0\n1\n1\n0\n0\n\n\n4\n35.000000\n0\n8.0500\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n27.000000\n2\n13.0000\n1\n0\n1\n0\n0\n\n\n887\n19.000000\n1\n30.0000\n0\n1\n1\n0\n0\n\n\n888\n29.699118\n0\n23.4500\n0\n1\n1\n0\n0\n\n\n889\n26.000000\n1\n30.0000\n1\n0\n0\n1\n0\n\n\n890\n32.000000\n0\n7.7500\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 8 columns\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/viz.html",
    "href": "tics411/notebooks/viz.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\na = np.array([4.5, 4, 4.1, 1, 2.3, 2.2, 2.4, 5, 5.5, 6.2, 6, 6, 6, 6])\nb = np.append(a, a + 1)\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(b, bins=5)\nax[\"A\"].set_title(\"Bins 5\")\nax[\"A\"].set_xlabel(\"Notas\")\nax[\"A\"].set_ylabel(\"Número de Estudiantes\")\nax[\"B\"].hist(b, bins=15)\nax[\"B\"].set_title(\"Bins 15\")\nax[\"B\"].set_xlabel(\"Notas\")\nax[\"C\"].hist(b, bins=30)\nax[\"C\"].set_title(\"Bins 30\")\nax[\"C\"].set_xlabel(\"Notas\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import KernelDensity\n\n\nkd_gauss = KernelDensity(kernel=\"epanechnikov\")\nkd_gauss.fit(b[:, np.newaxis])\nx_grid = np.linspace(1, 6, 1000)\nb_gauss = np.exp(kd_gauss.score_samples(x_grid[:, np.newaxis]))\nplt.hist(b)\nplt.plot(b_gauss)\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nnp.random.seed(0)\nx_grid = np.linspace(-4.5, 3.5, 1000)\nx = np.concatenate([norm(-1, 1.0).rvs(400), norm(1, 0.3).rvs(100)])\n\n\ndef kde_sklearn(x, x_grid, bandwidth=0.2, **kwargs):\n    \"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n    kde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n    kde_skl.fit(x[:, np.newaxis])\n    # score_samples() returns the log-likelihood of the samples\n    log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n    return np.exp(log_pdf)\n\n\npdf = kde_sklearn(x, x_grid, bandwidth=0.2)\n\nplt.hist(x)\nplt.plot(x_grid, pdf)\n\n\n\n\n\n\n\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"tophat\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_tophat = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_gaussian = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"epanechnikov\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_epa = np.exp(log_densidad_pred)\n\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"A\"].plot(X_grid, densidad_pred_tophat, color=\"red\", label=\"predicción\")\nax[\"A\"].set_title(\"Kernel Uniforme/Tophat h=0.2\")\n\nax[\"B\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"B\"].plot(X_grid, densidad_pred_gaussian, color=\"red\", label=\"predicción\")\nax[\"B\"].set_title(\"Kernel Gaussiano h=0.2\")\n\nax[\"C\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"C\"].plot(X_grid, densidad_pred_epa, color=\"red\", label=\"predicción\")\nax[\"C\"].set_title(\"Kernel Epanechnikov h=0.2\")\n\nText(0.5, 1.0, 'Kernel Epanechnikov h=0.2')\n\n\n\n\n\n\n\n\n\n\nn = 1000\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2))\n\nX_grid = np.linspace(-3, 4, 1000)\n\nmodelo_kde = KernelDensity(kernel=\"linear\", bandwidth=1)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred = np.exp(log_densidad_pred)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax.plot(X_grid, densidad_pred, color=\"red\", label=\"predicción\")\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df[\"embarked\"].value_counts().plot(\n    kind=\"bar\", rot=0, title=\"Personas embarcadas por puerto del Titanic\"\n)\n\n\n\n\n\n\n\n\n\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\n\n\n\ng = sns.catplot(\n    data=titanic_df, y=\"fare\", x=\"pclass\", kind=\"bar\", errorbar=None, hue=\"sex\"\n)\ng.figure.suptitle(\"Tarifa Promedio de Pasajeros del Titanic\\n por Clase y Sexo.\")\n\n\n\n\n\n\n\n\n\ndata = titanic_df[[\"age\", \"fare\"]].melt(value_vars=[\"age\", \"fare\"])\ndata\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nage\n22.00\n\n\n1\nage\n38.00\n\n\n2\nage\n26.00\n\n\n3\nage\n35.00\n\n\n4\nage\n35.00\n\n\n...\n...\n...\n\n\n1777\nfare\n13.00\n\n\n1778\nfare\n30.00\n\n\n1779\nfare\n23.45\n\n\n1780\nfare\n30.00\n\n\n1781\nfare\n7.75\n\n\n\n\n1782 rows × 2 columns\n\n\n\n\nsns.catplot(\n    kind=\"box\", x=\"variable\", y=\"value\", data=data, height=6, aspect=0.5, hue=\"variable\"\n)\n\n\n\n\n\n\n\n\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\ndata = iris_df.melt(\n    value_vars=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n)\ndata\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nsepal_length\n5.1\n\n\n1\nsepal_length\n4.9\n\n\n2\nsepal_length\n4.7\n\n\n3\nsepal_length\n4.6\n\n\n4\nsepal_length\n5.0\n\n\n...\n...\n...\n\n\n595\npetal_width\n2.3\n\n\n596\npetal_width\n1.9\n\n\n597\npetal_width\n2.0\n\n\n598\npetal_width\n2.3\n\n\n599\npetal_width\n1.8\n\n\n\n\n600 rows × 2 columns\n\n\n\n\ng = sns.catplot(x=\"variable\", y=\"value\", kind=\"box\", hue=\"variable\", data=data)\ng.figure.suptitle(\"Distribución de Medidas de Flores\")\ng._legend.remove()\ng.set(xlabel=None)\ng.set(ylabel=None)\n\n\n\n\n\n\n\n\n\nplt.scatter(iris_df.sepal_length, iris_df.petal_length)\nplt.title(\"Relación entre Largo del Sépalo y del Pétalo\")\nplt.xlabel(\"Largo del Sépalo\")\nplt.ylabel(\"Largo del Pétalo\")\n\nText(0, 0.5, 'Largo del Pétalo')\n\n\n\n\n\n\n\n\n\n\nanscombe = sns.load_dataset(\"anscombe\")\n\nfig = plt.figure(figsize=(10, 9))\nax = fig.subplot_mosaic(\n    \"\"\"AB\n                        CD\"\"\"\n)\nsns.regplot(data=anscombe.query(\"dataset == 'I'\"), x=\"x\", y=\"y\", ax=ax[\"A\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'II'\"), x=\"x\", y=\"y\", ax=ax[\"B\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'III'\"), x=\"x\", y=\"y\", ax=ax[\"C\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'IV'\"), x=\"x\", y=\"y\", ax=ax[\"D\"], ci=None)\nplt.suptitle(\"Cuarteto de Anscombe\")\n\nText(0.5, 0.98, 'Cuarteto de Anscombe')\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\ndata = pca.fit_transform(iris_df.drop(columns=\"species\"))\nx, y = data[:,0], data[:,1]\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nc = iris_df.species.astype(\"category\").cat.codes\nplt.scatter(x,y, c = c)\nplt.title(\"3 Clusters\");\n\n\n\n\n\n\n\n\n\nc_prima = (c == 0).astype(\"int64\")\nplt.scatter(x,y, c= c_prima)\nplt.title(\"2 Clusters\")\n\nText(0.5, 1.0, '2 Clusters')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/09-ex-apriori.html",
    "href": "tics411/notebooks/09-ex-apriori.html",
    "title": "Algoritmo Apriori",
    "section": "",
    "text": "%%capture\n!pip install mlxtend\n\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\nimport pandas as pd\n\n## Escribir acá las Transacciones\ntransactions = [\n    [\"Pan\", \"Mantequilla\", \"Leche\"],\n    [\"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Galletas\", \"Pañales\"],\n    [\"Leche\", \"Pañales\", \"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Pañales\"],\n]\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns=tre.columns_)\ndf_encoded\n\n\n\n\n\n\n\n\nCerveza\nGalletas\nLeche\nMantequilla\nPan\nPañales\n\n\n\n\n0\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n2\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\ndef apriori_algorithm(\n    df,\n    min_supp=0.4,\n    min_conf=0.7,\n    variables=[\n        \"antecedents\",\n        \"consequents\",\n        \"support\",\n        \"confidence\",\n        \"lift\",\n    ],\n):\n\n    frequent_itemsets = apriori(\n        df, min_support=min_supp, use_colnames=True\n    )\n    rules = association_rules(\n        frequent_itemsets, metric=\"confidence\", min_threshold=min_conf\n    )\n\n    return frequent_itemsets, rules[variables]\n\n\nfrequent_itemsets, rules = apriori_algorithm(\n    df_encoded, min_supp=0.4, min_conf=0.7\n)\ndisplay(frequent_itemsets)\ndisplay(rules)\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.4\n(Cerveza)\n\n\n1\n0.4\n(Leche)\n\n\n2\n0.6\n(Mantequilla)\n\n\n3\n0.6\n(Pan)\n\n\n4\n0.6\n(Pañales)\n\n\n5\n0.4\n(Pañales, Cerveza)\n\n\n6\n0.4\n(Mantequilla, Leche)\n\n\n7\n0.4\n(Leche, Pan)\n\n\n8\n0.6\n(Mantequilla, Pan)\n\n\n9\n0.4\n(Mantequilla, Leche, Pan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\n\n\n\n\n0\n(Cerveza)\n(Pañales)\n0.4\n1.0\n1.666667\n\n\n1\n(Leche)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n2\n(Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n3\n(Mantequilla)\n(Pan)\n0.6\n1.0\n1.666667\n\n\n4\n(Pan)\n(Mantequilla)\n0.6\n1.0\n1.666667\n\n\n5\n(Mantequilla, Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n6\n(Leche, Pan)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n7\n(Leche)\n(Mantequilla, Pan)\n0.4\n1.0\n1.666667\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/13-ex-DT.html",
    "href": "tics411/notebooks/13-ex-DT.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.survived\n\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\n\ndef make_pipeline(parameters):\n    scaler = SklearnTransformerWrapper(\n        StandardScaler(), variables=parameters[\"sc_variables\"]\n    )\n\n    print(\n        f\"Entrenamiento para Decision Tree y threshold = {parameters['threshold']}\"\n    )\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\n                \"num_imp\",\n                MeanMedianImputer(\n                    imputation_method=parameters[\"num_method\"]\n                ),\n            ),\n            (\n                \"cat_imp\",\n                CategoricalImputer(\n                    imputation_method=parameters[\"cat_method\"]\n                ),\n            ),\n            (\"ohe\", parameters[\"encoder\"]),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    DecisionTreeClassifier(\n                        random_state=42,\n                        min_samples_leaf=parameters[\"min_samples_leaf\"],\n                        min_samples_split=parameters[\"min_samples_split\"],\n                        max_depth=parameters[\"max_depth\"],\n                    ),\n                    threshold=parameters[\"threshold\"],\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\ndef show_tree(pipe, class_names=[\"No\", \"Sí\"], figsize=(20, 6)):\n    feature_names = pipe[-2].feature_names_in_\n    plt.figure(figsize=(20, 6))\n    plot_tree(\n        pipe[-1].estimator_,\n        filled=True,\n        feature_names=feature_names,\n        class_names=class_names,\n    )\n    plt.show()\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.9805389221556886\nTest Accuracy 0.7533632286995515\n===================================\nTrain Precision 0.9918032786885246\nTest Precision 0.6888888888888889\n===================================\nTrain Recall 0.9565217391304348\nTest Recall 0.6966292134831461\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.8488023952095808\nTest Accuracy 0.8071748878923767\n===================================\nTrain Precision 0.8333333333333334\nTest Precision 0.8194444444444444\n===================================\nTrain Recall 0.7509881422924901\nTest Recall 0.6629213483146067\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.2,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.2\n===================================\nTrain Accuracy 0.8068862275449101\nTest Accuracy 0.7892376681614349\n===================================\nTrain Precision 0.6962025316455697\nTest Precision 0.6944444444444444\n===================================\nTrain Recall 0.8695652173913043\nTest Recall 0.8426966292134831\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.9,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.9\n===================================\nTrain Accuracy 0.8173652694610778\nTest Accuracy 0.7713004484304933\n===================================\nTrain Precision 0.9851851851851852\nTest Precision 0.8958333333333334\n===================================\nTrain Recall 0.525691699604743\nTest Recall 0.48314606741573035\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=0.1,\n    min_samples_split=2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.2,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.2\n===================================\nTrain Accuracy 0.6212574850299402\nTest Accuracy 0.6143497757847534\n===================================\nTrain Precision 0.5\nTest Precision 0.50920245398773\n===================================\nTrain Recall 0.924901185770751\nTest Recall 0.9325842696629213\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=0.2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.8023952095808383\nTest Accuracy 0.7757847533632287\n===================================\nTrain Precision 0.9290780141843972\nTest Precision 0.8679245283018868\n===================================\nTrain Recall 0.5177865612648221\nTest Recall 0.5168539325842697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/pauta_guia2.html",
    "href": "tics411/notebooks/pauta_guia2.html",
    "title": "Pregunta 3:",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.DataFrame(\n    dict(x=[4, 5, 5, 6, 7, 7], y=[1, 1, 2, 7, 6, 7], c=[1, 1, 1, 2, 2, 2]),\n    index=[*range(1, 7)],\n)\ndf\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n1\n4\n1\n1\n\n\n2\n5\n1\n1\n\n\n3\n5\n2\n1\n\n\n4\n6\n7\n2\n\n\n5\n7\n6\n2\n\n\n6\n7\n7\n2\nfrom scipy.spatial import distance_matrix\nimport numpy as np\n\npoint = np.array([[8, 4]])\n\n## Distancias del Punto al resto de los puntos\npd.DataFrame(\n    distance_matrix(point, df[[\"x\", \"y\"]]), columns=[*range(1, 7)]\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n5.0\n4.242641\n3.605551\n3.605551\n2.236068\n3.162278\nfrom scipy.spatial.distance import cdist\n\npd.DataFrame(\n    cdist(point, df[[\"x\", \"y\"]], \"mahalanobis\"), columns=[*range(1, 7)]\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n3.07265\n2.182821\n2.357716\n3.24037\n1.855041\n2.338929"
  },
  {
    "objectID": "tics411/notebooks/pauta_guia2.html#pregunta-7",
    "href": "tics411/notebooks/pauta_guia2.html#pregunta-7",
    "title": "Pregunta 3:",
    "section": "Pregunta 7:",
    "text": "Pregunta 7:\n\nconf_mat = np.array([[8, 2, 0], [4, 13, 13], [5, 12, 43]])\n\n\n## Función para tomar una matriz de confusión y recrear las predicciones\ndef create_vectors(conf_mat):\n    rows, cols = conf_mat.shape\n\n    original_vals = np.empty((0, 2))\n    for i in range(rows):\n        for j in range(cols):\n            interim = np.repeat([[i, j]], conf_mat[i, j], axis=0)\n            original_vals = np.append(original_vals, interim, axis=0)\n\n    y = original_vals[:, 0]\n    y_pred = original_vals[:, 1]\n    return y, y_pred\n\n\ny, y_pred = create_vectors(conf_mat)\n\n\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n## Las predicciones recreadas generan esta matriz de confusión\n\n# OJO: ELIMINÉ un 1 para que hayan 100 valores, si no es lo que se busca,\n# se puede agregar el 1 en el objeto conf_mat de más arriba.\n\nConfusionMatrixDisplay.from_predictions(y, y_pred)\n\n## Esdto entrega el Accuracy único como mandé en un notebook y Precision, Recall y F1 por clases 0,1 y 2.\nprint(classification_report(y, y_pred, digits=3))\n\n              precision    recall  f1-score   support\n\n         0.0      0.471     0.800     0.593        10\n         1.0      0.481     0.433     0.456        30\n         2.0      0.768     0.717     0.741        60\n\n    accuracy                          0.640       100\n   macro avg      0.573     0.650     0.597       100\nweighted avg      0.652     0.640     0.641       100"
  },
  {
    "objectID": "tics411/notebooks/Ejercicio-Matriz-Confusión.html",
    "href": "tics411/notebooks/Ejercicio-Matriz-Confusión.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\n\ndf = pd.DataFrame(\n    dict(\n        y=[1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2],\n        y_pred=[1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 2],\n    )\n)\n\nprint(classification_report(df.y, df.y_pred, digits=3))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0      0.500     0.333     0.400         3\n           1      0.800     0.800     0.800         5\n           2      0.500     0.667     0.571         3\n\n    accuracy                          0.636        11\n   macro avg      0.600     0.600     0.590        11\nweighted avg      0.636     0.636     0.629        11\n\n\n\n\n\n\n\n\n\n\n\nEs importante mencionar que para el caso de más de 2 clases, el Accuracy se calcula como:\n\n\\[Accuracy = \\frac{TP_{Clase\\,0} + TP_{Clase\\,1} + TP_{Clase\\,2}}{Total} = \\frac{1 + 4 + 2}{11} = \\frac{7}{11} = 0.636\\]\nEl accuracy es un sólo valor para todas las clases. Y en el caso que mostramos ayer, el concepto de TN se pierde, por lo que es necesario reducir la matriz a un problema binario para poder poder darle sentido.\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/kmeans.html",
    "href": "tics411/notebooks/kmeans.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\n\n## Primera fila son coordenadas X\n## Segunda fila son coordenadas Y\n## Cada columna es el punto.\npuntos = np.array([[1, 2, 4, 5], [1, 1, 3, 4]])\npuntos\n\nc_1 = np.array([1, 1])\nc_2 = np.array([2, 1])\npuntos\n\narray([[1, 2, 4, 5],\n       [1, 1, 3, 4]])\n\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_clusters(puntos, c_1, c_2, c=None):\n    plt.scatter(puntos[0], puntos[1], s=500, c=c)\n    plt.scatter(\n        c_1[0],\n        c_1[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 1\",\n        edgecolors=\"k\",\n        s=200,\n        color=\"orange\",\n    )\n    plt.scatter(\n        c_2[0],\n        c_2[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 2\",\n        edgecolors=\"k\",\n        s=200,\n        c=\"green\",\n    )\n    plt.grid(alpha=0.5)\n    plt.legend()\n\n\nplot_clusters(puntos, c_1, c_2)\n\n\n\n\n\n\n\n\n\ndef distancia(p0, p1):\n    x0 = p0[0]\n    x1 = p1[0]\n    y0 = p0[1]\n    y1 = p1[1]\n    return np.sqrt((x1 - x0) ** 2 + (y1 - y0) ** 2)\n\n\ndef calculate_distances(puntos, c_1, c_2):\n\n    distancia_mat = np.zeros((2, 4))\n    distancia_mat\n\n    for i in range(4):\n        p = puntos[:, i]\n        distancia_mat[0, i] = distancia(p, c_1)\n        distancia_mat[1, i] = distancia(p, c_2)\n\n    return distancia_mat\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\ndistancia_mat\n\narray([[0.        , 1.        , 3.60555128, 5.        ],\n       [1.        , 0.        , 2.82842712, 4.24264069]])\n\n\n\ndef calculate_clusters(distancia_mat):\n    min_distancia_mat = np.min(distancia_mat, axis=0)\n\n    clusters = distancia_mat == min_distancia_mat\n    return clusters\n\n\nclusters = calculate_clusters(distancia_mat)\nclusters.astype(\"int64\")\n\n## Solo el punto 1 pertenece al cluster 1, todo el resto al cluster 2\n\narray([[1, 0, 0, 0],\n       [0, 1, 1, 1]])\n\n\n\ndef calculate_centroids(clusters):\n    c_1 = puntos[:, clusters[0]].mean(axis=1)\n    c_2 = puntos[:, clusters[1]].mean(axis=1)\n\n    return c_1, c_2\n\n\nc_1, c_2 = calculate_centroids(clusters)\nc_1, c_2\n\n(array([1., 1.]), array([3.66666667, 2.66666667]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"blue\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\nclusters = calculate_clusters(distancia_mat)\nc_1, c_2 = calculate_centroids(clusters)\nclusters\n\narray([[ True,  True, False, False],\n       [False, False,  True,  True]])\n\n\n\nc_1, c_2\n\n(array([1.5, 1. ]), array([4.5, 3.5]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"red\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/jerarquico.html",
    "href": "tics411/notebooks/jerarquico.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        alpha=[9, 10, 1, 6, 1],\n        beta=[3, 2, 9, 5, 10],\n        gamma=[7, 9, 4, 5, 3],\n    )\n)\n\nindices = [\"p53\", \"mdm2\", \"bcl2\", \"cylinE\", \"Caspade\"]\ndf.index = indices\ndf\n\n\n\n\n\n\n\n\nalpha\nbeta\ngamma\n\n\n\n\np53\n9\n3\n7\n\n\nmdm2\n10\n2\n9\n\n\nbcl2\n1\n9\n4\n\n\ncylinE\n6\n5\n5\n\n\nCaspade\n1\n10\n3\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\n\ndef calculate_global_min(dm):\n    data = np.triu(dm)\n\n    min_val = np.nanmin(data[np.nonzero(data)])\n    position = [dm.index[val[0]] for val in np.where(data == min_val)]\n    return min_val, position\n\n\noriginal_dm = distance_matrix(df, df, p=2)\noriginal_dm = pd.DataFrame(\n    original_dm, index=df.index, columns=df.index\n).round(2)\noriginal_dm.index = np.arange(5).astype(str)\noriginal_dm.columns = np.arange(5).astype(str)\noriginal_dm\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\n1\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\n2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\n3\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\n4\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\ndata = original_dm.copy()\ndata.index = indices\ndata.columns = indices\ndata\n\n\n\n\n\n\n\n\np53\nmdm2\nbcl2\ncylinE\nCaspade\n\n\n\n\np53\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\nmdm2\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\nbcl2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\ncylinE\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\nCaspade\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\ndef clean_position(position):\n    pos = []\n    for p in position:\n        pos.extend(p.split(\",\"))\n    return pos\n\n\ndef new_iteration(dm, original_dm, linkage=np.nanmean):\n    min_val, position = calculate_global_min(dm)\n    print(f\"El valor mínimo encontrado es: {min_val}\")\n    print(f\"Clusters a fusionar: {position}\")\n    non_position = [col for col in dm.columns if col not in position]\n    print(f\"Clusters que no se fusionan: {non_position}\")\n    new_position = \",\".join(position)\n    new_dm = dm.copy()\n    values = []\n    clean_pos = clean_position(position)\n    for n_p in non_position:\n        n_p = n_p.split(\",\")\n        v = linkage(original_dm.loc[n_p, clean_pos])\n        values.append(v)\n\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    new_dm = new_dm.T\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    return new_dm.drop(index=position, columns=position)\n\n\ndm_1 = new_iteration(original_dm, original_dm)\ndm_1\n\nEl valor mínimo encontrado es: 1.41\nClusters a fusionar: ['2', '4']\nClusters que no se fusionan: ['0', '1', '3']\n\n\n\n\n\n\n\n\n\n0\n1\n3\n2,4\n\n\n\n\n0\n0.00\n2.45\n4.120\n10.900\n\n\n1\n2.45\n0.00\n6.400\n12.950\n\n\n3\n4.12\n6.40\n0.000\n6.915\n\n\n2,4\n10.90\n12.95\n6.915\nNaN\n\n\n\n\n\n\n\n\ndm_2 = new_iteration(dm_1, original_dm)\ndm_2\n\nEl valor mínimo encontrado es: 2.45\nClusters a fusionar: ['0', '1']\nClusters que no se fusionan: ['3', '2,4']\n\n\n\n\n\n\n\n\n\n3\n2,4\n0,1\n\n\n\n\n3\n0.000\n6.915\n5.260\n\n\n2,4\n6.915\nNaN\n11.925\n\n\n0,1\n5.260\n11.925\nNaN\n\n\n\n\n\n\n\n\ndm_3 = new_iteration(dm_2, original_dm)\ndm_3\n\nEl valor mínimo encontrado es: 5.26\nClusters a fusionar: ['3', '0,1']\nClusters que no se fusionan: ['2,4']\n\n\n\n\n\n\n\n\n\n2,4\n3,0,1\n\n\n\n\n2,4\nNaN\n10.255\n\n\n3,0,1\n10.255\nNaN\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/05-ex-jerarquico.html",
    "href": "tics411/notebooks/05-ex-jerarquico.html",
    "title": "Ejemplo Clustering Aglomerativo",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"iris\")\ndf\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nX = df.drop(columns=\"species\")\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n3\n-1.506521\n0.098217\n-1.283389\n-1.315444\n\n\n4\n-1.021849\n1.249201\n-1.340227\n-1.315444\n\n\n...\n...\n...\n...\n...\n\n\n145\n1.038005\n-0.131979\n0.819596\n1.448832\n\n\n146\n0.553333\n-1.282963\n0.705921\n0.922303\n\n\n147\n0.795669\n-0.131979\n0.819596\n1.053935\n\n\n148\n0.432165\n0.788808\n0.933271\n1.448832\n\n\n149\n0.068662\n-0.131979\n0.762758\n0.790671\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_iris = pca.fit_transform(X_sc)\n\n\ndef pca_viz(pca, color=None, title=\"\"):\n    plt.scatter(pca_iris[\"pca0\"], pca_iris[\"pca1\"], c=color)\n    plt.title(title)\n    plt.show()\n\n\npca_viz(pca_iris, title=\"Visualización de Iris en 2 dimensiones\")\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlink_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in link_list:\n    plot_dendogram(X_sc, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nagc = AgglomerativeClustering(\n    n_clusters=3, metric=\"euclidean\", linkage=\"ward\"\n)\nlabels = agc.fit_predict(X_sc)\npca_viz(\n    pca_iris,\n    color=labels,\n    title=\"Clustering Iris. Método Average, 3 Clusters.\",\n)\n\n## Transformarlo en función para probar muchas combinaciones...\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html",
    "href": "tics411/notebooks/02-EDA.html",
    "title": "EDA",
    "section": "",
    "text": "El siguiente notebook tiene por propósito mostrar algunos comandos básicos para poder realizar Exploración de Datos utilizando Pandas.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Vamos a cargar los siguientes datos para poder explorarlos.\niris_df = sns.load_dataset(\"iris\")\ntitanic_df = sns.load_dataset(\"titanic\")\nts_df = sns.load_dataset(\"dowjones\")\niris_df\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "title": "EDA",
    "section": "Medidas de Tendencia Central",
    "text": "Medidas de Tendencia Central\nLos comandos .mean() y .median() permiten calcular la media y la mediana en datos numéricos. Como se ve en los ejemplos permite llamar una Serie de Pandas y calcular un valor.\n\nTip: En caso de querer aplicar estos comandos a un DataFrame se recomienda utilizar el flag numeric_only = True para evitar calcular estos valores en Datos Categóricos donde no hacen sentido.\n\n\nprint(f\"Promedio de Ancho de Petalo {iris_df['sepal_width'].mean()}\")\nprint(f\"Mediana de Largo de Petalo {iris_df['sepal_length'].median()}\")\n\nPromedio de Ancho de Petalo 3.0573333333333337\nMediana de Largo de Petalo 5.8\n\n\nPandas también cuenta con el comando .mode() el cuál devuelve la moda. A diferencia de los comandos anteriores, .mode() puede utilizarse tanto para datos categóricos como datos numéricos.\n\nprint(f\"Moda de Especies: \")\niris_df[\"species\"].mode()\n\nModa de Especies: \n\n\n0        setosa\n1    versicolor\n2     virginica\nName: species, dtype: object\n\n\nEl comando .quantile() permite calcular algún percentil de interés. q es un valor que va entre 0 y 1 para indicar el percentil requerido. Recordar que la mediana es equivalente al Percentil 50.\n\np25 = iris_df[\"sepal_width\"].quantile(q=0.25)\np50 = iris_df[\"sepal_width\"].quantile(q=0.50)\np75 = iris_df[\"sepal_width\"].quantile(q=0.75)\niris_df[\"sepal_width\"].median(), p25, p50, p75\n\n(3.0, 2.8, 3.0, 3.3)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-dispersión",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-dispersión",
    "title": "EDA",
    "section": "Medidas de Dispersión",
    "text": "Medidas de Dispersión\nPandas permite el cálculo de distintas medidas de dispersión. Al igual que los comandos anteriores contiene el flag numeric_only = True para evitar inconvenientes en DataFrames con distintos data types. Además contiene el comando ddof el cuál permitirá diferenciar si se quiere la medida poblacional (ddof = 0) o la muestral (ddof = 1).\n\n# Varianza Poblacional\niris_df.var(numeric_only=True, ddof=0)\n\nsepal_length    0.681122\nsepal_width     0.188713\npetal_length    3.095503\npetal_width     0.577133\ndtype: float64\n\n\n\n# Varianza Muestral\niris_df.var(numeric_only=True, ddof=1)\n\nsepal_length    0.685694\nsepal_width     0.189979\npetal_length    3.116278\npetal_width     0.581006\ndtype: float64\n\n\n\n# Desviación Estándar Muestral\niris_df.std(numeric_only=True, ddof=1)\n\nsepal_length    0.828066\nsepal_width     0.435866\npetal_length    1.765298\npetal_width     0.762238\ndtype: float64\n\n\n\n# Función para calcular el Rango Intercuartil...\ndef calculate_IQR(column):\n    quantiles = iris_df.quantile([0.25, 0.75], numeric_only=True)\n    iqr_sl = quantiles.loc[0.75, column] - quantiles.loc[0.25, column]\n    return iqr_sl\n\n\ncalculate_IQR(\"sepal_length\")\ncalculate_IQR(\"petal_width\")\n\n1.5\n\n\n\n# Coeficiente de Skewness o Asimetría.\niris_df.skew(numeric_only=True)\n\nsepal_length    0.314911\nsepal_width     0.318966\npetal_length   -0.274884\npetal_width    -0.102967\ndtype: float64"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#visualizaciones",
    "href": "tics411/notebooks/02-EDA.html#visualizaciones",
    "title": "EDA",
    "section": "Visualizaciones",
    "text": "Visualizaciones\nA continuación se mostrarán comandos propios de Pandas para poder generar los gráficos visto a lo largo de las clases. Se sugiere este tipo de gráficos cuando se trabaje con DataFrames ya que poseen buena documentación y una interfaz común para todos los gráficos.\nOpciones:\n\nkind: Permite indicar mediante un string el tipo de gráfico a mostrar.\nfigsize = (w,h): Permite fijar el tamaño de la figura. Notar que primero se entrega el ancho y luego el alto. Yo normalmente uso (20,6) ya que considero que queda bastante bien.\nedgecolor: Permite indicar el color del borde de las barras mediante un string. Tiene sentido para histogramás y bar plots.\ngrid = True/False: Permite mostrar o no una grilla.\nbins = n: Opción sólo para histogramas que permite indicar en cuántos bins se dividen los datos en el Histograma.\nalpha = 0.5: Corresponde al grado de transparecencia. Es un valor que va entre 0 y 1. Entre más pequeño el valor, más transparente.\ntitle: Permite agregar un Título como String.\nxlabel: Permite agregar un Título al Eje X.\nylabel: Permite agregar un Título al Eje Y.\n\n\nHistogramas\n\niris_df.plot(\n    kind=\"hist\", alpha=0.5, bins=30, figsize=(20, 6), edgecolor=\"black\"\n)\n# Notar que este genera todos los histogramas superpuestos...\n\nPor alguna razón Pandas tiene el comando .hist(). Este comando es bastante útil porque a diferencia del anterior no superpone los histogramas, lo cual la mayoría de las veces es lo que se busca.\n\niris_df.hist(figsize=(20, 6), bins=30, edgecolor=\"black\", grid=False)\n# tight_layout es opcional y a veces evita que hayan traslapes de títulos.\n# Usarlo si es que es necesario.\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nBarplots\nA diferencia de los Histográmas, los Barplots son utilizados para aplicar una agregación antes de gráficar. Esta agregación se puede utilizar mediante .value_counts() que permite contar valores, o mediante .groupby() el cuál permite aplicar otros tipos de agregación.\n\n# Acá por ejemplos contamos la cantidad de pasajeros por Sexo\ntitanic_df[\"sex\"].value_counts()\n\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\n\n# Una vez que tenemos contados los elementos podemos graficar...\ntitanic_df[\"sex\"].value_counts().plot(\n    kind=\"bar\",\n    figsize=(5, 6),\n    title=\"Número de Pasajeros por Sexo...\",\n    edgecolor=\"black\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Otro ejemplo, en este caso calculando el promedio por de Edad y Tarifa por Año.\ntitanic_df.groupby(\"pclass\")[[\"age\", \"fare\"]].mean()\n\n\n\n\n\n\n\n\nage\nfare\n\n\npclass\n\n\n\n\n\n\n1\n38.233441\n84.154687\n\n\n2\n29.877630\n20.662183\n\n\n3\n25.140620\n13.675550\n\n\n\n\n\n\n\n\n## En este caso, el índice Pclass irá al Eje X y los valores agregados de Age y Fare irán como barras.\niris_df.groupby(\"species\").mean().plot(kind=\"bar\", edgecolor=\"black\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#boxplots",
    "href": "tics411/notebooks/02-EDA.html#boxplots",
    "title": "EDA",
    "section": "Boxplots",
    "text": "Boxplots\n\niris_df.drop(columns=\"species\").plot(kind=\"box\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nPuntos\n\nNotar que a diferencia de los casos anteriores, el gráfico de puntos requiere que se definan qué columna irá en x y en y respectivamente.\n\n\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#lineplot",
    "href": "tics411/notebooks/02-EDA.html#lineplot",
    "title": "EDA",
    "section": "Lineplot",
    "text": "Lineplot\nEl lineplot es el gráfico por defecto de Pandas, por lo tanto no es necesario definir el parámetro kind. Al igual que el gráfico de Puntos se debe definir las variables x e y. Se recomienda siempre que x sea una variable de tipo temporal.\n\nts_df.plot(x=\"Date\", y=\"Price\", title=\"Evolución del Dow Jones\")\n\n\n## Este es un ejemplo de varias series de tiempo en conjunto.\n## Este código sólo genera datos sintéticos.\nfrom scipy.stats import norm\n\nts_df[\"AA\"] = ts_df[\"Price\"] + norm.rvs(size=649) * 55 + 1000\nts_df[\"BB\"] = -norm.rvs(size=649) * 55\n\nts_df.set_index(\"Date\").plot(title=\"Comparación distintas Tendencias\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#mosaico",
    "href": "tics411/notebooks/02-EDA.html#mosaico",
    "title": "EDA",
    "section": "Mosaico",
    "text": "Mosaico\nEn muchas ocaciones nosotros queremos mostrar una compilación de todos nuestros gráficos más que cada uno por separado. Para eso Matplotlib cuenta con la opción Mosaico.\nMosaico permite generar una grilla definida como un String. Si se fijan nuestra grilla se define por el string:\n\"\"\"AAA\n   BCC\"\"\"\nEn este caso nuestro canvas se divide en 6 partes, el gráfico que asigne a A utilizará las 3 secciones superiores, B utilizará sólo la sección de abajo a la izquierda y C utilizará las 2 restantes.\nPara asignar cada sección .plot() de pandas posee el parámetro ax donde se debe generar la asignación.\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.subplot_mosaic(\n    \"\"\"AAA\n       BCC\"\"\"\n)\n\n# Gráfico asignado a C\niris_df.drop(columns=\"species\").plot(\n    kind=\"box\", ax=ax[\"C\"], title=\"Distribución de Datos por Variable\"\n)\n\n## Gráfico asignado a B\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n    ax=ax[\"B\"],\n)\n\n## Gráfico asignado a A\niris_df.groupby(\"species\").mean().plot(\n    kind=\"bar\",\n    edgecolor=\"black\",\n    ax=ax[\"A\"],\n    rot=0,\n    title=\"Valores promedio por Especie\",\n)\n\n## Permite Agregar un título general a todo el Gráfico\nplt.suptitle(\"Este será un título para todo el Gráfico\", fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#matplotlib",
    "href": "tics411/notebooks/02-EDA.html#matplotlib",
    "title": "EDA",
    "section": "Matplotlib",
    "text": "Matplotlib\nLos comandos mostrados anteriormente son una adaptación de Matplotlib a Pandas. La gracia que tienen es que son fáciles de aprender y funcionarán directamente en Pandas que será nuestra principal fuente de datos.\nEn el caso de trabajar con Numpy, estos comandos NO FUNCIONARÁN. Por lo tanto es necesario utilizar la API de Matplotlib. La traducción no es 100% directa, pero normalmente todos los parámetros de .plot() se cambiarán por comandos del tipo plt.---"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#ejemplo",
    "href": "tics411/notebooks/02-EDA.html#ejemplo",
    "title": "EDA",
    "section": "Ejemplo",
    "text": "Ejemplo\nplt.plot(x,y, c = \"red\") #Existe también plt.bar, plt.hist, plt.scatter, plt.boxplot.\nplt.title(\"Este va a ser un título\")\nplt.xlabel(\"Este será una etiqueta del Eje X\")\nAprender Matplotlib es bastante más complicado pero tiene funcionalidades muchísimo más avanzadas que Pandas. Para este curso, no será necesario especializarse en Matplotlib, pero sí más adelante utilizaremos algunos gráficos que no se pueden hacer tan fácilmente en Pandas (pero serán casos puntuales)."
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html",
    "href": "tics411/notebooks/12-ex-CV.html",
    "title": "Holdout (Train Test Split)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape\n\n((891, 5), (891,))\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nnum_cols, cat_cols\n\n(['fare', 'age'], ['class', 'sex', 'embark_town'])\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nX_train.shape, X_test.shape\n\n((668, 5), (223, 5))\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef make_pipeline(k, scale=None):\n    if scale is not None:\n        scaler = SklearnTransformerWrapper(\n            StandardScaler(), variables=num_cols\n        )\n    else:\n        scaler = StandardScaler()\n\n    pipe = Pipeline(\n        steps=[\n            (\"ci\", CategoricalImputer(imputation_method=\"frequent\")),\n            (\"mmi\", MeanMedianImputer(imputation_method=\"mean\")),\n            (\"ohe\", OneHotEncoder()),\n            (\"sc\", scaler),\n            (\"model\", KNeighborsClassifier(n_neighbors=k)),\n        ]\n    )\n    return pipe\n\n\npipe = make_pipeline(k=5)\n# pipe = make_pipeline(k=5, scale = num_cols)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\npipe.score(X_test, y_test)\n\n0.7757847533632287"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#way-holdout-train-validation-test-split",
    "href": "tics411/notebooks/12-ex-CV.html#way-holdout-train-validation-test-split",
    "title": "Holdout (Train Test Split)",
    "section": "3-way Holdout (Train Validation Test Split)",
    "text": "3-way Holdout (Train Validation Test Split)\n\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\nfor k in [3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    pipe.fit(X_train, y_train)\n    metric = pipe.score(X_val, y_val)\n    print(f\"Puntaje del Modelo, k = {k}: {metric}\")\n    print(\"=============================================\")\n\nPuntaje del Modelo, k = 3: 0.7982062780269058\n=============================================\nPuntaje del Modelo, k = 5: 0.7757847533632287\n=============================================\nPuntaje del Modelo, k = 7: 0.7892376681614349\n=============================================\nPuntaje del Modelo, k = 9: 0.8026905829596412\n=============================================\nPuntaje del Modelo, k = 11: 0.7982062780269058\n============================================="
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#k-fold",
    "href": "tics411/notebooks/12-ex-CV.html#k-fold",
    "title": "Holdout (Train Test Split)",
    "section": "K-Fold",
    "text": "K-Fold\n\nimport numpy as np\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n\ndef make_kfold(X, y, k, kfold=5):\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=42)\n    score = []\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), start=1):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        pipe = make_pipeline(k=k)\n        pipe.fit(X_train, y_train)\n        metric = pipe.score(X_val, y_val)\n        score.append(metric)\n        print(f\"Metric for Fold {fold}: {metric:.3f}\")\n    return score\n\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    print(f\"k = {k}\")\n    kfold_score = make_kfold(X_trainval, y_trainval, k, kfold=5)\n    mean = np.mean(kfold_score)\n    std = np.std(kfold_score)\n    print(f\"K-Fold Score: {mean:.3f} +/- {std:.3f}\")\n    print(\"=========================================\")\n\nk = 1\nMetric for Fold 1: 0.748\nMetric for Fold 2: 0.811\nMetric for Fold 3: 0.725\nMetric for Fold 4: 0.803\nMetric for Fold 5: 0.746\nK-Fold Score: 0.767 +/- 0.034\n=========================================\nk = 3\nMetric for Fold 1: 0.769\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.789\nMetric for Fold 4: 0.789\nMetric for Fold 5: 0.810\nK-Fold Score: 0.802 +/- 0.029\n=========================================\nk = 5\nMetric for Fold 1: 0.790\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.803\nMetric for Fold 4: 0.754\nMetric for Fold 5: 0.789\nK-Fold Score: 0.798 +/- 0.032\n=========================================\nk = 7\nMetric for Fold 1: 0.783\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.803\nMetric for Fold 4: 0.761\nMetric for Fold 5: 0.817\nK-Fold Score: 0.803 +/- 0.031\n=========================================\nk = 9\nMetric for Fold 1: 0.790\nMetric for Fold 2: 0.846\nMetric for Fold 3: 0.789\nMetric for Fold 4: 0.761\nMetric for Fold 5: 0.810\nK-Fold Score: 0.799 +/- 0.028\n=========================================\nk = 11\nMetric for Fold 1: 0.776\nMetric for Fold 2: 0.839\nMetric for Fold 3: 0.796\nMetric for Fold 4: 0.754\nMetric for Fold 5: 0.789\nK-Fold Score: 0.791 +/- 0.028\n========================================="
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#versión-reducida",
    "href": "tics411/notebooks/12-ex-CV.html#versión-reducida",
    "title": "Holdout (Train Test Split)",
    "section": "Versión Reducida",
    "text": "Versión Reducida\n\nfrom sklearn.model_selection import cross_val_score\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor k in [1, 3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    vals = cross_val_score(pipe, X_trainval, y_trainval, cv=kf)\n    print(f\"k = {k}\")\n    print(f\"Metric: {np.mean(vals):.3f} +/- {np.std(vals):.3f}\")\n\nk = 1\nMetric: 0.767 +/- 0.034\nk = 3\nMetric: 0.802 +/- 0.029\nk = 5\nMetric: 0.798 +/- 0.032\nk = 7\nMetric: 0.803 +/- 0.031\nk = 9\nMetric: 0.799 +/- 0.028\nk = 11\nMetric: 0.791 +/- 0.028"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#calcular-test-scores",
    "href": "tics411/notebooks/12-ex-CV.html#calcular-test-scores",
    "title": "Holdout (Train Test Split)",
    "section": "Calcular Test Scores",
    "text": "Calcular Test Scores\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    pipe.fit(X_trainval, y_trainval)\n    metric = pipe.score(X_test, y_test)\n    print(f\"Score for k = {k}: {metric}\")\n\nScore for k = 1: 0.7821229050279329\nScore for k = 3: 0.8156424581005587\nScore for k = 5: 0.770949720670391\nScore for k = 7: 0.8044692737430168\nScore for k = 9: 0.8212290502793296\nScore for k = 11: 0.8156424581005587\n\n\n\nPero, qué está ocurriendo acá?"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html",
    "href": "tics411/notebooks/10-resolucion_guia.html",
    "title": "K-Means",
    "section": "",
    "text": "import pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(dict(x=[0, 0, 1, 4, 5, 6], y=[1, 0, 0, 4, 4, 6]))\ndisplay(df)\nd_matrix = pd.DataFrame(distance_matrix(df, df))\nd_matrix\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n1\n\n\n1\n0\n0\n\n\n2\n1\n0\n\n\n3\n4\n4\n\n\n4\n5\n4\n\n\n5\n6\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.000000\n1.000000\n1.414214\n5.000000\n5.830952\n7.810250\n\n\n1\n1.000000\n0.000000\n1.000000\n5.656854\n6.403124\n8.485281\n\n\n2\n1.414214\n1.000000\n0.000000\n5.000000\n5.656854\n7.810250\n\n\n3\n5.000000\n5.656854\n5.000000\n0.000000\n1.000000\n2.828427\n\n\n4\n5.830952\n6.403124\n5.656854\n1.000000\n0.000000\n2.236068\n\n\n5\n7.810250\n8.485281\n7.810250\n2.828427\n2.236068\n0.000000\ncentroides = pd.DataFrame(dict(x=[1, 5], y=[1, 6]))\ncentroides_2 = pd.DataFrame(dict(x=[1 / 3, 5], y=[1 / 3, 14 / 3]))\nimport matplotlib.pyplot as plt\n\nplt.scatter(df.x, df.y)\nplt.scatter(centroides.x, centroides.y, c=\"red\")\nplt.scatter(centroides_2.x, centroides_2.y, c=\"green\")\nplt.title(\"Centroides Iter 1: Rojo, Iter 2: Verde\")\nplt.tight_layout()\n## Distancia Centroides 1 a Puntos\npd.DataFrame(distance_matrix(centroides, df))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1.000000\n1.414214\n1.000000\n4.242641\n5.0\n7.071068\n\n\n1\n7.071068\n7.810250\n7.211103\n2.236068\n2.0\n1.000000\n## Distancia Centroides 2 a Puntos\npd.DataFrame(distance_matrix(centroides_2, df))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.745356\n0.471405\n0.745356\n5.18545\n5.934831\n8.013877\n\n\n1\n6.200358\n6.839428\n6.146363\n1.20185\n0.666667\n1.666667"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "href": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "title": "K-Means",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=2, eps=2)\ndbs.fit_predict(df)\n\narray([ 0,  0,  0,  1,  1, -1])\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=1, eps=1)\ndbs.fit_predict(df)\n\narray([0, 0, 0, 1, 1, 2])"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "href": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "title": "K-Means",
    "section": "Jerarquico Linkage Complete",
    "text": "Jerarquico Linkage Complete\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nplot_dendogram(df, link=\"complete\")"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#cohesión-y-separación",
    "href": "tics411/notebooks/10-resolucion_guia.html#cohesión-y-separación",
    "title": "K-Means",
    "section": "Cohesión y Separación",
    "text": "Cohesión y Separación\n\nimport numpy as np\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nlabels = np.array([0, 0, 0, 1, 1, 1])\ncenters = centroides_2.values\nsse, ssb = compute_clustering_metrics(df, labels, centers, is_df=True)\nsse, ssb\n\n(6.0, 60.833333333333336)"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "href": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "title": "K-Means",
    "section": "Silhouette",
    "text": "Silhouette\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, 1]\n    return a, b, bj, n_clusters\n\n\nd_matrix = distance_matrix(df, df)\na, b, bj, n_clusters = silhouette_score_m(d_matrix, labels)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.7517302154855591\n\n\n\ns_score_table\n\n\n\n\n\n\n\n\na\nb0\nb1\nb\ns\n\n\n\n\n0\n1.207107\n0.000000\n6.213734\n6.213734\n0.805736\n\n\n1\n1.000000\n0.000000\n6.848420\n6.848420\n0.853981\n\n\n2\n1.207107\n0.000000\n6.155701\n6.155701\n0.803904\n\n\n3\n1.914214\n5.218951\n0.000000\n5.218951\n0.633219\n\n\n4\n1.618034\n5.963643\n0.000000\n5.963643\n0.728684\n\n\n5\n2.532248\n8.035260\n0.000000\n8.035260\n0.684858"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html",
    "href": "tics411/notebooks/01-Preprocesamiento.html",
    "title": "Clases UAI",
    "section": "",
    "text": "%%capture\n## Ejecutar esta celda para instalar o actualizar Feature_Engine\n!pip install -U feature_engine\n## Chequear que la versión de Feature Engine sea al menos 1.7\nimport feature_engine\n\nfeature_engine.__version__\n\n'1.7.0'\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import set_config\n\n## (Opcional) Este comando permite que el output de Scikit-Learn sean Pandas DataFrames.\n## Por dejecto, Scikit-Learn transforma todo a Numpy, ya que es más eficiente computacionalmente.\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "href": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "title": "Clases UAI",
    "section": "Valores Faltantes",
    "text": "Valores Faltantes\n\n## Para detectar valores faltantes se utiliza el siguiente comando.\ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n## Opcionalmente se puede obtener el % o la fracción de nulos utilizando la siguiente variante.\ndf.isnull().mean()\n\nsurvived       0.000000\npclass         0.000000\nsex            0.000000\nage            0.198653\nsibsp          0.000000\nparch          0.000000\nfare           0.000000\nembarked       0.002245\nclass          0.000000\nwho            0.000000\nadult_male     0.000000\ndeck           0.772166\nembark_town    0.002245\nalive          0.000000\nalone          0.000000\ndtype: float64\n\n\nPandas: Es posible imputar valores usando Pandas con el comando .fillna().\n\nmedia = df[\"age\"].mean()\nmediana = df[\"age\"].median()\nprint(f\"Promedio de Edad: {media}\")\nprint(\n    f'Promedio de Edad con Imputación con Ceros: {df[\"age\"].fillna(0).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputación por Media: {df[\"age\"].fillna(media).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputación por Mediana: {df[\"age\"].fillna(mediana).mean()}'\n)\n\nPromedio de Edad: 29.69911764705882\nPromedio de Edad con Imputación con Ceros: 23.79929292929293\nPromedio de Edad con Imputación por Media: 29.69911764705882\nPromedio de Edad con Imputación por Mediana: 29.36158249158249\n\n\nScikit-Learn: Utiliza la clase SimpleImputer, el cual permite distintas estrategias de Imputación: \"mean\", \"median\", \"most_frequent\", \"constant\".\n\nfrom sklearn.impute import SimpleImputer\n\nsc = SimpleImputer(strategy=\"mean\")\n## En este caso uso [[]] ya que Scikit Learn espera Matrices o DataFrames.\n## Utilizar [[]] fuerza a que AGE sea un DataFrame de una Columna y no una Serie.\n\ndata_imputed = sc.fit_transform(df[[\"age\"]])\n## Se puede ver que los nuevos datos ya no poseen valores Perdidos.\ndata_imputed.isnull().sum()\n\nage    0\ndtype: int64"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "href": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "title": "Clases UAI",
    "section": "Outliers",
    "text": "Outliers\npandas: En Pandas se pueden acotar los outliers utilizando .clip()\n\nprint(f\"Promedio de Tarifas: {df.fare.mean()}\")\ndf[\"fare\"].agg([\"min\", \"max\"])\n\nPromedio de Tarifas: 32.204207968574636\n\n\nmin      0.0000\nmax    512.3292\nName: fare, dtype: float64\n\n\n\nlower: Define la cota inferior.\nupper: Define la cota superior.\n\n\nclipped_data = df[[\"fare\"]].clip(lower=10, upper=50)\nclipped_data.agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n10.0\n\n\nmax\n50.0\n\n\n\n\n\n\n\n\ndf[[\"fare\"]]\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows × 1 columns\n\n\n\n\n## Los valores menores a 10 fueron reemplazados por 10.\n## Los valores mayores a 50 fueron reemplazados por 50.\nclipped_data\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows × 1 columns\n\n\n\nsklearn: Para este caso nos apoyaremos de la librería feature_engine la cual posee herramientas para acotar. feature_engine sigue exactamente la misma lógica de Scikit-Learn.\n\nfrom feature_engine.outliers import ArbitraryOutlierCapper, Winsorizer\n\ncapper = ArbitraryOutlierCapper(\n    max_capping_dict=dict(fare=50), min_capping_dict=dict(fare=10)\n)\ncapper.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows × 1 columns\n\n\n\n\ncapping_method: Define la Estragegia a utilizar para el Winsorizer. Ver Docs.\n\n\n## \"gaussian\" permite acotar por mu +/- 3*std\n## \"iqr\" permite rellenar por Q1 - 3*iqr y Q3 + 3*iqr\nwin = Winsorizer(capping_method=\"gaussian\")\nwin.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows × 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#variables-categóricas",
    "href": "tics411/notebooks/01-Preprocesamiento.html#variables-categóricas",
    "title": "Clases UAI",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\npandas:\n\nOne Hot Encoding\nPara la conversión de variables categóricas utilizamos pd.get_dummies(). * drop_first: Si es True se elimina la primera categoría.\n\npd.get_dummies(df[\"embark_town\"], drop_first=False)\n\n\n\n\n\n\n\n\nCherbourg\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nFalse\nTrue\n\n\n1\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n\n\n886\nFalse\nFalse\nTrue\n\n\n887\nFalse\nFalse\nTrue\n\n\n888\nFalse\nFalse\nTrue\n\n\n889\nTrue\nFalse\nFalse\n\n\n890\nFalse\nTrue\nFalse\n\n\n\n\n891 rows × 3 columns\n\n\n\n\npd.get_dummies(df[\"embark_town\"], drop_first=True)\n# Una ventaja de este procedimiento es que no considera los Nulos como otra categoría...\n\n\n\n\n\n\n\n\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nTrue\n\n\n1\nFalse\nFalse\n\n\n2\nFalse\nTrue\n\n\n3\nFalse\nTrue\n\n\n4\nFalse\nTrue\n\n\n...\n...\n...\n\n\n886\nFalse\nTrue\n\n\n887\nFalse\nTrue\n\n\n888\nFalse\nTrue\n\n\n889\nFalse\nFalse\n\n\n890\nTrue\nFalse\n\n\n\n\n891 rows × 2 columns\n\n\n\n\n\nOrdinal Encoder\nSe utiliza pd.factorize(). * sort: Usar True ya que coloca las categorías en orden. Además de esta manera se comporta igual que OrdinalEncoder de Scikit-Learn.\n\npd.DataFrame(\n    pd.factorize(df[\"embark_town\"], sort=True)[0], columns=[\"new_column\"]\n)\n\n\n\n\n\n\n\n\nnew_column\n\n\n\n\n0\n2\n\n\n1\n0\n\n\n2\n2\n\n\n3\n2\n\n\n4\n2\n\n\n...\n...\n\n\n886\n2\n\n\n887\n2\n\n\n888\n2\n\n\n889\n0\n\n\n890\n1\n\n\n\n\n891 rows × 1 columns\n\n\n\nScikit-Learn:\n\n\nOne Hot Encoding\n\nsparse_output: Se debe fijar como False para poder ver el output como Pandas\ndrop: Se debe colocar \"first\" o el nombre de una sóla categoría a eliminar.\n\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nohe = OneHotEncoder(drop=\"first\", sparse_output=False)\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n\n\n890\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 3 columns\n\n\n\n\n\nOrdinal Encoder\n\nohe = OneHotEncoder(\n    drop=[\"Queenstown\"], sparse_output=False\n)  # También se puede colocar np.nan.\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\nembark_town_Cherbourg\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n1.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n\n\n\n\n891 rows × 3 columns\n\n\n\n\noe = OrdinalEncoder()\noe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\n2.0\n\n\n1\n0.0\n\n\n2\n2.0\n\n\n3\n2.0\n\n\n4\n2.0\n\n\n...\n...\n\n\n886\n2.0\n\n\n887\n2.0\n\n\n888\n2.0\n\n\n889\n0.0\n\n\n890\n1.0\n\n\n\n\n891 rows × 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "href": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "title": "Clases UAI",
    "section": "Escalamiento",
    "text": "Escalamiento\nEl escalamiento normalmente se realiza sólo en Scikit-Learn. Se mostrarán la Estandarización y Normalización.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n## Llamaremos esto Estandarización... (sólo por convención del curso)\nsc = StandardScaler()\ndata = sc.fit_transform(df[[\"fare\"]])\ndata.agg([\"mean\", \"std\"])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmean\n3.987333e-18\n\n\nstd\n1.000562e+00\n\n\n\n\n\n\n\n\n## Llamaremos esto Normalización... (sólo por convención del curso)\nmms = MinMaxScaler()\nmms.fit_transform(df[[\"fare\"]]).agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n0.0\n\n\nmax\n1.0"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-sólo-a-algunas-variables.",
    "href": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-sólo-a-algunas-variables.",
    "title": "Clases UAI",
    "section": "Aplicar Preprocesamientos sólo a algunas variables.",
    "text": "Aplicar Preprocesamientos sólo a algunas variables.\nScikit-Learn fue diseñado para el entrenamiento eficiente de modelos. Para ello, se basó en Numpy, el cuál no cuenta con nombre de columnas, por lo que para poder aplicar pre-procesamientos a ciertas partes del Dataset utiliza lo que se llama el ColumnTransformer(), el cuál va más allá del alcance del curso.\nPara simplificar el proceso de elegir ciertas columnas, feature_engine posee una el SklearnTransformerWrapper que permite elegir qué variables queremos pasar por cierta transformación.\n\n## Sin SklearnTransformerWrapper\n\nohe = OneHotEncoder(sparse_output=False)\nohe.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea columnas dummies incluso para las variables numéricas.\n\n\n\n\n\n\n\n\nage_0.42\nage_0.67\nage_0.75\nage_0.83\nage_0.92\nage_1.0\nage_2.0\nage_3.0\nage_4.0\nage_5.0\n...\nage_70.0\nage_70.5\nage_71.0\nage_74.0\nage_80.0\nage_nan\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 93 columns\n\n\n\n\n## Aplicar preprocesamientos a ciertas variables...\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nohe_w = SklearnTransformerWrapper(\n    OneHotEncoder(sparse_output=False), variables=\"embark_town\"\n)\nohe_w.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea dummies sólo para la variable embark_town y deja age como estaba.\n\n\n\n\n\n\n\n\nage\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n22.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n38.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\n27.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n19.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\nNaN\n0.0\n0.0\n1.0\n0.0\n\n\n889\n26.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n32.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 5 columns"
  },
  {
    "objectID": "tics411/clase-7.html#introducción",
    "href": "tics411/clase-7.html#introducción",
    "title": "TICS-411 Minería de Datos",
    "section": "Introducción",
    "text": "Introducción\n\nGracias a los planes de fidelización (juntar puntos, dar RUT, acumular millas, etc.) las empresas son capaces de detectar patrones:\n\n\nQué nos gusta,\nQué compramos,\nCon qué frecuencia lo compramos,\nJunto con qué lo compramos\netc.\n\n\n\n\n\n\n\nMarket Basket Analysis\n\n\nCorresponde al estudio de nuestra canasta de compras. De modo que podamos entender qué cosas son las que como clientes preferimos y una empresa pueda Recomendar de manera más apropiadas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones",
    "href": "tics411/clase-7.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nPatrón\n\n\nPredicado (output True/False) para verificar si una estructura buscada ocurre o no.\n\n\nTarea\n\n\nEncontrar reglas de asociación basado en patrones.\n\n\n\nEjemplos\n\nDatasets de supermercados:\n\n10% de los clientes totales compran vino y quedo (patrón: si compro vino, también llevo queso).\n\nDatasets de Alarmas:\n\nSi la alarma A y B suenan en un intervalo de 30 segundos, entonces la alarma C sonará dentro de un intervalo de 60 segundos con 50% de probabilidad."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado\n\nDatos Transaccionales\n\n\nUna transacción involucra un conjunto de elementos. Una boleta de supermercado muestra el conjunto de elementos comprados por un cliente. Los productos involucrados en una transacción se denominan items."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado"
  },
  {
    "objectID": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "href": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivo y Aplicaciones",
    "text": "Objetivo y Aplicaciones\n\n\n\n\n\n\nObjetivo\n\n\nEncontrar asociaciones entre elementos u objetos de bases de datos transaccionales.\n\n\n\n\n\n\n\n\n\nAplicaciones\n\n\n\nApoyo a toma de decisiones.\nAnálisis de Información de Ventas.\nDistribución y ubicación de Mercaderías.\nSegmentación de Clientes en base de patrones de compra.\nDiágnostico y predicción de alarmas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones-medidas",
    "href": "tics411/clase-7.html#definiciones-medidas",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones: Medidas",
    "text": "Definiciones: Medidas\n\n\n\n\n\n\nSupport (Soporte)\n\nFracción de Transacciones que contienen a \\(X\\). Probabilidad de que una transacción contenga a \\(X\\).\n\n\n\\[Supp(X) = P(X)\\]\n\n\n\n\n\n\n\nSupport Count\n\nNúmero de Transacciones que contienen a \\(X\\).\n\n\n\\[SuppCount(X) = Count(X)\\]\n\n\n\n\n\n\n\n\nConfidence (Confianza o Eficiencia)\n\nFracción de las Transacciones en las que aparece \\(X\\) que también incluyen \\(Z\\).\n\n\n\\[Conf(X \\implies Z) = \\frac{Supp(X \\cup Z)}{Supp(X)}\\] \\[Conf(X \\implies Z) = \\frac{SuppCount(X \\cup Z)}{SuppCount(X)}\\]\n\n\n\n\n\n\n\n\n\n\nOjo con la Notación \\(\\cup\\). En este caso significa que tanto el producto X como el Producto Z sean parte de la transacción."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "href": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplos: Support y Confidence",
    "text": "Ejemplos: Support y Confidence\n\n\n\n\n\n\n\n\n\\[ Supp({Pan}) = 4/7\\] \\[ Supp({Leche}) = 3/7\\] \\[ Supp({Pan, Huevo}) = 2/7\\]\n\\[ Conf({Pan} \\implies {Huevo}) = \\frac{Supp({Pan, Huevo})}{Supp(Pan)} = \\frac{2/7}{4/7}\\]\n\\[ Conf({Pan} \\implies {Leche}) = \\frac{Supp({Pan, Leche})}{Supp(Pan)} = \\frac{1/7}{4/7}\\] \\[ Conf({Leche} \\implies {Pan}) = \\frac{Supp({Pan, Leche})}{Supp(Leche)} = \\frac{1/7}{3/7}\\]"
  },
  {
    "objectID": "tics411/clase-7.html#problema",
    "href": "tics411/clase-7.html#problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Problema",
    "text": "Problema\n\nEn un dataset transaccional de n productos totales y \\(|U_i|\\) elementos para la Transacción \\(i\\).\n\nSe pueden generar un total de \\(N_{reglas}\\) de asociación:\n\\[N_{reglas} = \\sum_{i=1}^{2^{n}} \\sum_{j=0}^{|U_i|}\\binom{|U_i|}{j}\\]\n\n\n\n\n\n\n\n\n\nSi suponemos un supermercado que tiene 1000 productos, y transacciones que pueden ir entre 1 y 50 productos. El problema es muy costoso, y se podrían eventualmente generar demasiadas combinaciones."
  },
  {
    "objectID": "tics411/clase-7.html#algoritmo-apriori",
    "href": "tics411/clase-7.html#algoritmo-apriori",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo Apriori",
    "text": "Algoritmo Apriori\n\nApriori\n\n\nEs un algoritmo para aprender reglas de asociación que utiliza el principio Apriori para buscar de forma eficiente las reglas que satisfacen los límites de soporte y confianza.\n\n\n\n\nAlgoritmo\n\nFijar \\(k=1\\) y determinar lista de candidatos de tamaño \\(k\\).\n\nCalcular la frecuencia del conjunto.\nEliminar conjuntos con baja frecuencia (utilizando un umbral de soporte).\nUnir los conjuntos frecuentes para generar conjuntos de tamaño \\(k+1\\).\nSi existe la posibilidad de seguir creando combinaciones volver al paso a y repetir.\n\nUsar todos los conjuntos frecuentes para generar reglas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori",
    "href": "tics411/clase-7.html#ejemplo-apriori",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori",
    "text": "Ejemplo Apriori\n\nSupongamos el siguiente dataset transaccional:\n\nSupongamos que queremos calcular las reglas de asociación que tengan un MinSupp=40% y un MinConf=70%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodríamos pensar que MinSupp y MinConf son los hiperparámetros de este algoritmo."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 1",
    "text": "Ejemplo Apriori: Iteración 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGalletas NO CUMPLE con el Soporte Mínimo solicitado. Por lo tanto, lo elimino y genero relaciones de 2 productos sin considerar Galletas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-2",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 2",
    "text": "Ejemplo Apriori: Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcá NO SE ELIMINA ningún producto, ya que en los itemsets que sobrevivieron hay Pan, Mantequilla, Leche, Pañales y Cerveza."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-3",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 3",
    "text": "Ejemplo Apriori: Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe puede apreciar que los únicos 3 productos que sobreviven son Pan, Mantequilla y Leche. Por lo tanto, NO ES POSIBLE generar reglas con 4 productos."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas",
    "href": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Generación de Reglas",
    "text": "Ejemplo Apriori: Generación de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pan, Mantequilla}:\n\n\\(Conf(Pan \\implies Mantequilla) = \\frac{Supp(Pan, Mantequilla)}{Supp(Pan)} = \\frac{3}{3}\\)✅ \\(Conf(Mantequilla \\implies Pan) = \\frac{Supp(Pan, Mantequilla)}{Supp(Mantequilla)} = \\frac{3}{3}\\)✅\n\n\n\nPara {Pan, Leche}:\n\n\\(Conf(Pan \\implies Leche) = \\frac{Supp(Pan, Leche)}{Supp(Pan)} = \\frac{2}{3}\\) ❌ \\(Conf(Leche \\implies Pan) = \\frac{Supp(Pan, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ✅\n\n\n\nPara {Mantequilla, Leche}:\n\n\\(Conf(Mantequilla \\implies Leche) = \\frac{Supp(Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\) ❌ \\(Conf(Leche \\implies Mantequilla) = \\frac{Supp(Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ✅"
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Generación de Reglas",
    "text": "Ejemplo Apriori: Generación de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pañales, Cerveza}:\n\n\\(Conf(Pañales \\implies Cerveza) = \\frac{Supp(Pañales, Cerveza)}{Supp(Pañales)} = \\frac{2}{3}\\)❌ \\(Conf(Cerveza \\implies Pañales) = \\frac{Supp(Pañales, Cerveza)}{Supp(Cerveza)} = \\frac{2}{2}\\)✅\n\n\n\nPara {Pan, Mantequilla, Leche}:\n\n\\(Conf({Pan, Mantequilla} \\implies {Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Mantequilla)} = \\frac{2}{3}\\)❌ \\(Conf({Pan, Leche} \\implies {Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Leche)} = \\frac{2}{2}\\)✅ \\(Conf({Mantequilla, Leche} \\implies {Pan}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla, Leche)} = \\frac{2}{2}\\)✅\n\n\\(Conf({Leche} \\implies {Pan, Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\)✅ \\(Conf({Mantequilla} \\implies {Pan, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\)❌ \\(Conf({Pan} \\implies {Mantequilla, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan)} = \\frac{2}{3}\\)❌"
  },
  {
    "objectID": "tics411/clase-7.html#resultado-final",
    "href": "tics411/clase-7.html#resultado-final",
    "title": "TICS-411 Minería de Datos",
    "section": "Resultado Final",
    "text": "Resultado Final\n\n\nItemset MinSupp = 40%\n\n\n\n\n\n\nReglas Finales MinConf = 70%\n\\[Pan \\implies Mantequilla\\] \\[Mantequilla \\implies Pan\\] \\[Leche \\implies Pan\\] \\[Leche \\implies Mantequilla\\] \\[Cerveza \\implies Pañales\\] \\[\\{Pan, Leche\\} \\implies Mantequilla\\]\n\\[\\{Mantequilla, Leche\\} \\implies Pan\\] \\[Leche \\implies \\{Pan, Mantequilla\\}\\]\n\n\n\n\n\n\nInsights:\n\n\n\nEl Pan, la Leche y la Mantequilla están relacionados.\nParece ser que si llevo Cervezas también llevo Pañales."
  },
  {
    "objectID": "tics411/clase-7.html#evaluación-de-reglas-de-asociación",
    "href": "tics411/clase-7.html#evaluación-de-reglas-de-asociación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación de Reglas de Asociación",
    "text": "Evaluación de Reglas de Asociación\n\nLift\n\nMide qué tan lejos de la independencia están \\(X\\) e \\(Y\\). Lift varía entre 0 y \\(\\infty\\).\n\n\n\\[Lift(X,Y) = \\frac{Conf(X \\implies Y)}{s(Y)}\\]\n\n\\(Lift(X,Y) \\sim 1\\) implica independencia y la regla no es importante.\n\\(Lift(X,Y) &lt; 1\\) implica una asociación negativa de la regla.\n\\(Lift(X,Y) &gt; 1\\) implica una asociativa de la regla. Un mayor Lift implica que la regla es potencialmente útil para el futuro.\n\nEjemplo:\n\\[Lift(Cerveza, Pañales) = \\frac{Conf(Cerveza \\implies Pañales)}{Supp(Pañales)} = \\frac{1}{0.6} = 1.67\\]\n\n\n\n\n\n\nUna persona que compra Cerveza tiene 1.67 más chances de comprar Pañales."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-preprocesamiento",
    "href": "tics411/clase-7.html#implementación-en-python-preprocesamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Preprocesamiento",
    "text": "Implementación en Python: Preprocesamiento\nPre-procesamiento\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns = tre.columns_)\nL4: transactions debe ser una lista de listas. Cada fila, son distintas transacciones. Cada transaccion puede tener distinto número de elementos. L5: tre.columns_ extrae los nombres de los productos para que el DataFrame sea más entendible.\n\n\n\n\n\n\ndf_encoded es un DataFrame tipo OneHotEncoder pero con valores Booleanos (Esto es solicitado por la documentación)."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-itemsets",
    "href": "tics411/clase-7.html#implementación-en-python-itemsets",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Itemsets",
    "text": "Implementación en Python: Itemsets\nfrom mlxtend.frequent_patterns import apriori \n\nitemset = apriori(df_encoded, min_support=0.5, use_colnames = True)\nL3: df_encoded es el DataFrame preprocesado.\n\nmin_support: Corresponde al Soporte Mínimo para generar itemsets. Por defecto 0.5.\nuse_colnames: Permite que las reglas usen los nombres de las columnas para referirse a los productos. Por defecto es False, pero conviene usarlo como True.\nitemset será un DataFrame con los itemsets generados."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-reglas",
    "href": "tics411/clase-7.html#implementación-en-python-reglas",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Reglas",
    "text": "Implementación en Python: Reglas\nfrom mlxtend.frequent_patterns import association_rules\n\nrules = association_rules(itemsets, metric=\"confidence\", min_threshold=0.8)\nL3: itemset es el dataframe generado en el paso anterior.\n\nmetric: Métrica para definir reglas, puede ser “confidence” y otras definidas acá\nmin_threshold: Corresponde al umbral de la métrica a utilizar. Por defecto 0.8.\nrules corresponde a un Dataset que tiene las Reglas de Asociación detectadas y muchas métricas asociadas."
  },
  {
    "objectID": "tics411/clase-4.html#definiciones",
    "href": "tics411/clase-4.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nClustering Jerárquico\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender. Se basa en la construcción de Jerarquías para ir construyendo clusters.\n\n\nDendograma\n\n\nCorresponde a un diagrama en el que se muestran las distancias de atributos entre clases que son parte de un mismo cluster."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerarquía",
    "href": "tics411/clase-4.html#clustering-jerarquía",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Jerarquía",
    "text": "Clustering: Jerarquía\n\nLos algoritmos basados en jerarquía pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Algoritmo",
    "text": "Clustering Aglomerativo: Algoritmo\nAlgoritmo\n\nInicialmente se considera cada punto como un cluster.\nCalcula la matriz de proximidad/distancia entre cada cluster.\nRepetir (hasta que exista un solo cluster):\n\nUnir los cluster más cercanos.\nActualizar la matriz de proximidad/distancia.\n\n\n\n\n\n\n\n\nLo más importante de este proceso es el cálculo de la matriz de proximidad/distancia entre clusters\n\n\n\n\n\n\n\n\n\nDistintos enfoques de distancia entre clusters, segmentan los datos en forma distinta."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Ejemplo",
    "text": "Clustering Aglomerativo: Ejemplo\nSupongamos que tenemos cinco tipos de genes cuya expresión ha sido determinada por 3 caracteríticas. Las siguientes expresiones pueden ser vistas como la expresión dados los genes en tres experimentos. ​\n\nApliquemos un Clustering Jerárquico Aglomerativo utilizando como medida de similaridad la Distancia Euclideana.\n\n\n\n\n\n\n\nOtros tipos de distancia también son aplicables siguiendo un procedimiento análogo."
  },
  {
    "objectID": "tics411/clase-4.html#algoritmo-1era-iteración",
    "href": "tics411/clase-4.html#algoritmo-1era-iteración",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo: 1era Iteración",
    "text": "Algoritmo: 1era Iteración\n\n\n\n\n\n\nEl algoritmo considerará que todos los puntos inicialmente son un cluster. Por lo tanto, tratará de encontrar los 2 puntos más cercanos e intentará unirnos en un sólo cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblema: ¿Cómo actualizamos la matriz de Distancias?\n\n\n\n\n\n\n\n\n\n\n\nEntonces crearemos un nuevo cluster: bcl2-Caspade."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Single Linkage",
    "text": "Clustering Aglomerativo: Single Linkage\n\n\n\n\n\n\n\n\n\nDistancia entre clusters determinada por los puntos más similares entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = min\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nGenera Clusters largos y delgados.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nAfectado por Outliers"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Complete Linkage",
    "text": "Clustering Aglomerativo: Complete Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por la distancia ente los puntos más disímiles entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = max\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a quebrar Clusters Grandes.\nTiene tendencia a generar Clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Average Linkage",
    "text": "Clustering Aglomerativo: Average Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el promedio de las distancias que componen los clusters.\nPunto intermedio entre Single y Complete.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = avg\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a datos atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Ward Linkage",
    "text": "Clustering Aglomerativo: Ward Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el incremento del Within cluster distance.\nMinimiza la distancia intra cluster y maximiza la distancia entre clusters.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = wc(Cij) - wc(C_i) - wc(C_j) = \\frac{n_i\\cdot n_j}{n_i + n_j}||\\bar{C_i} - \\bar{C_j}||^2\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#hiperparámetros",
    "href": "tics411/clase-4.html#hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "Hiperparámetros",
    "text": "Hiperparámetros\nLos Hiperparámetros de este modelo serán:\n\n\n\n\n\n\nNote\n\n\n\nlinkage: La forma de calcular la distancia entre clusters.\ndistancia: La distancia utilizada como similaridad entre los clusters.\n\n\n\n\n\n\n\n\n\n\nA diferencia de K-Means, este método no requiere definir el número de Clusters a priori."
  },
  {
    "objectID": "tics411/clase-4.html#volvamos-a-la-iteración-1",
    "href": "tics411/clase-4.html#volvamos-a-la-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Volvamos a la Iteración 1",
    "text": "Volvamos a la Iteración 1\n\nSupongamos que por simplicidad utilizaremos Average Linkage. (El proceso para utilizar otro linkage es análogo).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos a extraer una Matriz entre los puntos a fusionar y los puntos de los clusters restantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 1era Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#iteración-2",
    "href": "tics411/clase-4.html#iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 2",
    "text": "Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 2da Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#iteración-3",
    "href": "tics411/clase-4.html#iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 3",
    "text": "Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 3ra Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#dendograma-resultante",
    "href": "tics411/clase-4.html#dendograma-resultante",
    "title": "TICS-411 Minería de Datos",
    "section": "Dendograma Resultante",
    "text": "Dendograma Resultante\n\n\n\n\n\n\nNo es necesario realizar la última iteración ya que se entiende que ambos clusters se unen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo encontramos los clusters una vez que tenemos el Dendograma?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos escoger un umbral de distancia y ver cuántos clusters se forman.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo regla general se deben escoger clusters más distanciados entre sí."
  },
  {
    "objectID": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "href": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "title": "TICS-411 Minería de Datos",
    "section": "Efecto del Linkage Escogido",
    "text": "Efecto del Linkage Escogido"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerárquico-detalles-técnicos",
    "href": "tics411/clase-4.html#clustering-jerárquico-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Jerárquico: Detalles Técnicos",
    "text": "Clustering Jerárquico: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nNo requiere definir el número de Clusters a priori.\nAl tener distintas variantes es posible que los puntos sean agrupados de manera completamente distintas.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nMuy ineficiente computacionalmente debido a que genera una nueva matriz de distancia en cada iteración lo que entrega una complejidad \\(O(n^2)\\) o \\(O(n^3)\\) dependiendo del linkage.\nUna vez que se decide combinar 2 clusters no es posible revertir esta decisión.\nNo tiene capacidad de generalización, ya que no es posible aplicarlo a datos nuevos."
  },
  {
    "objectID": "tics411/clase-4.html#implementación-en-scikit-learn",
    "href": "tics411/clase-4.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=2, metric=\"euclidean\",linkage=\"ward\")\n\n## Se entrena y se genera la predicción\nac.fit_predict(X)\n\n\nn_clusters: Define el número de clusters a crear, por defecto 2.\nmetric: Permite distancias L1, L2 y coseno. Por defecto “euclidean”.\nlinkage: Permite single, complete, average y ward. Por defecto “ward”.\n.fit_predict(): Entrenará el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento.\n\n\n\n\n\n\n\n\n\nSi bien el método de Aglomeración no requiere el número de clusters a generar, Scikit-Learn lo exige de modo de poder etiquetar cada elemento.\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué no existen los métodos .fit() y .predict() por separado?"
  },
  {
    "objectID": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "href": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "title": "TICS-411 Minería de Datos",
    "section": "Otras implementaciones (Dendograma)",
    "text": "Otras implementaciones (Dendograma)\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Genera los cálculos necesarios para construir el Histograma.\nZ = linkage(X, method='single', metric=\"euclidean\") \n\n# Graficar el Dendograma\nplt.figure(figsize=(10, 5)) # Define el tamaño del Gráfico\nplt.title('Dendograma Clustering Jerárquico') # Define un título para el dendograma\nplt.xlabel('Iris Samples')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\nplt.show()\n\n\nPrincipalmente este código permite graficar el Dendograma completo.\nL4: Genera una instancia del Dendograma. (Sería equivalente al .fit() de Scikit-Learn).\nL5-L12: Corresponde al código necesario para graficar el Dendograma."
  },
  {
    "objectID": "tics411/clase-4.html#sugerencias",
    "href": "tics411/clase-4.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que el clustering aglomerativo también es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala común.\n\n\n\n\n\n\n\n\n\n\nOtras técnicas como merge y split, no aplican a este tipo de clustering debido a las limitaciones del algoritmo."
  },
  {
    "objectID": "tics411/clase-4.html#variantes",
    "href": "tics411/clase-4.html#variantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes",
    "text": "Variantes\n\nEn casos en los que no es posible calcular distancias debido a la presencia de datos categóricos, es posible utilizar el Gower Dissimilarity como medida de similitud.\n\n\n\n\n\n\n\n\n\nGower\n\nSe define como la proporción de variables que tienen distinto valor con respecto al total sin considerar donde ambos son ceros.\n\n\n\n\\[Gower(p1,p2) = \\frac{3}{9}\\]"
  },
  {
    "objectID": "tics411/clase-3.html#definiciones",
    "href": "tics411/clase-3.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nAprendizaje No supervisado\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender.\n\n\n\n\n\n\n\n\n\nEn nuestro caso nos enfocaremos en un caso particular de Modelación Descriptiva llamada Clustering.\n\n\n\n\nClustering\n\n\nConsiste en agrupar los datos en un menor número de entidades o grupos. A estos grupos se les conoce como clusters y pueden ser generados de manera global, o modelando las principales características de los datos."
  },
  {
    "objectID": "tics411/clase-3.html#intuición",
    "href": "tics411/clase-3.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n¿Cuántos clusters se pueden apreciar?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-introducción",
    "href": "tics411/clase-3.html#clustering-introducción",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Introducción",
    "text": "Clustering: Introducción\n\n\n\n\n\n\nClustering: Consiste en buscar grupos de objetos tales que la similaridad intra-grupo sea alta, mientras que la similaridad inter-grupos sea baja. Normalmente la distancia es usada para determinar qué tan similares son estos grupos."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-evaluación",
    "href": "tics411/clase-3.html#clustering-evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Evaluación",
    "text": "Clustering: Evaluación\n\n\n\n\n\n\n\nEvaluar el nivel del éxito o logro del Clustering es complicado. ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-tipos",
    "href": "tics411/clase-3.html#clustering-tipos",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Tipos",
    "text": "Clustering: Tipos"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-partición",
    "href": "tics411/clase-3.html#clustering-partición",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Partición",
    "text": "Clustering: Partición\n\nLos datos son separados en K clusters, donde cada punto pertenece exclusivamente a un único cluster."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-densidad",
    "href": "tics411/clase-3.html#clustering-densidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-jerarquía",
    "href": "tics411/clase-3.html#clustering-jerarquía",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Jerarquía",
    "text": "Clustering: Jerarquía\n\nLos algoritmos basados en jerarquía pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-probabilístico",
    "href": "tics411/clase-3.html#clustering-probabilístico",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Probabilístico",
    "text": "Clustering: Probabilístico\nSe ajusta cada punto a una distribución de probabilidades que indica cuál es la probabilidad de pertenencia a dicho cluster."
  },
  {
    "objectID": "tics411/clase-3.html#partición",
    "href": "tics411/clase-3.html#partición",
    "title": "TICS-411 Minería de Datos",
    "section": "Partición",
    "text": "Partición\n\nLos datos son separados en K Clusters, donde cada punto pertenece exclusivamente a un único cluster. A K se le considera como un hiperparámetro.\n\n\n\n\n\n\n\n\nCluster Compactos: Minimizar la distancia intra-cluster (within cluster).\nClusters bien separados: Maximizar la distancia inter-cluster (between cluster).\n\n\n\n\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\nEl puntaje/score mide la calidad del clustering \\(C\\) para el Dataset \\(D\\)."
  },
  {
    "objectID": "tics411/clase-3.html#score",
    "href": "tics411/clase-3.html#score",
    "title": "TICS-411 Minería de Datos",
    "section": "Score",
    "text": "Score\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\n\n\n\n\nDistancia Between-Cluster: \\[bc(C) = \\sum_{1 \\le j \\le k \\le K} d(r_j, r_k)\\]\n\ndonde \\(r_k\\) representa el centro del cluster \\(k\\): \\[r_k = \\frac{1}{n_k} \\sum_{x_i \\in C_k} x_i\\]\n\n\nDistancia Within-Cluster (Inercia): \\[wc(C) = \\sum_{k=1}^K \\sum_{x_i \\in C_k} d(x_i, r_k)\\]\n\n\n\n\n\n\n\n\n\n\nDistancia entre los centros de cada cluster.\n\n\n\n\n\n\n\n\n\n\nDistancia entre todos los puntos del cluster y su respectivo centro."
  },
  {
    "objectID": "tics411/clase-3.html#k-means",
    "href": "tics411/clase-3.html#k-means",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means",
    "text": "K-Means\n\nK-Means\n\n\nDado un número de clusters \\(K\\) (determinado por el usuario), cada cluster es asociado a un centro (centroide). Luego, cada punto es asociado al cluster con el centroide más cercano.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utiliza la Distancia Euclideana como medida de similaridad.\n\nSe seleccionan \\(K\\) puntos como centroides iniciales.\nRepite:\n\nForma K clusters asignando todos los puntos al centroide más cercano.\nRecalcula el centroide para cada clase como la media de todos los puntos de dicho cluster.\n\n\n\nSe repite este procedimiento por un número finito de iteraciones o hasta que los centroides no cambien."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo",
    "href": "tics411/clase-3.html#k-means-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\nResolvamos el siguiente ejemplo.\nSupongamos que tenemos tipos de manzana, y cada una de ellas tiene 2 atributos (features). Agrupemos estos objetos en 2 grupos de manzanas basados en sus características."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-1",
    "href": "tics411/clase-3.html#k-means-ejemplo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n1era Iteración\n\n\n\n\nSupongamos los siguientes centroides iniciales: \\[C_1 = (1,1)\\] \\[C_2 = (2,1)\\]\n\n\n\n\n\n\n\n\n\nMatriz de Distancias al Centroide: (coordenada i,j representa distancia del punto j al centroide i)\n\n\n\n\n\n\\[D^1 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n1 & 0 & 2.83 & 4.24\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son: \\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-2",
    "href": "tics411/clase-3.html#k-means-ejemplo-2",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n2da Iteración\n\n\n\n\nLos nuevos centroides son:\n\n\\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]\n\n\n\nCalculamos la Matriz de Distancias al Centroide:\n\n\n\n\\[D^2 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n3.14 & 2.26 & 0.47 & 1.89\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^2 = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son:\n\\(C_1 = (\\frac{3}{2}, 1)\\) y \\(C_2 = (\\frac{9}{2}, \\frac{7}{2})\\)"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-3",
    "href": "tics411/clase-3.html#k-means-ejemplo-3",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\nSi seguimos iterando notaremos que ya no hay cambios en los clusters. El algoritmo converge.\nEste es el resultado de usar \\(K=2\\). Utilizar otro valor de \\(K\\) entregará valores distintos.\n¿Es este el número de clusters óptimos?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-número-de-clusters-óptimos",
    "href": "tics411/clase-3.html#k-means-número-de-clusters-óptimos",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Número de Clusters Óptimos",
    "text": "K-Means: Número de Clusters Óptimos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSiempre es posible encontrar el número de clusters indicados.\nEntonces,\n\n¿Cómo debería escoger el valor de \\(K\\)?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-número-de-clusters-óptimos-1",
    "href": "tics411/clase-3.html#k-means-número-de-clusters-óptimos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Número de Clusters Óptimos",
    "text": "K-Means: Número de Clusters Óptimos\n\nCurva del Codo\n\nEs una heurísitca en la cual gráfica el valor de una métrica de distancia (e.g. within distance) para distintos valores de \\(K\\). El valor óptimo de \\(K\\) será el codo de la curva, que es el valor donde se estabiliza la métrica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste valor del codo muchas veces es subjetivo y distintas apreciaciones pueden llegar a distintos \\(K\\) óptimos.\n\n\n\n\n\n\n\n\n\n\nEventualmente otras métricas distintas al within cluster distance podrían también ser usadas.\n\n\n\n\n\n\n\n\n\n¿Cuál es el efecto que está buscando la curva del codo? ¿Qué implica el valor de K escogido?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-detalles-técnicos",
    "href": "tics411/clase-3.html#k-means-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Detalles Técnicos",
    "text": "K-Means: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nAlgoritmo relativamente eficiente (\\(O(k \\cdot n \\cdot i)\\)). Donde \\(k\\) es el número de clusters, \\(n\\) el número de puntos, e \\(i\\) el número de iteraciones.\nEncuentra “clusters esféricos”.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nSensible al punto de inicio.\nSolo se puede aplicar cuando el promedio es calculable.\nSe requiere definir K a priori (K es un hiperparámetro).\nSuceptible al ruido y a mínimos locales (podría no converger)."
  },
  {
    "objectID": "tics411/clase-3.html#implementación-en-scikit-learn",
    "href": "tics411/clase-3.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=8, n_init=10,random_state=None)\nkm.fit(X)\nkm.predict(X)\n\n## opcionalmente\nkm.fit_predict(X)\n\n\nn_clusters: Define el número de clusters a crear, por defecto 8.\nn_init: Cuántas veces se ejecuta el algoritmo, por defecto 10.\nrandom_state: Define la semilla aleatoria. Por defecto sin semilla.\ninit: Permite agregar centroides de manera manual.\n.fit(): Entrenará el modelo en los datos suministrados.\n.predict() Entregará las clusters asignados a cada dato suministrado.\n.clusters_centers_: Entregará las coordenadas de los centroides de cada Cluster.\n.inertia_: Entrega valores correspondiente a la within cluster distance.\n\n\n👀 Veamos un ejemplo en Colab."
  },
  {
    "objectID": "tics411/clase-3.html#sugerencias",
    "href": "tics411/clase-3.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que K-Means es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala común."
  },
  {
    "objectID": "tics411/clase-3.html#interpretación-clusters",
    "href": "tics411/clase-3.html#interpretación-clusters",
    "title": "TICS-411 Minería de Datos",
    "section": "Interpretación Clusters",
    "text": "Interpretación Clusters\n\n\n\n\n\n\nRecordar, que el clustering no clasifica. Por lo tanto, a pesar de que K-Means nos indica a qué cluster pertenece cierto punto, debemos interpretar cada cluster para entender qué es lo que se agrupó.\n\n\n\n\n\n\n\n\n\nLa interpretación del cluster es principalmente intuición y exploración, por lo tanto el EDA puede ser de utilidad para analizar clusters."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge",
    "href": "tics411/clase-3.html#post-procesamiento-merge",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\nPost-Procesamiento\n\n\nSe define como el tratamiento que podemos realizar al algoritmo luego de haber entregado ya sus predicciones.\n\n\n\nEs posible generar más clusters de los necesarios y luego ir agrupando los más cercanos."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge-1",
    "href": "tics411/clase-3.html#post-procesamiento-merge-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el problema con este caso de Post-Procesamiento?"
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-split",
    "href": "tics411/clase-3.html#post-procesamiento-split",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Split",
    "text": "Post-Procesamiento: Split\n\n\n\n\n\n\n\n\n\n\n\nEn Scikit-Learn esto puede conseguirse utilizando el parámetro init. Se entregan los nuevos centroides para forzar a K-Means que separe ciertos clusters."
  },
  {
    "objectID": "tics411/clase-3.html#variantes-k-means",
    "href": "tics411/clase-3.html#variantes-k-means",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes K-Means",
    "text": "Variantes K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SMD(p_1,p_2) = 4\\]\n\n\n\n\n\n\n\nAcá pueden encontrar una implementación de K-Modes en Python."
  },
  {
    "objectID": "tics411/clase-1.html#avisos-1",
    "href": "tics411/clase-1.html#avisos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Avisos",
    "text": "Avisos\n\n\n\n\n\n\nAyudantía\n\n\nAvisos\nTenemos (posible) ayudante, pero tenemos un problema de horario.\n\nHorario Actual: Viernes 20:00 a 21:10 hrs.\nHorario Propuesto: Lunes 11:45 a 12:55 hrs.\n\n\n\n\n\n\n\n\n\n\n\n\nTarea 1\n\n\n\nEntrega el 7 de Abril: Parejas inscribirse en Webcursos.\nPlazo para inscribir parejas: Este Domingo.\n\n\n\n\n\n\n\n\n\n\n\n\nFechas de Prueba\n\n\n\nPrueba 1: Martes 30 de Abril 18:30 a 21:00\nPrueba 2: Martes 11 de Julio 18:30 a 21:00"
  },
  {
    "objectID": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, registros, instancias. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos más amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo.\nDistintos tipos, por lo que normalmente requiere de algún tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-numéricos",
    "href": "tics411/clase-1.html#data-types-numéricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Numéricos",
    "text": "Data Types: Numéricos\n\nNuméricos\n\n\nValores a los que se les puede aplicar alguna operación matemática.\n\n\n\n\n\n\n\n\n\n\nDiscretas: Número finito o contable de valores. Integers (Enteros). Ej: Número de Hijos, Cantidad de Productos, Edad.\nContinuas: Existen infinitos puntos entre dos puntos. Floats (punto flotando o decimales). Ej. Temperatura, Peso."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-categóricos",
    "href": "tics411/clase-1.html#data-types-categóricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Categóricos",
    "text": "Data Types: Categóricos\n\nCategóricos\n\n\nDatos que representan una categoría.\n\n\n\n\n\n\n\n\n\n\nNominales: Sólo nombres que no representan ningún orden. Ej: Nacionalidad, género, ocupación.\nOrdinales: Que tienen un orden o jerarquía inherente. Ej: Nivel de Escolaridad, tamaño.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo todas las operaciones matemáticas son aplicables. Ej: Media, Mediana, Sumas, Restas, etc."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-otros",
    "href": "tics411/clase-1.html#data-types-otros",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Otros",
    "text": "Data Types: Otros\n\nStrings\n\n\nDatos de texto, los cuales podrían eventualmente ser tratados y representar algo. Ej: Rescatar comunas de una dirección, rescatar sexo desde el nombre, etc.\n\n\nFechas\n\n\nDatos tipo fecha, los cuales podrían eventualmente ser tratados y representar variables de algún tipo. Ej: Rescatar Años, meses, días, semanas, trimestres (quarters), etc.\n\n\nDatos Geográficos\n\n\nDatos que representan la ubicación geográfica de un elemento. Ej: Latitud, Longitud, Coordenadas.\n\n\n\n\n\n\n\n\n\n\nSin importar el tipo de dato el mayor problema es su calidad."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "href": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Ruido",
    "text": "Calidad de los Datos: Ruido\n\nRuido\n\nCorresponde al error y extrema variabilidad en la medición en los datos. Este error puede ser aleatorio o sistemático.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe le llama Señal a la tendencia principal y representa la información significativa y valiosa de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "href": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Outliers",
    "text": "Calidad de los Datos: Outliers\n\nOutliers\n\nSon datos considerablemente diferentes a la mayoría del dataset. Dependiendo del caso pueden indicar casos \"interesantes\" o errores de medición.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que dependiendo del caso puede ser una buena idea deshacerse de ellos. ¿En qué casos podría no ser necesario eliminarlos?"
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "href": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Valores Faltantes",
    "text": "Calidad de los Datos: Valores Faltantes\n\nMissing Values\n\n\nSon valores que por alguna razón no están presentes.\n\n\n\n\nMissing at Random (MAR): Son valores que no están presentes por causas que no se pueden controlar. Ej: No se registró, no se preguntó, fallas en el sistema de recolección de datos, etc.\nInformative Missing: Es un valor no aplicable. Ej: Sueldo en niños, Precio de la entrada de un concierto si es que NO compró entrada."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "href": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Datos Duplicados",
    "text": "Calidad de los Datos: Datos Duplicados\n\nDuplicates\n\nSe refiere a registros que pueden estar total o parcialmente duplicados.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsto genera problemas en la confiabilidad de los datos. ¿Cuál es el registro correcto?\nEj: Caso particular de una Jooycar (una startup de seguros)."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "href": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Dominio del Problema",
    "text": "Calidad de los Datos: Dominio del Problema\n\n\n\n\n\n\n\n\n\n\n\n\n\nPor lejos el problema de calidad más difícil de encontrar.\nSe requiere experiencia y conocimiento profundo del negocio para detectarlo.\n\nEj: Caso de Super Avances en Cencosud."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-1",
    "href": "tics411/clase-1.html#feature-engineering-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nFeature Engineering\n\n\nTambién conocida como Ingeniería de Atributos, es el arte de trabajar las features existentes para limpiar o corregir variables existentes o crear nuevas variables.\n\n\nPreprocesamiento\n\n\nSe refiere al proceso de preparación de los datos para su ingreso a un modelo. En una primera parte puede incluir limpieza de datos corruptos, redundantes y/o irrelevantes. Por otra parte, también hace referencia a la transformación de datos para que puedan ser consumidos por un algoritmo."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-2",
    "href": "tics411/clase-1.html#feature-engineering-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nNo existe un procedimiento estándar.\nRevisar los datos y ver potenciales errores que puedan afectar el funcionamiento de un modelo."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "href": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Valores Faltantes",
    "text": "Preprocesamiento: Valores Faltantes\n\nImputación: Se refiere al proceso de rellenar datos faltantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependiendo del nivel de valores faltantes, es necesario evaluar la eliminación de registros o atributos completos de ser necesario."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Manejo de Outliers",
    "text": "Preprocesamiento: Manejo de Outliers\n\nCapping\n\nSe refiere al proceso de acotar un atributo eliminando los valores extremos o atípicos (outliers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAl igual que en el caso anterior, es necesario evaluar la eliminación de registros si es que representan valores atípicos."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categóricas",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categóricas",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Manejo de Variables Categóricas",
    "text": "Preprocesamiento: Manejo de Variables Categóricas\n\nLa mayoría de los modelos no tienen la capacidad de poder lidiar con variables categóricas por lo que deben ser transformadas en una representación numérica antes de ingresar a un modelo.\n\n\n\n\n\n\nOne Hot Encoder\n\n\n\n\n\n\nOrdinal Encoder\n\n\n\n\n\n\n\n\n\n\n\nOne Hot Encoder suele dar mejores resultados en modelos lineales modelos que dependan de distancias.\nOrdinal Encoder suele dar mejores resultados en modelos de árbol.\n\n\n\n\n\n\n¿Son necesarias todas las columnas en un One Hot Encoder?"
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\nEl escalamiento se refiere al proceso de llevar distintas variables a una misma escala.\n\n\n\n\n\n\n\n\n\nEvitar que la escala de una “sobre-importancia” a una cierta variable.\nPermitir una mejor convergencia de los algoritmos.\n\n\nStandardScaler (Normalización)\n\\[x_j=\\frac{x_j-\\mu_x}{\\sigma_x}\\]\n\n\n\n\n\n\n\nEste proceso fuerza (en la medida de lo posible) a tener media 0 y std 1.\nNotar que \\(\\sigma_x\\) hace referencia a la varianza poblacional.\n\n\n\n\nMinMax Scaler\n\\[x_j=\\frac{x_j-min(x_j)}{max(x_j)-min(x_j)}\\]\n\n\n\n\n\n\nEste proceso fuerza a los datos a distribuirse entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia: 0.75\nStd: 3.1875\nMin: -2\nMax: 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentering (Centrado): Se le llama a la diferencia entre la variable y su media.\nScaling (Escalado): Se le llama al cuociente entre la variable y su Desviación Estándar.\nStandardScaler (Normalización): Es Centrado y Escalado."
  },
  {
    "objectID": "tics411/clase-1.html#creación-de-variables",
    "href": "tics411/clase-1.html#creación-de-variables",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de Variables",
    "text": "Creación de Variables\n\nCombinación\n\n\nCombinar 2 o más variables. Ej: Calcular el área de un sitio a partir del ancho y largo.\n\n\nTransformación\n\n\nAplicar una operación a una variable. Ej: El logaritmo de las ganancias.\n\n\n\n\n\n\nDiscretización (Binning)\n\n\nGenerar categorías a partir de una variable continua."
  },
  {
    "objectID": "tics411/clase-1.html#creación-de-variables-1",
    "href": "tics411/clase-1.html#creación-de-variables-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de Variables",
    "text": "Creación de Variables\n\nRatios\n\nEs una medida que expresa la relación entre dos cantidades. Ej: Puntos por partido, cantidad de transacciones por mes, etc.\n\nAgregación\n\nAgregar o agrupar información resumida de ciertas variables. Ej: Promedio de tiempo en aprobar un tipo de crédito."
  },
  {
    "objectID": "tics411/clase-1.html#selección-de-variables",
    "href": "tics411/clase-1.html#selección-de-variables",
    "title": "TICS-411 Minería de Datos",
    "section": "Selección de Variables",
    "text": "Selección de Variables\n\nSe refiere al proceso de eliminar variables que pueden ser irrelevantes o poco significativas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcesos Manuales.\nProcesos Automáticos:\n\nPCA (Principal Component Analysis).\nRecursive Feature Elimination.\nRecursive Feature Addition.\nEliminación mediante alguna medida.\n\n\n\n\n\n\n\n\n\n\n\n\nObjetivo\n\n\n\nPuede ser una técnica apropiada para combatir la Maldición de la Dimensionalidad (Curse of Dimensionality)."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-1",
    "href": "tics411/clase-1.html#medidas-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas",
    "text": "Medidas\n\nSon métricas que permiten cuantificar la relación existente entre dos o más objetos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad",
    "href": "tics411/clase-1.html#medidas-similaridad",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad",
    "text": "Medidas: Similaridad"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-nominal",
    "href": "tics411/clase-1.html#medidas-similaridad-nominal",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Nominal",
    "text": "Medidas: Similaridad Nominal\n\n\n\nDisimilaridad: \\[D =\n\\begin{cases}\n0,  & \\text{if $p=q$} \\\\[2ex]\n1, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\nSimilaridad:\n\\[S =\n\\begin{cases}\n1,  & \\text{if $p=q$} \\\\[2ex]\n0, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 0\\] \\[D(p,q) = 1\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "href": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Ordinal",
    "text": "Medidas: Similaridad Ordinal\n\n\n\nDisimilaridad: \\[D = \\frac{|p-q|}{n}\\]\n\n\n\nSimilaridad:\n\\[S = 1 - \\frac{|p-q|}{n}\\]\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 1 - \\frac{5 - 4}{5} = 0.8\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "href": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Intervalo o Ratio",
    "text": "Medidas: Similaridad Intervalo o Ratio\n\n\n\nDisimilaridad: \\[D = |p-q|\\]\n\n\n\nSimilaridad:\n\\[S = -D\\] \\[S = \\frac{1}{1+D}\\]\n\n\n\nSea \\(p=35 °C\\) y \\(q = 40 °C\\). Luego:\n\\[ S(p,q) = -5\\] \\[S(p,q) = \\frac{1}{1 + 5} = 0.17\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos\n\nSea p y q vectores de dimensión \\(m\\) con sólo atributos categóricos. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\\[Sim(p,q) = \\sum_{i=1}^m S(p_i,q_i)\\]\n\n\n\n\nOverlap: \\[S(p_{a_i}, q_{a_i}) =\n\\begin{cases}\n1,  & \\text{if $p_{a_i} = q_{a_i}$} \\\\[2ex]\n0, & \\text{if $p_i\\neq q_i$}\n\\end{cases}\n\\]\n\n\n\nFrecuencia de Ocurrencia Inversa \\[S(p_i, q_i) = \\frac{1}{p_k(p_i)^2}\\]\n\n\n\nMedida de Goodall\n\n\\[S(p_i, q_i) = 1 - p_k(p_i)^2\\]\n\n\n\n\n\n\n\n\n\\(p_k()\\) se refiere a la probabilidad de ocurrencia del atributo k.\nTodas estas medidas son 0 si \\(p_i \\neq q_i\\)"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos\n\n\n\n\n\nEjercicio Propuesto: ¿Cuánto vale la similaridad entre los siguientes registros?\n\n1-4\n2-5\n7-8"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\nSea p y q vectores de dimensión \\(m\\) con sólo atributos binarios. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}}\\]\n\nSimple Matching Coefficient = Número de Coincidencias / Total de Atributos\n\n\n\\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}}\\]\n\nJaccard Coefficient = Número de Coincidencias 11 / Número de Atributos distintos de Ceros."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\np_i\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nq_i\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}} = \\] \\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}} = \\]\n\n\n\\[\\frac{7 + 0}{7 + 2 + 1 + 0} = 0.7\\]\n\n\n\\[\\frac{0}{2 + 1 + 0} = 0\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\nSean \\(d_1\\) y \\(d_2\\) dos vectores. La distancia coseno se calcula como:\n\n\\[cos(d_1, d_2) = \\frac{d_1 \\cdot d_2}{||d_1||||d_2||}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\nEjercicio Propuesto: ¿Cuánto vale \\(cos(d_1,d_2)\\) y \\(cos(d_1,d_3)\\)?"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-1",
    "href": "tics411/clase-1.html#distancias-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias",
    "text": "Distancias\n\nUna métrica o función de distancia es una función que define una distancia para cada par de elementos de un conjunto. Sean dos puntos x e y, una métrica o función de distancia debe satisfacer las siguientes condiciones:\n\n\nNo Negatividad:\n\n\\(d(x,y) = \\ge 0\\)\n\nIdentidad:\n\n\\(d(x,y) = 0 \\Leftrightarrow x = y\\)\n\nSimetría:\n\n\\(d(x,y) = d(y,x)\\)\n\nDesigualdad Triangular:\n\n\\(d(x,z) \\le d(x,y) + d(y,z)\\)"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Minkowski",
    "text": "Distancias: Distancia Minkowski\n\\[d(p,q) = \\left(\\sum_{k=1}^m |p_k - q_k|^r\\right)^{1/r}\\]\n\n\n\n\n\n\n\n\n\n\\(r=1 \\rightarrow\\) Distancia Manhattan (L1).\n\\(r=2 \\rightarrow\\) Distancia Euclideana (L2).\n\\(r=\\infty \\rightarrow\\) Distancia Chebyshev (L\\(\\infty\\)). \\[D_{ch}(p,q) = \\underset{k}{max} |p_k - q_k|\\]\n\n\n\n\n\n\n\nResolvamos en Colab\n\n\n\n\n\n\n\n\n\n\n\nSe denomina Matriz de Distancias a la Matriz que contiene la distancia \\(d(p_i,p_j)\\) en la coordenada \\(i,j\\)."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Minkowski (Resultados)",
    "text": "Distancias: Distancia Minkowski (Resultados)"
  },
  {
    "objectID": "tics411/clase-1.html#ayudantías",
    "href": "tics411/clase-1.html#ayudantías",
    "title": "TICS-411 Minería de Datos",
    "section": "Ayudantías",
    "text": "Ayudantías\nAyudante: Sofía Alvarez\nemail: sofalvarez@alumnos.uai.cl\n\n\n\n\n\n\n\nLas ayudantías serán en la manera que sean necesarias.\nEstarán enfocadas principalmente en aplicaciones, código y dudas sobre Tarea."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Mahalanobis",
    "text": "Distancias: Distancia Mahalanobis\n\\[d(p,q) = \\sqrt{(p-q)^T \\Sigma^{-1}(p-q)}\\]\ndonde \\(\\Sigma\\) es la Matriz de Covarianza de los datos de entrada.\n\\[cov(x,y) = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nPara 2 variables p y q:\n\n\\[\\Sigma = \\begin{bmatrix}\ncov(p,p) & cov(p,q) \\\\\ncov(q,p) & cov(q,q)\n\\end{bmatrix}\n\\]\nEjercicio: Supongamos las siguientes escalas de notas. Calcular la distancia entre la nota (1.0 y 7.0)\n\ntest #1: 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0\ntest #2: 1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Mahalanobis (Resultados)",
    "text": "Distancias: Distancia Mahalanobis (Resultados)\n\n\n\n\n\n\ntest #1: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{3.79}(7-1)} = 3.08\\)\ntest #2: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{1.59}(7-1)} = 4.76\\)\n\n\n\n\n\n\n\n\n\n\nEs importante notar que la covarianza existente entre los datos influye en la distancia."
  },
  {
    "objectID": "tics411/clase-1.html#correlación-1",
    "href": "tics411/clase-1.html#correlación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación",
    "text": "Correlación\n\nLa correlación mide la relación lineal entre 2 atributos.\n\n\n\n\nCorrelación Poblacional\n\n\\[\\rho(X,Y) = corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}\\]\n\n\n\n\nCorrelación Muestral o Pearson\n\n\\[r(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{S_xS_y}\\]"
  },
  {
    "objectID": "tics411/clase-1.html#correlación-no-es-causalidad",
    "href": "tics411/clase-1.html#correlación-no-es-causalidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación no es Causalidad",
    "text": "Correlación no es Causalidad\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante recalcar que Causalidad no es igual a Correlación. Ver video.\nLa Correlación no se ve afectada por la escala de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-2",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\n\\[d_1 \\cdot d_2 = 5\\] \\[d_1 \\cdot d_3 = 84\\]\n\\[||d_1|| = \\sqrt{42} = 6.481\\] \\[||d_2|| = \\sqrt{6} = 2.449\\] \\[||d_3|| = \\sqrt{168} = 12.962\\]\n\\[cos(d_1, d_2) = 0.3150\\] \\[cos(d_1, d_3) = 0.9999\\]"
  },
  {
    "objectID": "tics411/clase-12.html#intuición",
    "href": "tics411/clase-12.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\nSupongamos el siguiente dataset:\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo puedo separar ambas clases?"
  },
  {
    "objectID": "tics411/clase-12.html#intuición-1",
    "href": "tics411/clase-12.html#intuición-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición"
  },
  {
    "objectID": "tics411/clase-12.html#intuición-2",
    "href": "tics411/clase-12.html#intuición-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n\n\n\n\n\n\n\n\n\n\n\n\nLa frontera de decisión se puede caracterizar como la ecuación de una recta (en forma general).\n\n\n\n\n\n\n\n\n\n\n\nAdemás definiremos \\(h_\\theta(X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2\\)."
  },
  {
    "objectID": "tics411/clase-12.html#intuición-3",
    "href": "tics411/clase-12.html#intuición-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n\n\n\n\n\n\n\n\n\n\n\nPodríamos pensar que si \\(h_\\theta(X)\\) es positivo entonces pertenece a la clase 1 y si \\(h_\\theta(X)\\) es negativo pertenece a la clase 0."
  },
  {
    "objectID": "tics411/clase-12.html#la-función-sigmoide-o-logística",
    "href": "tics411/clase-12.html#la-función-sigmoide-o-logística",
    "title": "TICS-411 Minería de Datos",
    "section": "La Función Sigmoide o Logística",
    "text": "La Función Sigmoide o Logística\n\\[ g(z) = \\frac{1}{1 + e^{-z}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunción no lineal.\nFunción acotada entre 0 y 1.\n\\(g(\\varepsilon) = 0.5\\), \\(\\varepsilon = 0\\)\n\n\n\n\n\n\n\n\n\n\n\nDe acá sale la noción del umbral 0.5 que hemos visto en clases anteriores.\n\n\n\n\n\n\n\n\n\n\n\n¿Qué pasaría si ahora decimos que \\(z = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2\\)?"
  },
  {
    "objectID": "tics411/clase-12.html#la-regresión-logística",
    "href": "tics411/clase-12.html#la-regresión-logística",
    "title": "TICS-411 Minería de Datos",
    "section": "La Regresión Logística",
    "text": "La Regresión Logística\n\\[P[y = 1|X, \\theta] = g(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2)}}\\]\n\n\n\n\n\n\n\nRegla de Decisión:\n\n\n\nSi \\(g(z) \\ge 0.5 \\implies Clase \\, 1\\).\nSi \\(g(z) &lt; 0.5 \\implies Clase \\, 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(g(z)\\) se puede interpretar como una probabilidad de pertenecer a la Clase 1.\n\n\n\n\n\n\n\n\n\n\n\n\\(1 -g(z)\\) se puede interpretar como una probabilidad de NO pertenecer a la Clase 1, es decir, pertenecer a la Clase 0."
  },
  {
    "objectID": "tics411/clase-12.html#aprendizaje-del-modelo",
    "href": "tics411/clase-12.html#aprendizaje-del-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Aprendizaje del Modelo",
    "text": "Aprendizaje del Modelo\nSupongamos lo siguiente:\n\n\n\\[P(y = 1| X, \\theta) = g(z)\\]\n\n\\[P(y = 0| X, \\theta) = 1-g(z)\\]\n\nAmbas ecuaciones pueden comprimirse en una sola de la siguiente manera: \\[ P(y|X,\\theta) = g(z)^y (1 - g(z))^{1-y}\\]\n\n\n\n\n\n\nPara encontrar los parámetros \\(\\theta\\) podemos utilizar una técnica llamada Maximum Likelihood Estimation."
  },
  {
    "objectID": "tics411/clase-12.html#maximum-likelihood-estimation",
    "href": "tics411/clase-12.html#maximum-likelihood-estimation",
    "title": "TICS-411 Minería de Datos",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\\[\\mathcal{L}(\\theta) = \\prod_{i=1}^n P(y^{(i)} | x^{(i)}, \\theta)\\]\n\\[ \\underset{\\theta}{argmin} \\ -l(\\theta)\\] \\[l(\\theta) = log (\\mathcal{L(\\theta)}) = \\sum_{i=1}^n y^{(i)} \\cdot log(g(z)) + (1-y^{(i)})\\cdot log(1-g(z))\\]\n\n\n\n\n\n\nEsta ecuación se conoce como Entropía Cruzada o como Negative Log Loss (NLL) y tiene la gracia de que es una curva convexa lo que garantiza un valor único de los parámetros \\(\\theta\\)."
  },
  {
    "objectID": "tics411/clase-12.html#cálculo-de-coeficientes",
    "href": "tics411/clase-12.html#cálculo-de-coeficientes",
    "title": "TICS-411 Minería de Datos",
    "section": "Cálculo de Coeficientes",
    "text": "Cálculo de Coeficientes\n\n\n\n\n\n\nLa técnica más famosa para minimizar este tipo de problemas se conoce como Stochastic Gradient Descent. Lo que genera la siguiente solución:\n\n\n\n\\[\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{1}{n}\\sum_{i=1}^n\\left(g(z)-y^{(i)}\\right)x_j^{(i)}\\]\n\n\n\n\n\n\nA pesar de lo complicado que se ve la ecuación, implementarla en código es bastante sencillo."
  },
  {
    "objectID": "tics411/clase-12.html#frontera-de-decisión",
    "href": "tics411/clase-12.html#frontera-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Frontera de Decisión",
    "text": "Frontera de Decisión"
  },
  {
    "objectID": "tics411/clase-12.html#inference-time",
    "href": "tics411/clase-12.html#inference-time",
    "title": "TICS-411 Minería de Datos",
    "section": "Inference Time",
    "text": "Inference Time\nEn este caso se calcula: \\[g_\\theta(x^{(i)})=sigmoid(\\theta^t x^{(i)})\\]\n\n\\(\\theta\\): Corresponde a un vector con todos los parámetros calculados.\n\\(x^{(i)}\\): Corresponde a una instancia de \\(m\\) variables la cual generará una probabilidad.\n\n\\(\\theta^t x^{(i)}\\) corresponde al producto punto de dos vectores, que es equivalente a una “suma producto”.\n\n\\(g_\\theta(x^{(i)})\\): Generará un valor entre 0 y 1 al cuál se le aplica la Regla de Decisión."
  },
  {
    "objectID": "tics411/clase-12.html#implementación-en-python",
    "href": "tics411/clase-12.html#implementación-en-python",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python",
    "text": "Implementación en Python\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=1, penalty=\"l2\", random_state = 42)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\ny_proba = lr.predict_proba(X_test)\n\n## Visualizacion de los Parámetros \nlr.coef_\nlr.intercept_\n\n\nC: Corresponde a un parámetro de Regularización. Valores más pequeños implica mayor regularización. Por defecto 1.\npenalty: Corresponde al tipo de regularización. Por defecto “l2”.\n\n“l1”: Corresponde a la regularización Lasso. Genera que hayan parámetros cero, ayudando en la selección de variables.\n“l2”: Corresponde a la regularización Ridge. Genera que todos los parámetros sean pequeños, entregando estabilidad y buena interpretabilidad.\n“elasticnet”: Corresponde a la combinación de “l1” y “l2”.\nNone: No hay regularización.\n\n\n\n\n\n\n\n\n\nPara cambiar la regularización, consultar la documentación de Scikit-Learn."
  },
  {
    "objectID": "tics411/clase-12.html#interpretabilidad",
    "href": "tics411/clase-12.html#interpretabilidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Interpretabilidad",
    "text": "Interpretabilidad\n\nUna de las grandes ventajas que tiene la Regresión Logística es que sus predicciones son interpretables.\n\n\nTenemos un dataset de 2 variables:\n\nW: Corresponde al peso del Vehículo.\nqsec: Corresponde al tiempo en Segundos que lo toma en recorrer un cuarto de milla.\n\nQueremos predecir si el vehículo es Ecónomico o no (en términos de consumo de Bencina).\n\n\\[g_\\theta(x) = 0.5 - 3.5 \\cdot W + 1.5 \\cdot qsec \\]\n\n\n\n\n\n\n\n\nSi el vehículo se demora más en el cuarto de milla (qsec aumenta) entonces el vehículo es más económico.\n\nTiene menos potencia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi el vehículo es más pesado (W aumenta), entonces es menos económico.\n\nRequiere probablemente más combustible para mover dicho peso.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl valor del parámetro representa también la magnitud de la contribución."
  },
  {
    "objectID": "tics411/clase-12.html#sugerencias",
    "href": "tics411/clase-12.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\n\nEstandarización/Normalización de datos: Permite que la escala de los datos no afecte en la interpretabilidad.\nOne Hot Encoder: En general tiende a dar mejores resultados que el Ordinal.\nInteracciones: Combinación de variables.\nVariables no Lineales: Permite que la frontera de Decisión no sea necesariamente lineal (Regresión Polinómica)."
  },
  {
    "objectID": "tics411/clase-8.html#introducción-al-aprendizaje-supervisado",
    "href": "tics411/clase-8.html#introducción-al-aprendizaje-supervisado",
    "title": "TICS-411 Minería de Datos",
    "section": "Introducción al Aprendizaje Supervisado",
    "text": "Introducción al Aprendizaje Supervisado\nLos modelos Predictivos/Supervisados tienen la capacidad de predecir valores en datos no vistos.\n\nChip Huyen, Designing ML Systems (Profesora de Stanford)\n\n\n“Machine Learning Algorithms do not predict the future but encode the past, thus perpetuating the biases in the data and mode…”\n\n\n\nAprenden mediante un proceso de entrenamiento en un train set y evalúan su performance/rendimiento utilizando un test set."
  },
  {
    "objectID": "tics411/clase-8.html#definiciones",
    "href": "tics411/clase-8.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nFeatures\n\n\nTambién llamadas variables o atributos. Corresponden al input del Modelo y con el cuál el modelo aprende y predice. Normalmente es representado mediante una Matriz denominada \\(X\\).\n\n\nLabels o Etiquetas\n\n\nCorresponden a las respuestas que el modelo necesita mapear para poder descubrir patrones de manera automática. Normalmente se representa mediante un vector denominado \\(y\\)."
  },
  {
    "objectID": "tics411/clase-8.html#ejemplo",
    "href": "tics411/clase-8.html#ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nQueremos generar un algoritmo de aprendizaje tal que dado un cierto set de datos predigamos si es que a un niño se le dará o no permiso para jugar.\n\n\n\n\n\n\n\nProblema de Clasificación Binaria (Dos clases opuestas)."
  },
  {
    "objectID": "tics411/clase-8.html#definición-del-problema",
    "href": "tics411/clase-8.html#definición-del-problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Definición del Problema",
    "text": "Definición del Problema\n\\[h_\\theta(X) = f(X, \\theta)\\]\n\n\n\n\n\n\n\nA \\(h_\\theta(\\cdot)\\) la denominaremos hipótesis o simplemente modelo.\n\\(X\\) será nuestro set de features (\\(n\\times m\\) donde \\(n\\) es el número de observaciones y \\(m\\) el número de features).\n\nCada fila de \\(X\\) corresponde a un vector \\(x_i\\) que representa una observación de nuestro set de features.\n\\(\\theta\\) corresponde a los parámetros del modelo (existen modelos paramétricos y no paramétricos).\nCada algoritmo tendrá su propio mapeo \\(f(\\cdot)\\) para tratar de predecir una etiqueta.\n\n\n\n\n\n\n\n\n\n\nTipos de Hipótesis\n\n\n\nSi \\(h_\\theta(X)\\) devuelve valores discretos (o categóricos) hablaremos de un modelo de Clasificación.\nSi \\(h_\\theta(X)\\) devuelve valores continuos hablaremos de un modelo de Regresión."
  },
  {
    "objectID": "tics411/clase-8.html#tipos-de-problemas",
    "href": "tics411/clase-8.html#tipos-de-problemas",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Problemas",
    "text": "Tipos de Problemas\n\n\n\n\n\n\nClasificación:\n\n\n\nBinaria: La Clasificación es dicotómica, Perro o Gato, Sí o No, 1 o 0, Clase Positiva o Negativa.\nMulticlase: La clasificación puede tener más de 2 clases, pero sólo una es posible.\n\nEj: Perro, Gato o Canario; 0, 1, 2, 3, 4.\n\nMultilabel: La clasificación puede tener más de 2 clases, y más de una es posible a la vez.\n\nEj: Categorías de Libro: Puede ser Romance y Drama, Películas: Fantasía, Animación y Acción.\n\n\n\n\n\n\n\n\n\n\n\nRegresión:\n\n\n\nSimple: Predigo sólo un valor. Ej: Predecir la Temperatura.\nMultiple: Predigo varios valores continuos a la vez.\n\nEj: Modelo para intentar estimar Temperatura y Humedad a la vez.\n\nForecast: Donde se utilizan valores pasados para estimar valores futuros.\n\nDadas mis ganancias pasadas, estimar las futuras."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-intuición",
    "href": "tics411/clase-8.html#clasificación-intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Intuición",
    "text": "Clasificación: Intuición\n\n\n\n\n\n\nSupongamos el siguiente problema de clasificación. Tenemos un algoritmo, que dadas las variables Largo y Peso sean capaces de predecir si es que un Pez es una Reineta o una Sardina."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-intuición-1",
    "href": "tics411/clase-8.html#clasificación-intuición-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Intuición",
    "text": "Clasificación: Intuición\n\n\n\n\n\n\n\n\n\n\n\n\nQueremos encontrar una Regla de Decisión (Decision Rule) que permita clasificar correctamente un punto nuevo.\nDistintos modelos son capaces de encontrar distintas reglas de decisión. Por lo tanto, sus predicciones pueden ser completamente distintas."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-detalles",
    "href": "tics411/clase-8.html#clasificación-detalles",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Detalles",
    "text": "Clasificación: Detalles\nEs importante mencionar que un modelo de clasificación puede generar:\n\nHard Predictions: Es decir, la instancia a predecir es clase 0 o clase 1.\nSoft Prediction: Es decir, la instancia a predecir tiene una probabilidad \\(p\\) de pertenecer a la clase 1 y de \\(1-p\\) de pertenecer a la clase 0.\n\n\n\n\n\n\n\n\nCuando se hace predicción binaria, lo común es usar un Threshold de 0.5 para elegir la clase. Es decir si \\(p&lt;0\\) entonces clase 0, si \\(p \\ge 0.5\\) entonces clase 1.\n\n\n\n\n\n\n\n\n\n\nEn el caso de predicción multiclase o multilabel. Se calcula la probabilidad para cada clase. Por lo tanto se se asigna la clase de mayor probabilidad."
  },
  {
    "objectID": "tics411/clase-8.html#k-nearest-neighbors",
    "href": "tics411/clase-8.html#k-nearest-neighbors",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nEl modelo de vecinos más cercanos, o KNN por sus siglas en Inglés es un modelo basado en distancias. Su regla de decisión se basa en imitar el comportamiento de sus \\(K\\) vecinos más cercanos por votación (para clasificación) o la media (para regresión).\n\n\n\n\n\n\n\nK es un hiperparámetro de este modelo.\n\n\n\n\n\n\n\n\n\n\n\n\nSupongamos \\(K = 3\\).\nEs decir, tomaremos los 3 vecinos más cercanos.\n\n\n\n\n\n\n\nEn general es una buena idea elegir vecinos impares. ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-1-training-time",
    "href": "tics411/clase-8.html#knn-paso-1-training-time",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Paso 1 (Training Time)",
    "text": "KNN: Paso 1 (Training Time)\n\nTraining Time\n\nCorresponde al periodo donde el modelo aprende de los datos. Toma un patrón y ese modelo es utilizado para predecir.\n\n\n\n\n\n\n\n\nEn el caso de un KNN NO HAY APRENDIZAJE en esta etapa.\n\n\n\n\n\n\n\n\n\nEs considerado un modelo no-paramétrico ya que no aprende parámetros para realizar su predicción."
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-2-test-time",
    "href": "tics411/clase-8.html#knn-paso-2-test-time",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Paso 2 (Test Time)",
    "text": "KNN: Paso 2 (Test Time)\n\nInference Time\n\nCorresponde al periodo donde el modelo debe emitir una predicción.\n\n\nEn este caso, KNN calcula las distancias del punto a predecir (en verde) a todos los otros puntos existentes (proceso caro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa predicción corresponderá a la etiqueta mayoritaria por votacioń\n\n\n\n\n\n\n\n\nLa predicción corresponderá a la etiqueta mayoritaria por votacioń.\n¿Cuál sería una buena estrategia de predicción para un modelo de Regresión?"
  },
  {
    "objectID": "tics411/clase-8.html#fronteras-de-decisión",
    "href": "tics411/clase-8.html#fronteras-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Fronteras de Decisión",
    "text": "Fronteras de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\nImplicitamente, todo modelo de Machine Learning generará lo que se llama una Frontera de Decisión.\nSi un punto no visto cae dentro de su frontera entonces se le asigna dicha etiqueta."
  },
  {
    "objectID": "tics411/clase-8.html#implementación-clasificación-en-scikit-learn",
    "href": "tics411/clase-8.html#implementación-clasificación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación Clasificación en Scikit-Learn",
    "text": "Implementación Clasificación en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClasifier(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicción...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) número de vecinos a utilizar. Por defecto 5.\nmetric: Métrica de distancia. Por defecto “Minkowski”.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un parámetro interno para poder paralelizar los cálculos. Se recomienda utilizar -1 para utilizar todos sus cores."
  },
  {
    "objectID": "tics411/clase-8.html#implementación-regresión-en-scikit-learn",
    "href": "tics411/clase-8.html#implementación-regresión-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación Regresión en Scikit-Learn",
    "text": "Implementación Regresión en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn_clf = KNeighborsRegressor(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicción...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) número de vecinos a utilizar. Por defecto 5.\nmetric: Métrica de distancia. Por defecto “Minkowski”.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un parámetro interno para poder paralelizar los cálculos. Se recomienda utilizar -1 para utilizar todos sus cores.\n\n\n\n\n\n\n\n\n¿Cómo se encuentran las predicciones en un modelo de Regresión?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-detalles-técnicos",
    "href": "tics411/clase-8.html#knn-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Detalles Técnicos",
    "text": "KNN: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nModelo muy simple de implementar y entender.\nMuy eficiente en el aprendizaje.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nInferencia ineficiente: \\(O(mn^2)\\).\nCurse of Dimensionality: A medida que el número de dimensiones del problema crece, se requiere un incremento exponencial en la cantidad de datos para asegurar que existen suficientes vecinos cercanos para cualquier punto."
  },
  {
    "objectID": "tics411/clase-0.html#quién-soy",
    "href": "tics411/clase-0.html#quién-soy",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Quién soy?",
    "text": "¿Quién soy?\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia, estudié Ingeniería Civil pero llevo 9 años trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nTerminando mi Msc. y empezando mi PhD en la UAI.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqué mi primer paper el año pasado sobre Hate Speech en Español.\nJuego Tenis de Mesa, hago Agility con mi perrita Kira y escribo en mi Blog."
  },
  {
    "objectID": "tics411/clase-0.html#objetivos-del-curso",
    "href": "tics411/clase-0.html#objetivos-del-curso",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\nIdentificar Elementos Claves del Machine Learning (Terminología, Nomenclatura, Intuición).\nEntender como interactúan los algoritmos más importantes.\nAprender a seleccionar el mejor Algoritmo para el Problema.\nEjecutar y aplicar algoritmos clásicos de Machine Learning.\nEvaluar el desempeño esperado del Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#tópicos",
    "href": "tics411/clase-0.html#tópicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tópicos",
    "text": "Tópicos\n\n\n\n\n\n\n\nIntroducción a la Minería de Datos\nAnálisis Exploratorio de Datos (EDA)\nModelos No Supervisados/Descriptivos\nModelos Supervisados/Predictivos\n\n\n\n\n\n\nModelos no Supervisados\n\nK-Means\nHierarchical Clustering\nDBScan\nApriori\n\n\nModelos Supervisados\n\nKNN\nÁrboles de Decisión\nNaive Bayes\nRegresión Logística"
  },
  {
    "objectID": "tics411/clase-0.html#sobre-las-clases",
    "href": "tics411/clase-0.html#sobre-las-clases",
    "title": "TICS-411 Minería de Datos",
    "section": "Sobre las clases",
    "text": "Sobre las clases\n\nClases presenciales, con participación activa de los estudiantes.\nEs un curso coordinado.\nCanal oficial será Webcursos.\nMucha terminología y material de estudio será en Inglés.\nHorario: Jueves.\n\n15:30 a 16:40 (Cátedra)\n17:00 a 18:10 (Práctico)\nIdealmente!!\n\nAsistencia es voluntaria, pero altamente recomendada."
  },
  {
    "objectID": "tics411/clase-0.html#materiales-de-clases",
    "href": "tics411/clase-0.html#materiales-de-clases",
    "title": "TICS-411 Minería de Datos",
    "section": "Materiales de Clases",
    "text": "Materiales de Clases\n\nDiapositivas\nPrácticos\n\n\n\n\n\n\n\n\nSlides interactivas (Código se puede copiar e imágenes se pueden ver en grande).\nSe puede buscar contenido en las diapositivas mediante un buscador.\nSe dejarán copias en PDF en Webcursos (levemente distintas).\n\n\n\n\n\n\n\n\n\n\nSe espera que los estudiantes dominen las siguientes tecnologías:\n\nPython\nGoogle Colab\nPandas/Numpy\nScikit-Learn (Se enseñará a lo largo del curso)."
  },
  {
    "objectID": "tics411/clase-0.html#material-complementario",
    "href": "tics411/clase-0.html#material-complementario",
    "title": "TICS-411 Minería de Datos",
    "section": "Material Complementario",
    "text": "Material Complementario\n\n\n\n\n\n\nCurso de Scikit-Learn \n\nTutorial Colab\nAgregar Datos Externos a Colab"
  },
  {
    "objectID": "tics411/clase-0.html#evaluación",
    "href": "tics411/clase-0.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\n\n\n\n\n\n\nDos Evaluaciones Escritas (P1, P2) coordinadas y cuatro tareas prácticas en parejas (T1, T2, T3, T4) \\[NP = 0.35 \\cdot P1 + 0.35 \\cdot P2 + 0.3 \\cdot \\bar{T}\\] \\[ \\bar{T} = (T1 + T2 + T3 + T4)/4 \\]\n\n\n\n\n\n\n\n\n\n\nSi NP &gt; 5\n\n\n\\[NF = NP\\]\n\n\n\n\n\n\n\n\n\nEn caso contrario:\n\n\n\\[NF = 0.7 \\cdot NP + 0.3 \\cdot E\\]"
  },
  {
    "objectID": "tics411/clase-0.html#ayudantías",
    "href": "tics411/clase-0.html#ayudantías",
    "title": "TICS-411 Minería de Datos",
    "section": "Ayudantías",
    "text": "Ayudantías\nAyudante: TBD\nemail: TBD\n\n\n\n\n\n\n\nLas ayudantías serán en la manera que sean necesarias.\nEstarán enfocadas principalmente en aplicaciones y código."
  },
  {
    "objectID": "tics411/clase-0.html#revolución-de-los-datos",
    "href": "tics411/clase-0.html#revolución-de-los-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Revolución de los Datos",
    "text": "Revolución de los Datos\n\n\n\n\n\n\n\nHablar de los distintos tipos de Datos.\nTodo es datos, y está lleno de ellos en Internet y el mundo."
  },
  {
    "objectID": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "href": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Nace el Data Science (Ciencia de Datos)",
    "text": "Nace el Data Science (Ciencia de Datos)\n\n\n\n\n\n\n\nExplicar las distintas etapas. Qué son cada uno de ellos.\nExplicar que no estoy de acuerdo con todas las definiciones."
  },
  {
    "objectID": "tics411/clase-0.html#cómo-aprovechar-la-información-que-tenemos",
    "href": "tics411/clase-0.html#cómo-aprovechar-la-información-que-tenemos",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo aprovechar la información que tenemos?",
    "text": "¿Cómo aprovechar la información que tenemos?\n\n\nData Mining (Minería de Datos)\n\n\n“The process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.” (Fayyad, Piatetsky-Shapiro & Smith 1996)\n\n\n\n\n\n\nMachine Learning (Aprendizaje Automático)\n\n\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” (Mitchell, 2006)\n\n\n\n\n\n\nExplicar que estos son dos tipos de Approaches con el que hoy en día se enfrentan los datos.\nEl primero más enfocado en un análisis manual.\nEl segundo en un enfoque más automático."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos",
    "href": "tics411/clase-0.html#tipos-de-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos",
    "text": "Tipos de Datos\n\n\n\n\nDatos Estructurados\n\n\n\n\nDatos No Estructurados"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, instancias, registros. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos más amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo. También el que más disponible está en las empresas.\nDistintos data types, por lo que normalmente requiere de algún tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "href": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Series de Tiempo",
    "text": "Tipos de Datos: Series de Tiempo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Instancias temporales (Normalmente interdependientes).\nColumnas: Variables, Atributos, Features (Univariada o Multivariada).\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo.\nPropiedad temporal requiere preprocesamiento y modelos especiales."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-imágenes",
    "href": "tics411/clase-0.html#tipos-de-datos-imágenes",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Imágenes",
    "text": "Tipos de Datos: Imágenes\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste es el tipo de Datos que disparó la Inteligencia Artificial.\n¿Cuántos computadores para identificar un Gato? 16,000\n\n\n\n\n\n\n\n\n\n\n\n\nExplicar el concepto de Tensor, extensión de las matrices. Diferencia entre Grayscale y RGB."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "href": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Texto Libre",
    "text": "Tipos de Datos: Texto Libre\n\n\n\n\n\n\n\n\n\n\n\n\nDatos Masivos.\nDificiles de lidiar ya que deben ser llevarse a una representación numérica.\nAlto nivel de Sesgo y Subjetividad.\n\n\n\n\n\n\n\n\n\n\n\nGracias a este tipo de datos se han producido los avances más increíbles del último tiempo: Transformers"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-videos",
    "href": "tics411/clase-0.html#tipos-de-datos-videos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Videos",
    "text": "Tipos de Datos: Videos\n\n\n\n\n\n\n\n\nLos videos no son más que arreglos de imágenes.\nSon un tipo de dato muy pesado y difícil de lidiar.\nRequiere alto poder de Procesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-aprendizaje",
    "href": "tics411/clase-0.html#tipos-de-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Aprendizaje",
    "text": "Tipos de Aprendizaje"
  },
  {
    "objectID": "tics411/clase-0.html#reinforcement-learning",
    "href": "tics411/clase-0.html#reinforcement-learning",
    "title": "TICS-411 Minería de Datos",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este tipo de aprendizaje se enseña por refuerzo. Es decir se da una recompensa si el sistema aprende lo que queremos.\n\n\n\n\n\n\n\n\n\n\n\nSi el premio es mayor, se pueden obtener aprendizajes mayores.\n\n\n\n\n\n\n\n\n\n\n\nUn ejemplo de esto es AlphaTensor en el cual un modelo aprendió una nueva manera de multiplicar matrices que es más eficiente.\n\n\n\n\n\n\n\n\n\n\n\nOtro ejemplo es AlphaFold donde el modelo aprendió/descubrió cómo se doblan las proteínas cuando se vuelven aminoácidos."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-supervisados-regresión-y-clasificación",
    "href": "tics411/clase-0.html#problemas-supervisados-regresión-y-clasificación",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas Supervisados: Regresión y Clasificación",
    "text": "Problemas Supervisados: Regresión y Clasificación\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión: Se busca estimar un valor continuo.\n\n(Estimar el valor de una casa).\n\nClasificación: Se busca encontrar una categoría o un valor discreto.\n\n(Clasificar una imagen como Perro o Gato).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara entrenar este tipo de modelos se necesitan etiquetas, es decir, la respuesta esperada del modelo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmbos ejemplos se pueden realizar utilizando Largo (Eje Y) y Peso (Eje X)."
  },
  {
    "objectID": "tics411/clase-0.html#clustering",
    "href": "tics411/clase-0.html#clustering",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusters: Una categoría en la que sus componentes son similares. Los clusters normalmente no tienen un nombre propio, sino que uno les asigna uno.\nTambién se les llama segmentos. No usar la palabra clase.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo requiere de etiquetas, por lo tanto, no es posible evaluar su desempeño de manera 100% acertada."
  },
  {
    "objectID": "tics411/clase-0.html#reducción-de-dimensionalidad",
    "href": "tics411/clase-0.html#reducción-de-dimensionalidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Reducción de Dimensionalidad",
    "text": "Reducción de Dimensionalidad\n\n\n\n\n\n\n\n\n\n\n\n\nReducción de la Dimensionalidad: Eliminar complejidad sin perder información clave para poder entender su comportamiento."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML",
    "text": "Nuestro Sistema de ML\nCreemos un Sistema de ML que sea capaz de ver una imágen y pronunciar correctamente el uso de la letra C.\n\n\n\n\n\n\nVamos a Entrenar un Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Entrenamiento",
    "text": "Nuestro Sistema de ML: Entrenamiento\n\n\n\n\n\n\nKasa\n\n\n\n\n\n\n\nKokodrilo\n\n\n\n\n\n\n\nKubo\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué patrones está aprendiendo el modelo?\n\n\n\n\n\nEntrenamiento\n\n\nEs el proceso en el cuál se permite al modelo aprender. En este proceso se le entregan ejemplos (Train Set) para que el modelo de manera autónoma pueda aprender patrones que le permitan resolver la tarea dada."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Inferencia",
    "text": "Nuestro Sistema de ML: Inferencia\n\nInferencia/Predicción\n\n\nSe refiere al proceso en el que el modelo tiene que demostrar cuál sería su decisión de acuerdo a los patrones aprendidos en el proceso de entrenamiento. Los ejemplos en los que se prueba se le denomina Test Set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKukillo\n\n\nBikikleta\n\n\n\n\n\nGeneralización\n\n\nSe le llama generalización a la capacidad del modelo de aplicar lo aprendido de manera correcta en ejemplos no vistos."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento",
    "text": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento\n\n\n\n\n\n\nKuchillo\n\n\n\n\n\n\n\nChokolate\n\n\n\n\n\n\n\nSinsel\n\n\n\n\n\n\n\n\n\n\n\n\nNo es bueno entrenar con las mismas instancias de de Test, es decir, con las cuales se evalúa el modelo. ¿Por qué?\n\n\n\n\n\nMencionar el caso de error de ImageNet."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo",
    "text": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKuchillo\n\n\nBisikleta\n\n\n\n\n\nEvaluación\n\n\nUtilizar una métrica que permita ponerle nota al modelo.\n\n\n\n\n\n\n1er Modelo: 2 correctas de 4, es decir 50%.\n\n\n\n\n2do Modelo: 4 correctas de 4, es decir 100%."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas del Aprendizaje",
    "text": "Problemas del Aprendizaje\n\nSupongamos que queremos utilizar nuestro modelo para pronunciar palabras en otro idioma (otro Test Set).\n¿Qué problemas podemos encontrar?\n\n\n\n\nStomach \\(\\rightarrow\\) Stomak\nArcher \\(\\rightarrow\\) Archer\nChurch \\(\\rightarrow\\) Churk\n\nChurch.\n\nArcheology \\(\\rightarrow\\) Archeology\n\nArkeology.\n\nChicago \\(\\rightarrow\\) Chicago\n\nShicago.\n\nMuscle \\(\\rightarrow\\) Muskle\n\nMus_le.\n\nIch mag Schweinefleisch \\(\\rightarrow\\) Ich mag Schweinefleisk.\n\nIj mag Shvaineflaish.\n\n\n\n\n\n\n\n\n\n\nClaramente tenemos un problema. ¿A qué se debe esto?"
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas del Aprendizaje: Definiciones",
    "text": "Problemas del Aprendizaje: Definiciones\n\nOverfitting (Sobreajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, porque se ajusta demasiado bien (llegando a memorizar) a los datos de entrenamiento. ¿Cómo se puede mitigar este problema?\n\n\n\n\n\n\n\n\n\n\nSe le tiende a llamar sobreentrenamiento, pero no es del todo correcto para el caso de modelos de Machine Learning. Lo más correcto es que el sobreentrenamiento provoca overfitting.\n\n\n\n\n\nMostrar ejemplos en Pizarra de manera gráfica. Ejemplos típicos de Excel.\n\n\n\nUnderfitting (Subajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, pero a diferencia del overfitting no se ha ajustado correctamente a los datos. ¿Cómo se vería el underfitting en nuestro ejemplo?"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Crisp-DM",
    "text": "Etapas del Modelamiento: Crisp-DM"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: KDD",
    "text": "Etapas del Modelamiento: KDD"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Semma",
    "text": "Etapas del Modelamiento: Semma"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-metodología-propia",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-metodología-propia",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Metodología Propia",
    "text": "Etapas del Modelamiento: Metodología Propia"
  }
]