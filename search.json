[
  {
    "objectID": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning",
    "href": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning",
    "title": "TICS-579-Deep Learning",
    "section": "¬øPor qu√© estudiar Deep Learning?",
    "text": "¬øPor qu√© estudiar Deep Learning?\n\nPrincipalmente porque es el Estado del Arte en las aplicaciones m√°s Impresionantes en la Inteligencia Artificial.\n\n\n\n\nAlexnet (2012)\n\n\n\nAlphaGo (2016)\n\n\n\nTransformers (2017)\n\n\n\nGPT (2019)"
  },
  {
    "objectID": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning-1",
    "href": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning-1",
    "title": "TICS-579-Deep Learning",
    "section": "¬øPor qu√© estudiar Deep Learning?",
    "text": "¬øPor qu√© estudiar Deep Learning?\n\nPrincipalmente porque es el Estado del Arte en las aplicaciones m√°s Impresionantes en la Inteligencia Artificial.\n\n\n\n\nGPT-3 (2021)\n\n\n\nAlphaFold (2021)\n\n\n\nStable Diffussion/Dalle (2022)\n\n\n\nLLMs (2023) (ChatGPT/Llama)"
  },
  {
    "objectID": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning-2",
    "href": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning-2",
    "title": "TICS-579-Deep Learning",
    "section": "¬øPor qu√© estudiar Deep Learning?",
    "text": "¬øPor qu√© estudiar Deep Learning?\n\n\n\nIm√°gen tomada de la Clase de Zico Colter"
  },
  {
    "objectID": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning-3",
    "href": "tics579/clase-1.html#por-qu√©-estudiar-deep-learning-3",
    "title": "TICS-579-Deep Learning",
    "section": "¬øPor qu√© estudiar Deep Learning?",
    "text": "¬øPor qu√© estudiar Deep Learning?\n\n\n\n\n\n\nFacilidad y Autograd\n\n\n\nFrameworks como Tensorflow, Pytorch o Jax permiten realizar esto de manera mucho m√°s sencilla.\n\nFrameworks permiten calcular gradientes de manera autom√°tica.\nAntigua mente trabajar en Torch, Caffe o Theano pod√≠a tomar cerca de 50K l√≠neas de c√≥digo.\n\n\n\n\n\n\n\n\n\n\n\nC√≥mputo\n\n\n\nProliferaci√≥n de las GPUs, TPUs, HPUs, IPUs, como sistemas masivos de C√≥mputos.\n\nHow many computers to identify a cat? 16,000\n\n\n\n\n\n\n\n\n\n\n\nEstado del Arte\n\n\n\nModelos de Deep Learning pueden generar sistemas que entiendan im√°genes, textos, audios, videos, grafos, etc."
  },
  {
    "objectID": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales",
    "href": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales",
    "title": "TICS-579-Deep Learning",
    "section": "El nacimiento de las Redes Neuronales",
    "text": "El nacimiento de las Redes Neuronales\n\nLas redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad m√°s b√°sica es una Neurona."
  },
  {
    "objectID": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales-1",
    "href": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales-1",
    "title": "TICS-579-Deep Learning",
    "section": "El nacimiento de las Redes Neuronales",
    "text": "El nacimiento de las Redes Neuronales\n\nLas redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad m√°s b√°sica es una Neurona.\n\n\n\n\n\n\n\n\n\n\nEste c√°lculo se puede representar como:\n\n\\[ y = \\phi(w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_5 \\cdot x_5)\\] \\[ y = \\phi(w^T \\cdot x)\\]\ndonde \\(w = [w_1, w_2, w_3, w_4, w_5]\\) y \\(x = [x_1, x_2, x_3, x_4, x_5]\\).\n\n\n\n\n\n\n\n\n¬øQu√© pasa si \\(\\phi(.)\\) vale la funci√≥n identidad?\n¬øQu√© pasa si \\(\\phi(.)\\) vale la funci√≥n sigmoide?"
  },
  {
    "objectID": "tics579/clase-1.html#arquitectura-de-una-red",
    "href": "tics579/clase-1.html#arquitectura-de-una-red",
    "title": "TICS-579-Deep Learning",
    "section": "Arquitectura de una Red",
    "text": "Arquitectura de una Red\n\nNormalmente todas las neuronas de una capa anterior se conectan con las de una capa posterior (Hay excepciones).\nDependiendo de la forma en la que se conecten, cada Arquitectura recibe un nombre."
  },
  {
    "objectID": "tics579/clase-1.html#los-ingredientes-de-un-algoritmo-de-aprendizaje",
    "href": "tics579/clase-1.html#los-ingredientes-de-un-algoritmo-de-aprendizaje",
    "title": "TICS-579-Deep Learning",
    "section": "Los Ingredientes de un Algoritmo de Aprendizaje",
    "text": "Los Ingredientes de un Algoritmo de Aprendizaje\n\nHip√≥tesis\n\n\nUna funci√≥n que describe como mapear inputs (features) con outputs (labels) por medio de par√°metros.\n\n\nLoss Function\n\n\nUna funci√≥n que especifica cuanta informaci√≥n se pierde. Mayor p√©rdida m√°s error de estimaci√≥n.\n\n\nM√©todo de Optimizaci√≥n\n\n\nUn procedimiento para determinar los par√°metros de la hip√≥tesis, minimizando la suma de las p√©rdidas en un set de entrenamiento."
  },
  {
    "objectID": "tics579/clase-1.html#ejemplo-softmax-regression",
    "href": "tics579/clase-1.html#ejemplo-softmax-regression",
    "title": "TICS-579-Deep Learning",
    "section": "Ejemplo: Softmax Regression",
    "text": "Ejemplo: Softmax Regression\n\nSoftmax Regression\n\n\nCorresponde la versi√≥n multiclase de una Regresi√≥n Log√≠stica.\n\n\n\n\n\n\n\n\n\n\n\nConsideremos un problema de clasificaci√≥n multiclase de \\(k\\) clases tal que:\n\n\n\nDatos de Entrenamiento: \\(x^{(i)}, y^{(i)} \\in {1,...,k}\\) para \\(i=1,...,m\\).\n\n\\(n\\): Es el n√∫mero de Features.\n\\(m\\): Es el n√∫mero de puntos en el training set.\n\\(k\\): Es el n√∫mero de clases del problema.\n\n\n\n\n\n\n\n\n\n\n\nVamos a tener en total \\(n \\times k\\) par√°metros o pesos que actualizar."
  },
  {
    "objectID": "tics579/clase-1.html#softmax-regression-hip√≥tesis",
    "href": "tics579/clase-1.html#softmax-regression-hip√≥tesis",
    "title": "TICS-579-Deep Learning",
    "section": "Softmax Regression: Hip√≥tesis",
    "text": "Softmax Regression: Hip√≥tesis\n\nVamos a definir una funci√≥n que mapea valores de \\(x \\in \\mathbb{R}\\) a vectores de \\(k\\) dimensiones.\n\n\\[ h: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\\] \\[ x \\rightarrow h_\\theta(x) = \\theta^T x\\]\n\ndonde \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(x \\in \\mathbb{R}^{n\\times 1}\\)\n\n\n\n\n\n\n\nEn este caso usamos una hip√≥tesis lineal, ya que se usa una multiplicaci√≥n matricial (o producto punto) para relacionar \\(\\theta\\) y \\(x\\).\n\n\n\n\n\n\n\n\n\nEn este caso el output de \\(h_i(x)\\) devolver√° la probabilidad de pertenecer a una cierta clase \\(i\\).\n\n\n\n\n\n\n\n\n\n\n¬øCu√°l es el tama√±o/dimensi√≥n de \\(h_\\theta(x)\\)?"
  },
  {
    "objectID": "tics579/clase-1.html#notaci√≥n-matricial",
    "href": "tics579/clase-1.html#notaci√≥n-matricial",
    "title": "TICS-579-Deep Learning",
    "section": "Notaci√≥n Matricial",
    "text": "Notaci√≥n Matricial\n\nUna manera m√°s conveniente de escribir estas operaciones es utilizar (Matrix Batch Form).\n\n\n\nDesign Matrix\n\\[X \\in \\mathbb{R}^{m \\times n} = \\begin{bmatrix}\n&-x^{(1)T}-\\\\\n& \\vdots & \\\\\n&-x^{(m)T}- &\\\\\n\\end{bmatrix}\\]\n\nLabels Vector\n\\[y \\in {1,...,k} = \\begin{bmatrix}\n&-y^{(1)}-\\\\\n& \\vdots & \\\\\n&-y^{(m)}- &\\\\\n\\end{bmatrix}\\]\n\n\nLa hip√≥tesis tambi√©n se puede reescribir de manera matricial como:\n\n\n\\[h_\\theta(X) = \\begin{bmatrix}\n&-h_\\theta(x^{(1)})^T-\\\\\n& \\vdots & \\\\\n&-h_\\theta(x^{(m)})^T-\\\\\n\\end{bmatrix}\\]\n\n\\[h_\\theta(X)= \\begin{bmatrix}\n&-x^{(1)T} \\theta-\\\\\n& \\vdots & \\\\\n&-x^{(m)T} \\theta-\\\\\n\\end{bmatrix} = X  \\theta\\]\n\n\n\n\n\n\n\n\n\nNormalmente este tipo de operaciones son las que utilizaremos para hacer nuestro c√≥digo."
  },
  {
    "objectID": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss",
    "href": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function: Softmax/Cross-Entropy Loss",
    "text": "Loss Function: Softmax/Cross-Entropy Loss\nPara normalizar la salida de nuestra hip√≥tesis, vamos a convertirla en una ‚Äúprobabilidad‚Äù. Pra ello usaremos la funci√≥n Softmax tal que:\n\\[z_i = p(label = i) = \\frac{exp(h_i(x))}{\\sum_{j=1}^k exp(h_j(x))}\\]\nPara optimizar este proceso normalmente se usa MLE aplicado al Negative Log Loss, tambi√©n llamado Cross Entropy Loss.\n\\[l_{ce}(h(x), y) = -log \\, p(label = y) = - h_{(i=y)}(x) + log\\left(\\sum_{j = 1}^k exp(h_j(x))\\right)\\]"
  },
  {
    "objectID": "tics579/clase-1.html#m√©todo-de-optimizaci√≥n",
    "href": "tics579/clase-1.html#m√©todo-de-optimizaci√≥n",
    "title": "TICS-579-Deep Learning",
    "section": "M√©todo de Optimizaci√≥n",
    "text": "M√©todo de Optimizaci√≥n\n\nEl √∫ltimo ingrediente de un algoritmo de aprendizaje es el m√©todo de optimizaci√≥n. Es necesario minimizar la p√©rdida promedio asociada al set de entrenamiento. Para ello definimos esto formalmente como:\n\n\\[\\underset{\\theta}{minimize} = \\frac{1}{m} \\sum_{i=1}^m l(h_\\theta(x^{(i)}, y^{(i)}))\\]\n\n\n\n\n\n\n¬øC√≥mo encontramos los par√°metros \\(\\theta\\) que minimizan la p√©rdida de informaci√≥n/error de estimaci√≥n?\n\n\n\n\nGradient Descent\n\n\nEs un m√©todo num√©rico que permite minimizar funciones movi√©ndose en direcci√≥n contraria al Gradiente. Es computacionalmente muy eficiente y f√°cil de implementar en c√≥digo."
  },
  {
    "objectID": "tics579/clase-1.html#gradient-descent",
    "href": "tics579/clase-1.html#gradient-descent",
    "title": "TICS-579-Deep Learning",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\nSe define el gradiente como la matriz que contiene las derivadas parciales de una funci√≥n \\(f\\). Se denota como:\n\\[\\nabla_\\theta f(\\theta) \\in \\mathbb{R}^{n \\times k} = h_\\theta(X) = \\begin{bmatrix}\n\\frac{\\partial f(\\theta)}{\\partial \\theta_{11}} & \\cdots & \\frac{\\partial f(\\theta)}{\\partial \\theta_{1k}} \\\\\n\\cdots & \\ddots & \\cdots \\\\\n\\frac{\\partial f(\\theta)}{\\partial \\theta_{n1}} & \\cdots & \\frac{\\partial f(\\theta)}{\\partial \\theta_{nk}}\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl gradiente apunta a la direcci√≥n de m√°ximo crecimiento de la funci√≥n \\(f\\)."
  },
  {
    "objectID": "tics579/clase-1.html#gradient-descent-regla-de-actualizaci√≥n",
    "href": "tics579/clase-1.html#gradient-descent-regla-de-actualizaci√≥n",
    "title": "TICS-579-Deep Learning",
    "section": "Gradient Descent: Regla de Actualizaci√≥n",
    "text": "Gradient Descent: Regla de Actualizaci√≥n\nPara minimizar la funci√≥n, la idea es descender iterativamente por el trayecto en contra del gradiente. La regla de actualizaci√≥n se define como:\n\\[\\theta := \\theta - \\alpha \\nabla_\\theta f(\\theta)\\]\ndonde \\(\\alpha &gt; 0\\) corresponde al step size o learning rate.\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso \\(f\\) corresponder√° a nuestro \\(l_{ce}\\) calculado anteriormente. El problema es, ¬øcu√°nto vale el gradiente del Cross Entropy Loss?"
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\nNecesitamos calcular:\n\\[\\theta := \\theta - \\alpha \\nabla_\\theta l_{ce}(\\theta^Tx,y)\\]\n\n\n\n\n\n\n¬øCu√°nto vale el Gradiente?\n\nNo es tan sencillo, ya que derivamos respecto a \\(\\theta\\) que es una matriz.\nPero derivamos a \\(\\theta^T x\\) que es un vector.\nPara ello, lo correcto es utilizar Calculo Diferencial Matricial, Jacobianos y Productos de Kroenecker (que probablemente no han visto en ning√∫n curso).\n\nSPOILER: Yo tampoco lo he visto en ning√∫n curso.\n\nUsaremos un truco (sumamente hacky) que jam√°s deben revelar üò± y que nos deber√≠a avergonzar.\n\nPretenderemos que todos los valores son escalares y corregiremos las dimensiones al final."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-1",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-1",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\nSimplifiquemos el problema pensando que calcularemos el Gradiente para un s√≥lo vector \\(x\\).\n\n\nEs decir, \\(x \\in n\\times1\\).\n\nAdem√°s sabemos que \\(\\nabla_\\theta l_{ce}(\\theta^Tx, y)\\) debe tener dimensiones \\(n \\times k\\).\n\n\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = \\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^Tx}{\\partial \\theta}\\]\n\n\\[\\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} = \\frac{\\partial l_{ce}(\\theta^Tx, y)}{\\partial h} = \\begin{bmatrix}\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_1} \\\\\n\\vdots\\\\\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_k} \\\\\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\nLuego el gradiente de \\(l_{ce}\\) respecto a \\(h\\) tiene dimensiones \\(k \\times 1\\)."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-2",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-2",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\\[\\begin{align}\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_i} &= \\frac{\\partial }{\\partial h_i}\\left(-h_{(i = y)} + log \\sum_{j = 1}^k exp(h_j)\\right) \\\\\n&= -\\frac{\\partial h_{(i = y)}}{\\partial h_i}+ \\frac{1}{\\sum_{j = 1}^k exp(h_j)} \\cdot \\frac{\\partial}{\\partial h_i}\\left(\\sum_{j=1}^k exp(h_j)\\right) \\\\\n&= -\\frac{\\partial h_{(i = y)}}{\\partial h_i}+ \\frac{exp(h_i)}{\\sum_{j = 1}^k exp(h_j)} \\\\\n&= - 1\\{i=y\\} + z_i = z_i - 1\\{i=y\\}\n\\end{align}\n\\]\n\n\n\n\n\n\n\\[1\\{i = y\\} = \\begin{cases}\n1,  & \\text{i = y} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\nFinalmente:\n\n\n\\[\\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} = z - e_y\\]\n\n\n\n\n\n\n\nDonde \\(z\\), es el vector de Softmax y \\(e_y\\) es un vector con un 1 en la posici√≥n \\(y\\) y 0 en el resto."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-3",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-3",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\n\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = \\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^Tx}{\\partial \\theta}\\] \\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = (z-e_y)\\cdot x \\]\n\n\n\n\n\n\n\nOJO con las dimensiones\n\n\n\n\\(Z-e_y \\in \\mathbb{R}^{k \\times 1}\\)\n\\(x \\in \\mathbb{R}^{n \\times 1}\\)\n\n\n\n\n\n\n\nLuego:\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = x (z-e_y)^T\\]"
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-matrix-batch-form",
    "href": "tics579/clase-1.html#calculando-el-gradiente-matrix-batch-form",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente Matrix Batch Form",
    "text": "Calculando el Gradiente Matrix Batch Form\n\n\n\\[\\begin{align}\\nabla_\\theta l_{ce}(X\\theta,y) \\in \\mathbb{R}^{n\\times k} &= \\frac{\\partial l_{ce}(X\\theta,y)}{\\partial X\\theta} \\cdot \\frac{\\partial X\\theta}{\\partial \\theta}\\\\\n&= (Z - I_y) \\cdot X \\\\\n&= X^T \\cdot (Z - I_y)\n\\end{align}\\]\n\n\n\n\n\n\n\nOjo con las dimensiones\n\n\n\n\\(Z - I_y \\in \\mathbb{R}^{m \\times k}\\)\n\\(X \\in \\mathbb{R}^{m \\times n}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Z\\) corresponde al Softmax de \\(X\\theta\\) aplicado por filas.\n\\(I_y\\) corresponde al One Hot Encoder de las etiquetas.\n\n\n\n\nFinalmente la regla de actualizaci√≥n de par√°metros usando Gradient Descent queda como:\n\\[\\theta := \\theta - \\alpha X^T (Z - I_y)\\]\n\n\n\n\n\n\nAplicando esta simple regla se puede obtener cerca de un 8% de error clasificando d√≠gitos en MNIST.\n\n\n\n\n\n\n\n\n\nPr√≥xima clase vamos a hacer lo mismo para una Red Neuronal. La √∫nica diferencia es que la hip√≥tesis que utilizaremos es un poco m√°s fancy."
  },
  {
    "objectID": "tics579/clase-1.html#multiplicaci√≥n-matricial",
    "href": "tics579/clase-1.html#multiplicaci√≥n-matricial",
    "title": "TICS-579-Deep Learning",
    "section": "Multiplicaci√≥n Matricial",
    "text": "Multiplicaci√≥n Matricial\n\n\n\n\n\n\n\n\n\nDonde \\(B_{*,i}\\) corresponde a la columna \\(i\\) de B.\nDonde \\(A_{i,*}\\) corresponde a la fila \\(i\\) de A.\n\n\n\n\n\n\n\n\n\n\n\nIm√°gen tomada de la Clase de Zico Colter"
  },
  {
    "objectID": "charlas.html",
    "href": "charlas.html",
    "title": "Charlas",
    "section": "",
    "text": "Hate Speech UAI\n\n\nPresentaci√≥n del HateStack\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike a Needle in the HateStack\n\n\nPresentaci√≥n final para la Datat√≥n 2022\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRFM-Superlag\n\n\nPresentaci√≥n Final del Desaf√≠o Ita√∫-Binnario\n\n\n\nDec 5, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "tics411.html",
    "href": "tics411.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentaci√≥n del Curso\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 1\n\n\nCalidad de los Datos y Feature Engineering\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 2\n\n\nExploratory Data Analysis (EDA)\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase Bonus\n\n\nIntroducci√≥n a Scikit-Learn\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 3\n\n\nModelaci√≥n Descriptiva y K-Means\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 4\n\n\nClustering Jer√°rquico\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 5\n\n\nDBSCAN\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 6\n\n\nEvaluaci√≥n de Clusters\n\n\n\nApr 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 7\n\n\nAlgoritmo Apriori\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 8\n\n\nIntroducci√≥n al Aprendizaje Supervisado\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 9\n\n\nEvaluaci√≥n de Modelos\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 10\n\n\n√Årboles de Decisi√≥n\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 11\n\n\nNaive Bayes\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 12\n\n\nRegresi√≥n Log√≠stica\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 13\n\n\nDetecci√≥n de Anomal√≠as\n\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  },
  {
    "objectID": "tics579-labs.html",
    "href": "tics579-labs.html",
    "title": "Pr√°cticos",
    "section": "",
    "text": "Pr√°ctico\nColab\n\n\n\n\nPreprocesamiento\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     datacuber.cl\n  \n\n  \n  \nSoy Alfonso y he trabajado como Cient√≠fico de Datos por los √∫ltimos 10 a√±os. Adem√°s me gusta el Machine Learning Competitivo y hasta el momento he ganado 2 competencias.\nActualmente me encuentro cursando mi PhD. en Data Science. Mis intereses de investigaci√≥n tienen que ver con Machine Learning y Deep Learning enfoc√°ndome principalmente en la aplicaci√≥n de Transformers.\nEn mi tiempo libre practico Tenis de Mesa y escribo sobre Machine Learning en mi blog: datacuber.cl.\n\n\nUniversidad Adolfo Iba√±ez, Vi√±a del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Iba√±ez, Vi√±a del Mar | Msc. in Data Science | 2022 - 2024\nUniversidad T√©cnica Federico Santa Mar√≠a | Ingenier√≠a Civil | 2005 - 2013\nPuedes ver m√°s detalles de mi carrera ac√°.\n\n\n\n\nHate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "index.html#educaci√≥n",
    "href": "index.html#educaci√≥n",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "Universidad Adolfo Iba√±ez, Vi√±a del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Iba√±ez, Vi√±a del Mar | Msc. in Data Science | 2022 - 2024\nUniversidad T√©cnica Federico Santa Mar√≠a | Ingenier√≠a Civil | 2005 - 2013\nPuedes ver m√°s detalles de mi carrera ac√°."
  },
  {
    "objectID": "index.html#publicaciones",
    "href": "index.html#publicaciones",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "Hate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "tics411/clase-3.html#definiciones",
    "href": "tics411/clase-3.html#definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nAprendizaje No supervisado\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender.\n\n\n\n\n\n\n\n\n\nEn nuestro caso nos enfocaremos en un caso particular de Modelaci√≥n Descriptiva llamada Clustering.\n\n\n\n\nClustering\n\n\nConsiste en agrupar los datos en un menor n√∫mero de entidades o grupos. A estos grupos se les conoce como clusters y pueden ser generados de manera global, o modelando las principales caracter√≠sticas de los datos."
  },
  {
    "objectID": "tics411/clase-3.html#intuici√≥n",
    "href": "tics411/clase-3.html#intuici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Intuici√≥n",
    "text": "Intuici√≥n\n¬øCu√°ntos clusters se pueden apreciar?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-introducci√≥n",
    "href": "tics411/clase-3.html#clustering-introducci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Introducci√≥n",
    "text": "Clustering: Introducci√≥n\n\n\n\n\n\n\nClustering: Consiste en buscar grupos de objetos tales que la similaridad intra-grupo sea alta, mientras que la similaridad inter-grupos sea baja. Normalmente la distancia es usada para determinar qu√© tan similares son estos grupos."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-evaluaci√≥n",
    "href": "tics411/clase-3.html#clustering-evaluaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Evaluaci√≥n",
    "text": "Clustering: Evaluaci√≥n\n\n\n\n\n\n\n\nEvaluar el nivel del √©xito o logro del Clustering es complicado. ¬øPor qu√©?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-tipos",
    "href": "tics411/clase-3.html#clustering-tipos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Tipos",
    "text": "Clustering: Tipos"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-partici√≥n",
    "href": "tics411/clase-3.html#clustering-partici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Partici√≥n",
    "text": "Clustering: Partici√≥n\n\nLos datos son separados en K clusters, donde cada punto pertenece exclusivamente a un √∫nico cluster."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-densidad",
    "href": "tics411/clase-3.html#clustering-densidad",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (n√∫mero de objetos o puntos) en el vecindario sobrepase alg√∫n umbral."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-jerarqu√≠a",
    "href": "tics411/clase-3.html#clustering-jerarqu√≠a",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Jerarqu√≠a",
    "text": "Clustering: Jerarqu√≠a\n\nLos algoritmos basados en jerarqu√≠a pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a trav√©s de una m√©trica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una m√©trica de similaridad."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-probabil√≠stico",
    "href": "tics411/clase-3.html#clustering-probabil√≠stico",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Probabil√≠stico",
    "text": "Clustering: Probabil√≠stico\nSe ajusta cada punto a una distribuci√≥n de probabilidades que indica cu√°l es la probabilidad de pertenencia a dicho cluster."
  },
  {
    "objectID": "tics411/clase-3.html#partici√≥n",
    "href": "tics411/clase-3.html#partici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Partici√≥n",
    "text": "Partici√≥n\n\nLos datos son separados en K Clusters, donde cada punto pertenece exclusivamente a un √∫nico cluster. A K se le considera como un hiperpar√°metro.\n\n\n\n\n\n\n\n\nCluster Compactos: Minimizar la distancia intra-cluster (within cluster).\nClusters bien separados: Maximizar la distancia inter-cluster (between cluster).\n\n\n\n\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\nEl puntaje/score mide la calidad del clustering \\(C\\) para el Dataset \\(D\\)."
  },
  {
    "objectID": "tics411/clase-3.html#score",
    "href": "tics411/clase-3.html#score",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Score",
    "text": "Score\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\n\n\n\n\nDistancia Between-Cluster: \\[bc(C) = \\sum_{1 \\le j \\le k \\le K} d(r_j, r_k)\\]\n\ndonde \\(r_k\\) representa el centro del cluster \\(k\\): \\[r_k = \\frac{1}{n_k} \\sum_{x_i \\in C_k} x_i\\]\n\n\nDistancia Within-Cluster (Inercia): \\[wc(C) = \\sum_{k=1}^K \\sum_{x_i \\in C_k} d(x_i, r_k)\\]\n\n\n\n\n\n\n\n\n\n\n\nDistancia entre los centros de cada cluster.\n\n\n\n\n\n\n\n\n\n\nDistancia entre todos los puntos del cluster y su respectivo centro."
  },
  {
    "objectID": "tics411/clase-3.html#k-means",
    "href": "tics411/clase-3.html#k-means",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means",
    "text": "K-Means\n\nK-Means\n\n\nDado un n√∫mero de clusters \\(K\\) (determinado por el usuario), cada cluster es asociado a un centro (centroide). Luego, cada punto es asociado al cluster con el centroide m√°s cercano.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utiliza la Distancia Euclideana como medida de similaridad.\n\n\nSe seleccionan \\(K\\) puntos como centroides iniciales.\nRepite:\n\nForma K clusters asignando todos los puntos al centroide m√°s cercano.\nRecalcula el centroide para cada clase como la media de todos los puntos de dicho cluster.\n\n\n\nSe repite este procedimiento por un n√∫mero finito de iteraciones o hasta que los centroides no cambien."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo",
    "href": "tics411/clase-3.html#k-means-ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\nResolvamos el siguiente ejemplo.\nSupongamos que tenemos tipos de manzana, y cada una de ellas tiene 2 atributos (features). Agrupemos estos objetos en 2 grupos de manzanas basados en sus caracter√≠sticas."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-1",
    "href": "tics411/clase-3.html#k-means-ejemplo-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n1era Iteraci√≥n\n\n\n\n\nSupongamos los siguientes centroides iniciales: \\[C_1 = (1,1)\\] \\[C_2 = (2,1)\\]\n\n\n\n\n\n\n\n\n\nMatriz de Distancias al Centroide: (coordenada i,j representa distancia del punto j al centroide i)\n\n\n\n\n\n\\[D^1 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n1 & 0 & 2.83 & 4.24\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son: \\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-2",
    "href": "tics411/clase-3.html#k-means-ejemplo-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n2da Iteraci√≥n\n\n\n\n\nLos nuevos centroides son:\n\n\\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]\n\n\n\nCalculamos la Matriz de Distancias al Centroide:\n\n\n\n\\[D^2 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n3.14 & 2.26 & 0.47 & 1.89\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^2 = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son:\n\\(C_1 = (\\frac{3}{2}, 1)\\) y \\(C_2 = (\\frac{9}{2}, \\frac{7}{2})\\)"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-3",
    "href": "tics411/clase-3.html#k-means-ejemplo-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\nSi seguimos iterando notaremos que ya no hay cambios en los clusters. El algoritmo converge.\nEste es el resultado de usar \\(K=2\\). Utilizar otro valor de \\(K\\) entregar√° valores distintos.\n¬øEs este el n√∫mero de clusters √≥ptimos?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-n√∫mero-de-clusters-√≥ptimos",
    "href": "tics411/clase-3.html#k-means-n√∫mero-de-clusters-√≥ptimos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: N√∫mero de Clusters √ìptimos",
    "text": "K-Means: N√∫mero de Clusters √ìptimos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSiempre es posible encontrar el n√∫mero de clusters indicados.\nEntonces,\n\n¬øC√≥mo deber√≠a escoger el valor de \\(K\\)?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-n√∫mero-de-clusters-√≥ptimos-1",
    "href": "tics411/clase-3.html#k-means-n√∫mero-de-clusters-√≥ptimos-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: N√∫mero de Clusters √ìptimos",
    "text": "K-Means: N√∫mero de Clusters √ìptimos\n\nCurva del Codo\n\nEs una heur√≠sitca en la cual gr√°fica el valor de una m√©trica de distancia (e.g.¬†within distance) para distintos valores de \\(K\\). El valor √≥ptimo de \\(K\\) ser√° el codo de la curva, que es el valor donde se estabiliza la m√©trica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste valor del codo muchas veces es subjetivo y distintas apreciaciones pueden llegar a distintos \\(K\\) √≥ptimos.\n\n\n\n\n\n\n\n\n\n\nEventualmente otras m√©tricas distintas al within cluster distance podr√≠an tambi√©n ser usadas.\n\n\n\n\n\n\n\n\n\n¬øCu√°l es el efecto que est√° buscando la curva del codo? ¬øQu√© implica el valor de K escogido?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-detalles-t√©cnicos",
    "href": "tics411/clase-3.html#k-means-detalles-t√©cnicos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Means: Detalles T√©cnicos",
    "text": "K-Means: Detalles T√©cnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nAlgoritmo relativamente eficiente (\\(O(k \\cdot n \\cdot i)\\)). Donde \\(k\\) es el n√∫mero de clusters, \\(n\\) el n√∫mero de puntos, e \\(i\\) el n√∫mero de iteraciones.\nEncuentra ‚Äúclusters esf√©ricos‚Äù.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nSensible al punto de inicio.\nSolo se puede aplicar cuando el promedio es calculable.\nSe requiere definir K a priori (K es un hiperpar√°metro).\nSuceptible al ruido y a m√≠nimos locales (podr√≠a no converger)."
  },
  {
    "objectID": "tics411/clase-3.html#implementaci√≥n-en-scikit-learn",
    "href": "tics411/clase-3.html#implementaci√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Scikit-Learn",
    "text": "Implementaci√≥n en Scikit-Learn\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=8, n_init=10,random_state=None)\nkm.fit(X)\nkm.predict(X)\n\n## opcionalmente\nkm.fit_predict(X)\n\n\nn_clusters: Define el n√∫mero de clusters a crear, por defecto 8.\nn_init: Cu√°ntas veces se ejecuta el algoritmo, por defecto 10.\nrandom_state: Define la semilla aleatoria. Por defecto sin semilla.\ninit: Permite agregar centroides de manera manual.\n.fit(): Entrenar√° el modelo en los datos suministrados.\n.predict() Entregar√° las clusters asignados a cada dato suministrado.\n.clusters_centers_: Entregar√° las coordenadas de los centroides de cada Cluster.\n.inertia_: Entrega valores correspondiente a la within cluster distance.\n\n\nüëÄ Veamos un ejemplo en Colab."
  },
  {
    "objectID": "tics411/clase-3.html#sugerencias",
    "href": "tics411/clase-3.html#sugerencias",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que K-Means es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala com√∫n."
  },
  {
    "objectID": "tics411/clase-3.html#interpretaci√≥n-clusters",
    "href": "tics411/clase-3.html#interpretaci√≥n-clusters",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Interpretaci√≥n Clusters",
    "text": "Interpretaci√≥n Clusters\n\n\n\n\n\n\nRecordar, que el clustering no clasifica. Por lo tanto, a pesar de que K-Means nos indica a qu√© cluster pertenece cierto punto, debemos interpretar cada cluster para entender qu√© es lo que se agrup√≥.\n\n\n\n\n\n\n\n\n\nLa interpretaci√≥n del cluster es principalmente intuici√≥n y exploraci√≥n, por lo tanto el EDA puede ser de utilidad para analizar clusters."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge",
    "href": "tics411/clase-3.html#post-procesamiento-merge",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\nPost-Procesamiento\n\n\nSe define como el tratamiento que podemos realizar al algoritmo luego de haber entregado ya sus predicciones.\n\n\n\nEs posible generar m√°s clusters de los necesarios y luego ir agrupando los m√°s cercanos."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge-1",
    "href": "tics411/clase-3.html#post-procesamiento-merge-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\n\n\n\n\n\n\n\n\n\n\n\n¬øCu√°l es el problema con este caso de Post-Procesamiento?"
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-split",
    "href": "tics411/clase-3.html#post-procesamiento-split",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Post-Procesamiento: Split",
    "text": "Post-Procesamiento: Split\n\n\n\n\n\n\n\n\n\n\n\nEn Scikit-Learn esto puede conseguirse utilizando el par√°metro init. Se entregan los nuevos centroides para forzar a K-Means que separe ciertos clusters."
  },
  {
    "objectID": "tics411/clase-3.html#variantes-k-means",
    "href": "tics411/clase-3.html#variantes-k-means",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variantes K-Means",
    "text": "Variantes K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SMD(p_1,p_2) = 4\\]\n\n\n\n\n\n\n\n\nAc√° pueden encontrar una implementaci√≥n de K-Modes en Python."
  },
  {
    "objectID": "tics411/clase-9.html#intuici√≥n",
    "href": "tics411/clase-9.html#intuici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Intuici√≥n",
    "text": "Intuici√≥n\nSupongamos que tengo que estudiar para la prueba de Miner√≠a de Datos y tengo que aprender a calcular el Coeficiente de Silueta.\n\n\nQu√© pasa si s√≥lo les entrego una pregunta para estudiar y no tiene respuesta.\n¬øQu√© pasa si ahora les doy la respuesta?\n¬øQu√© pasa si te doy m√°s ejercicios?\n¬øQu√© pasa luego de que haces muchos ejercicios?\n\n\n\n\n\n\n\n\n\nVoy aprendiendo mejor la tarea de calcular el coeficiente de Silueta. Lo mismo pasa con los modelos.\n\n\n\n\n\n\n\n\n\n\n\nPero no puedo medir qu√© tan bien aprendiste en los ejercicios que yo ya entregu√© para practicar. Tengo que hacer una prueba que t√∫ no hayas visto, para ver si realmente aprendiste."
  },
  {
    "objectID": "tics411/clase-9.html#uso-de-un-modelo",
    "href": "tics411/clase-9.html#uso-de-un-modelo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Uso de un Modelo",
    "text": "Uso de un Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n¬øC√≥mo saber que el modelo est√° funcionando como esperamos?"
  },
  {
    "objectID": "tics411/clase-9.html#m√©tricas",
    "href": "tics411/clase-9.html#m√©tricas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "M√©tricas",
    "text": "M√©tricas\nEl Rendimieto de un Modelo de Clasificaci√≥n permite evaluar el error asociado al proceso de predicci√≥n.\n\n\n\n\nClase Positiva\n\nCorresponde a la clase/evento de inter√©s. Ej: Tiene cancer, va a pagar su deuda, es un gato. Normalmente se denota como la Clase 1.\n\nClase Negativa\n\nCorresponde a la clase/evento contrario al de inter√©s. Ej: No tiene cancer, no va a pagar su deuda, no es un gato. Normalmente se denota como la Clase 0.\n\n\n\n\n\n\n\n\n\n\n\nScikit-Learn usa la siguiente convenci√≥n:\n\nSi se llama *_score un mayor puntaje es mejor.\nSi se llama *_error o *_loss un mejor puntaje es mejor."
  },
  {
    "objectID": "tics411/clase-9.html#m√©tricas-matriz-de-confusi√≥n",
    "href": "tics411/clase-9.html#m√©tricas-matriz-de-confusi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "M√©tricas: Matriz de Confusi√≥n",
    "text": "M√©tricas: Matriz de Confusi√≥n\n\nLa Matriz de Confusi√≥n ordena los valores correctamente predichos y tambi√©n los distintos errores que el modelo puede cometer.\n\n\n\n\n\n\n\n\n\n\nTP (Verdaderos Positivos)\n\nCorresponde a valores reales de la clase 1 que fueron correctamente predichos como clase 1.\n\nTN (Verdaderos Negativos)\n\nCorresponde a valores reales de la clase 0 que fueron correctamente predichos como clase 0.\n\nFP (Falsos Positivos)\n\nCorresponde a valores reales de la clase 0 que fueron incorrectamente predichos como clase 1.\n\nFN (Falsos Negativos)\n\nCorresponde a valores reales de la clase 1 que fueron incorrectamente predichos como clase 0."
  },
  {
    "objectID": "tics411/clase-9.html#m√©tricas-a-partir-de-la-matriz-de-confusi√≥n",
    "href": "tics411/clase-9.html#m√©tricas-a-partir-de-la-matriz-de-confusi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "M√©tricas: A partir de la Matriz de Confusi√≥n",
    "text": "M√©tricas: A partir de la Matriz de Confusi√≥n\n\n\n\n\n\nAccuracy\n\n\n\\[\\frac{TP + TN}{TP + TN + FP + FN}\\]\n\n\n\n\n\n\nPrecision\n\n\n\\[\\frac{TP}{TP + FP}\\]\n\n\n\n\n\n\n\nRecall\n\n\n\\[\\frac{TP}{TP + FN}\\]\n\n\n\n\n\n\nF1-Score\n\n\n\\[\\frac{2\\cdot Precision \\cdot Recall}{Precision + Recall} = \\frac{2 \\cdot TP}{2\\cdot TP + FP + FN}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy es probablemente la m√©trica m√°s sencilla y m√°s utilizada.\nPrecision y Recall ponderar√°n distintos errores (FP y FN respectivamente) con mayor severidad. Ambas m√©tricas son Antagonistas.\nF1-Score corresponde a la media arm√≥nica del Precision y Recall, y tiende a ponderar los errores de manera m√°s balanceada.\n\n\n\n\n¬øCu√°ndo utilizar cada tipo de error?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-roc",
    "href": "tics411/clase-9.html#curva-roc",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Curva ROC",
    "text": "Curva ROC\nLa curva ROC fue desarrollada en 1950 para analizar se√±ales ruidosas. La curva ROC permite al operador contrapesar la tasa de verdaderos positivos (Eje \\(y\\)) versus los falsos positivos (Eje x).\n\nEl √°rea bajo la curva representa la calidad del modelo. Una manera de interpretarla es como la probabilidad de que una predicci√≥n de la clase positiva tenga mayor probabilidad que una de clase negativa. En otras palabras, mide que las probabilidades se encuentren correctamente ordenadas. Por lo tanto var√≠a entre 0.5 y 1.\n\n\n\n\n\n\n\n\n\nROC \\(\\sim\\) 0.5\n\n\n\n\n\nROC \\(\\sim\\) 1"
  },
  {
    "objectID": "tics411/clase-9.html#implementaci√≥n-en-python",
    "href": "tics411/clase-9.html#implementaci√≥n-en-python",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python",
    "text": "Implementaci√≥n en Python\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\naccuracy_score(y_true, y_pred)\nprecision_score(y_true, y_pred)\nrecall_score(y_true, y_pred)\nf1_score(y_true, y_pred)\nroc_auc_score(y_true, y_proba)\n\ny_true: Corresponde a las etiquetas reales del Dataset.\ny_pred: Corresponde a las predicciones realizadas por el modelo.\ny_proba: Corresponden a las probabilidades predichas por el modelo (si es que el modelo lo permite)."
  },
  {
    "objectID": "tics411/clase-9.html#implementaci√≥n-en-python-matriz-de-confusi√≥n",
    "href": "tics411/clase-9.html#implementaci√≥n-en-python-matriz-de-confusi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python: Matriz de Confusi√≥n",
    "text": "Implementaci√≥n en Python: Matriz de Confusi√≥n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
  },
  {
    "objectID": "tics411/clase-9.html#implementaci√≥n-en-python-curva-roc",
    "href": "tics411/clase-9.html#implementaci√≥n-en-python-curva-roc",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python: Curva ROC",
    "text": "Implementaci√≥n en Python: Curva ROC\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_predictions(y_true, y_proba)"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-training",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-training",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Curva de Aprendizaje: Training",
    "text": "Curva de Aprendizaje: Training\n\n\n\n\n\n\n\n\n\n\n\n¬øQu√© ser√≠a la Complejidad del Modelo?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-validaci√≥n",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-validaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Curva de Aprendizaje: Validaci√≥n",
    "text": "Curva de Aprendizaje: Validaci√≥n\n\n\n\n\n\n\n\n\n\n\n\n¬øPor qu√© el modelo pierde rendimiento cuando aumenta su Complejidad?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-mejor-ajuste",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-mejor-ajuste",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Curva de Aprendizaje: Mejor Ajuste",
    "text": "Curva de Aprendizaje: Mejor Ajuste\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting:\n\n\nGran diferencia entre Training y Validation Score.\n\n\n\n\n\n\n\n\n\nUnderfitting:\n\n\nPoca diferencia entre Training y Validation Score, pero con ambos puntajes ‚Äúrelativamente bajos‚Äù.\n\n\n\n\n\n\n\n\n\nProper fitting o Sweet Spot:\n\n\nCorresponde al mejor puntaje en el set de Validaci√≥n. Donde tambi√©n la distancia entre Train y Test es poca."
  },
  {
    "objectID": "tics411/clase-9.html#complejidad-de-un-modelo",
    "href": "tics411/clase-9.html#complejidad-de-un-modelo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Complejidad de un Modelo",
    "text": "Complejidad de un Modelo\n¬øQu√© modelo es un mejor clasificador?"
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff",
    "href": "tics411/clase-9.html#bias-variance-tradeoff",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Bias Variance Tradeoff",
    "text": "Bias Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos puntos azules ser√°n puntos que usaremos para entrenar.\nLos puntos verdes ser√°n puntos que usaremos para validar."
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff-bias",
    "href": "tics411/clase-9.html#bias-variance-tradeoff-bias",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Bias Variance Tradeoff: Bias",
    "text": "Bias Variance Tradeoff: Bias\n\nBias\n\n\nSe refiere a la incapacidad de un modelo de capturar la verdadera relaci√≥n entre los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo est√° ‚Äúsesgado‚Äù a tomar una cierta relaci√≥n que no necesariamente existe."
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff-variance",
    "href": "tics411/clase-9.html#bias-variance-tradeoff-variance",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Bias Variance Tradeoff: Variance",
    "text": "Bias Variance Tradeoff: Variance\n\nVariance\n\n\nSe refiere a la diferencia de ajuste entre datasets (Train y Validaci√≥n).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo var√≠a demasiado su comportamiento entre Training y Testing Time."
  },
  {
    "objectID": "tics411/clase-9.html#complejidad-de-un-modelo-1",
    "href": "tics411/clase-9.html#complejidad-de-un-modelo-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Complejidad de un Modelo",
    "text": "Complejidad de un Modelo\n\n\nOverfitting\n\n\n\n\n\n\n\n\n\n\n\n\nRegularizaci√≥n: Se refiere a una penalizaci√≥n para disminuir su complejidad.\n\nModelos m√°s simples: Utilizar modelos con una Frontera de Decisi√≥n m√°s simple.\nM√°s datos!!! M√°s datos m√°s dificil aprender, por lo tanto, modelos complejos se ven m√°s beneficiados de esto.\n\n\n\n\n\n\nUnderfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuitar Regularizaci√≥n\nModelos m√°s complejos\nM√°s variabilidad en los datos!!! Podr√≠a ser que los datos no permitan aprender patrones m√°s complejos."
  },
  {
    "objectID": "tics411/clase-9.html#c√≥mo-generamos-sets-de-validaci√≥n",
    "href": "tics411/clase-9.html#c√≥mo-generamos-sets-de-validaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øC√≥mo generamos sets de Validaci√≥n?",
    "text": "¬øC√≥mo generamos sets de Validaci√≥n?\n\nLa evaluaci√≥n de modelos supervisados es fundamental. De no hacerlo de forma correcta podemos quedarnos con una idea muy equivocada del rendimiento del modelo.\n\n\nCross Validation (Validaci√≥n Cruzada)\n\n\nSe debe evaluar el rendimiento de un modelo en un dataset diferente al que fue entrenado. Esta es la √∫nica manera en la que se puede medir el poder de generalizaci√≥n de un modelo.\n\n\nGeneralizaci√≥n\n\n\nCorresponde a la habilidad de un modelo de adaptarse apropiadamente a datos no vistos previamente.\n\n\n\n\n\n\n\n\n\nUtilizar una estrategia incorrecta de Validaci√≥n puede llevar a problemas de generalizaci√≥n. La estrategia de Validaci√≥n debe ser lo m√°s parecida posible a c√≥mo se utilizar√° el modelo en Producci√≥n.\n\n\n\n\n\n\n\n\n\nPara esto se asume que todos los datos son i.i.d (independent and identically distributed). De no lograr esto, lograr buenos rendimientos es m√°s dif√≠cil."
  },
  {
    "objectID": "tics411/clase-9.html#validaci√≥n-cruzada-holdout",
    "href": "tics411/clase-9.html#validaci√≥n-cruzada-holdout",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Validaci√≥n Cruzada: Holdout",
    "text": "Validaci√≥n Cruzada: Holdout\n\nTambi√©n es conocido como Train Test Split o simplemente Split. Corresponde a la separacion de nuestra data cuando con el proposito de aislar observaciones que el modelo no vea para una correcta evaluaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set es la porci√≥n de los datos que se utilizar√° exclusivamente para entrenar los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl test set es la porci√≥n de los datos que se utilizar√° exclusivamente para validar los datos.\nEl test set simula los datos que eventualmente entrar√°n el modelo para obtener una predicci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utilizan splits del tipo 70/30, 80/20 o 90/10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øCu√°l es el problema con este tipo de validaci√≥n?"
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout",
    "href": "tics411/clase-9.html#variante-holdout",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variante Holdout",
    "text": "Variante Holdout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe agrega un validation set el cu√°l se utilizar√° para escoger los hiperpar√°metros que muestren un mejor poder de generalizaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set y el test set cumplen la misma funci√≥n que ten√≠an antes."
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout-procedimiento",
    "href": "tics411/clase-9.html#variante-holdout-procedimiento",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variante Holdout: Procedimiento",
    "text": "Variante Holdout: Procedimiento\n\n\n\n\n\n\n\n\n\nProcedimiento\n\nRepetir para cada Modelo a probar.\n\n\n\n\n\n\n\n\nVamos a entender un modelo como la combinaci√≥n de un Algoritmo de Aprendizaje + Hiperpar√°metros + Preprocesamiento.\n\n\n\n\n\n\nSe entrena cada Modelo en el train set. Se mide una m√©trica de Evaluaci√≥n apropiada utilizando el Validation Set. La llamaremos m√©trica de Validaci√≥n.\nSe escoge el mejor Modelo como el que tenga la mejor m√©trica de Validaci√≥n.\nSe reentrena el modelo escogido pero ahora en un ‚Äúnuevo set‚Äù compuesto por el Train set + el Validation set.\nSe reporta el rendimiento final del mejor modelo (al momento del dise√±o) utilizando m√©tricas medidas en el Test Set."
  },
  {
    "objectID": "tics411/clase-9.html#k-fold-cv",
    "href": "tics411/clase-9.html#k-fold-cv",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Fold CV",
    "text": "K-Fold CV\n\n\n\n\n\n\n\nEl proceso de Holdout podr√≠a llevar a un proceso de overfitting del Test Set si el modelo no es lo suficientemente robusto.\n\n\n\n\n\n\n\n\n\n\n\n\nEl K-Fold CV se aplica s√≥lo al Train Set y la m√©trica final que se reporta utilizando el Test Set.\n\n\n\n\n\n\n\n\n\n\nFold\n\nEntenderemos Folds como divisiones que haremos a nuestro dataset. (En el ejemplo se divide el dataset en 5 Folds).\n\nSplit\n\nEntenderemos Splits, como iteraciones. En cada iteraci√≥n utilizaremos un Fold como Validation Set y todos los Folds restantes como Train Set.\n\n\n\n\n\n\n\n\n\nLa m√©trica final se calcular√° como el promedio de las M√©tricas de Validaci√≥n para cada Split.\nA veces la variabilidad (medido a trav√©s de la Desviaci√≥n Est√°ndar) tambi√©n es usado como criterio para elegir el mejor modelo.\n\n\n\n\n\n\n\n\n\n\n\n\nEn la pr√°ctica se le llama incorrectamente Cross Validation al K-Fold."
  },
  {
    "objectID": "tics411/clase-9.html#bootstrap",
    "href": "tics411/clase-9.html#bootstrap",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Bootstrap",
    "text": "Bootstrap\nConsiste en generar subgrupos aleatorios con repetici√≥n. Normalmente requiere espec√≠ficar el tama√±o de la muestra de entrenamiento. Y la cantidad de repeticiones que del proceso. Los sets de validaci√≥n (en morado) ac√° se denominan out-of-bag samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa m√©trica final a reportar se mide como el promedio de los out-of-bag samples."
  },
  {
    "objectID": "tics411/clase-9.html#variantes-y-consejos",
    "href": "tics411/clase-9.html#variantes-y-consejos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variantes y Consejos",
    "text": "Variantes y Consejos\n\nStratified K-Fold\n\nEs la variante m√°s utilizada de K-Fold el cual genera los folds considerando que se mantenga la proporci√≥n de etiquetas en cada Fold.\n\nLeave One Out\n\nSer√≠a una variante con \\(K=n\\). Por lo tanto, el Validation Set tiene s√≥lo una observaci√≥n.\n\n\n\n\n\n\n\n\n\n¬øCuando usar cada uno?\n\n\n\nSi se tiene una cantidad de datos suficiente (normalmente tama√±os muy grandes se prefiere) el Holdout.\n\nEntre m√°s registros, menos % de Validation Set se deja.\n\nSi se requiere robustez, o hay Test sets que son muy variables se prefiere K-Fold.\n\nSi es que hay desbalance de clases, se prefiere la versi√≥n Stratified.\n\n\nSi se tienen muy pocos datos, entonces utilizar Leave-One-Out.\nBootstrap tambi√©n es utilizado cuando se tengan pocos datos. Aunque suele ser un approach m√°s estad√≠stico."
  },
  {
    "objectID": "tics411/clase-9.html#baseline",
    "href": "tics411/clase-9.html#baseline",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Baseline",
    "text": "Baseline\n\nUn modelo Baseline es un modelo simple, normalmente sin aprendizaje asociado o con poder de aprendizaje m√°s limitado, el cu√°l ser√° utilizado como medida de referencia para ver si algoritmos m√°s complejos efectivamente est√°n aprendiendo.\n\n\n\n\n\n\n\nSi estamos probando un nuevo modelo y √©ste es capaz de superar el rendimiento de un Baseline, se considera como que estamos aprendiendo algo nuevo.\n\n\n\n\n\n\n\n\n\nModelos que no superaron el puntaje de un modelo Baseline normalmente son deshechados."
  },
  {
    "objectID": "tics411/clase-9.html#implementaci√≥n-en-python-baselines",
    "href": "tics411/clase-9.html#implementaci√≥n-en-python-baselines",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python: Baselines",
    "text": "Implementaci√≥n en Python: Baselines\nfrom sklearn.dummy import DummyClassifier\n\ndc = DummyClassifier(strategy=\"prior\", random_state = 42, constant=None)\ndc.fit(X_train,y_train)\ny_pred = dc.predict(X_test)\n\n\n\nstrategy: Corresponde a estrategias ‚Äúdummy‚Äù con las cuales generar predicciones.\n\n‚Äúprior‚Äù: predice siempre la clase m√°s frecuente observada en el entrenamiento. Si se predice la probabilidad, se devuelve la probabilidad emp√≠rica.\n‚Äúconstant‚Äù: Devuelve un valor constante provisto por el usuario.\n‚Äúuniform‚Äù: Predice probabilidades aleatorios obtenidas mediante una distribuci√≥n uniforme."
  },
  {
    "objectID": "tics411/clase-9.html#data-leakage",
    "href": "tics411/clase-9.html#data-leakage",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Data Leakage",
    "text": "Data Leakage\n\nFuga de Datos\n\n\nSe refiere al proceso donde el modelo por alguna raz√≥n conoce informaci√≥n que no deber√≠a conocer. Puede ser informaci√≥n del Test Set o variables que revelan informaci√≥n primordial sobre la etiqueta.\n\n\n\n\n\n\n\n\n\nCuando existe Data Leakage es posible que los resultados del modelo no reflejen correctamente su rendimiento dando una falsa sensaci√≥n de optimismo.\n\n\n\nEjemplos\n\nEstandarizar o aplicar preprocesamientos antes del Split de la Data.\nUtilizar variables que tienen directa relaci√≥n con el Target.\n\n\n\n\n\n\n\n\nSe recomienda siempre que sea posible utilizar Pipelines para poder evitar el Data Leakage."
  },
  {
    "objectID": "tics411/clase-11.html#naive-bayes-preliminares",
    "href": "tics411/clase-11.html#naive-bayes-preliminares",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Naive Bayes: Preliminares",
    "text": "Naive Bayes: Preliminares\n\nTambi√©n conocido como Clasificador Inexperto de Bayes, es uno de los clasificadores m√°s conocidos y sencillos.\n\n\n\n\n\n\n\nSe hizo particularmente conocido como uno de los primeros algoritmos en funcionar como Clasificador de Spam de manera efectiva.\n\n\n\n\nEs un modelo netamente probabil√≠stico basado en el Teorema de Bayes.\n\nAprende una distribucional de Probabilidad Condicional.\nDado un punto \\(x_i\\), el modelo retorna la ‚Äúprobabilidad‚Äù de que \\(x_i\\) pertenezca a una clase espec√≠fica."
  },
  {
    "objectID": "tics411/clase-11.html#definiciones",
    "href": "tics411/clase-11.html#definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\nProbabilidad Condicional\n\n\\[P(X|C) = \\frac{P(X \\cap C)}{P(C)}\\]\n\nTeorema de Bayes\n\n\\[P(C|X) = \\frac{P(X|C)P(C)}{P(X)}\\]\n\nIndependencia Condicional\n\n\\[P(X_1, X_2, ..., X_k|C) = \\prod_{i=1}^k P(X_i|C)\\]\n\n\n\n\n\n\n\n\n\n\nSe lee como la Probabilidad de que Ocurra \\(X\\) dado que tenemos \\(C\\).\n\n\n\n\n\n\n\n\n\n\nLa probabilidad a posteriori (LHS), depende de el Likelihood, la probabilidad a priori y la evidencia (RHS).\n\n\n\n\n\n\n\n\n\n\nSi asumimos independencia, entonces la probabilidad conjunta de \\(k\\) eventos condicionados, se calcula como la productoria de las probabilidades condicionales independientes."
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo-b√°sico",
    "href": "tics411/clase-11.html#ejemplo-b√°sico",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo b√°sico",
    "text": "Ejemplo b√°sico\n\n\n\nSupongamos que:\n\nSabemos que la Meningitis produce Tort√≠colis el 50% de las veces.\nLa probabilidad de tener meningitis es: \\(1/50000\\).\nLa probabilidad de tener Tort√≠colis: \\(1/20\\).\n\n\n\n\nSi su paciente tiene tort√≠colis, ¬øCu√°l es la probabilidad de que tenga Meningitis?\n\\[P(M|T) = \\frac{P(T|M)P(M)}{P(T)}=\\frac{0.5 \\cdot 1/50000}{1/20} = 0.0002\\]"
  },
  {
    "objectID": "tics411/clase-11.html#modelo-naive-bayes-aprendizaje",
    "href": "tics411/clase-11.html#modelo-naive-bayes-aprendizaje",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Modelo Naive Bayes: Aprendizaje",
    "text": "Modelo Naive Bayes: Aprendizaje\n\\[P(y = C_j|X_1, X_2, ..., X_k) = \\frac{P(X_1,X_2,..., X_k|y=C_j)P(y=C_j)}{P(X_1, X_2, ..., X_k)}\\]\n\n\n\n\\(P(y=C_j|X)\\) ser√≠a la probabilidad de que la predicci√≥n del modelo sea \\(C_j\\) dado que lo alimentamos con las variables \\(X\\).\nLuego \\(P(y=C_j)\\) es la probabilidad a priori de que la clase sea \\(C_j\\).\n\\(P(X|y=C_j)\\) es el likelihood (verosimilitud). Corresponde a la distribuci√≥n de probabilidad de las variables X cuando la clase es \\(C_j\\).\n\\(P(X)\\) es la evidencia, y normalmente es muy complejo de calcular.\n\n\n\n\n\n\n\n\nPor simplicidad reduciremos \\(X_1, X_2, ..., X_k\\) a \\(X\\).\n\n\n\n\n\n\n\n\n\n\\(P(X)\\) tiene como √∫nica funci√≥n la de normalizar la probabilidad para que vaya en un rango entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-11.html#modelo-naive-bayes-predicci√≥n",
    "href": "tics411/clase-11.html#modelo-naive-bayes-predicci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Modelo Naive Bayes: Predicci√≥n",
    "text": "Modelo Naive Bayes: Predicci√≥n\n\\[\\hat{y_i} = \\underset{C_j}{argmax} \\: P(y=C_j|X) \\]\ndonde, \\[P(y = C_j|X) \\propto \\prod_{i=1}^k P(X|y=C_j)P(y=C_j)\\]\n\n\n\n\n\n\nLa predicci√≥n de Naive Bayes corresponde a la clase que entrega [un estimado de] la Probabilidad a Posteriori m√°s grande."
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo",
    "href": "tics411/clase-11.html#ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n¬øC√≥mo clasificamos el siguiente punto?\n\\[ X = [C=Soleado,T=Media,H=Alta,V=D√©bil]\\]\n\n\n\n\n\n\nProbabilidad de S√≠\n\n\n\\[P(y = S√≠|X) = P(X|y=S√≠)P(y=S√≠)\\]\n\n\n\n\n\n\nProbabilidad de No\n\n\n\\[ P(y = No|X) = P(X|y=No)P(y=No)\\]"
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo-1",
    "href": "tics411/clase-11.html#ejemplo-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\\[ P(y = S√≠|X) = P(C=Soleado|y=S√≠)P(T=Media|y=S√≠)P(H=Alta|y=S√≠)P(V=D√©bil|y=S√≠)P(y=S√≠)\\]\n\n\n\n\n\n\n\\[ P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=D√©bil|y=No)P(y=No)\\]\n\n\n\n\n\nProbabilidad Condicional para clase S√≠\n\\[\\small P(C = Soleado|y = S√≠) = 2/9\\] \\[\\small P(T = Media|y = S√≠) = 4/9\\]\n\\[\\small P(H = Alta|y = S√≠) = 3/9\\] \\[\\small P(V=D√©bil|y = S√≠) = 6/9\\]\n\nProbabilidad Condicional para clase No\n\\[\\small P(C = Soleado|y = No) = 3/5\\] \\[\\small P(T = Media|y = No) = 2/5\\]\n\\[\\small P(H = Alta|y = No) = 4/5\\] \\[\\small P(V=D√©bil|y = No) = 2/5\\]\n\nProbabilidad a priori\n\\[P(y = S√≠) = \\frac{9}{14} = 0.642\\] \\[P(y = No) = \\frac{5}{14} = 0.357\\]"
  },
  {
    "objectID": "tics411/clase-11.html#predicci√≥n",
    "href": "tics411/clase-11.html#predicci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Predicci√≥n",
    "text": "Predicci√≥n\n\n\n\n\\[\\scriptsize P(y = S√≠|X) = P(C=Soleado|y=S√≠)P(T=Media|y=S√≠)P(H=Alta|y=S√≠)P(V=D√©bil|y=S√≠)P(y=S√≠)\\] \\[\\small P(y = S√≠|X) = \\frac{2}{9} \\cdot \\frac{4}{9} \\cdot \\frac{3}{9} \\cdot \\frac{6}{9} \\cdot \\frac{9}{14} = 0.0141\\]\n\n\n\n\n\n\n\\[\\scriptsize P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=D√©bil|y=No)P(y=No)\\] \\[\\small P(y = No|X) = \\frac{3}{5} \\cdot \\frac{2}{5} \\cdot \\frac{4}{5} \\cdot \\frac{2}{5} \\cdot \\frac{5}{14} = 0.0274\\]\n\n\n\n\n\n\n\n\n\n\\[\\hat{y} = argmax \\{0.0141, 0.0274\\} = No\\]"
  },
  {
    "objectID": "tics411/clase-11.html#smoothing",
    "href": "tics411/clase-11.html#smoothing",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Smoothing",
    "text": "Smoothing\nSupongamos otro dataset m√°s peque√±o:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDado que Naive Bayes se calcula como una Productoria, al tener probabilidades 0 inmediatamente la Probabilidad a Posteriori es 0.\n\n\n\n\\[ P(Clima = Soleado|y = S√≠) = \\frac{0}{6}\\] \\[ P(Clima = Soleado|y = No) = \\frac{5}{8}\\]\n\n\\[P(X_j|C = i) = \\frac{N_{yj} + \\alpha}{N_y + M\\alpha}\\]\n\n\\(\\alpha\\): Es un Hiperpar√°metro. Si \\(\\alpha = 1\\) se le llama Laplace Smoothing, si \\(\\alpha &lt;1\\) entonces se le llama Lidstone Smoothing.\nM: Corresponde al n√∫mero de posibles valores que puede tomar \\(X_j\\)\n\\(N_{yj}\\): Corresponde a la cantidad de registros que toman el valor de la variable \\(X_j\\) solicitado en la clase \\(y\\).\n\\(N_{y}\\): Corresponde a la cantidad de registros totales que tienen la clase \\(y\\)."
  },
  {
    "objectID": "tics411/clase-11.html#laplace-smoothing",
    "href": "tics411/clase-11.html#laplace-smoothing",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Laplace Smoothing",
    "text": "Laplace Smoothing\n\n\nSin Laplace\n\n\n\n\n\n\nCon Laplace\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso \\(\\alpha = 1\\) y \\(M=3\\) ya que Clima puede tomar 3 valores: Soleado, Cubierto y Lluvia."
  },
  {
    "objectID": "tics411/clase-11.html#variables-continuas",
    "href": "tics411/clase-11.html#variables-continuas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variables Continuas",
    "text": "Variables Continuas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos calcular el Likelihood como una PDF (Probability Density Function). La m√°s com√∫n: Distribuci√≥n Normal (Gaussian Naive Bayes).\n\n\n\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]"
  },
  {
    "objectID": "tics411/clase-11.html#variables-continuas-predicci√≥n",
    "href": "tics411/clase-11.html#variables-continuas-predicci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variables Continuas: Predicci√≥n",
    "text": "Variables Continuas: Predicci√≥n\n\n\n\n\\[P(humedad=74|y = S√≠) = \\frac{1}{\\sqrt{2\\pi \\cdot 10.2^2}}e^{-\\frac{(74-79.1)^2}{2\\cdot 10.2^2}} = 0.0345 \\]\n\n\n\n\n\n\n\\[P(humedad=74|y = No) = \\frac{1}{\\sqrt{2\\pi \\cdot 9.7^2}}e^{-\\frac{(74-86.2)^2}{2\\cdot 9.7^2}} = 0.01865 \\]\n\n\n\n\n\n\n\n\n\nLuego la predicci√≥n es S√≠."
  },
  {
    "objectID": "tics411/clase-11.html#detalles-t√©cnicos",
    "href": "tics411/clase-11.html#detalles-t√©cnicos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Detalles T√©cnicos",
    "text": "Detalles T√©cnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nF√°cil de Implementar\nA menudo tiene un rendimiento decente a pesar de que las variables pueden no ser independientes.\nPuede aprender de forma incremental.\nValores faltantes son ignorados en el proceso de Aprendizaje.\nModelo robusto frente a datos at√≠picos y/o irrelevantes.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAsumir clases condicionadas produce probabilidades sesgadas.\nDependencias entre las variables no pueden ser modeladas."
  },
  {
    "objectID": "tics411/clase-11.html#implementaci√≥n-en-scikit-learn",
    "href": "tics411/clase-11.html#implementaci√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Scikit-Learn",
    "text": "Implementaci√≥n en Scikit-Learn\nMultinomial Naive Bayes (Normal)\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB(alpha = 1)\nnb.fit(X_train, y_train)\n\ny_pred = nb.predict(X_test)\ny_proba = nb.predict_proba(X_test)\nGaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngb = GaussianNB()\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\ny_proba = gb.predict_proba(X_test)"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html",
    "href": "tics411/notebooks/10-resolucion_guia.html",
    "title": "K-Means",
    "section": "",
    "text": "import pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(dict(x=[0, 0, 1, 4, 5, 6], y=[1, 0, 0, 4, 4, 6]))\ndisplay(df)\nd_matrix = pd.DataFrame(distance_matrix(df, df))\nd_matrix\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n1\n\n\n1\n0\n0\n\n\n2\n1\n0\n\n\n3\n4\n4\n\n\n4\n5\n4\n\n\n5\n6\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.000000\n1.000000\n1.414214\n5.000000\n5.830952\n7.810250\n\n\n1\n1.000000\n0.000000\n1.000000\n5.656854\n6.403124\n8.485281\n\n\n2\n1.414214\n1.000000\n0.000000\n5.000000\n5.656854\n7.810250\n\n\n3\n5.000000\n5.656854\n5.000000\n0.000000\n1.000000\n2.828427\n\n\n4\n5.830952\n6.403124\n5.656854\n1.000000\n0.000000\n2.236068\n\n\n5\n7.810250\n8.485281\n7.810250\n2.828427\n2.236068\n0.000000"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#k-means",
    "href": "tics411/notebooks/10-resolucion_guia.html#k-means",
    "title": "K-Means",
    "section": "K-Means",
    "text": "K-Means\n\ncentroides = pd.DataFrame(dict(x=[1, 5], y=[1, 6]))\ncentroides_2 = pd.DataFrame(dict(x=[1 / 3, 5], y=[1 / 3, 14 / 3]))\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(df.x, df.y)\nplt.scatter(centroides.x, centroides.y, c=\"red\")\nplt.scatter(centroides_2.x, centroides_2.y, c=\"green\")\nplt.title(\"Centroides Iter 1: Rojo, Iter 2: Verde\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Distancia Centroides 1 a Puntos\npd.DataFrame(distance_matrix(centroides, df))\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1.000000\n1.414214\n1.000000\n4.242641\n5.0\n7.071068\n\n\n1\n7.071068\n7.810250\n7.211103\n2.236068\n2.0\n1.000000\n\n\n\n\n\n\n\n\n\n## Distancia Centroides 2 a Puntos\npd.DataFrame(distance_matrix(centroides_2, df))\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.745356\n0.471405\n0.745356\n5.18545\n5.934831\n8.013877\n\n\n1\n6.200358\n6.839428\n6.146363\n1.20185\n0.666667\n1.666667"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "href": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "title": "K-Means",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=2, eps=2)\ndbs.fit_predict(df)\n\narray([ 0,  0,  0,  1,  1, -1])\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=1, eps=1)\ndbs.fit_predict(df)\n\narray([0, 0, 0, 1, 1, 2])"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "href": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "title": "K-Means",
    "section": "Jerarquico Linkage Complete",
    "text": "Jerarquico Linkage Complete\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, M√©todo: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nplot_dendogram(df, link=\"complete\")"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#cohesi√≥n-y-separaci√≥n",
    "href": "tics411/notebooks/10-resolucion_guia.html#cohesi√≥n-y-separaci√≥n",
    "title": "K-Means",
    "section": "Cohesi√≥n y Separaci√≥n",
    "text": "Cohesi√≥n y Separaci√≥n\n\nimport numpy as np\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nlabels = np.array([0, 0, 0, 1, 1, 1])\ncenters = centroides_2.values\nsse, ssb = compute_clustering_metrics(df, labels, centers, is_df=True)\nsse, ssb\n\n(6.0, 60.833333333333336)"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "href": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "title": "K-Means",
    "section": "Silhouette",
    "text": "Silhouette\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, 1]\n    return a, b, bj, n_clusters\n\n\nd_matrix = distance_matrix(df, df)\na, b, bj, n_clusters = silhouette_score_m(d_matrix, labels)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.7517302154855591\n\n\n\ns_score_table\n\n\n\n\n\n\n\n\n\na\nb0\nb1\nb\ns\n\n\n\n\n0\n1.207107\n0.000000\n6.213734\n6.213734\n0.805736\n\n\n1\n1.000000\n0.000000\n6.848420\n6.848420\n0.853981\n\n\n2\n1.207107\n0.000000\n6.155701\n6.155701\n0.803904\n\n\n3\n1.914214\n5.218951\n0.000000\n5.218951\n0.633219\n\n\n4\n1.618034\n5.963643\n0.000000\n5.963643\n0.728684\n\n\n5\n2.532248\n8.035260\n0.000000\n8.035260\n0.684858"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html",
    "href": "tics411/notebooks/07-ex-evaluation.html",
    "title": "Evaluaci√≥n de Clusters",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pyclustertend import vat, hopkins, ivat\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nnp.random.seed(0)\n\nset_config(transform_output=\"pandas\")\n\nX = sns.load_dataset(\"iris\").drop(columns=\"species\")\nX_random = np.random.rand(150, 4)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#evaluaci√≥n-de-clusters",
    "href": "tics411/notebooks/07-ex-evaluation.html#evaluaci√≥n-de-clusters",
    "title": "Evaluaci√≥n de Clusters",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pyclustertend import vat, hopkins, ivat\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nnp.random.seed(0)\n\nset_config(transform_output=\"pandas\")\n\nX = sns.load_dataset(\"iris\").drop(columns=\"species\")\nX_random = np.random.rand(150, 4)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#visualizaci√≥n-de-ambos-datasets",
    "href": "tics411/notebooks/07-ex-evaluation.html#visualizaci√≥n-de-ambos-datasets",
    "title": "Evaluaci√≥n de Clusters",
    "section": "Visualizaci√≥n de ambos Datasets",
    "text": "Visualizaci√≥n de ambos Datasets\n\n!pip install pyclustertend\n\nRequirement already satisfied: pyclustertend in /home/datacuber/miniconda3/lib/python3.9/site-packages (1.8.2)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.1.2 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.4.1.post1)\nRequirement already satisfied: numba&lt;0.55.0,&gt;=0.54.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (0.54.1)\nRequirement already satisfied: matplotlib&lt;4.0.0,&gt;=3.3.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (3.7.5)\nRequirement already satisfied: numpy==1.20.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.20.3)\nRequirement already satisfied: pandas&lt;2.0.0,&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.5.3)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (9.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (23.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (0.11.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.2.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (6.4.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (4.37.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.4.4)\nRequirement already satisfied: setuptools in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (67.5.1)\nRequirement already satisfied: llvmlite&lt;0.38,&gt;=0.37.0rc1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (0.37.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pandas&lt;2.0.0,&gt;=1.2.0-&gt;pyclustertend) (2022.2.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (3.1.0)\nRequirement already satisfied: scipy&gt;=1.6.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.3.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.11.0)\nRequirement already satisfied: six&gt;=1.5 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.16.0)\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X)\npca = PCA(n_components=2)\npca_random = pca.fit_transform(X_random)\n\n\ndef compute_hopkins(X, p):\n    h_s = 1 - hopkins(X, p)\n    print(f\"Hopskins para p={p} es: {h_s}\")\n    return h_s\n\n\nhs_X = compute_hopkins(X, p=50)\nhs_random = compute_hopkins(X_random, p=50)\n\nfig, ax = plt.subplot_mosaic([[\"iris\", \"random\"]], figsize=(15, 6))\n\nax[\"iris\"].scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nax[\"random\"].scatter(pca_random[\"pca0\"], pca_random[\"pca1\"])\nax[\"random\"].set_title(\n    f\"Reducci√≥n a 2D de nuestros puntos aleatorios. H = {hs_random:.2f}\"\n)\nax[\"iris\"].set_title(f\"Reducci√≥n a 2D de Iris. H = {hs_X:.2f}\")\nplt.show()\n\nHopskins para p=50 es: 0.8241582644992403\nHopskins para p=50 es: 0.48048319214476964"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "title": "Evaluaci√≥n de Clusters",
    "section": "VAT: Iris",
    "text": "VAT: Iris\n\nimport matplotlib.pyplot as plt\n\nvat(X_sc)\nplt.title(\"VAT para Iris Escalado\")\nivat(X_sc)\nplt.title(\"iVAT para Iris Escalado\")\n\nText(0.5, 1.0, 'iVAT para Iris Escalado')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "title": "Evaluaci√≥n de Clusters",
    "section": "VAT: Random",
    "text": "VAT: Random\n\nvat(X_random)\nplt.title(\"VAT para Dataset Random\")\nivat(X_random)\nplt.title(\"iVAT para Dataset Random\")\n\nText(0.5, 1.0, 'iVAT para Dataset Random')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#correlaci√≥n",
    "href": "tics411/notebooks/07-ex-evaluation.html#correlaci√≥n",
    "title": "Evaluaci√≥n de Clusters",
    "section": "Correlaci√≥n",
    "text": "Correlaci√≥n\n\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance_matrix\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X_sc)\n\n\ndef cluster_correlation(X, labels, p=2):\n    \"\"\"p corresponde al nivel de la distancia de Minkowski\"\"\"\n    ideal_sim = (labels == labels.reshape(-1, 1)).astype(np.float32)\n\n    d_matrix = distance_matrix(X, X, p=p)\n    S = 1 / (d_matrix + 1)\n    return np.corrcoef(S.flatten(), ideal_sim.flatten()).min()\n\n\ncluster_correlation(X_sc, labels)\n\n0.6856891998862197"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#cohesi√≥n-y-separaci√≥n",
    "href": "tics411/notebooks/07-ex-evaluation.html#cohesi√≥n-y-separaci√≥n",
    "title": "Evaluaci√≥n de Clusters",
    "section": "Cohesi√≥n y Separaci√≥n",
    "text": "Cohesi√≥n y Separaci√≥n\n\ncenters = km.cluster_centers_\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nsse, ssb = compute_clustering_metrics(X_sc, labels, centers, is_df=True)\nsse, ssb\n\n(222.36170496502297, 377.638295034977)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "href": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "title": "Evaluaci√≥n de Clusters",
    "section": "Ejemplo de Clases",
    "text": "Ejemplo de Clases\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        x=[2, 3, 4, 8, 9, 10, 6, 7, 8],\n        y=[5, 4, 6, 3, 2, 5, 10, 8, 9],\n        c=[0, 0, 0, 1, 1, 1, 2, 2, 2],\n    )\n)\n\nd_matrix = distance_matrix(df[[\"x\", \"y\"]], df[[\"x\", \"y\"]], p=2)\nplt.scatter(df.x, df.y, c=df.c, s=200, edgecolors=\"k\")\n\ndf\n\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n0\n2\n5\n0\n\n\n1\n3\n4\n0\n\n\n2\n4\n6\n0\n\n\n3\n8\n3\n1\n\n\n4\n9\n2\n1\n\n\n5\n10\n5\n1\n\n\n6\n6\n10\n2\n\n\n7\n7\n8\n2\n\n\n8\n8\n9\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(df[[\"x\", \"y\"]], df.c)\n\n0.614855027897113\n\n\n\n## Esta funci√≥n se hizo s√≥lo para mostrar los pasos intermedios\n## Usen esta funci√≥n para revisar sus resultados cuando estudien para la prueba.\n\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, -1]\n    return a, b, bj, n_clusters\n\n\na, b, bj, n_clusters = silhouette_score_m(d_matrix, df.c.values)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.6395238095238095"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "href": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "title": "Evaluaci√≥n de Clusters",
    "section": "Silhouette Curve",
    "text": "Silhouette Curve\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X_sc, labels)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/09-ex-apriori.html",
    "href": "tics411/notebooks/09-ex-apriori.html",
    "title": "Algoritmo Apriori",
    "section": "",
    "text": "%%capture\n!pip install mlxtend"
  },
  {
    "objectID": "tics411/notebooks/09-ex-apriori.html#algoritmo-apriori",
    "href": "tics411/notebooks/09-ex-apriori.html#algoritmo-apriori",
    "title": "Algoritmo Apriori",
    "section": "Algoritmo Apriori",
    "text": "Algoritmo Apriori\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\nimport pandas as pd\n\n## Escribir ac√° las Transacciones\ntransactions = [\n    [\"Pan\", \"Mantequilla\", \"Leche\"],\n    [\"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Galletas\", \"Pa√±ales\"],\n    [\"Leche\", \"Pa√±ales\", \"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Pa√±ales\"],\n]\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns=tre.columns_)\ndf_encoded\n\n\n\n\n\n\n\n\n\nCerveza\nGalletas\nLeche\nMantequilla\nPan\nPa√±ales\n\n\n\n\n0\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n2\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n\ndef apriori_algorithm(\n    df,\n    min_supp=0.4,\n    min_conf=0.7,\n    variables=[\n        \"antecedents\",\n        \"consequents\",\n        \"support\",\n        \"confidence\",\n        \"lift\",\n    ],\n):\n\n    frequent_itemsets = apriori(\n        df, min_support=min_supp, use_colnames=True\n    )\n    rules = association_rules(\n        frequent_itemsets, metric=\"confidence\", min_threshold=min_conf\n    )\n\n    return frequent_itemsets, rules[variables]\n\n\nfrequent_itemsets, rules = apriori_algorithm(\n    df_encoded, min_supp=0.4, min_conf=0.7\n)\ndisplay(frequent_itemsets)\ndisplay(rules)\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.4\n(Cerveza)\n\n\n1\n0.4\n(Leche)\n\n\n2\n0.6\n(Mantequilla)\n\n\n3\n0.6\n(Pan)\n\n\n4\n0.6\n(Pa√±ales)\n\n\n5\n0.4\n(Pa√±ales, Cerveza)\n\n\n6\n0.4\n(Mantequilla, Leche)\n\n\n7\n0.4\n(Leche, Pan)\n\n\n8\n0.6\n(Mantequilla, Pan)\n\n\n9\n0.4\n(Mantequilla, Leche, Pan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\n\n\n\n\n0\n(Cerveza)\n(Pa√±ales)\n0.4\n1.0\n1.666667\n\n\n1\n(Leche)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n2\n(Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n3\n(Mantequilla)\n(Pan)\n0.6\n1.0\n1.666667\n\n\n4\n(Pan)\n(Mantequilla)\n0.6\n1.0\n1.666667\n\n\n5\n(Mantequilla, Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n6\n(Leche, Pan)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n7\n(Leche)\n(Mantequilla, Pan)\n0.4\n1.0\n1.666667"
  },
  {
    "objectID": "tics411/notebooks/preguntas-prueba-2.html",
    "href": "tics411/notebooks/preguntas-prueba-2.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\ndf = pd.DataFrame(\n    dict(\n        y=[0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n        y_pred=[1, 1, 0, 0, 0, 1, 1, 1, 0, 0],\n    )\n)\nprint(classification_report(df.y, df.y_pred, digits=2))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0       0.40      0.40      0.40         5\n           1       0.40      0.40      0.40         5\n\n    accuracy                           0.40        10\n   macro avg       0.40      0.40      0.40        10\nweighted avg       0.40      0.40      0.40        10\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\ndf\n\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n4\nMucho\nMedio\n1\n\n\n5\nMucho\nBajo\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\n\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\nX_train\n\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n4\nMucho\nMedio\n1\n\n\n5\nMucho\nBajo\n1\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(X=[7, 5, 3, 5, 2], Y=[4, 7, 5, 7, 3], Clase=[1, 1, 1, -1, -1])\n)\nprint(df.to_latex())\n\n\\begin{tabular}{lrrr}\n\\toprule\n & X & Y & Clase \\\\\n\\midrule\n0 & 7 & 4 & 1 \\\\\n1 & 5 & 7 & 1 \\\\\n2 & 3 & 5 & 1 \\\\\n3 & 5 & 7 & -1 \\\\\n4 & 2 & 3 & -1 \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\ndf = pd.DataFrame(\n    {\n        \"Humedad\": [\n            85,\n            90,\n            86,\n            96,\n            80,\n            65,\n            70,\n            70,\n            95,\n            80,\n            91,\n            70,\n            90,\n            75,\n        ],\n        \"Se juega?\": [\n            \"No\",\n            \"No\",\n            \"S√≠\",\n            \"S√≠\",\n            \"S√≠\",\n            \"S√≠\",\n            \"S√≠\",\n            \"No\",\n            \"No\",\n            \"S√≠\",\n            \"No\",\n            \"S√≠\",\n            \"S√≠\",\n            \"S√≠\",\n        ],\n    }\n)\ndf.index = [*range(1, 15)]\nprint(df.to_latex())\n\n\\begin{tabular}{lrl}\n\\toprule\n & Humedad & Se juega? \\\\\n\\midrule\n1 & 85 & No \\\\\n2 & 90 & No \\\\\n3 & 86 & S√≠ \\\\\n4 & 96 & S√≠ \\\\\n5 & 80 & S√≠ \\\\\n6 & 65 & S√≠ \\\\\n7 & 70 & S√≠ \\\\\n8 & 70 & No \\\\\n9 & 95 & No \\\\\n10 & 80 & S√≠ \\\\\n11 & 91 & No \\\\\n12 & 70 & S√≠ \\\\\n13 & 90 & S√≠ \\\\\n14 & 75 & S√≠ \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\n\ndf = pd.DataFrame(\n    dict(\n        y=[1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2],\n        y_pred=[1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 2],\n    )\n)\n\nprint(classification_report(df.y, df.y_pred, digits=3))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0      0.500     0.333     0.400         3\n           1      0.800     0.800     0.800         5\n           2      0.500     0.667     0.571         3\n\n    accuracy                          0.636        11\n   macro avg      0.600     0.600     0.590        11\nweighted avg      0.636     0.636     0.629        11\n\n\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\nX_train\n\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\n\ny_train = X_train.c\nX_train = X_train.drop(columns=\"c\")\n\noe = OrdinalEncoder()\nX_train_oe = oe.fit_transform(X_train)\nprint(oe.categories_)\ndt = DecisionTreeClassifier()\ndt.fit(X_train_oe, y_train)\n\nplt.figure(figsize=(20, 6))\nplot_tree(dt, filled=True, feature_names=X_train.columns)\nplt.tight_layout()\n\n[array(['Mucho', 'Poco'], dtype=object), array(['Alto', 'Bajo', 'Medio'], dtype=object)]\n\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        Brillo=[40, 50, 60, 10, 70, 60, 25],\n        Saturacion=[20, 50, 90, 25, 70, 10, 80],\n    )\n)\n\ndisplay(df)\ndf = pd.DataFrame(distance_matrix(df, df, p=2))\ndf.index = [*range(1, 8)]\ndf.columns = [*range(1, 8)]\ndf\n\n\n\n\n\n\n\n\n\nBrillo\nSaturacion\n\n\n\n\n0\n40\n20\n\n\n1\n50\n50\n\n\n2\n60\n90\n\n\n3\n10\n25\n\n\n4\n70\n70\n\n\n5\n60\n10\n\n\n6\n25\n80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n1\n0.000000\n31.622777\n72.801099\n30.413813\n58.309519\n22.360680\n61.846584\n\n\n2\n31.622777\n0.000000\n41.231056\n47.169906\n28.284271\n41.231056\n39.051248\n\n\n3\n72.801099\n41.231056\n0.000000\n82.006097\n22.360680\n80.000000\n36.400549\n\n\n4\n30.413813\n47.169906\n82.006097\n0.000000\n75.000000\n52.201533\n57.008771\n\n\n5\n58.309519\n28.284271\n22.360680\n75.000000\n0.000000\n60.827625\n46.097722\n\n\n6\n22.360680\n41.231056\n80.000000\n52.201533\n60.827625\n0.000000\n78.262379\n\n\n7\n61.846584\n39.051248\n36.400549\n57.008771\n46.097722\n78.262379\n0.000000\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nnp.sqrt((40 - 60) ** 2 + (20 - 10) ** 2)\n\n22.360679774997898\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html",
    "href": "tics411/notebooks/12-ex-CV.html",
    "title": "Holdout (Train Test Split)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape\n\n((891, 5), (891,))\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nnum_cols, cat_cols\n\n(['fare', 'age'], ['class', 'sex', 'embark_town'])"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#holdout-train-test-split",
    "href": "tics411/notebooks/12-ex-CV.html#holdout-train-test-split",
    "title": "Holdout (Train Test Split)",
    "section": "Holdout (Train Test Split)",
    "text": "Holdout (Train Test Split)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nX_train.shape, X_test.shape\n\n((668, 5), (223, 5))\n\n\n\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef make_pipeline(k, scale=None):\n    if scale is not None:\n        scaler = SklearnTransformerWrapper(\n            StandardScaler(), variables=num_cols\n        )\n    else:\n        scaler = StandardScaler()\n\n    pipe = Pipeline(\n        steps=[\n            (\"ci\", CategoricalImputer(imputation_method=\"frequent\")),\n            (\"mmi\", MeanMedianImputer(imputation_method=\"mean\")),\n            (\"ohe\", OneHotEncoder()),\n            (\"sc\", scaler),\n            (\"model\", KNeighborsClassifier(n_neighbors=k)),\n        ]\n    )\n    return pipe\n\n\npipe = make_pipeline(k=5)\n# pipe = make_pipeline(k=5, scale = num_cols)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\npipe.score(X_test, y_test)\n\n0.7757847533632287"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#way-holdout-train-validation-test-split",
    "href": "tics411/notebooks/12-ex-CV.html#way-holdout-train-validation-test-split",
    "title": "Holdout (Train Test Split)",
    "section": "3-way Holdout (Train Validation Test Split)",
    "text": "3-way Holdout (Train Validation Test Split)\n\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\nfor k in [3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    pipe.fit(X_train, y_train)\n    metric = pipe.score(X_val, y_val)\n    print(f\"Puntaje del Modelo, k = {k}: {metric}\")\n    print(\"=============================================\")\n\nPuntaje del Modelo, k = 3: 0.7982062780269058\n=============================================\nPuntaje del Modelo, k = 5: 0.7757847533632287\n=============================================\nPuntaje del Modelo, k = 7: 0.7892376681614349\n=============================================\nPuntaje del Modelo, k = 9: 0.8026905829596412\n=============================================\nPuntaje del Modelo, k = 11: 0.7982062780269058\n============================================="
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#k-fold",
    "href": "tics411/notebooks/12-ex-CV.html#k-fold",
    "title": "Holdout (Train Test Split)",
    "section": "K-Fold",
    "text": "K-Fold\n\nimport numpy as np\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n\ndef make_kfold(X, y, k, kfold=5):\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=42)\n    score = []\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), start=1):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        pipe = make_pipeline(k=k)\n        pipe.fit(X_train, y_train)\n        metric = pipe.score(X_val, y_val)\n        score.append(metric)\n        print(f\"Metric for Fold {fold}: {metric:.3f}\")\n    return score\n\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    print(f\"k = {k}\")\n    kfold_score = make_kfold(X_trainval, y_trainval, k, kfold=5)\n    mean = np.mean(kfold_score)\n    std = np.std(kfold_score)\n    print(f\"K-Fold Score: {mean:.3f} +/- {std:.3f}\")\n    print(\"=========================================\")\n\nk = 1\nMetric for Fold 1: 0.748\nMetric for Fold 2: 0.811\nMetric for Fold 3: 0.725\nMetric for Fold 4: 0.803\nMetric for Fold 5: 0.746\nK-Fold Score: 0.767 +/- 0.034\n=========================================\nk = 3\nMetric for Fold 1: 0.769\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.789\nMetric for Fold 4: 0.789\nMetric for Fold 5: 0.810\nK-Fold Score: 0.802 +/- 0.029\n=========================================\nk = 5\nMetric for Fold 1: 0.790\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.803\nMetric for Fold 4: 0.754\nMetric for Fold 5: 0.789\nK-Fold Score: 0.798 +/- 0.032\n=========================================\nk = 7\nMetric for Fold 1: 0.783\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.803\nMetric for Fold 4: 0.761\nMetric for Fold 5: 0.817\nK-Fold Score: 0.803 +/- 0.031\n=========================================\nk = 9\nMetric for Fold 1: 0.790\nMetric for Fold 2: 0.846\nMetric for Fold 3: 0.789\nMetric for Fold 4: 0.761\nMetric for Fold 5: 0.810\nK-Fold Score: 0.799 +/- 0.028\n=========================================\nk = 11\nMetric for Fold 1: 0.776\nMetric for Fold 2: 0.839\nMetric for Fold 3: 0.796\nMetric for Fold 4: 0.754\nMetric for Fold 5: 0.789\nK-Fold Score: 0.791 +/- 0.028\n========================================="
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#versi√≥n-reducida",
    "href": "tics411/notebooks/12-ex-CV.html#versi√≥n-reducida",
    "title": "Holdout (Train Test Split)",
    "section": "Versi√≥n Reducida",
    "text": "Versi√≥n Reducida\n\nfrom sklearn.model_selection import cross_val_score\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor k in [1, 3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    vals = cross_val_score(pipe, X_trainval, y_trainval, cv=kf)\n    print(f\"k = {k}\")\n    print(f\"Metric: {np.mean(vals):.3f} +/- {np.std(vals):.3f}\")\n\nk = 1\nMetric: 0.767 +/- 0.034\nk = 3\nMetric: 0.802 +/- 0.029\nk = 5\nMetric: 0.798 +/- 0.032\nk = 7\nMetric: 0.803 +/- 0.031\nk = 9\nMetric: 0.799 +/- 0.028\nk = 11\nMetric: 0.791 +/- 0.028"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#calcular-test-scores",
    "href": "tics411/notebooks/12-ex-CV.html#calcular-test-scores",
    "title": "Holdout (Train Test Split)",
    "section": "Calcular Test Scores",
    "text": "Calcular Test Scores\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    pipe.fit(X_trainval, y_trainval)\n    metric = pipe.score(X_test, y_test)\n    print(f\"Score for k = {k}: {metric}\")\n\nScore for k = 1: 0.7821229050279329\nScore for k = 3: 0.8156424581005587\nScore for k = 5: 0.770949720670391\nScore for k = 7: 0.8044692737430168\nScore for k = 9: 0.8212290502793296\nScore for k = 11: 0.8156424581005587\n\n\n\nPero, qu√© est√° ocurriendo ac√°?"
  },
  {
    "objectID": "tics411/notebooks/legacy_code.html",
    "href": "tics411/notebooks/legacy_code.html",
    "title": "Clases UAI",
    "section": "",
    "text": "## Otra forma de calcular lo mismo pero mucho m√°s ineficiente. No usar!!\n# def compute_ideal_sim(labels):\n#     labels = pd.Series(labels, name=\"labels\")\n#     labels_df = labels.to_frame().reset_index()\n#     return (\n#         labels_df.merge(labels_df, how=\"outer\", on=\"labels\")\n#         .add(1)\n#         .set_index([\"index_x\", \"index_y\"])\n#         .unstack(level=1)\n#         .fillna(0)\n#         .astype(bool)\n#         .astype(int)\n#     )\n\n\n# ideal_sim_pd = compute_ideal_sim(labels).to_numpy()\n\n\n## Es para demostrar que dan lo mismo.\n# np.array_equal(ideal_sim_np, ideal_sim_pd)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html",
    "href": "tics411/notebooks/04-analisis_centros.html",
    "title": "An√°lisis de Centros",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\ndf = sns.load_dataset(\"iris\")\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(df.drop(columns=\"species\"))\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nlabels = km.fit_predict(df.drop(columns=\"species\"))\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\nmean_table, std_table = create_tables(\n    df,\n    labels,\n    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n)\n\n\n## Corresponde a los valores promedios de cada variable por Cluster (los Centroides)\nmean_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n5.901613\n2.748387\n4.393548\n1.433871\n\n\n1\n5.006000\n3.428000\n1.462000\n0.246000\n\n\n2\n6.850000\n3.073684\n5.742105\n2.071053\n\n\n\n\n\n\n\n\n\n## Corresponde a la Desviaci√≥n Est√°ndar de cada variable por Cluster\nstd_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n0.466410\n0.296284\n0.508895\n0.297500\n\n\n1\n0.352490\n0.379064\n0.173664\n0.105386\n\n\n2\n0.494155\n0.290092\n0.488590\n0.279872"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html#an√°lisis-de-centros",
    "href": "tics411/notebooks/04-analisis_centros.html#an√°lisis-de-centros",
    "title": "An√°lisis de Centros",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\ndf = sns.load_dataset(\"iris\")\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(df.drop(columns=\"species\"))\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nlabels = km.fit_predict(df.drop(columns=\"species\"))\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\nmean_table, std_table = create_tables(\n    df,\n    labels,\n    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n)\n\n\n## Corresponde a los valores promedios de cada variable por Cluster (los Centroides)\nmean_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n5.901613\n2.748387\n4.393548\n1.433871\n\n\n1\n5.006000\n3.428000\n1.462000\n0.246000\n\n\n2\n6.850000\n3.073684\n5.742105\n2.071053\n\n\n\n\n\n\n\n\n\n## Corresponde a la Desviaci√≥n Est√°ndar de cada variable por Cluster\nstd_table\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n0.466410\n0.296284\n0.508895\n0.297500\n\n\n1\n0.352490\n0.379064\n0.173664\n0.105386\n\n\n2\n0.494155\n0.290092\n0.488590\n0.279872"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html#representaci√≥n-gr√°fica",
    "href": "tics411/notebooks/04-analisis_centros.html#representaci√≥n-gr√°fica",
    "title": "An√°lisis de Centros",
    "section": "Representaci√≥n Gr√°fica",
    "text": "Representaci√≥n Gr√°fica\nAc√° les dejo una Funci√≥n con la cual pueden realizar el An√°lisis de Centros. Para ello requieren un DataFrame que contenga las variables a analizar y su etiqueta.\nSe debe indicar, el df, el n√∫mero de Clusters creados, la columna de la etiqueta, y las columnas a analizar. Adicionalmente se puede agregar un t√≠tulo y cambiar las dimensiones del gr√°fico.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i].title())\n    plt.suptitle(title, fontsize=15)\n    plt.show()\n\n\ncolumns = df.drop(columns=[\"species\", \"labels\"]).columns.tolist()\ncenter_analysis_viz(\n    df,\n    n_clusters=3,\n    labels=labels,\n    columns=columns,\n    title=\"An√°lisis de Centros para Iris\",\n)"
  },
  {
    "objectID": "tics411/notebooks/05-ex-jerarquico.html",
    "href": "tics411/notebooks/05-ex-jerarquico.html",
    "title": "Ejemplo Clustering Aglomerativo",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"iris\")\ndf\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows √ó 5 columns\n\n\n\n\n\nX = df.drop(columns=\"species\")\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n3\n-1.506521\n0.098217\n-1.283389\n-1.315444\n\n\n4\n-1.021849\n1.249201\n-1.340227\n-1.315444\n\n\n...\n...\n...\n...\n...\n\n\n145\n1.038005\n-0.131979\n0.819596\n1.448832\n\n\n146\n0.553333\n-1.282963\n0.705921\n0.922303\n\n\n147\n0.795669\n-0.131979\n0.819596\n1.053935\n\n\n148\n0.432165\n0.788808\n0.933271\n1.448832\n\n\n149\n0.068662\n-0.131979\n0.762758\n0.790671\n\n\n\n\n150 rows √ó 4 columns\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_iris = pca.fit_transform(X_sc)\n\n\ndef pca_viz(pca, color=None, title=\"\"):\n    plt.scatter(pca_iris[\"pca0\"], pca_iris[\"pca1\"], c=color)\n    plt.title(title)\n    plt.show()\n\n\npca_viz(pca_iris, title=\"Visualizaci√≥n de Iris en 2 dimensiones\")\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, M√©todo: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlink_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in link_list:\n    plot_dendogram(X_sc, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nagc = AgglomerativeClustering(\n    n_clusters=3, metric=\"euclidean\", linkage=\"ward\"\n)\nlabels = agc.fit_predict(X_sc)\npca_viz(\n    pca_iris,\n    color=labels,\n    title=\"Clustering Iris. M√©todo Average, 3 Clusters.\",\n)\n\n## Transformarlo en funci√≥n para probar muchas combinaciones..."
  },
  {
    "objectID": "tics411/notebooks/05-ex-jerarquico.html#ejemplo-clustering-aglomerativo",
    "href": "tics411/notebooks/05-ex-jerarquico.html#ejemplo-clustering-aglomerativo",
    "title": "Ejemplo Clustering Aglomerativo",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"iris\")\ndf\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows √ó 5 columns\n\n\n\n\n\nX = df.drop(columns=\"species\")\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n3\n-1.506521\n0.098217\n-1.283389\n-1.315444\n\n\n4\n-1.021849\n1.249201\n-1.340227\n-1.315444\n\n\n...\n...\n...\n...\n...\n\n\n145\n1.038005\n-0.131979\n0.819596\n1.448832\n\n\n146\n0.553333\n-1.282963\n0.705921\n0.922303\n\n\n147\n0.795669\n-0.131979\n0.819596\n1.053935\n\n\n148\n0.432165\n0.788808\n0.933271\n1.448832\n\n\n149\n0.068662\n-0.131979\n0.762758\n0.790671\n\n\n\n\n150 rows √ó 4 columns\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_iris = pca.fit_transform(X_sc)\n\n\ndef pca_viz(pca, color=None, title=\"\"):\n    plt.scatter(pca_iris[\"pca0\"], pca_iris[\"pca1\"], c=color)\n    plt.title(title)\n    plt.show()\n\n\npca_viz(pca_iris, title=\"Visualizaci√≥n de Iris en 2 dimensiones\")\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, M√©todo: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlink_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in link_list:\n    plot_dendogram(X_sc, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nagc = AgglomerativeClustering(\n    n_clusters=3, metric=\"euclidean\", linkage=\"ward\"\n)\nlabels = agc.fit_predict(X_sc)\npca_viz(\n    pca_iris,\n    color=labels,\n    title=\"Clustering Iris. M√©todo Average, 3 Clusters.\",\n)\n\n## Transformarlo en funci√≥n para probar muchas combinaciones..."
  },
  {
    "objectID": "tics411/notebooks/kmeans.html",
    "href": "tics411/notebooks/kmeans.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\n\n## Primera fila son coordenadas X\n## Segunda fila son coordenadas Y\n## Cada columna es el punto.\npuntos = np.array([[1, 2, 4, 5], [1, 1, 3, 4]])\npuntos\n\nc_1 = np.array([1, 1])\nc_2 = np.array([2, 1])\npuntos\n\narray([[1, 2, 4, 5],\n       [1, 1, 3, 4]])\n\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_clusters(puntos, c_1, c_2, c=None):\n    plt.scatter(puntos[0], puntos[1], s=500, c=c)\n    plt.scatter(\n        c_1[0],\n        c_1[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 1\",\n        edgecolors=\"k\",\n        s=200,\n        color=\"orange\",\n    )\n    plt.scatter(\n        c_2[0],\n        c_2[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 2\",\n        edgecolors=\"k\",\n        s=200,\n        c=\"green\",\n    )\n    plt.grid(alpha=0.5)\n    plt.legend()\n\n\nplot_clusters(puntos, c_1, c_2)\n\n\n\n\n\n\n\n\n\ndef distancia(p0, p1):\n    x0 = p0[0]\n    x1 = p1[0]\n    y0 = p0[1]\n    y1 = p1[1]\n    return np.sqrt((x1 - x0) ** 2 + (y1 - y0) ** 2)\n\n\ndef calculate_distances(puntos, c_1, c_2):\n\n    distancia_mat = np.zeros((2, 4))\n    distancia_mat\n\n    for i in range(4):\n        p = puntos[:, i]\n        distancia_mat[0, i] = distancia(p, c_1)\n        distancia_mat[1, i] = distancia(p, c_2)\n\n    return distancia_mat\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\ndistancia_mat\n\narray([[0.        , 1.        , 3.60555128, 5.        ],\n       [1.        , 0.        , 2.82842712, 4.24264069]])\n\n\n\ndef calculate_clusters(distancia_mat):\n    min_distancia_mat = np.min(distancia_mat, axis=0)\n\n    clusters = distancia_mat == min_distancia_mat\n    return clusters\n\n\nclusters = calculate_clusters(distancia_mat)\nclusters.astype(\"int64\")\n\n## Solo el punto 1 pertenece al cluster 1, todo el resto al cluster 2\n\narray([[1, 0, 0, 0],\n       [0, 1, 1, 1]])\n\n\n\ndef calculate_centroids(clusters):\n    c_1 = puntos[:, clusters[0]].mean(axis=1)\n    c_2 = puntos[:, clusters[1]].mean(axis=1)\n\n    return c_1, c_2\n\n\nc_1, c_2 = calculate_centroids(clusters)\nc_1, c_2\n\n(array([1., 1.]), array([3.66666667, 2.66666667]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"blue\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\nclusters = calculate_clusters(distancia_mat)\nc_1, c_2 = calculate_centroids(clusters)\nclusters\n\narray([[ True,  True, False, False],\n       [False, False,  True,  True]])\n\n\n\nc_1, c_2\n\n(array([1.5, 1. ]), array([4.5, 3.5]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"red\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/15-ex-LR.html",
    "href": "tics411/notebooks/15-ex-LR.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\n\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.survived\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nX_train.shape, X_test.shape\n\n((668, 5), (223, 5))\n\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\n\n\ndef make_pipeline(\n    num_method, cat_method, encoder, sc_variables, C, threshold=0.5\n):\n    scaler = SklearnTransformerWrapper(\n        StandardScaler(), variables=sc_variables\n    )\n\n    print(f\"Entrenamiento para C={C} y threshold = {threshold}\")\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", encoder),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    LogisticRegression(\n                        C=C, random_state=42, max_iter=10000\n                    ),\n                    threshold=threshold,\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    print(\"===================================\")\n    print(f\"Coeficientes: {model[-1].estimator_.coef_}\")\n    print(f\"Coeficientes: {model[-1].estimator_.intercept_}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\npipe = make_pipeline(\n    num_method=\"mean\",\n    cat_method=\"missing\",\n    encoder=OneHotEncoder(),\n    sc_variables=[\"age\", \"fare\"],\n    C=10,\n    threshold=0.6,\n)\n\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para C=10 y threshold = 0.6\n===================================\nTrain Accuracy 0.8068862275449101\nTest Accuracy 0.8026905829596412\n===================================\nTrain Precision 0.8444444444444444\nTest Precision 0.8082191780821918\n===================================\nTrain Recall 0.6007905138339921\nTest Recall 0.6629213483146067\n===================================\nCoeficientes: [[ 0.08468769 -0.35930833  0.91502373 -1.02074655  0.36843929 -1.13894327\n   1.40165975 -0.60754774  0.03083727 -0.10686131  0.94628825]]\nCoeficientes: [0.3325951]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportante, normalemente es buena idea escoger s√≥lo una m√©trica. O darle m√°s preponderancia a una m√©trica, ya que el mejor modelo puede ser muy distinto dependiendo de la m√©trica a utilizar.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/preguntas-prueba.html",
    "href": "tics411/notebooks/preguntas-prueba.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        x=[2, 8, 13, 16, 19, 2, 8, 13, 16, 19],\n        y=[7, 7, 7, 7, 7, 3, 3, 5, 5, 5],\n    )\n)\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n2\n7\n\n\n1\n8\n7\n\n\n2\n13\n7\n\n\n3\n16\n7\n\n\n4\n19\n7\n\n\n5\n2\n3\n\n\n6\n8\n3\n\n\n7\n13\n5\n\n\n8\n16\n5\n\n\n9\n19\n5\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\nimport numpy as np\n\nc1 = df.iloc[0]\nc2 = df.iloc[9]\n\nvals = np.arange(1, 11)\nd_m = distance_matrix(df.to_numpy(), df.to_numpy(), p=2)\nd_m = pd.DataFrame(d_m, index=vals, columns=vals).round(2)\nd_m\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n0.00\n6.00\n11.00\n14.00\n17.00\n4.00\n7.21\n11.18\n14.14\n17.12\n\n\n2\n6.00\n0.00\n5.00\n8.00\n11.00\n7.21\n4.00\n5.39\n8.25\n11.18\n\n\n3\n11.00\n5.00\n0.00\n3.00\n6.00\n11.70\n6.40\n2.00\n3.61\n6.32\n\n\n4\n14.00\n8.00\n3.00\n0.00\n3.00\n14.56\n8.94\n3.61\n2.00\n3.61\n\n\n5\n17.00\n11.00\n6.00\n3.00\n0.00\n17.46\n11.70\n6.32\n3.61\n2.00\n\n\n6\n4.00\n7.21\n11.70\n14.56\n17.46\n0.00\n6.00\n11.18\n14.14\n17.12\n\n\n7\n7.21\n4.00\n6.40\n8.94\n11.70\n6.00\n0.00\n5.39\n8.25\n11.18\n\n\n8\n11.18\n5.39\n2.00\n3.61\n6.32\n11.18\n5.39\n0.00\n3.00\n6.00\n\n\n9\n14.14\n8.25\n3.61\n2.00\n3.61\n14.14\n8.25\n3.00\n0.00\n3.00\n\n\n10\n17.12\n11.18\n6.32\n3.61\n2.00\n17.12\n11.18\n6.00\n3.00\n0.00\n\n\n\n\n\n\n\n\n\nD = d_m.loc[[1, 10]]\nD\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n0.00\n6.00\n11.00\n14.00\n17.0\n4.00\n7.21\n11.18\n14.14\n17.12\n\n\n10\n17.12\n11.18\n6.32\n3.61\n2.0\n17.12\n11.18\n6.00\n3.00\n0.00\n\n\n\n\n\n\n\n\n\nG = D == D.min()\nG.astype(\"int64\")\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n\n\n10\n0\n0\n1\n1\n1\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\n\n\nG\n\n\nA = np.array([-5, -1, 1, 2, 3])\nA.mean()\n\n0.0\n\n\n\nA.std(ddof=0)\n\n2.8284271247461903\n\n\n\nprint(A.mean())\n(A**2).sum()\n\n0.0\n\n\n40\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nsc = StandardScaler()\nmm = MinMaxScaler()\nsc.fit_transform(A.reshape(-1, 1))\n\narray([[-1.76776695],\n       [-0.35355339],\n       [ 0.35355339],\n       [ 0.70710678],\n       [ 1.06066017]])\n\n\n\nmm.fit_transform(A.reshape(-1, 1))\n\narray([[0.   ],\n       [0.5  ],\n       [0.75 ],\n       [0.875],\n       [1.   ]])\n\n\n\ndf = pd.DataFrame(\n    dict(\n        Item=[1, 2, 3, 4, 5, 6, 7, 8],\n        Forma=[\n            \"cuadrado\",\n            \"cuadrado\",\n            \"circulo\",\n            \"circulo\",\n            \"triangulo\",\n            \"cuadrado\",\n            \"circulo\",\n            \"circulo\",\n        ],\n        Color=[\n            \"rojo\",\n            \"azul\",\n            \"rojo\",\n            \"verde\",\n            \"rojo\",\n            \"amarillo\",\n            \"amarillo\",\n            \"verde\",\n        ],\n    )\n)\n\nprint(df.to_latex(index=False, caption=\"Dataset\"))\n\n\\begin{table}\n\\centering\n\\caption{Dataset}\n\\begin{tabular}{rll}\n\\toprule\n Item &     Forma &    Color \\\\\n\\midrule\n    1 &  cuadrado &     rojo \\\\\n    2 &  cuadrado &     azul \\\\\n    3 &   circulo &     rojo \\\\\n    4 &   circulo &    verde \\\\\n    5 & triangulo &     rojo \\\\\n    6 &  cuadrado & amarillo \\\\\n    7 &   circulo & amarillo \\\\\n    8 &   circulo &    verde \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n/tmp/ipykernel_8516/3999806737.py:27: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(df.to_latex(index=False, caption=\"Dataset\"))\n\n\n\ndata = [\n    [0, 0.5, 1, 1.6, 0.8, 1.3, 3.6, 3.6, 4.3, 4.5, 5, 4],\n    [0.5, 0, 0.5, 1.1, 0.8, 1.4, 3.5, 3.3, 4, 4.2, 4.7, 4.2],\n    [1, 0.5, 0, 0.8, 0.8, 1.3, 3, 2.8, 3.5, 3.7, 4.2, 4.1],\n    [1.6, 1.1, 0.8, 0, 1.6, 2.1, 3.4, 3, 3.7, 3.8, 4.4, 4.8],\n    [0.8, 0.8, 0.8, 1.6, 0, 0.6, 2.9, 2.9, 3.6, 3.8, 4.3, 3.4],\n    [1.3, 1.4, 1.3, 2.1, 0.6, 0, 2.5, 2.7, 3.3, 3.6, 4, 2.8],\n    [3.6, 3.5, 3, 3.4, 2.9, 2.5, 0, 0.9, 1, 1.4, 1.6, 2.7],\n    [3.6, 3.3, 2.8, 3, 2.9, 2.7, 0.9, 0, 0.7, 0.9, 1.4, 3.6],\n    [4.3, 4, 3.5, 3.7, 3.6, 3.3, 1, 0.7, 0, 0.4, 0.7, 3.7],\n    [4.5, 4.2, 3.7, 3.8, 3.8, 3.6, 1.4, 0.9, 0.4, 0, 0.6, 4.1],\n    [5, 4.7, 4.2, 4.4, 4.3, 4, 1.6, 1.4, 0.7, 0.6, 0, 4.1],\n    [4, 4.2, 4.1, 4.8, 3.4, 2.8, 2.7, 3.6, 3.7, 4.1, 4.1, 0],\n]\n\n\ndf = pd.DataFrame(data)\ndf.index = [*range(1, 13)]\ndf.columns = [*range(1, 13)]\ndisplay(df)\nprint(df.to_latex(caption=\"DBSCAN\"))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n1\n0.0\n0.5\n1.0\n1.6\n0.8\n1.3\n3.6\n3.6\n4.3\n4.5\n5.0\n4.0\n\n\n2\n0.5\n0.0\n0.5\n1.1\n0.8\n1.4\n3.5\n3.3\n4.0\n4.2\n4.7\n4.2\n\n\n3\n1.0\n0.5\n0.0\n0.8\n0.8\n1.3\n3.0\n2.8\n3.5\n3.7\n4.2\n4.1\n\n\n4\n1.6\n1.1\n0.8\n0.0\n1.6\n2.1\n3.4\n3.0\n3.7\n3.8\n4.4\n4.8\n\n\n5\n0.8\n0.8\n0.8\n1.6\n0.0\n0.6\n2.9\n2.9\n3.6\n3.8\n4.3\n3.4\n\n\n6\n1.3\n1.4\n1.3\n2.1\n0.6\n0.0\n2.5\n2.7\n3.3\n3.6\n4.0\n2.8\n\n\n7\n3.6\n3.5\n3.0\n3.4\n2.9\n2.5\n0.0\n0.9\n1.0\n1.4\n1.6\n2.7\n\n\n8\n3.6\n3.3\n2.8\n3.0\n2.9\n2.7\n0.9\n0.0\n0.7\n0.9\n1.4\n3.6\n\n\n9\n4.3\n4.0\n3.5\n3.7\n3.6\n3.3\n1.0\n0.7\n0.0\n0.4\n0.7\n3.7\n\n\n10\n4.5\n4.2\n3.7\n3.8\n3.8\n3.6\n1.4\n0.9\n0.4\n0.0\n0.6\n4.1\n\n\n11\n5.0\n4.7\n4.2\n4.4\n4.3\n4.0\n1.6\n1.4\n0.7\n0.6\n0.0\n4.1\n\n\n12\n4.0\n4.2\n4.1\n4.8\n3.4\n2.8\n2.7\n3.6\n3.7\n4.1\n4.1\n0.0\n\n\n\n\n\n\n\n\n\\begin{table}\n\\centering\n\\caption{DBSCAN}\n\\begin{tabular}{lrrrrrrrrrrrr}\n\\toprule\n{} &   1  &   2  &   3  &   4  &   5  &   6  &   7  &   8  &   9  &   10 &   11 &   12 \\\\\n\\midrule\n1  &  0.0 &  0.5 &  1.0 &  1.6 &  0.8 &  1.3 &  3.6 &  3.6 &  4.3 &  4.5 &  5.0 &  4.0 \\\\\n2  &  0.5 &  0.0 &  0.5 &  1.1 &  0.8 &  1.4 &  3.5 &  3.3 &  4.0 &  4.2 &  4.7 &  4.2 \\\\\n3  &  1.0 &  0.5 &  0.0 &  0.8 &  0.8 &  1.3 &  3.0 &  2.8 &  3.5 &  3.7 &  4.2 &  4.1 \\\\\n4  &  1.6 &  1.1 &  0.8 &  0.0 &  1.6 &  2.1 &  3.4 &  3.0 &  3.7 &  3.8 &  4.4 &  4.8 \\\\\n5  &  0.8 &  0.8 &  0.8 &  1.6 &  0.0 &  0.6 &  2.9 &  2.9 &  3.6 &  3.8 &  4.3 &  3.4 \\\\\n6  &  1.3 &  1.4 &  1.3 &  2.1 &  0.6 &  0.0 &  2.5 &  2.7 &  3.3 &  3.6 &  4.0 &  2.8 \\\\\n7  &  3.6 &  3.5 &  3.0 &  3.4 &  2.9 &  2.5 &  0.0 &  0.9 &  1.0 &  1.4 &  1.6 &  2.7 \\\\\n8  &  3.6 &  3.3 &  2.8 &  3.0 &  2.9 &  2.7 &  0.9 &  0.0 &  0.7 &  0.9 &  1.4 &  3.6 \\\\\n9  &  4.3 &  4.0 &  3.5 &  3.7 &  3.6 &  3.3 &  1.0 &  0.7 &  0.0 &  0.4 &  0.7 &  3.7 \\\\\n10 &  4.5 &  4.2 &  3.7 &  3.8 &  3.8 &  3.6 &  1.4 &  0.9 &  0.4 &  0.0 &  0.6 &  4.1 \\\\\n11 &  5.0 &  4.7 &  4.2 &  4.4 &  4.3 &  4.0 &  1.6 &  1.4 &  0.7 &  0.6 &  0.0 &  4.1 \\\\\n12 &  4.0 &  4.2 &  4.1 &  4.8 &  3.4 &  2.8 &  2.7 &  3.6 &  3.7 &  4.1 &  4.1 &  0.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n/tmp/ipykernel_8516/1107703924.py:21: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(df.to_latex(caption=\"DBSCAN\"))\n\n\n\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        ai=np.random.rand(5),\n        biA=np.random.rand(5),\n        biB=np.random.rand(5),\n        bi=np.random.rand(5),\n        si=np.random.rand(5),\n    )\n)\nprint(df.to_latex(caption=\"Tabla Silhouette\"))\n\n\\begin{table}\n\\centering\n\\caption{Tabla Silhouette}\n\\begin{tabular}{lrrrrr}\n\\toprule\n{} &        ai &       biA &       biB &        bi &        si \\\\\n\\midrule\n0 &  0.096053 &  0.128639 &  0.841865 &  0.942217 &  0.000843 \\\\\n1 &  0.856134 &  0.549423 &  0.721880 &  0.292111 &  0.063252 \\\\\n2 &  0.436851 &  0.914593 &  0.261301 &  0.486813 &  0.830686 \\\\\n3 &  0.877781 &  0.886552 &  0.762853 &  0.487446 &  0.127603 \\\\\n4 &  0.667224 &  0.334679 &  0.423831 &  0.758756 &  0.887252 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n/tmp/ipykernel_8516/7425570.py:12: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n  print(df.to_latex(caption=\"Tabla Silhouette\"))\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html",
    "href": "tics411/notebooks/11-ex-knn.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\ndf.dtypes.value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Tipos de Variable presente en Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "href": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "title": "Clases UAI",
    "section": "Supongamos que utilizaremos las siguientes variables",
    "text": "Supongamos que utilizaremos las siguientes variables\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape\n\n((891, 5), (891,))"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#eda",
    "href": "tics411/notebooks/11-ex-knn.html#eda",
    "title": "Clases UAI",
    "section": "EDA",
    "text": "EDA\n\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nprint(f\"Variables Num√©ricas: {num_cols}\")\nprint(f\"Variables Categ√≥ricas: {cat_cols}\")\n\nVariables Num√©ricas: ['fare', 'age']\nVariables Categ√≥ricas: ['class', 'sex', 'embark_town']\n\n\n\nValores Faltantes (Nulos)\n\nX.isnull().mean().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Cantidad de Valores Nulos en el Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-num√©ricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-num√©ricas",
    "title": "Clases UAI",
    "section": "Variables Num√©ricas",
    "text": "Variables Num√©ricas\n\nX.hist(grid=False, edgecolor=\"k\")\nplt.suptitle(\"Distribuci√≥n de Variables Num√©ricas\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-categ√≥ricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-categ√≥ricas",
    "title": "Clases UAI",
    "section": "Variables Categ√≥ricas",
    "text": "Variables Categ√≥ricas\n\ncolor = [\"red\", \"blue\", \"green\"]\nfor cat, color in zip(cat_cols, color):\n    df[cat].value_counts().plot(\n        kind=\"bar\",\n        edgecolor=\"k\",\n        color=color,\n        title=f\"Categor√≠as para '{cat}'\",\n    )\n    plt.show()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "href": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "title": "Clases UAI",
    "section": "Preprocesamiento",
    "text": "Preprocesamiento\n\nfrom feature_engine.imputation import CategoricalImputer\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X)\nX_imp\n\n\n\n\n\n\n\n\n\nclass\nsex\nembark_town\nfare\nage\n\n\n\n\n0\nThird\nmale\nSouthampton\n7.2500\n22.0\n\n\n1\nFirst\nfemale\nCherbourg\n71.2833\n38.0\n\n\n2\nThird\nfemale\nSouthampton\n7.9250\n26.0\n\n\n3\nFirst\nfemale\nSouthampton\n53.1000\n35.0\n\n\n4\nThird\nmale\nSouthampton\n8.0500\n35.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nSecond\nmale\nSouthampton\n13.0000\n27.0\n\n\n887\nFirst\nfemale\nSouthampton\n30.0000\n19.0\n\n\n888\nThird\nfemale\nSouthampton\n23.4500\nNaN\n\n\n889\nFirst\nmale\nCherbourg\n30.0000\n26.0\n\n\n890\nThird\nmale\nQueenstown\n7.7500\n32.0\n\n\n\n\n891 rows √ó 5 columns\n\n\n\n\n\nfrom feature_engine.imputation import MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X_imp)\nX_imp\n\n\n\n\n\n\n\n\n\nclass\nsex\nembark_town\nfare\nage\n\n\n\n\n0\nThird\nmale\nSouthampton\n7.2500\n22.000000\n\n\n1\nFirst\nfemale\nCherbourg\n71.2833\n38.000000\n\n\n2\nThird\nfemale\nSouthampton\n7.9250\n26.000000\n\n\n3\nFirst\nfemale\nSouthampton\n53.1000\n35.000000\n\n\n4\nThird\nmale\nSouthampton\n8.0500\n35.000000\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nSecond\nmale\nSouthampton\n13.0000\n27.000000\n\n\n887\nFirst\nfemale\nSouthampton\n30.0000\n19.000000\n\n\n888\nThird\nfemale\nSouthampton\n23.4500\n29.699118\n\n\n889\nFirst\nmale\nCherbourg\n30.0000\n26.000000\n\n\n890\nThird\nmale\nQueenstown\n7.7500\n32.000000\n\n\n\n\n891 rows √ó 5 columns\n\n\n\n\n\nfrom feature_engine.encoding import OneHotEncoder\n\nohe = OneHotEncoder()\nX_ohe = ohe.fit_transform(X_imp)\nX_ohe\n\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n7.2500\n22.000000\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n71.2833\n38.000000\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n2\n7.9250\n26.000000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n3\n53.1000\n35.000000\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n4\n8.0500\n35.000000\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n13.0000\n27.000000\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n887\n30.0000\n19.000000\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n888\n23.4500\n29.699118\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n889\n30.0000\n26.000000\n0\n1\n0\n1\n0\n0\n1\n0\n\n\n890\n7.7500\n32.000000\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n891 rows √ó 10 columns\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_ohe)\nX_sc_all\n\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.502445\n-0.592481\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n1\n0.786845\n0.638789\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n-1.623803\n2.074505\n-0.307562\n\n\n2\n-0.488854\n-0.284663\n0.902587\n-0.565685\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n3\n0.420730\n0.407926\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n4\n-0.486337\n0.407926\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.386671\n-0.207709\n-1.107926\n-0.565685\n1.960202\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n887\n-0.044381\n-0.823344\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n888\n-0.176263\n0.000000\n0.902587\n-0.565685\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n889\n-0.044381\n-0.284663\n-1.107926\n1.767767\n-0.510152\n0.737695\n-0.737695\n-1.623803\n2.074505\n-0.307562\n\n\n890\n-0.492378\n0.177063\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n-1.623803\n-0.482043\n3.251373\n\n\n\n\n891 rows √ó 10 columns\n\n\n\n\n\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"fare\", \"age\"])\nX_sc = sc.fit_transform(X_ohe)\nX_sc\n\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.502445\n-0.592481\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n0.786845\n0.638789\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n2\n-0.488854\n-0.284663\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n3\n0.420730\n0.407926\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n4\n-0.486337\n0.407926\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.386671\n-0.207709\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n887\n-0.044381\n-0.823344\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n888\n-0.176263\n0.000000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n889\n-0.044381\n-0.284663\n0\n1\n0\n1\n0\n0\n1\n0\n\n\n890\n-0.492378\n0.177063\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n891 rows √ó 10 columns"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "href": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "title": "Clases UAI",
    "section": "Entrenamiento del Modelo",
    "text": "Entrenamiento del Modelo\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn_clf(X, y, k=5, prep=\"\"):\n    knn = KNeighborsClassifier(\n        n_neighbors=k, metric=\"euclidean\", n_jobs=-1\n    )\n    ## Notar que es posible utilizar Variables categ√≥ricas como Etiquetas...\n    knn.fit(X, y)\n    y_pred = knn.predict(X)\n    print(\n        f\"Score k = {k}, y Preprocesamiento: {prep}: {knn.score(X,y):.4f}\"\n    )\n    return y_pred\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    print(\n        \"=================================================================\"\n    )\n    y_pred_sc = knn_clf(X_sc, y, k=k, prep=\"StandardScaler Num√©rico\")\n    y_pred_sc_all = knn_clf(X_sc_all, y, k=k, prep=\"StandardScaler a todo\")\n    y_pred_ohe = knn_clf(X_ohe, y, k=k, prep=\"Sin Escalar\")\n\n=================================================================\nScore k = 3, y Preprocesamiento: StandardScaler Num√©rico: 0.8844\nScore k = 3, y Preprocesamiento: StandardScaler a todo: 0.8855\nScore k = 3, y Preprocesamiento: Sin Escalar: 0.8384\n=================================================================\nScore k = 5, y Preprocesamiento: StandardScaler Num√©rico: 0.8698\nScore k = 5, y Preprocesamiento: StandardScaler a todo: 0.8698\nScore k = 5, y Preprocesamiento: Sin Escalar: 0.8204\n=================================================================\nScore k = 7, y Preprocesamiento: StandardScaler Num√©rico: 0.8608\nScore k = 7, y Preprocesamiento: StandardScaler a todo: 0.8575\nScore k = 7, y Preprocesamiento: Sin Escalar: 0.7834\n=================================================================\nScore k = 9, y Preprocesamiento: StandardScaler Num√©rico: 0.8418\nScore k = 9, y Preprocesamiento: StandardScaler a todo: 0.8406\nScore k = 9, y Preprocesamiento: Sin Escalar: 0.7733\n=================================================================\nScore k = 11, y Preprocesamiento: StandardScaler Num√©rico: 0.8373\nScore k = 11, y Preprocesamiento: StandardScaler a todo: 0.8361\nScore k = 11, y Preprocesamiento: Sin Escalar: 0.7643\n=================================================================\nScore k = 13, y Preprocesamiento: StandardScaler Num√©rico: 0.8272\nScore k = 13, y Preprocesamiento: StandardScaler a todo: 0.8283\nScore k = 13, y Preprocesamiento: Sin Escalar: 0.7587\n=================================================================\nScore k = 15, y Preprocesamiento: StandardScaler Num√©rico: 0.8215\nScore k = 15, y Preprocesamiento: StandardScaler a todo: 0.8249\nScore k = 15, y Preprocesamiento: Sin Escalar: 0.7486\n\n\n\nConclusi√≥n: Los Preprocesamientos afectan de manera importante el entrenamiento de un modelo."
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "href": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "title": "Clases UAI",
    "section": "Uso de Pipelines",
    "text": "Uso de Pipelines\n\nfrom sklearn.pipeline import Pipeline\n\n\ndef model_pipeline(num_method, cat_method, sc_variables, k=5):\n\n    sc = SklearnTransformerWrapper(\n        StandardScaler(), variables=sc_variables\n    )\n\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", OneHotEncoder()),\n            # (\"sc\", StandardScaler()),\n            (\"sc\", sc),\n            (\"model\", KNeighborsClassifier(n_neighbors=k, n_jobs=-1)),\n        ]\n    )\n\n    return pipe\n\n\npipe = model_pipeline(\n    num_method=\"mean\", cat_method=\"frequent\", sc_variables=[\"age\", \"fare\"]\n)\npipe\n\nPipeline(steps=[('num_imp', MeanMedianImputer(imputation_method='mean')),\n                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n                ('ohe', OneHotEncoder()), ('sc', StandardScaler()),\n                ('model', KNeighborsClassifier(n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('num_imp', MeanMedianImputer(imputation_method='mean')),\n                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n                ('ohe', OneHotEncoder()), ('sc', StandardScaler()),\n                ('model', KNeighborsClassifier(n_jobs=-1))]) MeanMedianImputerMeanMedianImputer(imputation_method='mean') CategoricalImputerCategoricalImputer(imputation_method='frequent') OneHotEncoderOneHotEncoder() ¬†StandardScaler?Documentation for StandardScalerStandardScaler() ¬†KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_jobs=-1) \n\n\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\npipe.score(X, y)\n\n0.8698092031425365\n\n\n\ny_pred\n\narray(['no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes',\n       'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no',\n       'yes', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no',\n       'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes',\n       'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no',\n       'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no',\n       'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no',\n       'yes', 'no', 'no', 'no'], dtype=object)"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html",
    "href": "tics411/notebooks/03-ex_kmeans.html",
    "title": "Ejemplo K-Means",
    "section": "",
    "text": "import seaborn as sns\n\n# Importamos el Dataset Iris\ndf = sns.load_dataset(\"iris\")\ndf\n\n\ndf[\"species\"].value_counts()\n\n\nSupongamos que utilizaremos s√≥lo las variables num√©ricas‚Ä¶ ‚ÄúSpecies‚Äù, es de hecho la respuesta correcta (la etiqueta).\n\n\n# Definimos X como una Matriz sin la variable Species.\nX = df.drop(columns=\"species\")\nX"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ejemplo-k-means",
    "href": "tics411/notebooks/03-ex_kmeans.html#ejemplo-k-means",
    "title": "Ejemplo K-Means",
    "section": "",
    "text": "import seaborn as sns\n\n# Importamos el Dataset Iris\ndf = sns.load_dataset(\"iris\")\ndf\n\n\ndf[\"species\"].value_counts()\n\n\nSupongamos que utilizaremos s√≥lo las variables num√©ricas‚Ä¶ ‚ÄúSpecies‚Äù, es de hecho la respuesta correcta (la etiqueta).\n\n\n# Definimos X como una Matriz sin la variable Species.\nX = df.drop(columns=\"species\")\nX"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "href": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "title": "Ejemplo K-Means",
    "section": "Ayuda Visual",
    "text": "Ayuda Visual\nVamos a utilizar PCA para poder reducir las dimensiones a un tama√±o el cual podamos visualizar: 2D.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n## Esto es s√≥lo una ayuda para poder visualizar datos\n# que est√°n en m√°s dimensiones de las que podemos ver.\npca = PCA(n_components=2, random_state=1)\npca_X = pca.fit_transform(X)\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(pca_X[:, 0], pca_X[:, 1])\nplt.title(\"Visualizaci√≥n de Iris en 2D.\")\nplt.tight_layout()\n\n\n## Esta es una funci√≥n que nos permitir√° visualizar nuestras etiquetas en un espacio reducido por PCA.\n## Adem√°s permite la visualizaci√≥n de los centroides de nuestro proceso...\n\n\ndef pca_viz(pca_X, pca_centroids, labels, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    plt.scatter(\n        pca_centroids[:, 0],\n        pca_centroids[:, 1],\n        marker=\"*\",\n        c=\"red\",\n        s=150,\n    )\n    plt.title(title)\n\n\nImplementaci√≥n de K-Means\n\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\n\npca_viz(\n    pca_X,\n    pca_centroids,\n    labels=labels,\n    title=\"Visualizaci√≥n de K-Means en Iris 2D\",\n)\n\n\n\nEfecto del Escalamiento en K-Means\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_sc = pca.fit_transform(X_sc)\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nsc_labels = km.fit_predict(X_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\npca_viz(\n    pca_X_sc,\n    pca_centroids,\n    sc_labels,\n    title=\"K-Means de Iris en 2D luego de Estandarizar los datos. \",\n)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\nX_mm = mm.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_mm = pca.fit_transform(X_mm)\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nmm_labels = km.fit_predict(X_mm)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(\n    pca_X_mm,\n    pca_centroids,\n    mm_labels,\n    title=\"K-Means de Iris en 2D luego de Normalizar los datos.\",\n)"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ejemplo-m√°s-avanzado-sin-entrenar-con-todos-los-datos",
    "href": "tics411/notebooks/03-ex_kmeans.html#ejemplo-m√°s-avanzado-sin-entrenar-con-todos-los-datos",
    "title": "Ejemplo K-Means",
    "section": "Ejemplo m√°s avanzado sin entrenar con todos los datos‚Ä¶",
    "text": "Ejemplo m√°s avanzado sin entrenar con todos los datos‚Ä¶\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(X, test_size=0.25, random_state=1)\n\n\nEstamos dejando un 25% de los datos fuera para poder chequear cu√°l ser√≠a la predicci√≥n que se le dan a dichos datos.\n\n\npca = PCA(n_components=2)\nkm = KMeans(n_clusters=2, n_init=10)\nsc = StandardScaler()\n## Fit siempre se hace con datos de `Entrenamiento`.\n\n## Escalamos los datos...\nsc.fit(X_train)\nX_train_sc = sc.transform(X_train)\nX_test_sc = sc.transform(X_test)\n\n# Generamos las coordenadas del PCA para visualizar\npca.fit(X_train_sc)\npca_train = pca.transform(X_train_sc)\npca_test = pca.transform(X_test_sc)\n\ntrain_labels = km.fit_predict(X_train_sc)\ntest_labels = km.predict(X_test_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(pca_train, pca_centroids, train_labels)\npca_viz(pca_test, pca_centroids, test_labels, cmap=\"tab20b\")"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#cu√°l-es-el-k-√≥ptimo",
    "href": "tics411/notebooks/03-ex_kmeans.html#cu√°l-es-el-k-√≥ptimo",
    "title": "Ejemplo K-Means",
    "section": "Cu√°l es el K √≥ptimo?",
    "text": "Cu√°l es el K √≥ptimo?\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"N√∫mero de Cl√∫sters\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X_train,\n    k_max=15,\n    color=\"red\",\n    title=\"Curva del Codo para el Dataset Iris, s√≥lo con Train Set.\",\n)\n\n\nwc"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html",
    "href": "tics411/notebooks/06-ex-DBSCAN.html",
    "title": "DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\n\n\ndf = sns.load_dataset(\"iris\")\nX = df.drop(columns=\"species\")\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows √ó 4 columns"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#dbscan",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#dbscan",
    "title": "DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\n\n\ndf = sns.load_dataset(\"iris\")\nX = df.drop(columns=\"species\")\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows √ó 4 columns"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#funci√≥n-para-visualizar",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#funci√≥n-para-visualizar",
    "title": "DBSCAN",
    "section": "Funci√≥n para Visualizar",
    "text": "Funci√≥n para Visualizar\n\nimport matplotlib.pyplot as plt\n\n\n## Funci√≥n ligeramente modificada para no requerir centroides en caso que no sea aplicable.\ndef pca_viz(pca_X, labels, pca_centroids=None, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    if pca_centroids is not None:\n        plt.scatter(\n            pca_centroids[:, 0],\n            pca_centroids[:, 1],\n            marker=\"*\",\n            c=\"red\",\n            s=150,\n        )\n    plt.title(title)\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=13, eps=0.6)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\n\n\nfrom sklearn.decomposition import PCA\n\n## Probar minPts = 4 y eps = 0.2\n## Probar minPts = 10 y eps = 0.5\n## Probar minPts = 13 y eps = 0.6\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X_sc)\n\npca_viz(\n    pca_X,\n    labels=labels,\n    title=\"Visualizaci√≥n de DBSCAN para Iris en 2D\",\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino m√°s cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino m√°s cercano\")\n    plt.title(f\"B√∫squeda de EPS para DBSCAN con k={k}\")\n\n\n# k = 5 escogido ya que tenemos 4 dimensiones.\ndbscan_elbow_plot(X_sc, k=5)"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperpar√°metros-√≥ptimos",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperpar√°metros-√≥ptimos",
    "title": "DBSCAN",
    "section": "Modelo entrenado con Hiperpar√°metros √ìptimos",
    "text": "Modelo entrenado con Hiperpar√°metros √ìptimos\n\nMIN_PTS = 20\nEPS = 0.75\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=MIN_PTS, eps=EPS)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\npca_viz(\n    pca_X,\n    labels=labels,\n    title=f\"Visualizaci√≥n de DBSCAN para Iris en 2D con los mejores Hiperpar√°metros: MinPts: {MIN_PTS} y eps = {EPS}\",\n)"
  },
  {
    "objectID": "tics411/clase-0.html#qui√©n-soy",
    "href": "tics411/clase-0.html#qui√©n-soy",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øQui√©n soy?",
    "text": "¬øQui√©n soy?\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia, estudi√© Ingenier√≠a Civil pero llevo 9 a√±os trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nTerminando mi Msc. y empezando mi PhD en la UAI.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqu√© mi primer paper el a√±o pasado sobre Hate Speech en Espa√±ol.\nJuego Tenis de Mesa, hago Agility con mi perrita Kira y escribo en mi Blog."
  },
  {
    "objectID": "tics411/clase-0.html#objetivos-del-curso",
    "href": "tics411/clase-0.html#objetivos-del-curso",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\n\nIdentificar Elementos Claves del Machine Learning (Terminolog√≠a, Nomenclatura, Intuici√≥n).\nEntender como interact√∫an los algoritmos m√°s importantes.\nAprender a seleccionar el mejor Algoritmo para el Problema.\nEjecutar y aplicar algoritmos cl√°sicos de Machine Learning.\nEvaluar el desempe√±o esperado del Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#t√≥picos",
    "href": "tics411/clase-0.html#t√≥picos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "T√≥picos",
    "text": "T√≥picos\n\n\n\n\n\n\n\nIntroducci√≥n a la Miner√≠a de Datos\nAn√°lisis Exploratorio de Datos (EDA)\nModelos No Supervisados/Descriptivos\nModelos Supervisados/Predictivos\n\n\n\n\n\n\nModelos no Supervisados\n\nK-Means\nHierarchical Clustering\nDBScan\nApriori\n\n\nModelos Supervisados\n\nKNN\n√Årboles de Decisi√≥n\nNaive Bayes\nRegresi√≥n Log√≠stica"
  },
  {
    "objectID": "tics411/clase-0.html#sobre-las-clases",
    "href": "tics411/clase-0.html#sobre-las-clases",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Sobre las clases",
    "text": "Sobre las clases\n\n\nClases presenciales, con participaci√≥n activa de los estudiantes.\nEs un curso coordinado.\nCanal oficial ser√° Webcursos.\nMucha terminolog√≠a y material de estudio ser√° en Ingl√©s.\nHorario: Jueves.\n\n15:30 a 16:40 (C√°tedra)\n17:00 a 18:10 (Pr√°ctico)\nIdealmente!!\n\nAsistencia es voluntaria, pero altamente recomendada."
  },
  {
    "objectID": "tics411/clase-0.html#materiales-de-clases",
    "href": "tics411/clase-0.html#materiales-de-clases",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Materiales de Clases",
    "text": "Materiales de Clases\n\nDiapositivas\nPr√°cticos\n\n\n\n\n\n\n\n\nSlides interactivas (C√≥digo se puede copiar e im√°genes se pueden ver en grande).\nSe puede buscar contenido en las diapositivas mediante un buscador.\nSe dejar√°n copias en PDF en Webcursos (levemente distintas).\n\n\n\n\n\n\n\n\n\n\nSe espera que los estudiantes dominen las siguientes tecnolog√≠as:\n\nPython\nGoogle Colab\nPandas/Numpy\nScikit-Learn (Se ense√±ar√° a lo largo del curso)."
  },
  {
    "objectID": "tics411/clase-0.html#material-complementario",
    "href": "tics411/clase-0.html#material-complementario",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Material Complementario",
    "text": "Material Complementario\n\n\n\n\n\n\nCurso de Scikit-Learn \n\nTutorial Colab\nAgregar Datos Externos a Colab"
  },
  {
    "objectID": "tics411/clase-0.html#evaluaci√≥n",
    "href": "tics411/clase-0.html#evaluaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\n\n\n\n\n\n\n\nDos Evaluaciones Escritas (P1, P2) coordinadas y cuatro tareas pr√°cticas en parejas (T1, T2, T3, T4) \\[NP = 0.35 \\cdot P1 + 0.35 \\cdot P2 + 0.3 \\cdot \\bar{T}\\] \\[ \\bar{T} = (T1 + T2 + T3 + T4)/4 \\]\n\n\n\n\n\n\n\n\n\n\nSi NP &gt; 5\n\n\n\\[NF = NP\\]\n\n\n\n\n\n\n\n\n\nEn caso contrario:\n\n\n\\[NF = 0.7 \\cdot NP + 0.3 \\cdot E\\]"
  },
  {
    "objectID": "tics411/clase-0.html#ayudant√≠as",
    "href": "tics411/clase-0.html#ayudant√≠as",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ayudant√≠as",
    "text": "Ayudant√≠as\nAyudante: TBD\nemail: TBD\n\n\n\n\n\n\n\nLas ayudant√≠as ser√°n en la manera que sean necesarias.\nEstar√°n enfocadas principalmente en aplicaciones y c√≥digo."
  },
  {
    "objectID": "tics411/clase-0.html#revoluci√≥n-de-los-datos",
    "href": "tics411/clase-0.html#revoluci√≥n-de-los-datos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Revoluci√≥n de los Datos",
    "text": "Revoluci√≥n de los Datos\n\n\n\n\n\n\n\nHablar de los distintos tipos de Datos.\nTodo es datos, y est√° lleno de ellos en Internet y el mundo."
  },
  {
    "objectID": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "href": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Nace el Data Science (Ciencia de Datos)",
    "text": "Nace el Data Science (Ciencia de Datos)\n\n\n\n\n\n\n\nExplicar las distintas etapas. Qu√© son cada uno de ellos.\nExplicar que no estoy de acuerdo con todas las definiciones."
  },
  {
    "objectID": "tics411/clase-0.html#c√≥mo-aprovechar-la-informaci√≥n-que-tenemos",
    "href": "tics411/clase-0.html#c√≥mo-aprovechar-la-informaci√≥n-que-tenemos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øC√≥mo aprovechar la informaci√≥n que tenemos?",
    "text": "¬øC√≥mo aprovechar la informaci√≥n que tenemos?\n\n\nData Mining (Miner√≠a de Datos)\n\n\n‚ÄúThe process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.‚Äù (Fayyad, Piatetsky-Shapiro & Smith 1996)\n\n\n\n\n\n\nMachine Learning (Aprendizaje Autom√°tico)\n\n\n‚ÄúA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.‚Äù (Mitchell, 2006)\n\n\n\n\n\n\nExplicar que estos son dos tipos de Approaches con el que hoy en d√≠a se enfrentan los datos.\nEl primero m√°s enfocado en un an√°lisis manual.\nEl segundo en un enfoque m√°s autom√°tico."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos",
    "href": "tics411/clase-0.html#tipos-de-datos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos",
    "text": "Tipos de Datos\n\n\n\n\n\nDatos Estructurados\n\n\n\n\n\nDatos No Estructurados"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, instancias, registros. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos m√°s amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % baj√≠simo del total de datos existentes en el Mundo. Tambi√©n el que m√°s disponible est√° en las empresas.\nDistintos data types, por lo que normalmente requiere de alg√∫n tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "href": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos: Series de Tiempo",
    "text": "Tipos de Datos: Series de Tiempo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Instancias temporales (Normalmente interdependientes).\nColumnas: Variables, Atributos, Features (Univariada o Multivariada).\n\n\n\n\n\n\n\n\n\n\n\nEs un % baj√≠simo del total de datos existentes en el Mundo.\nPropiedad temporal requiere preprocesamiento y modelos especiales."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-im√°genes",
    "href": "tics411/clase-0.html#tipos-de-datos-im√°genes",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos: Im√°genes",
    "text": "Tipos de Datos: Im√°genes\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste es el tipo de Datos que dispar√≥ la Inteligencia Artificial.\n¬øCu√°ntos computadores para identificar un Gato? 16,000\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplicar el concepto de Tensor, extensi√≥n de las matrices. Diferencia entre Grayscale y RGB."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "href": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos: Texto Libre",
    "text": "Tipos de Datos: Texto Libre\n\n\n\n\n\n\n\n\n\n\n\n\nDatos Masivos.\nDificiles de lidiar ya que deben ser llevarse a una representaci√≥n num√©rica.\nAlto nivel de Sesgo y Subjetividad.\n\n\n\n\n\n\n\n\n\n\n\nGracias a este tipo de datos se han producido los avances m√°s incre√≠bles del √∫ltimo tiempo: Transformers"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-videos",
    "href": "tics411/clase-0.html#tipos-de-datos-videos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos: Videos",
    "text": "Tipos de Datos: Videos\n\n\n\n\n\n\n\n\nLos videos no son m√°s que arreglos de im√°genes.\nSon un tipo de dato muy pesado y dif√≠cil de lidiar.\nRequiere alto poder de Procesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-aprendizaje",
    "href": "tics411/clase-0.html#tipos-de-aprendizaje",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Aprendizaje",
    "text": "Tipos de Aprendizaje"
  },
  {
    "objectID": "tics411/clase-0.html#reinforcement-learning",
    "href": "tics411/clase-0.html#reinforcement-learning",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este tipo de aprendizaje se ense√±a por refuerzo. Es decir se da una recompensa si el sistema aprende lo que queremos.\n\n\n\n\n\n\n\n\n\n\n\nSi el premio es mayor, se pueden obtener aprendizajes mayores.\n\n\n\n\n\n\n\n\n\n\n\nUn ejemplo de esto es AlphaTensor en el cual un modelo aprendi√≥ una nueva manera de multiplicar matrices que es m√°s eficiente.\n\n\n\n\n\n\n\n\n\n\n\nOtro ejemplo es AlphaFold donde el modelo aprendi√≥/descubri√≥ c√≥mo se doblan las prote√≠nas cuando se vuelven amino√°cidos."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-supervisados-regresi√≥n-y-clasificaci√≥n",
    "href": "tics411/clase-0.html#problemas-supervisados-regresi√≥n-y-clasificaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Problemas Supervisados: Regresi√≥n y Clasificaci√≥n",
    "text": "Problemas Supervisados: Regresi√≥n y Clasificaci√≥n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegresi√≥n: Se busca estimar un valor continuo.\n\n(Estimar el valor de una casa).\n\nClasificaci√≥n: Se busca encontrar una categor√≠a o un valor discreto.\n\n(Clasificar una imagen como Perro o Gato).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara entrenar este tipo de modelos se necesitan etiquetas, es decir, la respuesta esperada del modelo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmbos ejemplos se pueden realizar utilizando Largo (Eje Y) y Peso (Eje X)."
  },
  {
    "objectID": "tics411/clase-0.html#clustering",
    "href": "tics411/clase-0.html#clustering",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusters: Una categor√≠a en la que sus componentes son similares. Los clusters normalmente no tienen un nombre propio, sino que uno les asigna uno.\nTambi√©n se les llama segmentos. No usar la palabra clase.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo requiere de etiquetas, por lo tanto, no es posible evaluar su desempe√±o de manera 100% acertada."
  },
  {
    "objectID": "tics411/clase-0.html#reducci√≥n-de-dimensionalidad",
    "href": "tics411/clase-0.html#reducci√≥n-de-dimensionalidad",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Reducci√≥n de Dimensionalidad",
    "text": "Reducci√≥n de Dimensionalidad\n\n\n\n\n\n\n\n\n\n\n\n\nReducci√≥n de la Dimensionalidad: Eliminar complejidad sin perder informaci√≥n clave para poder entender su comportamiento."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Nuestro Sistema de ML",
    "text": "Nuestro Sistema de ML\nCreemos un Sistema de ML que sea capaz de ver una im√°gen y pronunciar correctamente el uso de la letra C.\n\n\n\n\n\n\nVamos a Entrenar un Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Nuestro Sistema de ML: Entrenamiento",
    "text": "Nuestro Sistema de ML: Entrenamiento\n\n\n\n\n\n\nKasa\n\n\n\n\n\n\n\nKokodrilo\n\n\n\n\n\n\n\nKubo\n\n\n\n\n\n\n\n\n\n\n\n\n¬øQu√© patrones est√° aprendiendo el modelo?\n\n\n\n\n\nEntrenamiento\n\n\nEs el proceso en el cu√°l se permite al modelo aprender. En este proceso se le entregan ejemplos (Train Set) para que el modelo de manera aut√≥noma pueda aprender patrones que le permitan resolver la tarea dada."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Nuestro Sistema de ML: Inferencia",
    "text": "Nuestro Sistema de ML: Inferencia\n\nInferencia/Predicci√≥n\n\n\nSe refiere al proceso en el que el modelo tiene que demostrar cu√°l ser√≠a su decisi√≥n de acuerdo a los patrones aprendidos en el proceso de entrenamiento. Los ejemplos en los que se prueba se le denomina Test Set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKukillo\n\n\nBikikleta\n\n\n\n\n\nGeneralizaci√≥n\n\n\nSe le llama generalizaci√≥n a la capacidad del modelo de aplicar lo aprendido de manera correcta en ejemplos no vistos."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento",
    "text": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento\n\n\n\n\n\n\nKuchillo\n\n\n\n\n\n\n\nChokolate\n\n\n\n\n\n\n\nSinsel\n\n\n\n\n\n\n\n\n\n\n\n\nNo es bueno entrenar con las mismas instancias de de Test, es decir, con las cuales se eval√∫a el modelo. ¬øPor qu√©?\n\n\n\n\n\nMencionar el caso de error de ImageNet."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo",
    "text": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKuchillo\n\n\nBisikleta\n\n\n\n\n\nEvaluaci√≥n\n\n\nUtilizar una m√©trica que permita ponerle nota al modelo.\n\n\n\n\n\n\n1er Modelo: 2 correctas de 4, es decir 50%.\n\n\n\n\n2do Modelo: 4 correctas de 4, es decir 100%."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Problemas del Aprendizaje",
    "text": "Problemas del Aprendizaje\n\nSupongamos que queremos utilizar nuestro modelo para pronunciar palabras en otro idioma (otro Test Set).\n¬øQu√© problemas podemos encontrar?\n\n\n\n\nStomach \\(\\rightarrow\\) Stomak\nArcher \\(\\rightarrow\\) Archer\nChurch \\(\\rightarrow\\) Churk\n\nChurch.\n\nArcheology \\(\\rightarrow\\) Archeology\n\nArkeology.\n\nChicago \\(\\rightarrow\\) Chicago\n\nShicago.\n\nMuscle \\(\\rightarrow\\) Muskle\n\nMus_le.\n\nIch mag Schweinefleisch \\(\\rightarrow\\) Ich mag Schweinefleisk.\n\nIj mag Shvaineflaish.\n\n\n\n\n\n\n\n\n\n\nClaramente tenemos un problema. ¬øA qu√© se debe esto?"
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Problemas del Aprendizaje: Definiciones",
    "text": "Problemas del Aprendizaje: Definiciones\n\nOverfitting (Sobreajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, porque se ajusta demasiado bien (llegando a memorizar) a los datos de entrenamiento. ¬øC√≥mo se puede mitigar este problema?\n\n\n\n\n\n\n\n\n\n\nSe le tiende a llamar sobreentrenamiento, pero no es del todo correcto para el caso de modelos de Machine Learning. Lo m√°s correcto es que el sobreentrenamiento provoca overfitting.\n\n\n\n\n\nMostrar ejemplos en Pizarra de manera gr√°fica. Ejemplos t√≠picos de Excel.\n\n\n\nUnderfitting (Subajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, pero a diferencia del overfitting no se ha ajustado correctamente a los datos. ¬øC√≥mo se ver√≠a el underfitting en nuestro ejemplo?"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Etapas del Modelamiento: Crisp-DM",
    "text": "Etapas del Modelamiento: Crisp-DM"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Etapas del Modelamiento: KDD",
    "text": "Etapas del Modelamiento: KDD"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Etapas del Modelamiento: Semma",
    "text": "Etapas del Modelamiento: Semma"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-metodolog√≠a-propia",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-metodolog√≠a-propia",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Etapas del Modelamiento: Metodolog√≠a Propia",
    "text": "Etapas del Modelamiento: Metodolog√≠a Propia"
  },
  {
    "objectID": "tics411/clase-13.html#definici√≥n",
    "href": "tics411/clase-13.html#definici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definici√≥n",
    "text": "Definici√≥n\n\nAnomal√≠as\n\n\nConjunto de puntos que son considerablemente diferentes al resto.\n\n\n\n\n\n\n\n\n\nPor definici√≥n las Anomal√≠as son relativamente raras. * Pueden ocurrir en proporciones extremadamente bajas en los datos. Ej: 1 entre mil. * El contexto es importante. Ej: Temperaturas bajo cero en Verano.\n\n\n\nEjemplos:\n\nTelecomunicaciones: Detecci√≥n de Abusos de Roaming.\nBanca: Compras/Ventas inusualmente elevados.\nFinanzas y Seguros: Detectar y prevenir patrones de gastos fraudulentos.\nMantenci√≥n: Predicci√≥n de comportamiento irregular/fallas.\nSmart Homes: Detecciones de fugas de Energ√≠a\netc."
  },
  {
    "objectID": "tics411/clase-13.html#m√°s-ejemplos",
    "href": "tics411/clase-13.html#m√°s-ejemplos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "M√°s ejemplos",
    "text": "M√°s ejemplos\n\n\n\n\n\n\nLa definici√≥n de Anomal√≠a es altamente subjetiva y depende mucho del Dominio en el cu√°l se est√° trabajando."
  },
  {
    "objectID": "tics411/clase-13.html#tipos-de-anomal√≠as-series-de-tiempo",
    "href": "tics411/clase-13.html#tipos-de-anomal√≠as-series-de-tiempo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Anomal√≠as (Series de Tiempo)",
    "text": "Tipos de Anomal√≠as (Series de Tiempo)"
  },
  {
    "objectID": "tics411/clase-13.html#desaf√≠os",
    "href": "tics411/clase-13.html#desaf√≠os",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Desaf√≠os",
    "text": "Desaf√≠os\n\n\n\n\n\n\nDesaf√≠os\n\n\n\n¬øCu√°ntos Atributos/Variables usamos para definir un Outlier?\n¬øCu√°ntos Outliers existen?\nEste tipo de problema suele ser complicado de Etiquetar, por lo que es dif√≠cil resolverlo como un problema supervisado.\nPuede ser como ‚ÄúEncontrar una aguja en un pajar‚Äù."
  },
  {
    "objectID": "tics411/clase-13.html#enfoques",
    "href": "tics411/clase-13.html#enfoques",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Enfoques",
    "text": "Enfoques"
  },
  {
    "objectID": "tics411/clase-13.html#t√©cnicas-visuales",
    "href": "tics411/clase-13.html#t√©cnicas-visuales",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "T√©cnicas Visuales",
    "text": "T√©cnicas Visuales\n\n\n\n\n\n\nEstas t√©cnicas son muy subjetivas ya que dependen del criterio/apreciaci√≥n del usuario.\n\n\n\n\n\nBox Plots\n\n\n\n\n\n\nScatter Plots"
  },
  {
    "objectID": "tics411/clase-13.html#t√©cnicas-estad√≠sticas-test-de-grubbs",
    "href": "tics411/clase-13.html#t√©cnicas-estad√≠sticas-test-de-grubbs",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "T√©cnicas Estad√≠sticas: Test de Grubbs",
    "text": "T√©cnicas Estad√≠sticas: Test de Grubbs\n\n\n\n\n\n\nEl test de Grubbs detecta si alg√∫n dato es un outlier sobre una variable asumiendo que se distribuyen de manera normal.\n\n\n\n\\[G = \\frac{\\underset{i = 1,2,...,n}{max}|x_i - \\bar{X}|}{S_x}\\]\ndonde \\(\\bar{X}\\) y \\(S_x\\) corresponden a la media y Desviaci√≥n Est√°ndar Muestral.\n\nEso implica que \\(G\\) se distribuye como una t-student de \\(n-2\\) grados de libertad, por lo tanto si:\n\n\\[ G_{critico} = \\frac{n-1}{\\sqrt{n}}\\sqrt{\\frac{t^2_{(\\alpha/n, n-2)}}{n-2+t_{(\\alpha/n, n-2)^2}}}\\]\n\n\n\n\n\n\nSi \\(G &gt; G_{critico}\\), \\(x_i\\) es considerado un outlier con una significancia \\(\\alpha/n\\) para una t-student con \\(n-2\\) grados de libertad."
  },
  {
    "objectID": "tics411/clase-13.html#test-de-grubs-en-python",
    "href": "tics411/clase-13.html#test-de-grubs-en-python",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Test de Grubs en Python",
    "text": "Test de Grubs en Python\n\n\n\n\n\n\nEste c√≥digo debiera entregar una lista de todos los puntos que son considerados outliers.\n\n\n\nfrom scipy import stats\nimport numpy as np\n\nn = 16 # N√∫mero de Datos\nalpha = 0.05 # nivel de confianza\n\nt_crit = stats.t.ppf(1-alpha/n, n-2)\nG_crit = (n-1)/np.sqrt(n)*np.sqrt(t_crit**2/(n-2 + t_crit**2))\n\ndata = np.array([5,14,15,15,19,17,16,20,22,8,21,28,11,9,29,40])\nG_test = np.abs(data-np.mean(data)/np.std(data))\n\ntest_grubbs = np.where(G_test&gt;G_crit)\nprint(f\"Outliers: {data[test_grubbs]}\")"
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado",
    "href": "tics411/clase-13.html#caso-multivariado",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\n\n\n\n\nLa idea es calcular la distancia de cada punto al centro tomando en consideraci√≥n la covarianza."
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-1",
    "href": "tics411/clase-13.html#caso-multivariado-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 1\n\n\n\nCalcular el punto central de todos los puntos (Promedio) \\[\\mu = (3.16, 3.16)\\]\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 2\n\n\n\nCalcular la Inversa de la Matriz de Covarianza:"
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-continuaci√≥n",
    "href": "tics411/clase-13.html#caso-multivariado-continuaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Caso Multivariado: Continuaci√≥n",
    "text": "Caso Multivariado: Continuaci√≥n\n\n\n\n\n\n\nPaso 3\n\n\nCalcular la distancia de cada punto con respecto a la media y la inversa de la Covarianza.\n\n\n\n\\[d_i = (p_i - \\mu)^T \\Sigma^{-1}(p_i - \\mu)\\]\n\\[d_1 = (p_1 - \\mu)^T \\sigma^{-1}(p_1 - \\mu)\\]\n\\[d_1 = ([0,0] - [3.16, 3.16])^T \\begin{bmatrix}\n                            0.147 & -0.147  \\\\\n                            -0.147 & 1.911  \\\\\n                            \\end{bmatrix}\n                            ([0,0] - [3.16, 3.16])\\]\n\n\n\n\n\n\nSe debe repetir este procedimiento para cada punto."
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-continuaci√≥n-1",
    "href": "tics411/clase-13.html#caso-multivariado-continuaci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Caso Multivariado: Continuaci√≥n",
    "text": "Caso Multivariado: Continuaci√≥n\n\n\n\n\n\n\nPaso 4:\n\n\nSe debe calcular el punto cr√≠tico seg√∫n t-student con 95% confianza, y orden de magnitud \\(m\\) dimensiones.\n\n\n\n\\[t_{(\\alpha = 0.95,2)} = 5.99\\]\n\n\n\n\n\n\nPaso 5\n\n\nComparar, Si \\(d_i&gt;t_{crit}\\) entonces \\(d_i\\) es Outlier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso ning√∫n valor de \\(d_i\\) es mayor al $t_{(crit)}, por lo tanto, no hay outliers."
  },
  {
    "objectID": "tics411/clase-13.html#distancia-de-mahalanobis",
    "href": "tics411/clase-13.html#distancia-de-mahalanobis",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Distancia de Mahalanobis",
    "text": "Distancia de Mahalanobis\nLa distancia de Mahalanobis corresponde a:\n\\[d_i = \\sqrt{(p_i - \\mu)^T \\Sigma^{-1}(p_i - \\mu)}\\]\n\n\n\n\n\n\nSe puede repetir el mismo procedimiento anterior, s√≥lo que se define una Distancia de Mahalonobis umbral. Las que superen dicho umbral son considerados como Outliers."
  },
  {
    "objectID": "tics411/clase-13.html#dbscan",
    "href": "tics411/clase-13.html#dbscan",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\nPodemos utilizar el procedimiento que aprendimos de DBSCAN. Todos los puntos Noise ser√°n considerados como Anomal√≠as.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point. Lo cu√°l en nuestro caso particular se considerar√° una Anomal√≠a."
  },
  {
    "objectID": "tics411/clase-13.html#k-nearest-neighbor",
    "href": "tics411/clase-13.html#k-nearest-neighbor",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Nearest Neighbor",
    "text": "K-Nearest Neighbor\n\n\n\n\n\n\nSe puede utilizar los modelos de vecinos m√°s cercanos para determinar outliers siguiendo el siguiente procedimiento:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 1: Definir el valor de \\(k\\) para encontrar los vecinos m√°s cercanos.\nEj: Sea \\(k=3\\)\nPaso 2: Calcular la Matriz de Distancias y determinar los vecinos m√°s cercanos."
  },
  {
    "objectID": "tics411/clase-13.html#k-nearest-neighbor-continuaci√≥n",
    "href": "tics411/clase-13.html#k-nearest-neighbor-continuaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Nearest Neighbor: Continuaci√≥n",
    "text": "K-Nearest Neighbor: Continuaci√≥n\n\n\n\nPaso 3: Calcular la distancias Promedio.\n\n\n\n\n\n\nPaso 4: Escoger un Umbral. Si es que la distancia es mayor al Umbral entonces es un Outlier. Ej: \\(Dist_crit: 3\\)"
  },
  {
    "objectID": "tics411/clase-13.html#local-outlier-factor-lof",
    "href": "tics411/clase-13.html#local-outlier-factor-lof",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\n\nLocal Outlier Factor (LOF) detecta anomal√≠as con sus vecindarios locales, en lugar de la distribuci√≥n glocal de los datos.\n\n\n\n\n\n\n\nEn la Figura, \\(O1\\) y \\(O2\\) son anomal√≠as locales en comparaci√≥n con \\(C1\\), \\(O3\\) es una anomal√≠a global, y \\(O4\\) no es una anomal√≠a."
  },
  {
    "objectID": "tics411/clase-13.html#algoritmo",
    "href": "tics411/clase-13.html#algoritmo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Algoritmo",
    "text": "Algoritmo\n\nDeterminar \\(N(x,k)\\), los k-vecinos m√°s cercanos de cada punto x.\nPara todo punto \\(y\\), calcular la distancia a su k-√©simo vecino m√°s cercano.\nCalcular la reach-distance entre todos los puntos: \\[reach-distance_k(x,y) = max\\{k-distance(y), d(x,y)\\}\\]\nCalcule la densidad del vecindario local sobre sus \\(k\\) vecinos, donde \\(|N(x,k)| = k\\).\n\n\\[density(x,k) = lrd_k(x) = \\left(\\frac{\\sum_{y \\in N(x,k)} reach-distance_k(x,y)}{|N(x,k)|}\\right)^{-1}\\]\n\nCalcule el Local Outlier Factor para el punto x como la proporci√≥n de la densidad de sus \\(k\\) vecinos m√°s cercanos, con respecto a la densidad del punto \\(x\\).\n\n\n\n\\[ LOF(x) = \\frac{\\sum_{y \\in N(x,k) density(y,k)}}{|N(x,k)|density(x,k)} \\]\n\n\n\n\n\n\n\nLOF(X) &gt;&gt; 1 implica anomal√≠a."
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor",
    "text": "Ejemplo Local Outlier Factor\n\n\n\nConsideremos los siguientes 4 puntos de datos: a(0,0), b(0,1), c(1,1), d(3,0). Calcular el LOF para cada punto y mostrar la anomal√≠a principal.\n\n\n\n\n\n\n\n\n\nUtilizar \\(K = 2\\) y Distancia Manhattan.\n\n\n\n\n\nPaso 1: Calcular Distancias\n\ndist(a,b) = 1\ndist(a,c) = 2\ndist(a,d) = 3\ndist(b,c) = 1\ndist(b,d) = 4\ndist(c,d) = 3"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuaci√≥n",
    "text": "Ejemplo Local Outlier Factor: Continuaci√≥n\nPaso 2: Para todo punto \\(y\\), calcule la distancia a su k-√©simo vecino m√°s cercano.\n\n\\(dist_2(a) = dist(a,c) = 2\\) (c es el 2do vecino m√°s cercano)\n\\(dist_2(b) = dist(b,a) = 1\\) (a/c es el 2do vecino m√°s cercano)\n\\(dist_2(c) = dist(c,a) = 2\\) (a es el 2do vecino m√°s cercano)\n\\(dist_2(d) = dist(d,a) = 3\\) (a/c es el 2do vecino m√°s cercano)"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-1",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuaci√≥n",
    "text": "Ejemplo Local Outlier Factor: Continuaci√≥n\nPaso 3: Calcular la reach-distance entre todos los puntos, es decir, los puntos vecindarios a una distancia k.\n\n\n\n\\(N_k(o)\\): Vecindario de \\(k\\)-distancia de \\(o\\), \\(N_k(o)=\\{o'\\|o' \\in D, dist(o,o') \\le dist_k(o)\\}\\)\n\n\n\n\n\\(N_2(a) = \\{b,c\\}\\)\n\\(N_2(b) = \\{a,c\\}\\)\n\\(N_2(c) = \\{b,a\\}\\)\n\\(N_2(d) = \\{a,c\\}\\)"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-2",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuaci√≥n",
    "text": "Ejemplo Local Outlier Factor: Continuaci√≥n\n\n\nPaso 4: Calcular la densidad del vecinadario local sobre sus \\(k\\) vecinos.\n\n\n\n\n\n\n\n\n\n\\(reach-dist_2(b \\leftarrow a) = max\\{dist_2(b), dist(b,a)\\} = max\\{1,1\\} = 1\\)\n\\(reach-dist_2(c \\leftarrow a) = max\\{dist_2(c), dist(c,a)\\} = max\\{2,2\\} = 2\\)\n\n\n\n\n\n\n\nCalcular el resto de manera an√°loga."
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-3",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuaci√≥n",
    "text": "Ejemplo Local Outlier Factor: Continuaci√≥n\nEntonces, \\(lrd_k(o)\\): Densidad de alcanzabilidad local de \\(o\\).\n\\[lrd_2(a) = \\frac{|\\mathcal{N}(a)|}{reach-dist_2(b\\leftarrow a) + reach-dist_2(c \\leftarrow a)} = \\frac{2}{1 + 2} = 0.667\\] \\[lrd_2(b) = \\frac{|\\mathcal{N}(b)|}{reach-dist_2(a\\leftarrow b) + reach-dist_2(b \\leftarrow b)} = \\frac{2}{2 + 2} = 0.5\\] \\[lrd_2(c) = \\frac{|\\mathcal{N}(c)|}{reach-dist_2(b\\leftarrow c) + reach-dist_2(a \\leftarrow c)} = \\frac{2}{1 + 2} = 0.667\\] \\[lrd_2(d) = \\frac{|\\mathcal{N}(d)|}{reach-dist_2(a\\leftarrow d) + reach-dist_2(c \\leftarrow d)} = \\frac{2}{1 + 2} = 0.33\\]"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-4",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-4",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuaci√≥n",
    "text": "Ejemplo Local Outlier Factor: Continuaci√≥n\nPaso 5: Calcular el Local Outlier Factor para el punto \\(x\\) como la proporci√≥n de la densidad de sus \\(k\\) vecinos m√°s cercanos, con respecto a la densidad del punto \\(x\\).\n\n\n\\[LOF(x) = \\frac{\\sum_{y \\in N(x,k)} density(y,k)}{|N(x,k)|density(x,k)}\\]\n\\[LOF_2(a) = \\frac{lrd_2(b) + lrd_2(c)}{N_2(a) \\cdot lrd_2(a)} = \\frac{0.5 + 0.667}{2 \\cdot 0.667} = 0.87\\] \\[LOF_2(b) = \\frac{lrd_2(a) + lrd_2(c)}{N_2(b) \\cdot lrd_2(b)} = \\frac{0.667 + 0.667}{2 \\cdot 0.5} = 1.334\\] \\[LOF_2(c) = \\frac{lrd_2(b) + lrd_2(a)}{N_2(c) \\cdot lrd_2(c)} = \\frac{0.5 + 0.667}{2 \\cdot 0.667} = 0.87\\] \\[LOF_2(d) = \\frac{lrd_2(a) + lrd_2(c)}{N_2(d) \\cdot lrd_2(d)} = \\frac{0.667 + 0.667}{2 \\cdot 0.33} = 2\\]"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-5",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuaci√≥n-5",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuaci√≥n",
    "text": "Ejemplo Local Outlier Factor: Continuaci√≥n\nPaso 6: Ordena todas las LOF_k(o)\n\nLOF_2(d) = 2 \\(\\implies\\) el punto paraecer una anomal√≠a (LOF &gt;&gt; 1)\nLOF_2(b) = 1.334\nLOF_2(a) = 0.87\nLOF_2(c) = 0.87\n\n\n\n\n\n\n\nDetalles T√©cnicos\n\n\n\nDado que esto sigue un enfoque local, la resoluci√≥n depende de la elecci√≥n del usuario para \\(k\\).\nGenera una puntuaci√≥n (Anomaly Score) para cada punto.\nComo \\(LOF\\) es una raz√≥n, es dif√≠cil de interpretar. No existe un valor umbral espec√≠fico por encima del cual un punto se define como un valor at√≠pico. La identificaci√≥n de un valor at√≠pico depende del problema y del usuario\nComo \\(LOF\\) es una raz√≥n, es dif√≠cil de interpretar. No existe un valor umbral espec√≠fico por encima del cual un punto se define como un valor at√≠pico. La identificaci√≥n de un valor at√≠pico depende del problema y del usuario."
  },
  {
    "objectID": "tics411/clase-4.html#definiciones",
    "href": "tics411/clase-4.html#definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nClustering Jer√°rquico\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender. Se basa en la construcci√≥n de Jerarqu√≠as para ir construyendo clusters.\n\n\nDendograma\n\n\nCorresponde a un diagrama en el que se muestran las distancias de atributos entre clases que son parte de un mismo cluster."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerarqu√≠a",
    "href": "tics411/clase-4.html#clustering-jerarqu√≠a",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Jerarqu√≠a",
    "text": "Clustering: Jerarqu√≠a\n\nLos algoritmos basados en jerarqu√≠a pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a trav√©s de una m√©trica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una m√©trica de similaridad."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Aglomerativo: Algoritmo",
    "text": "Clustering Aglomerativo: Algoritmo\nAlgoritmo\n\nInicialmente se considera cada punto como un cluster.\nCalcula la matriz de proximidad/distancia entre cada cluster.\nRepetir (hasta que exista un solo cluster):\n\nUnir los cluster m√°s cercanos.\nActualizar la matriz de proximidad/distancia.\n\n\n\n\n\n\n\n\nLo m√°s importante de este proceso es el c√°lculo de la matriz de proximidad/distancia entre clusters\n\n\n\n\n\n\n\n\n\nDistintos enfoques de distancia entre clusters, segmentan los datos en forma distinta."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Aglomerativo: Ejemplo",
    "text": "Clustering Aglomerativo: Ejemplo\nSupongamos que tenemos cinco tipos de genes cuya expresi√≥n ha sido determinada por 3 caracter√≠ticas. Las siguientes expresiones pueden ser vistas como la expresi√≥n dados los genes en tres experimentos. ‚Äã\n\nApliquemos un Clustering Jer√°rquico Aglomerativo utilizando como medida de similaridad la Distancia Euclideana.\n\n\n\n\n\n\n\nOtros tipos de distancia tambi√©n son aplicables siguiendo un procedimiento an√°logo."
  },
  {
    "objectID": "tics411/clase-4.html#algoritmo-1era-iteraci√≥n",
    "href": "tics411/clase-4.html#algoritmo-1era-iteraci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Algoritmo: 1era Iteraci√≥n",
    "text": "Algoritmo: 1era Iteraci√≥n\n\n\n\n\n\n\nEl algoritmo considerar√° que todos los puntos inicialmente son un cluster. Por lo tanto, tratar√° de encontrar los 2 puntos m√°s cercanos e intentar√° unirnos en un s√≥lo cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblema: ¬øC√≥mo actualizamos la matriz de Distancias?\n\n\n\n\n\n\n\n\n\n\n\nEntonces crearemos un nuevo cluster: bcl2-Caspade."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Aglomerativo: Single Linkage",
    "text": "Clustering Aglomerativo: Single Linkage\n\n\n\n\n\n\n\n\n\nDistancia entre clusters determinada por los puntos m√°s similares entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = min\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nGenera Clusters largos y delgados.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nAfectado por Outliers"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Aglomerativo: Complete Linkage",
    "text": "Clustering Aglomerativo: Complete Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por la distancia ente los puntos m√°s dis√≠miles entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = max\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato at√≠picos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a quebrar Clusters Grandes.\nTiene tendencia a generar Clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Aglomerativo: Average Linkage",
    "text": "Clustering Aglomerativo: Average Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el promedio de las distancias que componen los clusters.\nPunto intermedio entre Single y Complete.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = avg\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a datos at√≠picos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Aglomerativo: Ward Linkage",
    "text": "Clustering Aglomerativo: Ward Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el incremento del Within cluster distance.\nMinimiza la distancia intra cluster y maximiza la distancia entre clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = wc(Cij) - wc(C_i) - wc(C_j) = \\frac{n_i\\cdot n_j}{n_i + n_j}||\\bar{C_i} - \\bar{C_j}||^2\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato at√≠picos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#hiperpar√°metros",
    "href": "tics411/clase-4.html#hiperpar√°metros",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Hiperpar√°metros",
    "text": "Hiperpar√°metros\nLos Hiperpar√°metros de este modelo ser√°n:\n\n\n\n\n\n\nNote\n\n\n\nlinkage: La forma de calcular la distancia entre clusters.\ndistancia: La distancia utilizada como similaridad entre los clusters.\n\n\n\n\n\n\n\n\n\n\nA diferencia de K-Means, este m√©todo no requiere definir el n√∫mero de Clusters a priori."
  },
  {
    "objectID": "tics411/clase-4.html#volvamos-a-la-iteraci√≥n-1",
    "href": "tics411/clase-4.html#volvamos-a-la-iteraci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Volvamos a la Iteraci√≥n 1",
    "text": "Volvamos a la Iteraci√≥n 1\n\nSupongamos que por simplicidad utilizaremos Average Linkage. (El proceso para utilizar otro linkage es an√°logo).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos a extraer una Matriz entre los puntos a fusionar y los puntos de los clusters restantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 1era Iteraci√≥n"
  },
  {
    "objectID": "tics411/clase-4.html#iteraci√≥n-2",
    "href": "tics411/clase-4.html#iteraci√≥n-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Iteraci√≥n 2",
    "text": "Iteraci√≥n 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 2da Iteraci√≥n"
  },
  {
    "objectID": "tics411/clase-4.html#iteraci√≥n-3",
    "href": "tics411/clase-4.html#iteraci√≥n-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Iteraci√≥n 3",
    "text": "Iteraci√≥n 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 3ra Iteraci√≥n"
  },
  {
    "objectID": "tics411/clase-4.html#dendograma-resultante",
    "href": "tics411/clase-4.html#dendograma-resultante",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Dendograma Resultante",
    "text": "Dendograma Resultante\n\n\n\n\n\n\nNo es necesario realizar la √∫ltima iteraci√≥n ya que se entiende que ambos clusters se unen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øC√≥mo encontramos los clusters una vez que tenemos el Dendograma?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos escoger un umbral de distancia y ver cu√°ntos clusters se forman.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo regla general se deben escoger clusters m√°s distanciados entre s√≠."
  },
  {
    "objectID": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "href": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Efecto del Linkage Escogido",
    "text": "Efecto del Linkage Escogido"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jer√°rquico-detalles-t√©cnicos",
    "href": "tics411/clase-4.html#clustering-jer√°rquico-detalles-t√©cnicos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering Jer√°rquico: Detalles T√©cnicos",
    "text": "Clustering Jer√°rquico: Detalles T√©cnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nNo requiere definir el n√∫mero de Clusters a priori.\nAl tener distintas variantes es posible que los puntos sean agrupados de manera completamente distintas.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nMuy ineficiente computacionalmente debido a que genera una nueva matriz de distancia en cada iteraci√≥n lo que entrega una complejidad \\(O(n^2)\\) o \\(O(n^3)\\) dependiendo del linkage.\nUna vez que se decide combinar 2 clusters no es posible revertir esta decisi√≥n.\nNo tiene capacidad de generalizaci√≥n, ya que no es posible aplicarlo a datos nuevos."
  },
  {
    "objectID": "tics411/clase-4.html#implementaci√≥n-en-scikit-learn",
    "href": "tics411/clase-4.html#implementaci√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Scikit-Learn",
    "text": "Implementaci√≥n en Scikit-Learn\nfrom sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=2, metric=\"euclidean\",linkage=\"ward\")\n\n## Se entrena y se genera la predicci√≥n\nac.fit_predict(X)\n\n\nn_clusters: Define el n√∫mero de clusters a crear, por defecto 2.\nmetric: Permite distancias L1, L2 y coseno. Por defecto ‚Äúeuclidean‚Äù.\nlinkage: Permite single, complete, average y ward. Por defecto ‚Äúward‚Äù.\n.fit_predict(): Entrenar√° el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento.\n\n\n\n\n\n\n\n\n\nSi bien el m√©todo de Aglomeraci√≥n no requiere el n√∫mero de clusters a generar, Scikit-Learn lo exige de modo de poder etiquetar cada elemento.\n\n\n\n\n\n\n\n\n\n\n\n¬øPor qu√© no existen los m√©todos .fit() y .predict() por separado?"
  },
  {
    "objectID": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "href": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Otras implementaciones (Dendograma)",
    "text": "Otras implementaciones (Dendograma)\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Genera los c√°lculos necesarios para construir el Histograma.\nZ = linkage(X, method='single', metric=\"euclidean\") \n\n# Graficar el Dendograma\nplt.figure(figsize=(10, 5)) # Define el tama√±o del Gr√°fico\nplt.title('Dendograma Clustering Jer√°rquico') # Define un t√≠tulo para el dendograma\nplt.xlabel('Iris Samples')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\nplt.show()\n\n\nPrincipalmente este c√≥digo permite graficar el Dendograma completo.\nL4: Genera una instancia del Dendograma. (Ser√≠a equivalente al .fit() de Scikit-Learn).\nL5-L12: Corresponde al c√≥digo necesario para graficar el Dendograma."
  },
  {
    "objectID": "tics411/clase-4.html#sugerencias",
    "href": "tics411/clase-4.html#sugerencias",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que el clustering aglomerativo tambi√©n es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala com√∫n.\n\n\n\n\n\n\n\n\n\n\nOtras t√©cnicas como merge y split, no aplican a este tipo de clustering debido a las limitaciones del algoritmo."
  },
  {
    "objectID": "tics411/clase-4.html#variantes",
    "href": "tics411/clase-4.html#variantes",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Variantes",
    "text": "Variantes\n\nEn casos en los que no es posible calcular distancias debido a la presencia de datos categ√≥ricos, es posible utilizar el Gower Dissimilarity como medida de similitud.\n\n\n\n\n\n\n\n\n\nGower\n\nSe define como la proporci√≥n de variables que tienen distinto valor con respecto al total sin considerar donde ambos son ceros.\n\n\n\n\\[Gower(p1,p2) = \\frac{3}{9}\\]"
  },
  {
    "objectID": "tics411/clase-12.html#intuici√≥n",
    "href": "tics411/clase-12.html#intuici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Intuici√≥n",
    "text": "Intuici√≥n\nSupongamos el siguiente dataset:\n\n\n\n\n\n\n\n\n\n\n\n¬øC√≥mo puedo separar ambas clases?"
  },
  {
    "objectID": "tics411/clase-12.html#intuici√≥n-1",
    "href": "tics411/clase-12.html#intuici√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Intuici√≥n",
    "text": "Intuici√≥n"
  },
  {
    "objectID": "tics411/clase-12.html#intuici√≥n-2",
    "href": "tics411/clase-12.html#intuici√≥n-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Intuici√≥n",
    "text": "Intuici√≥n\n\n\n\n\n\n\n\n\n\n\n\n\nLa frontera de decisi√≥n se puede caracterizar como la ecuaci√≥n de una recta (en forma general).\n\n\n\n\n\n\n\n\n\n\n\nAdem√°s definiremos \\(h_\\theta(X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2\\)."
  },
  {
    "objectID": "tics411/clase-12.html#intuici√≥n-3",
    "href": "tics411/clase-12.html#intuici√≥n-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Intuici√≥n",
    "text": "Intuici√≥n\n\n\n\n\n\n\n\n\n\n\n\nPodr√≠amos pensar que si \\(h_\\theta(X)\\) es positivo entonces pertenece a la clase 1 y si \\(h_\\theta(X)\\) es negativo pertenece a la clase 0."
  },
  {
    "objectID": "tics411/clase-12.html#la-funci√≥n-sigmoide-o-log√≠stica",
    "href": "tics411/clase-12.html#la-funci√≥n-sigmoide-o-log√≠stica",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "La Funci√≥n Sigmoide o Log√≠stica",
    "text": "La Funci√≥n Sigmoide o Log√≠stica\n\\[ g(z) = \\frac{1}{1 + e^{-z}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunci√≥n no lineal.\nFunci√≥n acotada entre 0 y 1.\n\\(g(\\varepsilon) = 0.5\\), \\(\\varepsilon = 0\\)\n\n\n\n\n\n\n\n\n\n\n\nDe ac√° sale la noci√≥n del umbral 0.5 que hemos visto en clases anteriores.\n\n\n\n\n\n\n\n\n\n\n\n\n¬øQu√© pasar√≠a si ahora decimos que \\(z = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2\\)?"
  },
  {
    "objectID": "tics411/clase-12.html#la-regresi√≥n-log√≠stica",
    "href": "tics411/clase-12.html#la-regresi√≥n-log√≠stica",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "La Regresi√≥n Log√≠stica",
    "text": "La Regresi√≥n Log√≠stica\n\\[P[y = 1|X, \\theta] = g(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2)}}\\]\n\n\n\n\n\n\n\nRegla de Decisi√≥n:\n\n\n\nSi \\(g(z) \\ge 0.5 \\implies Clase \\, 1\\).\nSi \\(g(z) &lt; 0.5 \\implies Clase \\, 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(g(z)\\) se puede interpretar como una probabilidad de pertenecer a la Clase 1.\n\n\n\n\n\n\n\n\n\n\n\n\\(1 -g(z)\\) se puede interpretar como una probabilidad de NO pertenecer a la Clase 1, es decir, pertenecer a la Clase 0."
  },
  {
    "objectID": "tics411/clase-12.html#aprendizaje-del-modelo",
    "href": "tics411/clase-12.html#aprendizaje-del-modelo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Aprendizaje del Modelo",
    "text": "Aprendizaje del Modelo\nSupongamos lo siguiente:\n\n\n\\[P(y = 1| X, \\theta) = g(z)\\]\n\n\\[P(y = 0| X, \\theta) = 1-g(z)\\]\n\n\nAmbas ecuaciones pueden comprimirse en una sola de la siguiente manera: \\[ P(y|X,\\theta) = g(z)^y (1 - g(z))^{1-y}\\]\n\n\n\n\n\n\nPara encontrar los par√°metros \\(\\theta\\) podemos utilizar una t√©cnica llamada Maximum Likelihood Estimation."
  },
  {
    "objectID": "tics411/clase-12.html#maximum-likelihood-estimation",
    "href": "tics411/clase-12.html#maximum-likelihood-estimation",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\\[\\mathcal{L}(\\theta) = \\prod_{i=1}^n P(y^{(i)} | x^{(i)}, \\theta)\\]\n\\[ \\underset{\\theta}{argmin} \\ -l(\\theta)\\] \\[l(\\theta) = log (\\mathcal{L(\\theta)}) = \\sum_{i=1}^n y^{(i)} \\cdot log(g(z)) + (1-y^{(i)})\\cdot log(1-g(z))\\]\n\n\n\n\n\n\nEsta ecuaci√≥n se conoce como Entrop√≠a Cruzada o como Negative Log Loss (NLL) y tiene la gracia de que es una curva convexa lo que garantiza un valor √∫nico de los par√°metros \\(\\theta\\)."
  },
  {
    "objectID": "tics411/clase-12.html#c√°lculo-de-coeficientes",
    "href": "tics411/clase-12.html#c√°lculo-de-coeficientes",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "C√°lculo de Coeficientes",
    "text": "C√°lculo de Coeficientes\n\n\n\n\n\n\nLa t√©cnica m√°s famosa para minimizar este tipo de problemas se conoce como Stochastic Gradient Descent. Lo que genera la siguiente soluci√≥n:\n\n\n\n\\[\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{1}{n}\\sum_{i=1}^n\\left(g(z)-y^{(i)}\\right)x_j^{(i)}\\]\n\n\n\n\n\n\nA pesar de lo complicado que se ve la ecuaci√≥n, implementarla en c√≥digo es bastante sencillo."
  },
  {
    "objectID": "tics411/clase-12.html#frontera-de-decisi√≥n",
    "href": "tics411/clase-12.html#frontera-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Frontera de Decisi√≥n",
    "text": "Frontera de Decisi√≥n"
  },
  {
    "objectID": "tics411/clase-12.html#inference-time",
    "href": "tics411/clase-12.html#inference-time",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Inference Time",
    "text": "Inference Time\nEn este caso se calcula: \\[g_\\theta(x^{(i)})=sigmoid(\\theta^t x^{(i)})\\]\n\n\\(\\theta\\): Corresponde a un vector con todos los par√°metros calculados.\n\\(x^{(i)}\\): Corresponde a una instancia de \\(m\\) variables la cual generar√° una probabilidad.\n\n\\(\\theta^t x^{(i)}\\) corresponde al producto punto de dos vectores, que es equivalente a una ‚Äúsuma producto‚Äù.\n\n\\(g_\\theta(x^{(i)})\\): Generar√° un valor entre 0 y 1 al cu√°l se le aplica la Regla de Decisi√≥n."
  },
  {
    "objectID": "tics411/clase-12.html#implementaci√≥n-en-python",
    "href": "tics411/clase-12.html#implementaci√≥n-en-python",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python",
    "text": "Implementaci√≥n en Python\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=1, penalty=\"l2\", random_state = 42)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\ny_proba = lr.predict_proba(X_test)\n\n## Visualizacion de los Par√°metros \nlr.coef_\nlr.intercept_\n\n\nC: Corresponde a un par√°metro de Regularizaci√≥n. Valores m√°s peque√±os implica mayor regularizaci√≥n. Por defecto 1.\npenalty: Corresponde al tipo de regularizaci√≥n. Por defecto ‚Äúl2‚Äù.\n\n‚Äúl1‚Äù: Corresponde a la regularizaci√≥n Lasso. Genera que hayan par√°metros cero, ayudando en la selecci√≥n de variables.\n‚Äúl2‚Äù: Corresponde a la regularizaci√≥n Ridge. Genera que todos los par√°metros sean peque√±os, entregando estabilidad y buena interpretabilidad.\n‚Äúelasticnet‚Äù: Corresponde a la combinaci√≥n de ‚Äúl1‚Äù y ‚Äúl2‚Äù.\nNone: No hay regularizaci√≥n.\n\n\n\n\n\n\n\n\n\nPara cambiar la regularizaci√≥n, consultar la documentaci√≥n de Scikit-Learn."
  },
  {
    "objectID": "tics411/clase-12.html#interpretabilidad",
    "href": "tics411/clase-12.html#interpretabilidad",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Interpretabilidad",
    "text": "Interpretabilidad\n\nUna de las grandes ventajas que tiene la Regresi√≥n Log√≠stica es que sus predicciones son interpretables.\n\n\nTenemos un dataset de 2 variables:\n\nW: Corresponde al peso del Veh√≠culo.\nqsec: Corresponde al tiempo en Segundos que lo toma en recorrer un cuarto de milla.\n\nQueremos predecir si el veh√≠culo es Ec√≥nomico o no (en t√©rminos de consumo de Bencina).\n\n\\[g_\\theta(x) = 0.5 - 3.5 \\cdot W + 1.5 \\cdot qsec \\]\n\n\n\n\n\n\n\n\nSi el veh√≠culo se demora m√°s en el cuarto de milla (qsec aumenta) entonces el veh√≠culo es m√°s econ√≥mico.\n\nTiene menos potencia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi el veh√≠culo es m√°s pesado (W aumenta), entonces es menos econ√≥mico.\n\nRequiere probablemente m√°s combustible para mover dicho peso.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl valor del par√°metro representa tambi√©n la magnitud de la contribuci√≥n."
  },
  {
    "objectID": "tics411/clase-12.html#sugerencias",
    "href": "tics411/clase-12.html#sugerencias",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\n\nEstandarizaci√≥n/Normalizaci√≥n de datos: Permite que la escala de los datos no afecte en la interpretabilidad.\nOne Hot Encoder: En general tiende a dar mejores resultados que el Ordinal.\nInteracciones: Combinaci√≥n de variables.\nVariables no Lineales: Permite que la frontera de Decisi√≥n no sea necesariamente lineal (Regresi√≥n Polin√≥mica)."
  },
  {
    "objectID": "tics411/clase-7.html#introducci√≥n",
    "href": "tics411/clase-7.html#introducci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\n\nGracias a los planes de fidelizaci√≥n (juntar puntos, dar RUT, acumular millas, etc.) las empresas son capaces de detectar patrones:\n\n\nQu√© nos gusta,\nQu√© compramos,\nCon qu√© frecuencia lo compramos,\nJunto con qu√© lo compramos\netc.\n\n\n\n\n\n\n\nMarket Basket Analysis\n\n\nCorresponde al estudio de nuestra canasta de compras. De modo que podamos entender qu√© cosas son las que como clientes preferimos y una empresa pueda Recomendar de manera m√°s apropiadas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones",
    "href": "tics411/clase-7.html#definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nPatr√≥n\n\n\nPredicado (output True/False) para verificar si una estructura buscada ocurre o no.\n\n\nTarea\n\n\nEncontrar reglas de asociaci√≥n basado en patrones.\n\n\n\nEjemplos\n\nDatasets de supermercados:\n\n10% de los clientes totales compran vino y quedo (patr√≥n: si compro vino, tambi√©n llevo queso).\n\nDatasets de Alarmas:\n\nSi la alarma A y B suenan en un intervalo de 30 segundos, entonces la alarma C sonar√° dentro de un intervalo de 60 segundos con 50% de probabilidad."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado\n\nDatos Transaccionales\n\n\nUna transacci√≥n involucra un conjunto de elementos. Una boleta de supermercado muestra el conjunto de elementos comprados por un cliente. Los productos involucrados en una transacci√≥n se denominan items."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado"
  },
  {
    "objectID": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "href": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Objetivo y Aplicaciones",
    "text": "Objetivo y Aplicaciones\n\n\n\n\n\n\nObjetivo\n\n\nEncontrar asociaciones entre elementos u objetos de bases de datos transaccionales.\n\n\n\n\n\n\n\n\n\nAplicaciones\n\n\n\nApoyo a toma de decisiones.\nAn√°lisis de Informaci√≥n de Ventas.\nDistribuci√≥n y ubicaci√≥n de Mercader√≠as.\nSegmentaci√≥n de Clientes en base de patrones de compra.\nDi√°gnostico y predicci√≥n de alarmas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones-medidas",
    "href": "tics411/clase-7.html#definiciones-medidas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definiciones: Medidas",
    "text": "Definiciones: Medidas\n\n\n\n\n\n\nSupport (Soporte)\n\nFracci√≥n de Transacciones que contienen a \\(X\\). Probabilidad de que una transacci√≥n contenga a \\(X\\).\n\n\n\\[Supp(X) = P(X)\\]\n\n\n\n\n\n\n\nSupport Count\n\nN√∫mero de Transacciones que contienen a \\(X\\).\n\n\n\\[SuppCount(X) = Count(X)\\]\n\n\n\n\n\n\n\n\nConfidence (Confianza o Eficiencia)\n\nFracci√≥n de las Transacciones en las que aparece \\(X\\) que tambi√©n incluyen \\(Z\\).\n\n\n\\[Conf(X \\implies Z) = \\frac{Supp(X \\cup Z)}{Supp(X)}\\] \\[Conf(X \\implies Z) = \\frac{SuppCount(X \\cup Z)}{SuppCount(X)}\\]\n\n\n\n\n\n\n\n\n\n\n\nOjo con la Notaci√≥n \\(\\cup\\). En este caso significa que tanto el producto X como el Producto Z sean parte de la transacci√≥n."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "href": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplos: Support y Confidence",
    "text": "Ejemplos: Support y Confidence\n\n\n\n\n\n\n\n\n\\[ Supp({Pan}) = 4/7\\] \\[ Supp({Leche}) = 3/7\\] \\[ Supp({Pan, Huevo}) = 2/7\\]\n\\[ Conf({Pan} \\implies {Huevo}) = \\frac{Supp({Pan, Huevo})}{Supp(Pan)} = \\frac{2/7}{4/7}\\]\n\\[ Conf({Pan} \\implies {Leche}) = \\frac{Supp({Pan, Leche})}{Supp(Pan)} = \\frac{1/7}{4/7}\\] \\[ Conf({Leche} \\implies {Pan}) = \\frac{Supp({Pan, Leche})}{Supp(Leche)} = \\frac{1/7}{3/7}\\]"
  },
  {
    "objectID": "tics411/clase-7.html#problema",
    "href": "tics411/clase-7.html#problema",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Problema",
    "text": "Problema\n\nEn un dataset transaccional de n productos totales y \\(|U_i|\\) elementos para la Transacci√≥n \\(i\\).\n\nSe pueden generar un total de \\(N_{reglas}\\) de asociaci√≥n:\n\\[N_{reglas} = \\sum_{i=1}^{2^{n}} \\sum_{j=0}^{|U_i|}\\binom{|U_i|}{j}\\]\n\n\n\n\n\n\n\n\n\nSi suponemos un supermercado que tiene 1000 productos, y transacciones que pueden ir entre 1 y 50 productos. El problema es muy costoso, y se podr√≠an eventualmente generar demasiadas combinaciones."
  },
  {
    "objectID": "tics411/clase-7.html#algoritmo-apriori",
    "href": "tics411/clase-7.html#algoritmo-apriori",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Algoritmo Apriori",
    "text": "Algoritmo Apriori\n\nApriori\n\n\nEs un algoritmo para aprender reglas de asociaci√≥n que utiliza el principio Apriori para buscar de forma eficiente las reglas que satisfacen los l√≠mites de soporte y confianza.\n\n\n\n\nAlgoritmo\n\nFijar \\(k=1\\) y determinar lista de candidatos de tama√±o \\(k\\).\n\nCalcular la frecuencia del conjunto.\nEliminar conjuntos con baja frecuencia (utilizando un umbral de soporte).\nUnir los conjuntos frecuentes para generar conjuntos de tama√±o \\(k+1\\).\nSi existe la posibilidad de seguir creando combinaciones volver al paso a y repetir.\n\nUsar todos los conjuntos frecuentes para generar reglas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori",
    "href": "tics411/clase-7.html#ejemplo-apriori",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Apriori",
    "text": "Ejemplo Apriori\n\nSupongamos el siguiente dataset transaccional:\n\nSupongamos que queremos calcular las reglas de asociaci√≥n que tengan un MinSupp=40% y un MinConf=70%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodr√≠amos pensar que MinSupp y MinConf son los hiperpar√°metros de este algoritmo."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteraci√≥n-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteraci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Apriori: Iteraci√≥n 1",
    "text": "Ejemplo Apriori: Iteraci√≥n 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGalletas NO CUMPLE con el Soporte M√≠nimo solicitado. Por lo tanto, lo elimino y genero relaciones de 2 productos sin considerar Galletas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteraci√≥n-2",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteraci√≥n-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Apriori: Iteraci√≥n 2",
    "text": "Ejemplo Apriori: Iteraci√≥n 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nAc√° NO SE ELIMINA ning√∫n producto, ya que en los itemsets que sobrevivieron hay Pan, Mantequilla, Leche, Pa√±ales y Cerveza."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteraci√≥n-3",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteraci√≥n-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Apriori: Iteraci√≥n 3",
    "text": "Ejemplo Apriori: Iteraci√≥n 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe puede apreciar que los √∫nicos 3 productos que sobreviven son Pan, Mantequilla y Leche. Por lo tanto, NO ES POSIBLE generar reglas con 4 productos."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generaci√≥n-de-reglas",
    "href": "tics411/clase-7.html#ejemplo-apriori-generaci√≥n-de-reglas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Apriori: Generaci√≥n de Reglas",
    "text": "Ejemplo Apriori: Generaci√≥n de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pan, Mantequilla}:\n\n\\(Conf(Pan \\implies Mantequilla) = \\frac{Supp(Pan, Mantequilla)}{Supp(Pan)} = \\frac{3}{3}\\)‚úÖ \\(Conf(Mantequilla \\implies Pan) = \\frac{Supp(Pan, Mantequilla)}{Supp(Mantequilla)} = \\frac{3}{3}\\)‚úÖ\n\n\n\nPara {Pan, Leche}:\n\n\\(Conf(Pan \\implies Leche) = \\frac{Supp(Pan, Leche)}{Supp(Pan)} = \\frac{2}{3}\\) ‚ùå \\(Conf(Leche \\implies Pan) = \\frac{Supp(Pan, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ‚úÖ\n\n\n\nPara {Mantequilla, Leche}:\n\n\\(Conf(Mantequilla \\implies Leche) = \\frac{Supp(Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\) ‚ùå \\(Conf(Leche \\implies Mantequilla) = \\frac{Supp(Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ‚úÖ"
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generaci√≥n-de-reglas-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-generaci√≥n-de-reglas-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo Apriori: Generaci√≥n de Reglas",
    "text": "Ejemplo Apriori: Generaci√≥n de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pa√±ales, Cerveza}:\n\n\\(Conf(Pa√±ales \\implies Cerveza) = \\frac{Supp(Pa√±ales, Cerveza)}{Supp(Pa√±ales)} = \\frac{2}{3}\\)‚ùå \\(Conf(Cerveza \\implies Pa√±ales) = \\frac{Supp(Pa√±ales, Cerveza)}{Supp(Cerveza)} = \\frac{2}{2}\\)‚úÖ\n\n\n\nPara {Pan, Mantequilla, Leche}:\n\n\\(Conf({Pan, Mantequilla} \\implies {Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Mantequilla)} = \\frac{2}{3}\\)‚ùå \\(Conf({Pan, Leche} \\implies {Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Leche)} = \\frac{2}{2}\\)‚úÖ \\(Conf({Mantequilla, Leche} \\implies {Pan}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla, Leche)} = \\frac{2}{2}\\)‚úÖ\n\n\\(Conf({Leche} \\implies {Pan, Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\)‚úÖ \\(Conf({Mantequilla} \\implies {Pan, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\)‚ùå \\(Conf({Pan} \\implies {Mantequilla, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan)} = \\frac{2}{3}\\)‚ùå"
  },
  {
    "objectID": "tics411/clase-7.html#resultado-final",
    "href": "tics411/clase-7.html#resultado-final",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Resultado Final",
    "text": "Resultado Final\n\n\nItemset MinSupp = 40%\n\n\n\n\n\n\nReglas Finales MinConf = 70%\n\\[Pan \\implies Mantequilla\\] \\[Mantequilla \\implies Pan\\] \\[Leche \\implies Pan\\] \\[Leche \\implies Mantequilla\\] \\[Cerveza \\implies Pa√±ales\\] \\[\\{Pan, Leche\\} \\implies Mantequilla\\]\n\\[\\{Mantequilla, Leche\\} \\implies Pan\\] \\[Leche \\implies \\{Pan, Mantequilla\\}\\]\n\n\n\n\n\n\nInsights:\n\n\n\nEl Pan, la Leche y la Mantequilla est√°n relacionados.\nParece ser que si llevo Cervezas tambi√©n llevo Pa√±ales."
  },
  {
    "objectID": "tics411/clase-7.html#evaluaci√≥n-de-reglas-de-asociaci√≥n",
    "href": "tics411/clase-7.html#evaluaci√≥n-de-reglas-de-asociaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Evaluaci√≥n de Reglas de Asociaci√≥n",
    "text": "Evaluaci√≥n de Reglas de Asociaci√≥n\n\nLift\n\nMide qu√© tan lejos de la independencia est√°n \\(X\\) e \\(Y\\). Lift var√≠a entre 0 y \\(\\infty\\).\n\n\n\\[Lift(X,Y) = \\frac{Conf(X \\implies Y)}{s(Y)}\\]\n\n\\(Lift(X,Y) \\sim 1\\) implica independencia y la regla no es importante.\n\\(Lift(X,Y) &lt; 1\\) implica una asociaci√≥n negativa de la regla.\n\\(Lift(X,Y) &gt; 1\\) implica una asociativa de la regla. Un mayor Lift implica que la regla es potencialmente √∫til para el futuro.\n\nEjemplo:\n\\[Lift(Cerveza, Pa√±ales) = \\frac{Conf(Cerveza \\implies Pa√±ales)}{Supp(Pa√±ales)} = \\frac{1}{0.6} = 1.67\\]\n\n\n\n\n\n\nUna persona que compra Cerveza tiene 1.67 m√°s chances de comprar Pa√±ales."
  },
  {
    "objectID": "tics411/clase-7.html#implementaci√≥n-en-python-preprocesamiento",
    "href": "tics411/clase-7.html#implementaci√≥n-en-python-preprocesamiento",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python: Preprocesamiento",
    "text": "Implementaci√≥n en Python: Preprocesamiento\nPre-procesamiento\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns = tre.columns_)\nL4: transactions debe ser una lista de listas. Cada fila, son distintas transacciones. Cada transaccion puede tener distinto n√∫mero de elementos. L5: tre.columns_ extrae los nombres de los productos para que el DataFrame sea m√°s entendible.\n\n\n\n\n\n\ndf_encoded es un DataFrame tipo OneHotEncoder pero con valores Booleanos (Esto es solicitado por la documentaci√≥n)."
  },
  {
    "objectID": "tics411/clase-7.html#implementaci√≥n-en-python-itemsets",
    "href": "tics411/clase-7.html#implementaci√≥n-en-python-itemsets",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python: Itemsets",
    "text": "Implementaci√≥n en Python: Itemsets\nfrom mlxtend.frequent_patterns import apriori \n\nitemset = apriori(df_encoded, min_support=0.5, use_colnames = True)\nL3: df_encoded es el DataFrame preprocesado.\n\nmin_support: Corresponde al Soporte M√≠nimo para generar itemsets. Por defecto 0.5.\nuse_colnames: Permite que las reglas usen los nombres de las columnas para referirse a los productos. Por defecto es False, pero conviene usarlo como True.\nitemset ser√° un DataFrame con los itemsets generados."
  },
  {
    "objectID": "tics411/clase-7.html#implementaci√≥n-en-python-reglas",
    "href": "tics411/clase-7.html#implementaci√≥n-en-python-reglas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Python: Reglas",
    "text": "Implementaci√≥n en Python: Reglas\nfrom mlxtend.frequent_patterns import association_rules\n\nrules = association_rules(itemsets, metric=\"confidence\", min_threshold=0.8)\nL3: itemset es el dataframe generado en el paso anterior.\n\nmetric: M√©trica para definir reglas, puede ser ‚Äúconfidence‚Äù y otras definidas ac√°\nmin_threshold: Corresponde al umbral de la m√©trica a utilizar. Por defecto 0.8.\nrules corresponde a un Dataset que tiene las Reglas de Asociaci√≥n detectadas y muchas m√©tricas asociadas."
  },
  {
    "objectID": "tics411/clase-8.html#introducci√≥n-al-aprendizaje-supervisado",
    "href": "tics411/clase-8.html#introducci√≥n-al-aprendizaje-supervisado",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Introducci√≥n al Aprendizaje Supervisado",
    "text": "Introducci√≥n al Aprendizaje Supervisado\nLos modelos Predictivos/Supervisados tienen la capacidad de predecir valores en datos no vistos.\n\nChip Huyen, Designing ML Systems (Profesora de Stanford)\n\n\n‚ÄúMachine Learning Algorithms do not predict the future but encode the past, thus perpetuating the biases in the data and mode‚Ä¶‚Äù\n\n\n\nAprenden mediante un proceso de entrenamiento en un train set y eval√∫an su performance/rendimiento utilizando un test set."
  },
  {
    "objectID": "tics411/clase-8.html#definiciones",
    "href": "tics411/clase-8.html#definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nFeatures\n\n\nTambi√©n llamadas variables o atributos. Corresponden al input del Modelo y con el cu√°l el modelo aprende y predice. Normalmente es representado mediante una Matriz denominada \\(X\\).\n\n\nLabels o Etiquetas\n\n\nCorresponden a las respuestas que el modelo necesita mapear para poder descubrir patrones de manera autom√°tica. Normalmente se representa mediante un vector denominado \\(y\\)."
  },
  {
    "objectID": "tics411/clase-8.html#ejemplo",
    "href": "tics411/clase-8.html#ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nQueremos generar un algoritmo de aprendizaje tal que dado un cierto set de datos predigamos si es que a un ni√±o se le dar√° o no permiso para jugar.\n\n\n\n\n\n\n\nProblema de Clasificaci√≥n Binaria (Dos clases opuestas)."
  },
  {
    "objectID": "tics411/clase-8.html#definici√≥n-del-problema",
    "href": "tics411/clase-8.html#definici√≥n-del-problema",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Definici√≥n del Problema",
    "text": "Definici√≥n del Problema\n\\[h_\\theta(X) = f(X, \\theta)\\]\n\n\n\n\n\n\n\nA \\(h_\\theta(\\cdot)\\) la denominaremos hip√≥tesis o simplemente modelo.\n\\(X\\) ser√° nuestro set de features (\\(n\\times m\\) donde \\(n\\) es el n√∫mero de observaciones y \\(m\\) el n√∫mero de features).\n\nCada fila de \\(X\\) corresponde a un vector \\(x_i\\) que representa una observaci√≥n de nuestro set de features.\n\\(\\theta\\) corresponde a los par√°metros del modelo (existen modelos param√©tricos y no param√©tricos).\nCada algoritmo tendr√° su propio mapeo \\(f(\\cdot)\\) para tratar de predecir una etiqueta.\n\n\n\n\n\n\n\n\n\n\nTipos de Hip√≥tesis\n\n\n\nSi \\(h_\\theta(X)\\) devuelve valores discretos (o categ√≥ricos) hablaremos de un modelo de Clasificaci√≥n.\nSi \\(h_\\theta(X)\\) devuelve valores continuos hablaremos de un modelo de Regresi√≥n."
  },
  {
    "objectID": "tics411/clase-8.html#tipos-de-problemas",
    "href": "tics411/clase-8.html#tipos-de-problemas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Problemas",
    "text": "Tipos de Problemas\n\n\n\n\n\n\nClasificaci√≥n:\n\n\n\nBinaria: La Clasificaci√≥n es dicot√≥mica, Perro o Gato, S√≠ o No, 1 o 0, Clase Positiva o Negativa.\nMulticlase: La clasificaci√≥n puede tener m√°s de 2 clases, pero s√≥lo una es posible.\n\nEj: Perro, Gato o Canario; 0, 1, 2, 3, 4.\n\nMultilabel: La clasificaci√≥n puede tener m√°s de 2 clases, y m√°s de una es posible a la vez.\n\nEj: Categor√≠as de Libro: Puede ser Romance y Drama, Pel√≠culas: Fantas√≠a, Animaci√≥n y Acci√≥n.\n\n\n\n\n\n\n\n\n\n\n\nRegresi√≥n:\n\n\n\nSimple: Predigo s√≥lo un valor. Ej: Predecir la Temperatura.\nMultiple: Predigo varios valores continuos a la vez.\n\nEj: Modelo para intentar estimar Temperatura y Humedad a la vez.\n\nForecast: Donde se utilizan valores pasados para estimar valores futuros.\n\nDadas mis ganancias pasadas, estimar las futuras."
  },
  {
    "objectID": "tics411/clase-8.html#clasificaci√≥n-intuici√≥n",
    "href": "tics411/clase-8.html#clasificaci√≥n-intuici√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clasificaci√≥n: Intuici√≥n",
    "text": "Clasificaci√≥n: Intuici√≥n\n\n\n\n\n\n\nSupongamos el siguiente problema de clasificaci√≥n. Tenemos un algoritmo, que dadas las variables Largo y Peso sean capaces de predecir si es que un Pez es una Reineta o una Sardina."
  },
  {
    "objectID": "tics411/clase-8.html#clasificaci√≥n-intuici√≥n-1",
    "href": "tics411/clase-8.html#clasificaci√≥n-intuici√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clasificaci√≥n: Intuici√≥n",
    "text": "Clasificaci√≥n: Intuici√≥n\n\n\n\n\n\n\n\n\n\n\n\n\nQueremos encontrar una Regla de Decisi√≥n (Decision Rule) que permita clasificar correctamente un punto nuevo.\nDistintos modelos son capaces de encontrar distintas reglas de decisi√≥n. Por lo tanto, sus predicciones pueden ser completamente distintas."
  },
  {
    "objectID": "tics411/clase-8.html#clasificaci√≥n-detalles",
    "href": "tics411/clase-8.html#clasificaci√≥n-detalles",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clasificaci√≥n: Detalles",
    "text": "Clasificaci√≥n: Detalles\nEs importante mencionar que un modelo de clasificaci√≥n puede generar:\n\nHard Predictions: Es decir, la instancia a predecir es clase 0 o clase 1.\nSoft Prediction: Es decir, la instancia a predecir tiene una probabilidad \\(p\\) de pertenecer a la clase 1 y de \\(1-p\\) de pertenecer a la clase 0.\n\n\n\n\n\n\n\n\nCuando se hace predicci√≥n binaria, lo com√∫n es usar un Threshold de 0.5 para elegir la clase. Es decir si \\(p&lt;0\\) entonces clase 0, si \\(p \\ge 0.5\\) entonces clase 1.\n\n\n\n\n\n\n\n\n\n\nEn el caso de predicci√≥n multiclase o multilabel. Se calcula la probabilidad para cada clase. Por lo tanto se se asigna la clase de mayor probabilidad."
  },
  {
    "objectID": "tics411/clase-8.html#k-nearest-neighbors",
    "href": "tics411/clase-8.html#k-nearest-neighbors",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nEl modelo de vecinos m√°s cercanos, o KNN por sus siglas en Ingl√©s es un modelo basado en distancias. Su regla de decisi√≥n se basa en imitar el comportamiento de sus \\(K\\) vecinos m√°s cercanos por votaci√≥n (para clasificaci√≥n) o la media (para regresi√≥n).\n\n\n\n\n\n\n\nK es un hiperpar√°metro de este modelo.\n\n\n\n\n\n\n\n\n\n\n\n\nSupongamos \\(K = 3\\).\nEs decir, tomaremos los 3 vecinos m√°s cercanos.\n\n\n\n\n\n\n\nEn general es una buena idea elegir vecinos impares. ¬øPor qu√©?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-1-training-time",
    "href": "tics411/clase-8.html#knn-paso-1-training-time",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "KNN: Paso 1 (Training Time)",
    "text": "KNN: Paso 1 (Training Time)\n\nTraining Time\n\nCorresponde al periodo donde el modelo aprende de los datos. Toma un patr√≥n y ese modelo es utilizado para predecir.\n\n\n\n\n\n\n\n\nEn el caso de un KNN NO HAY APRENDIZAJE en esta etapa.\n\n\n\n\n\n\n\n\n\nEs considerado un modelo no-param√©trico ya que no aprende par√°metros para realizar su predicci√≥n."
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-2-test-time",
    "href": "tics411/clase-8.html#knn-paso-2-test-time",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "KNN: Paso 2 (Test Time)",
    "text": "KNN: Paso 2 (Test Time)\n\nInference Time\n\nCorresponde al periodo donde el modelo debe emitir una predicci√≥n.\n\n\nEn este caso, KNN calcula las distancias del punto a predecir (en verde) a todos los otros puntos existentes (proceso caro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa predicci√≥n corresponder√° a la etiqueta mayoritaria por votacio≈Ñ\n\n\n\n\n\n\n\n\nLa predicci√≥n corresponder√° a la etiqueta mayoritaria por votacio≈Ñ.\n¬øCu√°l ser√≠a una buena estrategia de predicci√≥n para un modelo de Regresi√≥n?"
  },
  {
    "objectID": "tics411/clase-8.html#fronteras-de-decisi√≥n",
    "href": "tics411/clase-8.html#fronteras-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Fronteras de Decisi√≥n",
    "text": "Fronteras de Decisi√≥n\n\n\n\n\n\n\n\n\n\n\n\n\nImplicitamente, todo modelo de Machine Learning generar√° lo que se llama una Frontera de Decisi√≥n.\nSi un punto no visto cae dentro de su frontera entonces se le asigna dicha etiqueta."
  },
  {
    "objectID": "tics411/clase-8.html#implementaci√≥n-clasificaci√≥n-en-scikit-learn",
    "href": "tics411/clase-8.html#implementaci√≥n-clasificaci√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n Clasificaci√≥n en Scikit-Learn",
    "text": "Implementaci√≥n Clasificaci√≥n en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClasifier(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicci√≥n...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) n√∫mero de vecinos a utilizar. Por defecto 5.\nmetric: M√©trica de distancia. Por defecto ‚ÄúMinkowski‚Äù.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un par√°metro interno para poder paralelizar los c√°lculos. Se recomienda utilizar -1 para utilizar todos sus cores."
  },
  {
    "objectID": "tics411/clase-8.html#implementaci√≥n-regresi√≥n-en-scikit-learn",
    "href": "tics411/clase-8.html#implementaci√≥n-regresi√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n Regresi√≥n en Scikit-Learn",
    "text": "Implementaci√≥n Regresi√≥n en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn_clf = KNeighborsRegressor(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicci√≥n...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) n√∫mero de vecinos a utilizar. Por defecto 5.\nmetric: M√©trica de distancia. Por defecto ‚ÄúMinkowski‚Äù.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un par√°metro interno para poder paralelizar los c√°lculos. Se recomienda utilizar -1 para utilizar todos sus cores.\n\n\n\n\n\n\n\n\n¬øC√≥mo se encuentran las predicciones en un modelo de Regresi√≥n?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-detalles-t√©cnicos",
    "href": "tics411/clase-8.html#knn-detalles-t√©cnicos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "KNN: Detalles T√©cnicos",
    "text": "KNN: Detalles T√©cnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nModelo muy simple de implementar y entender.\nMuy eficiente en el aprendizaje.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nInferencia ineficiente: \\(O(mn^2)\\).\nCurse of Dimensionality: A medida que el n√∫mero de dimensiones del problema crece, se requiere un incremento exponencial en la cantidad de datos para asegurar que existen suficientes vecinos cercanos para cualquier punto."
  },
  {
    "objectID": "tics411/clase-10.html#√°rboles-de-decisi√≥n",
    "href": "tics411/clase-10.html#√°rboles-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årboles de Decisi√≥n",
    "text": "√Årboles de Decisi√≥n\n\nT√©cnica de clasificaci√≥n supervisada que genera una decisi√≥n basada en √°rboles de decisi√≥n para clasificar instancias no conocidas."
  },
  {
    "objectID": "tics411/clase-10.html#√°rboles-de-decisi√≥n-ejemplo",
    "href": "tics411/clase-10.html#√°rboles-de-decisi√≥n-ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årboles de Decisi√≥n: Ejemplo",
    "text": "√Årboles de Decisi√≥n: Ejemplo\n\nVisualmente, un √°rbol de decisi√≥n segmenta el espacio separando los datos en subgrupos.\n\n\n\n\n\n\n\nEsto permite la generacion de fronteras de decisi√≥n sumamente complejas.\n\n\n\nSupongamos el siguiente ejemplo:"
  },
  {
    "objectID": "tics411/clase-10.html#√°rboles-de-decisi√≥n-frontera-de-decisi√≥n",
    "href": "tics411/clase-10.html#√°rboles-de-decisi√≥n-frontera-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årboles de Decisi√≥n: Frontera de Decisi√≥n",
    "text": "√Årboles de Decisi√≥n: Frontera de Decisi√≥n"
  },
  {
    "objectID": "tics411/clase-10.html#√°rboles-de-decisi√≥n-frontera-de-decisi√≥n-1",
    "href": "tics411/clase-10.html#√°rboles-de-decisi√≥n-frontera-de-decisi√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årboles de Decisi√≥n: Frontera de Decisi√≥n",
    "text": "√Årboles de Decisi√≥n: Frontera de Decisi√≥n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øCu√°l ser√≠a el Nivel de Ajuste de un modelo de este tipo?"
  },
  {
    "objectID": "tics411/clase-10.html#√°rboles-de-decisi√≥n-inferencia",
    "href": "tics411/clase-10.html#√°rboles-de-decisi√≥n-inferencia",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årboles de Decisi√≥n: Inferencia",
    "text": "√Årboles de Decisi√≥n: Inferencia\nUna vez construido el √°rbol de decisi√≥n basta con recorrerlo para poder generar la predicci√≥n para una instancia dada:"
  },
  {
    "objectID": "tics411/clase-10.html#caracter√≠sticas-de-√°rboles",
    "href": "tics411/clase-10.html#caracter√≠sticas-de-√°rboles",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Caracter√≠sticas de √Årboles",
    "text": "Caracter√≠sticas de √Årboles\n\nPueden trabajar con valores discretos o continuos. Adem√°s pueden ser usados como modelos de Clasificaci√≥n o Regres√≠on.\nUna vez seleccionado un atributo no es posible devolverse (backtracking).\nDebido al poder de un √°rbol de Decisi√≥n la mayor√≠a de las veces tienden al Overfitting. Una forma de evitar esto es usar t√©cnicas de Pruning.\nEs preferible usar √°rboles cortos (Principio de Parsimonia o Occam's Razor).\n\n\n\n\n\n\n\nEl principio de Parsimonia recomienda encontrar soluciones a problemas utilizando la menor cantidad de elementos/par√°metros."
  },
  {
    "objectID": "tics411/clase-10.html#tipos-de-√°rboles-de-decisi√≥n",
    "href": "tics411/clase-10.html#tipos-de-√°rboles-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de √Årboles de Decisi√≥n",
    "text": "Tipos de √Årboles de Decisi√≥n\n\n\n\n\n\nBinary Split\n\n\n\n\n\n\nMulti-way Split\n\n\n\n\n\n\n\n\nHunt‚Äôs Algorithm \\(\\implies\\) Primer M√©todo.\nID3 \\(\\implies\\) S√≥lo utiliza variables categ√≥ricas.\nC4.5 \\(\\implies\\) incluye variables continuas.\nC5.0 \\(\\implies\\) Permite separaci√≥n en M√∫ltiples Splits (No ha sido implementado en Sklearn).\nCART (Classification and Regression Trees) \\(\\implies\\) Permite que el output sea continuo pero solo utilizando Splits binarios.\n\n\n\n\n\n\n\n\n\n\nLos CARTs son por lejos los √°rboles m√°s utilizados en las librer√≠as m√°s famosas y potentes: Scikit-Learn, XGboost, LightGBM, Catboost."
  },
  {
    "objectID": "tics411/clase-10.html#creaci√≥n-de-un-√°rbol-de-decisi√≥n",
    "href": "tics411/clase-10.html#creaci√≥n-de-un-√°rbol-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Creaci√≥n de un √Årbol de Decisi√≥n",
    "text": "Creaci√≥n de un √Årbol de Decisi√≥n\n\nPureza\n\nCorresponde a la probabilidad de no sacar dos registros de un Nodo que pertenezcan a la misma clase.\n\n\n\n\n\n\n\n\nEl √°rbol de Decisi√≥n busca crear Nodos lo m√°s puro posible. Para ello puede utilizar las siguentes m√©tricas:\n\n\n\n\n\n√çndice Gini\n\\[Gini(X) = 1 - \\sum_{x_i}p(x_i)^2\\]\n\nEntrop√≠a\n\\[H(X) = -\\sum_{x_i}p(x_i)log_2p(x_i)\\]\n\n\n\n\n\n\n\n\nA mayor valor, mayor nivel de impureza. 0 implica Nodo completamente puro."
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ejemplo",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ejemplo",
    "text": "√Årbol de Decisi√≥n: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC√°lculo de Impureza en Hoja\n\n\n\\[Gini_{(leaf)} = 1 - p(Yes)^2 - p(No)^2\\]\n\n\n\n\n\n\n\n\n\nC√°lculo de Impureza en Split\n\n\n\\[ Gini_{(split)} = \\frac{n_{(yes)}}{n} Gini_{(yes)} + \\frac{n_{(no)}}{n} Gini_{(no)}\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-popcorn",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-popcorn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Popcorn",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Popcorn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{4}\\right)^2 - \\left(\\frac{3}{4}\\right)^2 = 0.375\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.375 + \\frac{3}{7} \\cdot 0.444 = 0.405\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-soda",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-soda",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Soda",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Soda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{4}\\right)^2 - \\left(\\frac{1}{4}\\right)^2 = 0.375\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{3}\\right)^2 - \\left(\\frac{3}{3}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.375 = 0.214\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Age",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5\\]\n\n\n\n\\[Gini_{(split)} = \\frac{6}{7}\\cdot 0.5 = 0.429\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-1",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Age",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{2}\\right)^2 - \\left(\\frac{2}{2}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{5}\\right)^2 - \\left(\\frac{2}{5}\\right)^2 = 0.48\\]\n\n\n\n\\[Gini_{(split)} = \\frac{5}{7}\\cdot 0.48 = 0.343\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-2",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Age",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{4}\\right)^2 - \\left(\\frac{2}{4}\\right)^2 = 0.5\\]\n\n\n\n\\[Gini_{(split)} = \\frac{3}{7}\\cdot 0.444 + \\frac{4}{7} \\cdot 0.5 = 0.476\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-3",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Age",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{2}{4}\\right)^2 - \\left(\\frac{2}{4}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.5 + \\frac{3}{7} \\cdot 0.444 = 0.476\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-4",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-4",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Age",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{2}\\right)^2 - \\left(\\frac{2}{5}\\right)^2 = 0.48\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{2}\\right)^2 - \\left(\\frac{2}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{5}{7}\\cdot 0.48 = 0.343\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-5",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-ra√≠z-age-5",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: Ra√≠z Age",
    "text": "√Årbol de Decisi√≥n: Ra√≠z Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{6}{7}\\cdot 0.5 = 0.429\\]"
  },
  {
    "objectID": "tics411/clase-10.html#qu√©-split-elegiremos",
    "href": "tics411/clase-10.html#qu√©-split-elegiremos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øQu√© Split elegiremos?",
    "text": "¬øQu√© Split elegiremos?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEscogeremos el Split m√°s peque√±o que representa el que genera m√°s pureza.\n\n\n\n\n\n\n\n\n\n\n\nEl nodo que no le gusta la Soda qued√≥ completamente puro. Por lo tanto, no puede seguir dividi√©ndose. Seguiremos trabajando s√≥lo con aquellos que s√≠ les gusta la Soda."
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: 2do Nivel",
    "text": "√Årbol de Decisi√≥n: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{2}\\right)^2 - \\left(\\frac{0}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{2}{4}\\cdot 0.5 = 0.25\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel-1",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: 2do Nivel",
    "text": "√Årbol de Decisi√≥n: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{3}\\right)^2 - \\left(\\frac{0}{3}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = 0\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel-2",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: 2do Nivel",
    "text": "√Årbol de Decisi√≥n: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{2}\\right)^2 - \\left(\\frac{0}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{2}{4} \\cdot 0.5 = 0.25\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel-3",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n-2do-nivel-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n: 2do Nivel",
    "text": "√Årbol de Decisi√≥n: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\\] \\[Gini_{(no)} = 1 - \\left(\\frac{1}{1}\\right)^2 - \\left(\\frac{0}{1}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{3}{4} \\cdot 0.444 = 0.333\\]"
  },
  {
    "objectID": "tics411/clase-10.html#√°rbol-de-decisi√≥n",
    "href": "tics411/clase-10.html#√°rbol-de-decisi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "√Årbol de Decisi√≥n",
    "text": "√Årbol de Decisi√≥n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øCu√°l ser√≠a la predicci√≥n?"
  },
  {
    "objectID": "tics411/clase-10.html#crecimiento-de-un-√°rbol",
    "href": "tics411/clase-10.html#crecimiento-de-un-√°rbol",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Crecimiento de un √Årbol",
    "text": "Crecimiento de un √Årbol\n\nUn √°rbol s√≥lo dejar√° de crecer si:\n\nNo hay m√°s puntos a separar.\n\nTodas las muestras de un nodo pertenecen a la misma clase.\n\nNo hay m√°s variables a separar.\n\n\n\n\n\n\n\n\n\nEsto normalmente termina en Overfitting.\n\n\n\n\n\n\n\n\n\n\nPara solucionar esto se aplica regularizaci√≥n. En el caso de √Årboles esto se denomina prunning."
  },
  {
    "objectID": "tics411/clase-10.html#pruning",
    "href": "tics411/clase-10.html#pruning",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Pruning",
    "text": "Pruning\nPrepruning: Define/Evita que el √°rbol crezca hasta:\n\nUn cierto nivel o n√∫mero de hojas.\nAplicar un test estad√≠stico (normalmente un proceso muy costoso).\nUsar medidas de complejidad para penalizar √°rboles de gran tama√±o.\n\nPostpruning: Decide eliminar nodos, luego de que el √°rbol crezca.\n\nUsar un par√°metro de Costo de Impureza."
  },
  {
    "objectID": "tics411/clase-10.html#hiperpar√°metros",
    "href": "tics411/clase-10.html#hiperpar√°metros",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Hiperpar√°metros",
    "text": "Hiperpar√°metros\n\n\n\n\n\n\n\ncriterion: Elegir bajo qu√© criterio se mide la impureza.\nmax_depth: El nivel es la altura que tendr√° el √°rbol. Niveles m√°s bajos generan √°rboles m√°s simples.\nmin_samples_split: N√∫mero de instancias necesarias para generar un split. Un mayor n√∫mero o proporci√≥n generar√° √°rboles m√°s simples.\nmin_samples_leaf: N√∫mero m√≠nimo de instancias necesarias para que un nodo sea hoja. Un n√∫mero o proporci√≥n m√°s alta generar√° √°rboles m√°s simples.\nccp_alpha: Est√° asociado a la pureza total del √°rbol. Para m√°s informaci√≥n ver ac√°.\n\n\n\n\n\n\n\n\n\n\n¬øC√≥mo se ve la complejidad/simplicidad en un √°rbol de Decisi√≥n?"
  },
  {
    "objectID": "tics411/clase-10.html#implementaci√≥n-en-scikit-learn",
    "href": "tics411/clase-10.html#implementaci√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Scikit-Learn",
    "text": "Implementaci√≥n en Scikit-Learn\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndt = DecisionTreeClassifier(criterion=\"gini\", max_depth=None, min_sample_split=2, \n                            min_samples_leaf=1,min_impurity_decrease=0, \n                            ccp_alpha=0, random_state=42)\ndt.fit(X_train, y_train)\n\ny_pred = dt.predict(X_test)\ny_proba = dt.predict_proba(X_test)\n\n## Permite Visualizar el √Årbol de Decisi√≥n\nplt_tree(dt, filled = True, feature_names=None, class_names=None)\n\n\ncriterion: Puede ser gini o entrop√≠a. Por defecto \"gini\".\nmax_depth: N√∫mero de niveles que se permita que crezca el nivel, por defecto None, significa todos los que pueda.\nmin_samples_split: El n√∫mero m√≠nimo de elementos dentro de un nodo para permitir el split. Por defecto 2.\nmin_samples_leaf: El n√∫mero m√≠nimo de elementos para que un nodo pueda ser considerado hoja. Por defecto 1.\nmin_impurity_decreased: Decrecimiento m√≠nimo de la impureza. Si no se cumple, no hay Split. Por defecto 0.\nccp_alpha: Par√°metro de Post-Pruning. Valores m√°s altos genera la poda de m√°s nodos."
  },
  {
    "objectID": "tics411/clase-2.html#eda",
    "href": "tics411/clase-2.html#eda",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "EDA",
    "text": "EDA\n\nEl Analisis Exploratorio de Datos (EDA, por sus siglas en ingl√©s) es procedimiento en el cual se analiza un dataset para explorar sus caracter√≠sticas principales.\n\n\nSu objetivo principal es poder familiarizarse con los datos adem√°s de encontrar potenciales problemas en su calidad.\nPrincipalmente hace uso de t√©cnicas de manipulaci√≥n de datos y visualizaciones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos hallazgos importantes dentro del proceso se les denomina insights.\n\n\n\n\n\n\n\n\n\nEl uso de visualizaciones inadecuadas podr√≠a llevar a conclusiones err√≥neas.\n\n\n\n\n\n\n\n\n\n\nSummary.\nVisualizaci√≥n."
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-tendencia-central",
    "href": "tics411/clase-2.html#medidas-de-tendencia-central",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas de Tendencia Central",
    "text": "Medidas de Tendencia Central"
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-dispersi√≥n-y-asimetr√≠a",
    "href": "tics411/clase-2.html#medidas-de-dispersi√≥n-y-asimetr√≠a",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas de Dispersi√≥n y Asimetr√≠a",
    "text": "Medidas de Dispersi√≥n y Asimetr√≠a"
  },
  {
    "objectID": "tics411/clase-2.html#eda-visualizaci√≥n",
    "href": "tics411/clase-2.html#eda-visualizaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "EDA: Visualizaci√≥n",
    "text": "EDA: Visualizaci√≥n\n\nLa visualizaci√≥n de datos es la presentaci√≥n de datos en forma gr√°fica. Permite simplificar conceptos m√°s complejos en especial a altos mandos.\n\n\nGracias a la evoluci√≥n del cerebro humano somos capaces de detectar patrones complejos en la naturaleza a partir de la Visi√≥n.\n\n\n\n\n\n\n\n\nPuede ser dif√≠cil de aplicar si el tama√±o de los datos es grande (sea en instancias o atributos). Por ejemplo, si los datos est√°n en 4 dimensiones.\n\n\n\n\n\n\n\n\n\n\n\n\nSe suelen resumir los datos en estad√≠sticas simples.\nGraficar datos en 1D, 2D y 3D (evitar dentro de lo posible).\nLa visualizaci√≥n debe ser comprensible ojal√° sin ninguna explicaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\nEn caso de datos de alta dimensionalidad puede ser una buena idea reducir dimensiones mediante t√©cnicas como:\n\nPCA\nUMAP\netc."
  },
  {
    "objectID": "tics411/clase-2.html#caso-de-visualizaci√≥n",
    "href": "tics411/clase-2.html#caso-de-visualizaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Caso de Visualizaci√≥n",
    "text": "Caso de Visualizaci√≥n\n\n\n\n\n\n\n\nFiguras\nEscala de Colores.\nTama√±o de los puntos.\nDemasiada informaci√≥n en un s√≥lo gr√°fico.\nNo se entiende el mensaje."
  },
  {
    "objectID": "tics411/clase-2.html#canales-visuales",
    "href": "tics411/clase-2.html#canales-visuales",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Canales Visuales",
    "text": "Canales Visuales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe les llama canales visuales a elementos visuales que pueden utilizarse para expresar informaci√≥n (Clase Visualizacion Andreas Mueller).\nLa idea es poder mapear cada uno de estos canales a valores que queremos visualizar.\n\n\n\n\n\n\n\n\n\n\n\nNo todos los canales son igual de √∫tiles ni f√°ciles de entender."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nHistograma\n\n\nEl histograma permite visualizar distribuciones univariadas acumulando los datos en rangos de igual tama√±o (bins).\n\n\n\n\nPermite visualizar el centro, la extensi√≥n, la asimetr√≠a y outliers.\n\n\n\n\n\n\n\n\nEl histograma puede ser ‚Äúenga√±oso‚Äù para conjuntos de datos peque√±os.\nLa visualizaci√≥n puede resultar de manera muy distintas dependiendo del n√∫mero de bins."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nKernel Density\n\n\nCorresponde a un suavizamiento de un Histograma en el cu√°l se usa un Kernel (funci√≥n no negativa que suma 1 y tiene media 0) para agrupar los puntos vecinos.\n\n\n\n\n\nLa funci√≥n estimada es:\n\\[f(x) = \\frac{1}{n} = \\sum_{i=1}^n K \\left(\\frac{x - x(i)}{h}\\right)\\]\n\n\\(K(u)\\) es el Kernel.\n\\(h\\) es el ancho de banda."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nBoxplot (Caja y Bigotes)\n\nEs un tipo de gr√°fico que muestra la distribuci√≥n de manera univariada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiene la capacidad de mostrar varias distribuciones a la vez.\nAdem√°s presenta estad√≠sticos de inter√©s: Mediana, IQR y outliers.\nLos puntos fuera de los bigotes son considerados Outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos bigotes pueden representar:\n\nM√≠nimo y M√°ximo. (En este caso no hay outliers).\n\\(\\mu \\pm 3\\sigma\\)\nPercentiles 5 y 95.\nOtros valores."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-barras",
    "href": "tics411/clase-2.html#visualizaciones-barras",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visualizaciones: Barras",
    "text": "Visualizaciones: Barras\n\nBar Plot\n\n\nLa altura de la barra (normalmente Eje y) representa una agregaci√≥n asociada a una categor√≠a (normalmente Eje x).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOtras convenciones llaman a este gr√°fico Column Plot, mientras que el Bar Plot tiene las barras de manera horizontal."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-puntos",
    "href": "tics411/clase-2.html#visualizaciones-puntos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visualizaciones: Puntos",
    "text": "Visualizaciones: Puntos\n\nScatter\n\n\nGr√°fico empleado para mostrar distribuci√≥n de datos bivariados\n\n\n\n\nMuestra la relaci√≥n entre una variable independiente (Eje X) y una variable dependiente (Eje Y).\nPermite mostrar relaciones lineales o no-lineales (Correlaciones).\nOutliers.\nSimplemente ubicaci√≥n de Puntos en el Espacio."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-l√≠neas",
    "href": "tics411/clase-2.html#visualizaciones-l√≠neas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visualizaciones: L√≠neas",
    "text": "Visualizaciones: L√≠neas\n\nLineplot\n\n\nGr√°fico empleado para visualizar tendencias y su evoluci√≥n de una medida (Eje Y) en el tiempo (Eje X).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi bien es posible utilizarlo para gr√°ficar dos medidas continuas, las buenas pr√°cticas indican que el eje X siempre deber√≠a contener una componente temporal."
  },
  {
    "objectID": "tics411/clase-2.html#estad√≠sticos-vs-visualizaciones",
    "href": "tics411/clase-2.html#estad√≠sticos-vs-visualizaciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Estad√≠sticos vs Visualizaciones",
    "text": "Estad√≠sticos vs Visualizaciones"
  },
  {
    "objectID": "tics411/clase-2.html#otras-visualizaciones",
    "href": "tics411/clase-2.html#otras-visualizaciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øOtras Visualizaciones?",
    "text": "¬øOtras Visualizaciones?"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html",
    "href": "tics411/notebooks/01-Preprocesamiento.html",
    "title": "Clases UAI",
    "section": "",
    "text": "%%capture\n## Ejecutar esta celda para instalar o actualizar Feature_Engine\n!pip install -U feature_engine\n## Chequear que la versi√≥n de Feature Engine sea al menos 1.7\nimport feature_engine\n\nfeature_engine.__version__\n\n'1.7.0'\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import set_config\n\n## (Opcional) Este comando permite que el output de Scikit-Learn sean Pandas DataFrames.\n## Por dejecto, Scikit-Learn transforma todo a Numpy, ya que es m√°s eficiente computacionalmente.\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "href": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "title": "Clases UAI",
    "section": "Valores Faltantes",
    "text": "Valores Faltantes\n\n## Para detectar valores faltantes se utiliza el siguiente comando.\ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n## Opcionalmente se puede obtener el % o la fracci√≥n de nulos utilizando la siguiente variante.\ndf.isnull().mean()\n\nsurvived       0.000000\npclass         0.000000\nsex            0.000000\nage            0.198653\nsibsp          0.000000\nparch          0.000000\nfare           0.000000\nembarked       0.002245\nclass          0.000000\nwho            0.000000\nadult_male     0.000000\ndeck           0.772166\nembark_town    0.002245\nalive          0.000000\nalone          0.000000\ndtype: float64\n\n\nPandas: Es posible imputar valores usando Pandas con el comando .fillna().\n\nmedia = df[\"age\"].mean()\nmediana = df[\"age\"].median()\nprint(f\"Promedio de Edad: {media}\")\nprint(\n    f'Promedio de Edad con Imputaci√≥n con Ceros: {df[\"age\"].fillna(0).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputaci√≥n por Media: {df[\"age\"].fillna(media).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputaci√≥n por Mediana: {df[\"age\"].fillna(mediana).mean()}'\n)\n\nPromedio de Edad: 29.69911764705882\nPromedio de Edad con Imputaci√≥n con Ceros: 23.79929292929293\nPromedio de Edad con Imputaci√≥n por Media: 29.69911764705882\nPromedio de Edad con Imputaci√≥n por Mediana: 29.36158249158249\n\n\nScikit-Learn: Utiliza la clase SimpleImputer, el cual permite distintas estrategias de Imputaci√≥n: \"mean\", \"median\", \"most_frequent\", \"constant\".\n\nfrom sklearn.impute import SimpleImputer\n\nsc = SimpleImputer(strategy=\"mean\")\n## En este caso uso [[]] ya que Scikit Learn espera Matrices o DataFrames.\n## Utilizar [[]] fuerza a que AGE sea un DataFrame de una Columna y no una Serie.\n\ndata_imputed = sc.fit_transform(df[[\"age\"]])\n## Se puede ver que los nuevos datos ya no poseen valores Perdidos.\ndata_imputed.isnull().sum()\n\nage    0\ndtype: int64"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "href": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "title": "Clases UAI",
    "section": "Outliers",
    "text": "Outliers\npandas: En Pandas se pueden acotar los outliers utilizando .clip()\n\nprint(f\"Promedio de Tarifas: {df.fare.mean()}\")\ndf[\"fare\"].agg([\"min\", \"max\"])\n\nPromedio de Tarifas: 32.204207968574636\n\n\nmin      0.0000\nmax    512.3292\nName: fare, dtype: float64\n\n\n\nlower: Define la cota inferior.\nupper: Define la cota superior.\n\n\nclipped_data = df[[\"fare\"]].clip(lower=10, upper=50)\nclipped_data.agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n10.0\n\n\nmax\n50.0\n\n\n\n\n\n\n\n\n\ndf[[\"fare\"]]\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows √ó 1 columns\n\n\n\n\n\n## Los valores menores a 10 fueron reemplazados por 10.\n## Los valores mayores a 50 fueron reemplazados por 50.\nclipped_data\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows √ó 1 columns\n\n\n\n\nsklearn: Para este caso nos apoyaremos de la librer√≠a feature_engine la cual posee herramientas para acotar. feature_engine sigue exactamente la misma l√≥gica de Scikit-Learn.\n\nfrom feature_engine.outliers import ArbitraryOutlierCapper, Winsorizer\n\ncapper = ArbitraryOutlierCapper(\n    max_capping_dict=dict(fare=50), min_capping_dict=dict(fare=10)\n)\ncapper.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows √ó 1 columns\n\n\n\n\n\ncapping_method: Define la Estragegia a utilizar para el Winsorizer. Ver Docs.\n\n\n## \"gaussian\" permite acotar por mu +/- 3*std\n## \"iqr\" permite rellenar por Q1 - 3*iqr y Q3 + 3*iqr\nwin = Winsorizer(capping_method=\"gaussian\")\nwin.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows √ó 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#variables-categ√≥ricas",
    "href": "tics411/notebooks/01-Preprocesamiento.html#variables-categ√≥ricas",
    "title": "Clases UAI",
    "section": "Variables Categ√≥ricas",
    "text": "Variables Categ√≥ricas\npandas:\n\nOne Hot Encoding\nPara la conversi√≥n de variables categ√≥ricas utilizamos pd.get_dummies(). * drop_first: Si es True se elimina la primera categor√≠a.\n\npd.get_dummies(df[\"embark_town\"], drop_first=False)\n\n\n\n\n\n\n\n\n\nCherbourg\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nFalse\nTrue\n\n\n1\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n\n\n886\nFalse\nFalse\nTrue\n\n\n887\nFalse\nFalse\nTrue\n\n\n888\nFalse\nFalse\nTrue\n\n\n889\nTrue\nFalse\nFalse\n\n\n890\nFalse\nTrue\nFalse\n\n\n\n\n891 rows √ó 3 columns\n\n\n\n\n\npd.get_dummies(df[\"embark_town\"], drop_first=True)\n# Una ventaja de este procedimiento es que no considera los Nulos como otra categor√≠a...\n\n\n\n\n\n\n\n\n\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nTrue\n\n\n1\nFalse\nFalse\n\n\n2\nFalse\nTrue\n\n\n3\nFalse\nTrue\n\n\n4\nFalse\nTrue\n\n\n...\n...\n...\n\n\n886\nFalse\nTrue\n\n\n887\nFalse\nTrue\n\n\n888\nFalse\nTrue\n\n\n889\nFalse\nFalse\n\n\n890\nTrue\nFalse\n\n\n\n\n891 rows √ó 2 columns\n\n\n\n\n\n\nOrdinal Encoder\nSe utiliza pd.factorize(). * sort: Usar True ya que coloca las categor√≠as en orden. Adem√°s de esta manera se comporta igual que OrdinalEncoder de Scikit-Learn.\n\npd.DataFrame(\n    pd.factorize(df[\"embark_town\"], sort=True)[0], columns=[\"new_column\"]\n)\n\n\n\n\n\n\n\n\n\nnew_column\n\n\n\n\n0\n2\n\n\n1\n0\n\n\n2\n2\n\n\n3\n2\n\n\n4\n2\n\n\n...\n...\n\n\n886\n2\n\n\n887\n2\n\n\n888\n2\n\n\n889\n0\n\n\n890\n1\n\n\n\n\n891 rows √ó 1 columns\n\n\n\n\nScikit-Learn:\n\n\nOne Hot Encoding\n\nsparse_output: Se debe fijar como False para poder ver el output como Pandas\ndrop: Se debe colocar \"first\" o el nombre de una s√≥la categor√≠a a eliminar.\n\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nohe = OneHotEncoder(drop=\"first\", sparse_output=False)\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\n\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n\n\n890\n1.0\n0.0\n0.0\n\n\n\n\n891 rows √ó 3 columns\n\n\n\n\n\n\nOrdinal Encoder\n\nohe = OneHotEncoder(\n    drop=[\"Queenstown\"], sparse_output=False\n)  # Tambi√©n se puede colocar np.nan.\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\n\nembark_town_Cherbourg\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n1.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n\n\n\n\n891 rows √ó 3 columns\n\n\n\n\n\noe = OrdinalEncoder()\noe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\n2.0\n\n\n1\n0.0\n\n\n2\n2.0\n\n\n3\n2.0\n\n\n4\n2.0\n\n\n...\n...\n\n\n886\n2.0\n\n\n887\n2.0\n\n\n888\n2.0\n\n\n889\n0.0\n\n\n890\n1.0\n\n\n\n\n891 rows √ó 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "href": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "title": "Clases UAI",
    "section": "Escalamiento",
    "text": "Escalamiento\nEl escalamiento normalmente se realiza s√≥lo en Scikit-Learn. Se mostrar√°n la Estandarizaci√≥n y Normalizaci√≥n.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n## Llamaremos esto Estandarizaci√≥n... (s√≥lo por convenci√≥n del curso)\nsc = StandardScaler()\ndata = sc.fit_transform(df[[\"fare\"]])\ndata.agg([\"mean\", \"std\"])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmean\n3.987333e-18\n\n\nstd\n1.000562e+00\n\n\n\n\n\n\n\n\n\n## Llamaremos esto Normalizaci√≥n... (s√≥lo por convenci√≥n del curso)\nmms = MinMaxScaler()\nmms.fit_transform(df[[\"fare\"]]).agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n0.0\n\n\nmax\n1.0"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-s√≥lo-a-algunas-variables.",
    "href": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-s√≥lo-a-algunas-variables.",
    "title": "Clases UAI",
    "section": "Aplicar Preprocesamientos s√≥lo a algunas variables.",
    "text": "Aplicar Preprocesamientos s√≥lo a algunas variables.\nScikit-Learn fue dise√±ado para el entrenamiento eficiente de modelos. Para ello, se bas√≥ en Numpy, el cu√°l no cuenta con nombre de columnas, por lo que para poder aplicar pre-procesamientos a ciertas partes del Dataset utiliza lo que se llama el ColumnTransformer(), el cu√°l va m√°s all√° del alcance del curso.\nPara simplificar el proceso de elegir ciertas columnas, feature_engine posee una el SklearnTransformerWrapper que permite elegir qu√© variables queremos pasar por cierta transformaci√≥n.\n\n## Sin SklearnTransformerWrapper\n\nohe = OneHotEncoder(sparse_output=False)\nohe.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea columnas dummies incluso para las variables num√©ricas.\n\n\n\n\n\n\n\n\n\nage_0.42\nage_0.67\nage_0.75\nage_0.83\nage_0.92\nage_1.0\nage_2.0\nage_3.0\nage_4.0\nage_5.0\n...\nage_70.0\nage_70.5\nage_71.0\nage_74.0\nage_80.0\nage_nan\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows √ó 93 columns\n\n\n\n\n\n## Aplicar preprocesamientos a ciertas variables...\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nohe_w = SklearnTransformerWrapper(\n    OneHotEncoder(sparse_output=False), variables=\"embark_town\"\n)\nohe_w.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea dummies s√≥lo para la variable embark_town y deja age como estaba.\n\n\n\n\n\n\n\n\n\nage\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n22.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n38.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\n27.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n19.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\nNaN\n0.0\n0.0\n1.0\n0.0\n\n\n889\n26.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n32.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows √ó 5 columns"
  },
  {
    "objectID": "tics411/notebooks/distancia.html",
    "href": "tics411/notebooks/distancia.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(dict(x=[0, 2, 3, 5], y=[2, 0, 1, 1]))\ndf.index = [1, 2, 3, 4]\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n1\n0\n2\n\n\n2\n2\n0\n\n\n3\n3\n1\n\n\n4\n5\n1\n\n\n\n\n\n\n\n\n\nnp.zeros((4, 4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\ndef distancia_l1(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n\n    return np.abs(x1 - x2) + np.abs(y1 - y2)\n\n\ndef distancia_l2(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n\n\ndef distancia_linf(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    d_x = np.abs(x1 - x2)\n    d_y = np.abs(y1 - y2)\n    return np.max([d_x, d_y])\n\n\ndef calculate_matrix(distance, n_puntos):\n    m = np.zeros((n_puntos, n_puntos))\n    for i in range(n_puntos):\n        for j in range(n_puntos):\n            p = df.iloc[i]\n            q = df.iloc[j]\n            m[i, j] = distance(p, q)\n\n    return m\n\n\nm_m = calculate_matrix(distancia_l1, 4)\nm_e = calculate_matrix(distancia_l2, 4)\nm_c = calculate_matrix(distancia_linf, 4)\n\n\nm_m\n\narray([[0., 4., 4., 6.],\n       [4., 0., 2., 4.],\n       [4., 2., 0., 2.],\n       [6., 4., 2., 0.]])\n\n\n\nm_e\n\narray([[0.        , 2.82842712, 3.16227766, 5.09901951],\n       [2.82842712, 0.        , 1.41421356, 3.16227766],\n       [3.16227766, 1.41421356, 0.        , 2.        ],\n       [5.09901951, 3.16227766, 2.        , 0.        ]])\n\n\n\nm_c\n\narray([[0., 2., 3., 5.],\n       [2., 0., 1., 3.],\n       [3., 1., 0., 2.],\n       [5., 3., 2., 0.]])\n\n\n\n\na = pd.Series(\n    [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0]\n)\nb = pd.Series(\n    [1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0]\n)\n\na.var(ddof=0)  # Varianza Poblacional\n\n3.5\n\n\n\na.var(ddof=1)  # Varianza Muestral\n\n3.7916666666666665\n\n\n\nb.var(ddof=1)\n\n1.5916666666666668\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/hopkins.html",
    "href": "tics411/notebooks/hopkins.html",
    "title": "Ejemplos de Hopkins",
    "section": "",
    "text": "from pyclustertend import hopkins\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef blobs_examples(\n    n_centers, cluster_std, n_samples=4000, p=100, center_box=(-10, 10)\n):\n    df_spread, labels = make_blobs(\n        n_samples=n_samples,\n        centers=n_centers,\n        n_features=2,\n        random_state=42,\n        center_box=center_box,\n        cluster_std=cluster_std,\n    )\n    df_spread = pd.DataFrame(df_spread, columns=[\"x\", \"y\"])\n    plt.scatter(df_spread.x, df_spread.y, c=labels)\n    plt.title(f\"H = {1-hopkins(df_spread, p)}\")\n    plt.tight_layout()\n\n\n## Una sola nube, muy compacta...\nblobs_examples(n_centers=1, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchos nubes muy compactos...\nblobs_examples(n_centers=5, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchas nubes extremadamente compactos\nblobs_examples(n_centers=5, cluster_std=0.001, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso utilizamos la funci√≥n make_blobs para simular clusters ficticios. Los clusters siempre son esf√©ricos, es por eso que el Hopkins tiende a dar valores bastante buenos. Aunque, dependiendo de qu√© tan compacto sea la nube tiende a 1 de manera muy fuerte.\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\n\ndef random_examples(n_samples=4000, p=100):\n    df_random = pd.DataFrame(\n        np.random.rand(n_samples, 2), columns=[\"x\", \"y\"]\n    )\n    plt.scatter(df_random.x, df_random.y)\n    plt.title(f\"H = {1-hopkins(df_random, p)}\")\n    plt.tight_layout()\n\n\n## Puntos m√°s dispersos\nrandom_examples(n_samples=100, p=10)\n\n\n\n\n\n\n\n\n\n## M√°s denso, pero a√∫n aleatorio...\nrandom_examples(n_samples=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso estamos usando np.random.rand para simular s√≥lo valores aleatorios. Se puede ver que entre m√°s lleno est√° el espacio, Hopkins tiende a 0.5.\n\n\n## valores uniformemente distribuidos y con poca tendencia a agruparse (normalmente pocos puntos)\n## tienen H m√°s peque√±os, pero es d√≠ficil obtenerlos...\nnp.random.seed(0)\n\n\ndef uniform_example(n_samples=10, max_val=10, p=10):\n    df_uniform = pd.DataFrame(\n        dict(\n            x=np.random.randint(0, max_val + 1, size=n_samples),\n            y=np.random.randint(0, max_val, size=n_samples),\n        )\n    )\n\n    plt.scatter(df_uniform.x, df_uniform.y)\n    plt.title(f\"H = {1-hopkins(df_uniform, p)}\")\n\n\nuniform_example(n_samples=11, max_val=10, p=10)\n\n\n\n\n\n\n\n\n\nuniform_example(n_samples=4000, max_val=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso tambi√©n estamos forzando aleatoriedad pero con uniformidad de distancia. Para eso simulamos usando np.random.randint para generar valores aleatorios pero m√°s o menos equiespaciados uniformemente. Es bien interesante este caso, porque si usamos muchos datos, se tiende a valores completamente aleatorios, es decir H \\(\\sim\\) 0.5."
  },
  {
    "objectID": "tics411/notebooks/hopkins.html#ejemplos-de-hopkins",
    "href": "tics411/notebooks/hopkins.html#ejemplos-de-hopkins",
    "title": "Ejemplos de Hopkins",
    "section": "",
    "text": "from pyclustertend import hopkins\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef blobs_examples(\n    n_centers, cluster_std, n_samples=4000, p=100, center_box=(-10, 10)\n):\n    df_spread, labels = make_blobs(\n        n_samples=n_samples,\n        centers=n_centers,\n        n_features=2,\n        random_state=42,\n        center_box=center_box,\n        cluster_std=cluster_std,\n    )\n    df_spread = pd.DataFrame(df_spread, columns=[\"x\", \"y\"])\n    plt.scatter(df_spread.x, df_spread.y, c=labels)\n    plt.title(f\"H = {1-hopkins(df_spread, p)}\")\n    plt.tight_layout()\n\n\n## Una sola nube, muy compacta...\nblobs_examples(n_centers=1, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchos nubes muy compactos...\nblobs_examples(n_centers=5, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchas nubes extremadamente compactos\nblobs_examples(n_centers=5, cluster_std=0.001, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso utilizamos la funci√≥n make_blobs para simular clusters ficticios. Los clusters siempre son esf√©ricos, es por eso que el Hopkins tiende a dar valores bastante buenos. Aunque, dependiendo de qu√© tan compacto sea la nube tiende a 1 de manera muy fuerte.\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\n\ndef random_examples(n_samples=4000, p=100):\n    df_random = pd.DataFrame(\n        np.random.rand(n_samples, 2), columns=[\"x\", \"y\"]\n    )\n    plt.scatter(df_random.x, df_random.y)\n    plt.title(f\"H = {1-hopkins(df_random, p)}\")\n    plt.tight_layout()\n\n\n## Puntos m√°s dispersos\nrandom_examples(n_samples=100, p=10)\n\n\n\n\n\n\n\n\n\n## M√°s denso, pero a√∫n aleatorio...\nrandom_examples(n_samples=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso estamos usando np.random.rand para simular s√≥lo valores aleatorios. Se puede ver que entre m√°s lleno est√° el espacio, Hopkins tiende a 0.5.\n\n\n## valores uniformemente distribuidos y con poca tendencia a agruparse (normalmente pocos puntos)\n## tienen H m√°s peque√±os, pero es d√≠ficil obtenerlos...\nnp.random.seed(0)\n\n\ndef uniform_example(n_samples=10, max_val=10, p=10):\n    df_uniform = pd.DataFrame(\n        dict(\n            x=np.random.randint(0, max_val + 1, size=n_samples),\n            y=np.random.randint(0, max_val, size=n_samples),\n        )\n    )\n\n    plt.scatter(df_uniform.x, df_uniform.y)\n    plt.title(f\"H = {1-hopkins(df_uniform, p)}\")\n\n\nuniform_example(n_samples=11, max_val=10, p=10)\n\n\n\n\n\n\n\n\n\nuniform_example(n_samples=4000, max_val=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso tambi√©n estamos forzando aleatoriedad pero con uniformidad de distancia. Para eso simulamos usando np.random.randint para generar valores aleatorios pero m√°s o menos equiespaciados uniformemente. Es bien interesante este caso, porque si usamos muchos datos, se tiende a valores completamente aleatorios, es decir H \\(\\sim\\) 0.5."
  },
  {
    "objectID": "tics411/notebooks/Ejercicio-Matriz-Confusi√≥n.html",
    "href": "tics411/notebooks/Ejercicio-Matriz-Confusi√≥n.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\n\ndf = pd.DataFrame(\n    dict(\n        y=[1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2],\n        y_pred=[1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 2],\n    )\n)\n\nprint(classification_report(df.y, df.y_pred, digits=3))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0      0.500     0.333     0.400         3\n           1      0.800     0.800     0.800         5\n           2      0.500     0.667     0.571         3\n\n    accuracy                          0.636        11\n   macro avg      0.600     0.600     0.590        11\nweighted avg      0.636     0.636     0.629        11\n\n\n\n\n\n\n\n\n\n\n\nEs importante mencionar que para el caso de m√°s de 2 clases, el Accuracy se calcula como:\n\n\\[Accuracy = \\frac{TP_{Clase\\,0} + TP_{Clase\\,1} + TP_{Clase\\,2}}{Total} = \\frac{1 + 4 + 2}{11} = \\frac{7}{11} = 0.636\\]\nEl accuracy es un s√≥lo valor para todas las clases. Y en el caso que mostramos ayer, el concepto de TN se pierde, por lo que es necesario reducir la matriz a un problema binario para poder poder darle sentido.\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/jerarquico.html",
    "href": "tics411/notebooks/jerarquico.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        alpha=[9, 10, 1, 6, 1],\n        beta=[3, 2, 9, 5, 10],\n        gamma=[7, 9, 4, 5, 3],\n    )\n)\n\nindices = [\"p53\", \"mdm2\", \"bcl2\", \"cylinE\", \"Caspade\"]\ndf.index = indices\ndf\n\n\n\n\n\n\n\n\n\nalpha\nbeta\ngamma\n\n\n\n\np53\n9\n3\n7\n\n\nmdm2\n10\n2\n9\n\n\nbcl2\n1\n9\n4\n\n\ncylinE\n6\n5\n5\n\n\nCaspade\n1\n10\n3\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\n\ndef calculate_global_min(dm):\n    data = np.triu(dm)\n\n    min_val = np.nanmin(data[np.nonzero(data)])\n    position = [dm.index[val[0]] for val in np.where(data == min_val)]\n    return min_val, position\n\n\noriginal_dm = distance_matrix(df, df, p=2)\noriginal_dm = pd.DataFrame(\n    original_dm, index=df.index, columns=df.index\n).round(2)\noriginal_dm.index = np.arange(5).astype(str)\noriginal_dm.columns = np.arange(5).astype(str)\noriginal_dm\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\n1\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\n2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\n3\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\n4\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\n\ndata = original_dm.copy()\ndata.index = indices\ndata.columns = indices\ndata\n\n\n\n\n\n\n\n\n\np53\nmdm2\nbcl2\ncylinE\nCaspade\n\n\n\n\np53\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\nmdm2\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\nbcl2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\ncylinE\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\nCaspade\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\n\ndef clean_position(position):\n    pos = []\n    for p in position:\n        pos.extend(p.split(\",\"))\n    return pos\n\n\ndef new_iteration(dm, original_dm, linkage=np.nanmean):\n    min_val, position = calculate_global_min(dm)\n    print(f\"El valor m√≠nimo encontrado es: {min_val}\")\n    print(f\"Clusters a fusionar: {position}\")\n    non_position = [col for col in dm.columns if col not in position]\n    print(f\"Clusters que no se fusionan: {non_position}\")\n    new_position = \",\".join(position)\n    new_dm = dm.copy()\n    values = []\n    clean_pos = clean_position(position)\n    for n_p in non_position:\n        n_p = n_p.split(\",\")\n        v = linkage(original_dm.loc[n_p, clean_pos])\n        values.append(v)\n\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    new_dm = new_dm.T\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    return new_dm.drop(index=position, columns=position)\n\n\ndm_1 = new_iteration(original_dm, original_dm)\ndm_1\n\nEl valor m√≠nimo encontrado es: 1.41\nClusters a fusionar: ['2', '4']\nClusters que no se fusionan: ['0', '1', '3']\n\n\n\n\n\n\n\n\n\n\n0\n1\n3\n2,4\n\n\n\n\n0\n0.00\n2.45\n4.120\n10.900\n\n\n1\n2.45\n0.00\n6.400\n12.950\n\n\n3\n4.12\n6.40\n0.000\n6.915\n\n\n2,4\n10.90\n12.95\n6.915\nNaN\n\n\n\n\n\n\n\n\n\ndm_2 = new_iteration(dm_1, original_dm)\ndm_2\n\nEl valor m√≠nimo encontrado es: 2.45\nClusters a fusionar: ['0', '1']\nClusters que no se fusionan: ['3', '2,4']\n\n\n\n\n\n\n\n\n\n\n3\n2,4\n0,1\n\n\n\n\n3\n0.000\n6.915\n5.260\n\n\n2,4\n6.915\nNaN\n11.925\n\n\n0,1\n5.260\n11.925\nNaN\n\n\n\n\n\n\n\n\n\ndm_3 = new_iteration(dm_2, original_dm)\ndm_3\n\nEl valor m√≠nimo encontrado es: 5.26\nClusters a fusionar: ['3', '0,1']\nClusters que no se fusionan: ['2,4']\n\n\n\n\n\n\n\n\n\n\n2,4\n3,0,1\n\n\n\n\n2,4\nNaN\n10.255\n\n\n3,0,1\n10.255\nNaN\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/pauta_guia2.html",
    "href": "tics411/notebooks/pauta_guia2.html",
    "title": "Pregunta 3:",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.DataFrame(\n    dict(x=[4, 5, 5, 6, 7, 7], y=[1, 1, 2, 7, 6, 7], c=[1, 1, 1, 2, 2, 2]),\n    index=[*range(1, 7)],\n)\ndf\n\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n1\n4\n1\n1\n\n\n2\n5\n1\n1\n\n\n3\n5\n2\n1\n\n\n4\n6\n7\n2\n\n\n5\n7\n6\n2\n\n\n6\n7\n7\n2\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\nimport numpy as np\n\npoint = np.array([[8, 4]])\n\n## Distancias del Punto al resto de los puntos\npd.DataFrame(\n    distance_matrix(point, df[[\"x\", \"y\"]]), columns=[*range(1, 7)]\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n5.0\n4.242641\n3.605551\n3.605551\n2.236068\n3.162278\n\n\n\n\n\n\n\n\n\nSi k es 1, la distancia mas corta es 5, luego la predicci√≥n es 2.\n\n\nSi k = 3, las distancias son 5,6 y empate entre 3 y 4. Dado que 5 y 6 son 2, ya es mayoria por lo que el empate no importa, la predicci√≥n es 2.\n\n\nfrom scipy.spatial.distance import cdist\n\npd.DataFrame(\n    cdist(point, df[[\"x\", \"y\"]], \"mahalanobis\"), columns=[*range(1, 7)]\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n3.07265\n2.182821\n2.357716\n3.24037\n1.855041\n2.338929\n\n\n\n\n\n\n\n\n\nPara k = 1 la distancia m√°s peque√±a es 5, luego la predicci√≥n es 2.\n\n\nPara k = 3, las 3 distancias m√°s cortas son 5,2 y 6, luego la predicci√≥n tambi√©n es 2.\n\n\n\n\nconf_mat = np.array([[8, 2, 0], [4, 13, 13], [5, 12, 43]])\n\n\n## Funci√≥n para tomar una matriz de confusi√≥n y recrear las predicciones\ndef create_vectors(conf_mat):\n    rows, cols = conf_mat.shape\n\n    original_vals = np.empty((0, 2))\n    for i in range(rows):\n        for j in range(cols):\n            interim = np.repeat([[i, j]], conf_mat[i, j], axis=0)\n            original_vals = np.append(original_vals, interim, axis=0)\n\n    y = original_vals[:, 0]\n    y_pred = original_vals[:, 1]\n    return y, y_pred\n\n\ny, y_pred = create_vectors(conf_mat)\n\n\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n## Las predicciones recreadas generan esta matriz de confusi√≥n\n\n# OJO: ELIMIN√â un 1 para que hayan 100 valores, si no es lo que se busca,\n# se puede agregar el 1 en el objeto conf_mat de m√°s arriba.\n\nConfusionMatrixDisplay.from_predictions(y, y_pred)\n\n## Esdto entrega el Accuracy √∫nico como mand√© en un notebook y Precision, Recall y F1 por clases 0,1 y 2.\nprint(classification_report(y, y_pred, digits=3))\n\n              precision    recall  f1-score   support\n\n         0.0      0.471     0.800     0.593        10\n         1.0      0.481     0.433     0.456        30\n         2.0      0.768     0.717     0.741        60\n\n    accuracy                          0.640       100\n   macro avg      0.573     0.650     0.597       100\nweighted avg      0.652     0.640     0.641       100"
  },
  {
    "objectID": "tics411/notebooks/pauta_guia2.html#pregunta-7",
    "href": "tics411/notebooks/pauta_guia2.html#pregunta-7",
    "title": "Pregunta 3:",
    "section": "",
    "text": "conf_mat = np.array([[8, 2, 0], [4, 13, 13], [5, 12, 43]])\n\n\n## Funci√≥n para tomar una matriz de confusi√≥n y recrear las predicciones\ndef create_vectors(conf_mat):\n    rows, cols = conf_mat.shape\n\n    original_vals = np.empty((0, 2))\n    for i in range(rows):\n        for j in range(cols):\n            interim = np.repeat([[i, j]], conf_mat[i, j], axis=0)\n            original_vals = np.append(original_vals, interim, axis=0)\n\n    y = original_vals[:, 0]\n    y_pred = original_vals[:, 1]\n    return y, y_pred\n\n\ny, y_pred = create_vectors(conf_mat)\n\n\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n## Las predicciones recreadas generan esta matriz de confusi√≥n\n\n# OJO: ELIMIN√â un 1 para que hayan 100 valores, si no es lo que se busca,\n# se puede agregar el 1 en el objeto conf_mat de m√°s arriba.\n\nConfusionMatrixDisplay.from_predictions(y, y_pred)\n\n## Esdto entrega el Accuracy √∫nico como mand√© en un notebook y Precision, Recall y F1 por clases 0,1 y 2.\nprint(classification_report(y, y_pred, digits=3))\n\n              precision    recall  f1-score   support\n\n         0.0      0.471     0.800     0.593        10\n         1.0      0.481     0.433     0.456        30\n         2.0      0.768     0.717     0.741        60\n\n    accuracy                          0.640       100\n   macro avg      0.573     0.650     0.597       100\nweighted avg      0.652     0.640     0.641       100"
  },
  {
    "objectID": "tics411/notebooks/viz.html",
    "href": "tics411/notebooks/viz.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\na = np.array([4.5, 4, 4.1, 1, 2.3, 2.2, 2.4, 5, 5.5, 6.2, 6, 6, 6, 6])\nb = np.append(a, a + 1)\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(b, bins=5)\nax[\"A\"].set_title(\"Bins 5\")\nax[\"A\"].set_xlabel(\"Notas\")\nax[\"A\"].set_ylabel(\"N√∫mero de Estudiantes\")\nax[\"B\"].hist(b, bins=15)\nax[\"B\"].set_title(\"Bins 15\")\nax[\"B\"].set_xlabel(\"Notas\")\nax[\"C\"].hist(b, bins=30)\nax[\"C\"].set_title(\"Bins 30\")\nax[\"C\"].set_xlabel(\"Notas\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import KernelDensity\n\n\nkd_gauss = KernelDensity(kernel=\"epanechnikov\")\nkd_gauss.fit(b[:, np.newaxis])\nx_grid = np.linspace(1, 6, 1000)\nb_gauss = np.exp(kd_gauss.score_samples(x_grid[:, np.newaxis]))\nplt.hist(b)\nplt.plot(b_gauss)\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nnp.random.seed(0)\nx_grid = np.linspace(-4.5, 3.5, 1000)\nx = np.concatenate([norm(-1, 1.0).rvs(400), norm(1, 0.3).rvs(100)])\n\n\ndef kde_sklearn(x, x_grid, bandwidth=0.2, **kwargs):\n    \"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n    kde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n    kde_skl.fit(x[:, np.newaxis])\n    # score_samples() returns the log-likelihood of the samples\n    log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n    return np.exp(log_pdf)\n\n\npdf = kde_sklearn(x, x_grid, bandwidth=0.2)\n\nplt.hist(x)\nplt.plot(x_grid, pdf)\n\n\n\n\n\n\n\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"tophat\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_tophat = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_gaussian = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"epanechnikov\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_epa = np.exp(log_densidad_pred)\n\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"A\"].plot(X_grid, densidad_pred_tophat, color=\"red\", label=\"predicci√≥n\")\nax[\"A\"].set_title(\"Kernel Uniforme/Tophat h=0.2\")\n\nax[\"B\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"B\"].plot(X_grid, densidad_pred_gaussian, color=\"red\", label=\"predicci√≥n\")\nax[\"B\"].set_title(\"Kernel Gaussiano h=0.2\")\n\nax[\"C\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"C\"].plot(X_grid, densidad_pred_epa, color=\"red\", label=\"predicci√≥n\")\nax[\"C\"].set_title(\"Kernel Epanechnikov h=0.2\")\n\nText(0.5, 1.0, 'Kernel Epanechnikov h=0.2')\n\n\n\n\n\n\n\n\n\n\nn = 1000\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2))\n\nX_grid = np.linspace(-3, 4, 1000)\n\nmodelo_kde = KernelDensity(kernel=\"linear\", bandwidth=1)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred = np.exp(log_densidad_pred)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax.plot(X_grid, densidad_pred, color=\"red\", label=\"predicci√≥n\")\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df[\"embarked\"].value_counts().plot(\n    kind=\"bar\", rot=0, title=\"Personas embarcadas por puerto del Titanic\"\n)\n\n\n\n\n\n\n\n\n\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\n\n\n\ng = sns.catplot(\n    data=titanic_df, y=\"fare\", x=\"pclass\", kind=\"bar\", errorbar=None, hue=\"sex\"\n)\ng.figure.suptitle(\"Tarifa Promedio de Pasajeros del Titanic\\n por Clase y Sexo.\")\n\n\n\n\n\n\n\n\n\ndata = titanic_df[[\"age\", \"fare\"]].melt(value_vars=[\"age\", \"fare\"])\ndata\n\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nage\n22.00\n\n\n1\nage\n38.00\n\n\n2\nage\n26.00\n\n\n3\nage\n35.00\n\n\n4\nage\n35.00\n\n\n...\n...\n...\n\n\n1777\nfare\n13.00\n\n\n1778\nfare\n30.00\n\n\n1779\nfare\n23.45\n\n\n1780\nfare\n30.00\n\n\n1781\nfare\n7.75\n\n\n\n\n1782 rows √ó 2 columns\n\n\n\n\n\nsns.catplot(\n    kind=\"box\", x=\"variable\", y=\"value\", data=data, height=6, aspect=0.5, hue=\"variable\"\n)\n\n\n\n\n\n\n\n\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows √ó 5 columns\n\n\n\n\n\ndata = iris_df.melt(\n    value_vars=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n)\ndata\n\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nsepal_length\n5.1\n\n\n1\nsepal_length\n4.9\n\n\n2\nsepal_length\n4.7\n\n\n3\nsepal_length\n4.6\n\n\n4\nsepal_length\n5.0\n\n\n...\n...\n...\n\n\n595\npetal_width\n2.3\n\n\n596\npetal_width\n1.9\n\n\n597\npetal_width\n2.0\n\n\n598\npetal_width\n2.3\n\n\n599\npetal_width\n1.8\n\n\n\n\n600 rows √ó 2 columns\n\n\n\n\n\ng = sns.catplot(x=\"variable\", y=\"value\", kind=\"box\", hue=\"variable\", data=data)\ng.figure.suptitle(\"Distribuci√≥n de Medidas de Flores\")\ng._legend.remove()\ng.set(xlabel=None)\ng.set(ylabel=None)\n\n\n\n\n\n\n\n\n\nplt.scatter(iris_df.sepal_length, iris_df.petal_length)\nplt.title(\"Relaci√≥n entre Largo del S√©palo y del P√©talo\")\nplt.xlabel(\"Largo del S√©palo\")\nplt.ylabel(\"Largo del P√©talo\")\n\nText(0, 0.5, 'Largo del P√©talo')\n\n\n\n\n\n\n\n\n\n\nanscombe = sns.load_dataset(\"anscombe\")\n\nfig = plt.figure(figsize=(10, 9))\nax = fig.subplot_mosaic(\n    \"\"\"AB\n                        CD\"\"\"\n)\nsns.regplot(data=anscombe.query(\"dataset == 'I'\"), x=\"x\", y=\"y\", ax=ax[\"A\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'II'\"), x=\"x\", y=\"y\", ax=ax[\"B\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'III'\"), x=\"x\", y=\"y\", ax=ax[\"C\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'IV'\"), x=\"x\", y=\"y\", ax=ax[\"D\"], ci=None)\nplt.suptitle(\"Cuarteto de Anscombe\")\n\nText(0.5, 0.98, 'Cuarteto de Anscombe')\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows √ó 5 columns\n\n\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\ndata = pca.fit_transform(iris_df.drop(columns=\"species\"))\nx, y = data[:,0], data[:,1]\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nc = iris_df.species.astype(\"category\").cat.codes\nplt.scatter(x,y, c = c)\nplt.title(\"3 Clusters\");\n\n\n\n\n\n\n\n\n\nc_prima = (c == 0).astype(\"int64\")\nplt.scatter(x,y, c= c_prima)\nplt.title(\"2 Clusters\")\n\nText(0.5, 1.0, '2 Clusters')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/knn_desarrollo.html",
    "href": "tics411/notebooks/knn_desarrollo.html",
    "title": "Preprocesamiento",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf.dtypes.value_counts().plot(kind=\"bar\")\nplt.tight_layout()\nX = df[[\"sex\", \"age\", \"class\", \"embark_town\", \"fare\"]]\ny = df.alive\ny\n\n0       no\n1      yes\n2      yes\n3      yes\n4       no\n      ... \n886     no\n887    yes\n888     no\n889    yes\n890     no\nName: alive, Length: 891, dtype: object\ny.value_counts(normalize=True).plot(kind=\"bar\")\nimport numpy as np\n\nnum_vars = X.select_dtypes(np.number).columns.tolist()\ncat_vars = [col for col in X.columns if col not in num_vars]\ncat_vars\n\n['sex', 'class', 'embark_town']\nX[num_vars].hist(figsize=(20, 6))\n\narray([[&lt;Axes: title={'center': 'age'}&gt;,\n        &lt;Axes: title={'center': 'fare'}&gt;]], dtype=object)\nX.groupby(\"age\").fare.mean()\n\nage\n0.42       8.5167\n0.67      14.5000\n0.75      19.2583\n0.83      23.8750\n0.92     151.5500\n           ...   \n70.00     40.7500\n70.50      7.7500\n71.00     42.0792\n74.00      7.7750\n80.00     30.0000\nName: fare, Length: 88, dtype: float64\ndf[\"rango_edad\"] = pd.cut(X[\"age\"], 5)\ndf.groupby(\"rango_edad\").fare.median()\n\nrango_edad\n(0.34, 16.336]      26.00000\n(16.336, 32.252]    10.50000\n(32.252, 48.168]    24.86875\n(48.168, 64.084]    29.70000\n(64.084, 80.0]      26.55000\nName: fare, dtype: float64\ndf.groupby(\"rango_edad\").fare.mean()\n\nrango_edad\n(0.34, 16.336]      31.588877\n(16.336, 32.252]    28.260499\n(32.252, 48.168]    42.788940\n(48.168, 64.084]    50.327235\n(64.084, 80.0]      28.905691\nName: fare, dtype: float64\ndf.fare.plot(kind=\"box\")\nfor cat in cat_vars:\n    X[cat].value_counts().plot(\n        kind=\"bar\", title=f\"Gr√°fico de variable {cat}\"\n    )\n    plt.show()\nX.groupby(\"class\").fare.mean()\n\nclass\nFirst     84.154687\nSecond    20.662183\nThird     13.675550\nName: fare, dtype: float64"
  },
  {
    "objectID": "tics411/notebooks/knn_desarrollo.html#preprocesamiento",
    "href": "tics411/notebooks/knn_desarrollo.html#preprocesamiento",
    "title": "Preprocesamiento",
    "section": "Preprocesamiento",
    "text": "Preprocesamiento\n\nX\n\n\n\n\n\n\n\n\n\nsex\nage\nclass\nembark_town\nfare\n\n\n\n\n0\nmale\n22.0\nThird\nSouthampton\n7.2500\n\n\n1\nfemale\n38.0\nFirst\nCherbourg\n71.2833\n\n\n2\nfemale\n26.0\nThird\nSouthampton\n7.9250\n\n\n3\nfemale\n35.0\nFirst\nSouthampton\n53.1000\n\n\n4\nmale\n35.0\nThird\nSouthampton\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nmale\n27.0\nSecond\nSouthampton\n13.0000\n\n\n887\nfemale\n19.0\nFirst\nSouthampton\n30.0000\n\n\n888\nfemale\nNaN\nThird\nSouthampton\n23.4500\n\n\n889\nmale\n26.0\nFirst\nCherbourg\n30.0000\n\n\n890\nmale\n32.0\nThird\nQueenstown\n7.7500\n\n\n\n\n891 rows √ó 5 columns\n\n\n\n\n\nX.isnull().mean().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X)\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X_imp)\nX_imp\n\n\n\n\n\n\n\n\n\nsex\nage\nclass\nembark_town\nfare\n\n\n\n\n0\nmale\n22.000000\nThird\nSouthampton\n7.2500\n\n\n1\nfemale\n38.000000\nFirst\nCherbourg\n71.2833\n\n\n2\nfemale\n26.000000\nThird\nSouthampton\n7.9250\n\n\n3\nfemale\n35.000000\nFirst\nSouthampton\n53.1000\n\n\n4\nmale\n35.000000\nThird\nSouthampton\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nmale\n27.000000\nSecond\nSouthampton\n13.0000\n\n\n887\nfemale\n19.000000\nFirst\nSouthampton\n30.0000\n\n\n888\nfemale\n29.699118\nThird\nSouthampton\n23.4500\n\n\n889\nmale\n26.000000\nFirst\nCherbourg\n30.0000\n\n\n890\nmale\n32.000000\nThird\nQueenstown\n7.7500\n\n\n\n\n891 rows √ó 5 columns\n\n\n\n\n\nohe = OneHotEncoder(variables=[\"sex\", \"embark_town\"])\nX_enc = ohe.fit_transform(X_imp)\nod = OrdinalEncoder(encoding_method=\"arbitrary\")\nX_enc = od.fit_transform(X_enc)\nX_enc\n\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n22.000000\n0\n7.2500\n1\n0\n1\n0\n0\n\n\n1\n38.000000\n1\n71.2833\n0\n1\n0\n1\n0\n\n\n2\n26.000000\n0\n7.9250\n0\n1\n1\n0\n0\n\n\n3\n35.000000\n1\n53.1000\n0\n1\n1\n0\n0\n\n\n4\n35.000000\n0\n8.0500\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n27.000000\n2\n13.0000\n1\n0\n1\n0\n0\n\n\n887\n19.000000\n1\n30.0000\n0\n1\n1\n0\n0\n\n\n888\n29.699118\n0\n23.4500\n0\n1\n1\n0\n0\n\n\n889\n26.000000\n1\n30.0000\n1\n0\n0\n1\n0\n\n\n890\n32.000000\n0\n7.7500\n1\n0\n0\n0\n1\n\n\n\n\n891 rows √ó 8 columns\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"age\", \"fare\"])\nX_sc = sc.fit_transform(X_enc)\n\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_enc)\nX_sc_all\n\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.592481\n-0.820037\n-0.502445\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n1\n0.638789\n0.431081\n0.786845\n-1.355574\n1.355574\n-1.623803\n2.074505\n-0.307562\n\n\n2\n-0.284663\n-0.820037\n-0.488854\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n3\n0.407926\n0.431081\n0.420730\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n4\n0.407926\n-0.820037\n-0.486337\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.207709\n1.682199\n-0.386671\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n887\n-0.823344\n0.431081\n-0.044381\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n888\n0.000000\n-0.820037\n-0.176263\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n889\n-0.284663\n0.431081\n-0.044381\n0.737695\n-0.737695\n-1.623803\n2.074505\n-0.307562\n\n\n890\n0.177063\n-0.820037\n-0.492378\n0.737695\n-0.737695\n-1.623803\n-0.482043\n3.251373\n\n\n\n\n891 rows √ó 8 columns\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn(X, y, k=3):\n    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n    knn.fit(X, y)\n    print(f\"Puntaje para k = {k}: {knn.score(X, y)}\")\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_sc, y, k=k)\n\nPuntaje para k = 3: 0.8843995510662177\nPuntaje para k = 5: 0.8686868686868687\nPuntaje para k = 7: 0.8608305274971941\nPuntaje para k = 9: 0.8428731762065096\nPuntaje para k = 11: 0.835016835016835\nPuntaje para k = 13: 0.8249158249158249\nPuntaje para k = 15: 0.819304152637486\n\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_sc_all, y, k=k)\n\nPuntaje para k = 3: 0.8866442199775533\nPuntaje para k = 5: 0.8698092031425365\nPuntaje para k = 7: 0.8552188552188552\nPuntaje para k = 9: 0.8383838383838383\nPuntaje para k = 11: 0.835016835016835\nPuntaje para k = 13: 0.8282828282828283\nPuntaje para k = 15: 0.8237934904601572\n\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_enc, y, k=k)\n\nPuntaje para k = 3: 0.8372615039281706\nPuntaje para k = 5: 0.8204264870931538\nPuntaje para k = 7: 0.7867564534231201\nPuntaje para k = 9: 0.7721661054994389\nPuntaje para k = 11: 0.7676767676767676\nPuntaje para k = 13: 0.7575757575757576\nPuntaje para k = 15: 0.7508417508417509\n\n\n\nX_enc\n\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n22.000000\n0\n7.2500\n1\n0\n1\n0\n0\n\n\n1\n38.000000\n1\n71.2833\n0\n1\n0\n1\n0\n\n\n2\n26.000000\n0\n7.9250\n0\n1\n1\n0\n0\n\n\n3\n35.000000\n1\n53.1000\n0\n1\n1\n0\n0\n\n\n4\n35.000000\n0\n8.0500\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n27.000000\n2\n13.0000\n1\n0\n1\n0\n0\n\n\n887\n19.000000\n1\n30.0000\n0\n1\n1\n0\n0\n\n\n888\n29.699118\n0\n23.4500\n0\n1\n1\n0\n0\n\n\n889\n26.000000\n1\n30.0000\n1\n0\n0\n1\n0\n\n\n890\n32.000000\n0\n7.7500\n1\n0\n0\n0\n1\n\n\n\n\n891 rows √ó 8 columns"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html",
    "href": "tics411/notebooks/02-EDA.html",
    "title": "EDA",
    "section": "",
    "text": "El siguiente notebook tiene por prop√≥sito mostrar algunos comandos b√°sicos para poder realizar Exploraci√≥n de Datos utilizando Pandas.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Vamos a cargar los siguientes datos para poder explorarlos.\niris_df = sns.load_dataset(\"iris\")\ntitanic_df = sns.load_dataset(\"titanic\")\nts_df = sns.load_dataset(\"dowjones\")\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows √ó 5 columns\n\n\n\n\n\n\nLos comandos .mean() y .median() permiten calcular la media y la mediana en datos num√©ricos. Como se ve en los ejemplos permite llamar una Serie de Pandas y calcular un valor.\n\nTip: En caso de querer aplicar estos comandos a un DataFrame se recomienda utilizar el flag numeric_only = True para evitar calcular estos valores en Datos Categ√≥ricos donde no hacen sentido.\n\n\nprint(f\"Promedio de Ancho de Petalo {iris_df['sepal_width'].mean()}\")\nprint(f\"Mediana de Largo de Petalo {iris_df['sepal_length'].median()}\")\n\nPromedio de Ancho de Petalo 3.0573333333333337\nMediana de Largo de Petalo 5.8\n\n\nPandas tambi√©n cuenta con el comando .mode() el cu√°l devuelve la moda. A diferencia de los comandos anteriores, .mode() puede utilizarse tanto para datos categ√≥ricos como datos num√©ricos.\n\nprint(f\"Moda de Especies: \")\niris_df[\"species\"].mode()\n\nModa de Especies: \n\n\n0        setosa\n1    versicolor\n2     virginica\nName: species, dtype: object\n\n\nEl comando .quantile() permite calcular alg√∫n percentil de inter√©s. q es un valor que va entre 0 y 1 para indicar el percentil requerido. Recordar que la mediana es equivalente al Percentil 50.\n\np25 = iris_df[\"sepal_width\"].quantile(q=0.25)\np50 = iris_df[\"sepal_width\"].quantile(q=0.50)\np75 = iris_df[\"sepal_width\"].quantile(q=0.75)\niris_df[\"sepal_width\"].median(), p25, p50, p75\n\n(3.0, 2.8, 3.0, 3.3)\n\n\n\n\n\nPandas permite el c√°lculo de distintas medidas de dispersi√≥n. Al igual que los comandos anteriores contiene el flag numeric_only = True para evitar inconvenientes en DataFrames con distintos data types. Adem√°s contiene el comando ddof el cu√°l permitir√° diferenciar si se quiere la medida poblacional (ddof = 0) o la muestral (ddof = 1).\n\n# Varianza Poblacional\niris_df.var(numeric_only=True, ddof=0)\n\nsepal_length    0.681122\nsepal_width     0.188713\npetal_length    3.095503\npetal_width     0.577133\ndtype: float64\n\n\n\n# Varianza Muestral\niris_df.var(numeric_only=True, ddof=1)\n\nsepal_length    0.685694\nsepal_width     0.189979\npetal_length    3.116278\npetal_width     0.581006\ndtype: float64\n\n\n\n# Desviaci√≥n Est√°ndar Muestral\niris_df.std(numeric_only=True, ddof=1)\n\nsepal_length    0.828066\nsepal_width     0.435866\npetal_length    1.765298\npetal_width     0.762238\ndtype: float64\n\n\n\n# Funci√≥n para calcular el Rango Intercuartil...\ndef calculate_IQR(column):\n    quantiles = iris_df.quantile([0.25, 0.75], numeric_only=True)\n    iqr_sl = quantiles.loc[0.75, column] - quantiles.loc[0.25, column]\n    return iqr_sl\n\n\ncalculate_IQR(\"sepal_length\")\ncalculate_IQR(\"petal_width\")\n\n1.5\n\n\n\n# Coeficiente de Skewness o Asimetr√≠a.\niris_df.skew(numeric_only=True)\n\nsepal_length    0.314911\nsepal_width     0.318966\npetal_length   -0.274884\npetal_width    -0.102967\ndtype: float64\n\n\n\n\n\nA continuaci√≥n se mostrar√°n comandos propios de Pandas para poder generar los gr√°ficos visto a lo largo de las clases. Se sugiere este tipo de gr√°ficos cuando se trabaje con DataFrames ya que poseen buena documentaci√≥n y una interfaz com√∫n para todos los gr√°ficos.\nOpciones:\n\nkind: Permite indicar mediante un string el tipo de gr√°fico a mostrar.\nfigsize = (w,h): Permite fijar el tama√±o de la figura. Notar que primero se entrega el ancho y luego el alto. Yo normalmente uso (20,6) ya que considero que queda bastante bien.\nedgecolor: Permite indicar el color del borde de las barras mediante un string. Tiene sentido para histogram√°s y bar plots.\ngrid = True/False: Permite mostrar o no una grilla.\nbins = n: Opci√≥n s√≥lo para histogramas que permite indicar en cu√°ntos bins se dividen los datos en el Histograma.\nalpha = 0.5: Corresponde al grado de transparecencia. Es un valor que va entre 0 y 1. Entre m√°s peque√±o el valor, m√°s transparente.\ntitle: Permite agregar un T√≠tulo como String.\nxlabel: Permite agregar un T√≠tulo al Eje X.\nylabel: Permite agregar un T√≠tulo al Eje Y.\n\n\n\n\niris_df.plot(\n    kind=\"hist\", alpha=0.5, bins=30, figsize=(20, 6), edgecolor=\"black\"\n)\n# Notar que este genera todos los histogramas superpuestos...\n\nPor alguna raz√≥n Pandas tiene el comando .hist(). Este comando es bastante √∫til porque a diferencia del anterior no superpone los histogramas, lo cual la mayor√≠a de las veces es lo que se busca.\n\niris_df.hist(figsize=(20, 6), bins=30, edgecolor=\"black\", grid=False)\n# tight_layout es opcional y a veces evita que hayan traslapes de t√≠tulos.\n# Usarlo si es que es necesario.\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nA diferencia de los Histogr√°mas, los Barplots son utilizados para aplicar una agregaci√≥n antes de gr√°ficar. Esta agregaci√≥n se puede utilizar mediante .value_counts() que permite contar valores, o mediante .groupby() el cu√°l permite aplicar otros tipos de agregaci√≥n.\n\n# Ac√° por ejemplos contamos la cantidad de pasajeros por Sexo\ntitanic_df[\"sex\"].value_counts()\n\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\n\n# Una vez que tenemos contados los elementos podemos graficar...\ntitanic_df[\"sex\"].value_counts().plot(\n    kind=\"bar\",\n    figsize=(5, 6),\n    title=\"N√∫mero de Pasajeros por Sexo...\",\n    edgecolor=\"black\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Otro ejemplo, en este caso calculando el promedio por de Edad y Tarifa por A√±o.\ntitanic_df.groupby(\"pclass\")[[\"age\", \"fare\"]].mean()\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\npclass\n\n\n\n\n\n\n1\n38.233441\n84.154687\n\n\n2\n29.877630\n20.662183\n\n\n3\n25.140620\n13.675550\n\n\n\n\n\n\n\n\n\n## En este caso, el √≠ndice Pclass ir√° al Eje X y los valores agregados de Age y Fare ir√°n como barras.\niris_df.groupby(\"species\").mean().plot(kind=\"bar\", edgecolor=\"black\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\niris_df.drop(columns=\"species\").plot(kind=\"box\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nNotar que a diferencia de los casos anteriores, el gr√°fico de puntos requiere que se definan qu√© columna ir√° en x y en y respectivamente.\n\n\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de P√©talo vs Ancho de P√©talo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n)\n\n\n\n\n\nEl lineplot es el gr√°fico por defecto de Pandas, por lo tanto no es necesario definir el par√°metro kind. Al igual que el gr√°fico de Puntos se debe definir las variables x e y. Se recomienda siempre que x sea una variable de tipo temporal.\n\nts_df.plot(x=\"Date\", y=\"Price\", title=\"Evoluci√≥n del Dow Jones\")\n\n\n## Este es un ejemplo de varias series de tiempo en conjunto.\n## Este c√≥digo s√≥lo genera datos sint√©ticos.\nfrom scipy.stats import norm\n\nts_df[\"AA\"] = ts_df[\"Price\"] + norm.rvs(size=649) * 55 + 1000\nts_df[\"BB\"] = -norm.rvs(size=649) * 55\n\nts_df.set_index(\"Date\").plot(title=\"Comparaci√≥n distintas Tendencias\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nEn muchas ocaciones nosotros queremos mostrar una compilaci√≥n de todos nuestros gr√°ficos m√°s que cada uno por separado. Para eso Matplotlib cuenta con la opci√≥n Mosaico.\nMosaico permite generar una grilla definida como un String. Si se fijan nuestra grilla se define por el string:\n\"\"\"AAA\n   BCC\"\"\"\nEn este caso nuestro canvas se divide en 6 partes, el gr√°fico que asigne a A utilizar√° las 3 secciones superiores, B utilizar√° s√≥lo la secci√≥n de abajo a la izquierda y C utilizar√° las 2 restantes.\nPara asignar cada secci√≥n .plot() de pandas posee el par√°metro ax donde se debe generar la asignaci√≥n.\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.subplot_mosaic(\n    \"\"\"AAA\n       BCC\"\"\"\n)\n\n# Gr√°fico asignado a C\niris_df.drop(columns=\"species\").plot(\n    kind=\"box\", ax=ax[\"C\"], title=\"Distribuci√≥n de Datos por Variable\"\n)\n\n## Gr√°fico asignado a B\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de P√©talo vs Ancho de P√©talo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n    ax=ax[\"B\"],\n)\n\n## Gr√°fico asignado a A\niris_df.groupby(\"species\").mean().plot(\n    kind=\"bar\",\n    edgecolor=\"black\",\n    ax=ax[\"A\"],\n    rot=0,\n    title=\"Valores promedio por Especie\",\n)\n\n## Permite Agregar un t√≠tulo general a todo el Gr√°fico\nplt.suptitle(\"Este ser√° un t√≠tulo para todo el Gr√°fico\", fontsize=20)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nLos comandos mostrados anteriormente son una adaptaci√≥n de Matplotlib a Pandas. La gracia que tienen es que son f√°ciles de aprender y funcionar√°n directamente en Pandas que ser√° nuestra principal fuente de datos.\nEn el caso de trabajar con Numpy, estos comandos NO FUNCIONAR√ÅN. Por lo tanto es necesario utilizar la API de Matplotlib. La traducci√≥n no es 100% directa, pero normalmente todos los par√°metros de .plot() se cambiar√°n por comandos del tipo plt.---\n\n\n\nplt.plot(x,y, c = \"red\") #Existe tambi√©n plt.bar, plt.hist, plt.scatter, plt.boxplot.\nplt.title(\"Este va a ser un t√≠tulo\")\nplt.xlabel(\"Este ser√° una etiqueta del Eje X\")\nAprender Matplotlib es bastante m√°s complicado pero tiene funcionalidades much√≠simo m√°s avanzadas que Pandas. Para este curso, no ser√° necesario especializarse en Matplotlib, pero s√≠ m√°s adelante utilizaremos algunos gr√°ficos que no se pueden hacer tan f√°cilmente en Pandas (pero ser√°n casos puntuales)."
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "title": "EDA",
    "section": "",
    "text": "Los comandos .mean() y .median() permiten calcular la media y la mediana en datos num√©ricos. Como se ve en los ejemplos permite llamar una Serie de Pandas y calcular un valor.\n\nTip: En caso de querer aplicar estos comandos a un DataFrame se recomienda utilizar el flag numeric_only = True para evitar calcular estos valores en Datos Categ√≥ricos donde no hacen sentido.\n\n\nprint(f\"Promedio de Ancho de Petalo {iris_df['sepal_width'].mean()}\")\nprint(f\"Mediana de Largo de Petalo {iris_df['sepal_length'].median()}\")\n\nPromedio de Ancho de Petalo 3.0573333333333337\nMediana de Largo de Petalo 5.8\n\n\nPandas tambi√©n cuenta con el comando .mode() el cu√°l devuelve la moda. A diferencia de los comandos anteriores, .mode() puede utilizarse tanto para datos categ√≥ricos como datos num√©ricos.\n\nprint(f\"Moda de Especies: \")\niris_df[\"species\"].mode()\n\nModa de Especies: \n\n\n0        setosa\n1    versicolor\n2     virginica\nName: species, dtype: object\n\n\nEl comando .quantile() permite calcular alg√∫n percentil de inter√©s. q es un valor que va entre 0 y 1 para indicar el percentil requerido. Recordar que la mediana es equivalente al Percentil 50.\n\np25 = iris_df[\"sepal_width\"].quantile(q=0.25)\np50 = iris_df[\"sepal_width\"].quantile(q=0.50)\np75 = iris_df[\"sepal_width\"].quantile(q=0.75)\niris_df[\"sepal_width\"].median(), p25, p50, p75\n\n(3.0, 2.8, 3.0, 3.3)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-dispersi√≥n",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-dispersi√≥n",
    "title": "EDA",
    "section": "",
    "text": "Pandas permite el c√°lculo de distintas medidas de dispersi√≥n. Al igual que los comandos anteriores contiene el flag numeric_only = True para evitar inconvenientes en DataFrames con distintos data types. Adem√°s contiene el comando ddof el cu√°l permitir√° diferenciar si se quiere la medida poblacional (ddof = 0) o la muestral (ddof = 1).\n\n# Varianza Poblacional\niris_df.var(numeric_only=True, ddof=0)\n\nsepal_length    0.681122\nsepal_width     0.188713\npetal_length    3.095503\npetal_width     0.577133\ndtype: float64\n\n\n\n# Varianza Muestral\niris_df.var(numeric_only=True, ddof=1)\n\nsepal_length    0.685694\nsepal_width     0.189979\npetal_length    3.116278\npetal_width     0.581006\ndtype: float64\n\n\n\n# Desviaci√≥n Est√°ndar Muestral\niris_df.std(numeric_only=True, ddof=1)\n\nsepal_length    0.828066\nsepal_width     0.435866\npetal_length    1.765298\npetal_width     0.762238\ndtype: float64\n\n\n\n# Funci√≥n para calcular el Rango Intercuartil...\ndef calculate_IQR(column):\n    quantiles = iris_df.quantile([0.25, 0.75], numeric_only=True)\n    iqr_sl = quantiles.loc[0.75, column] - quantiles.loc[0.25, column]\n    return iqr_sl\n\n\ncalculate_IQR(\"sepal_length\")\ncalculate_IQR(\"petal_width\")\n\n1.5\n\n\n\n# Coeficiente de Skewness o Asimetr√≠a.\niris_df.skew(numeric_only=True)\n\nsepal_length    0.314911\nsepal_width     0.318966\npetal_length   -0.274884\npetal_width    -0.102967\ndtype: float64"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#visualizaciones",
    "href": "tics411/notebooks/02-EDA.html#visualizaciones",
    "title": "EDA",
    "section": "",
    "text": "A continuaci√≥n se mostrar√°n comandos propios de Pandas para poder generar los gr√°ficos visto a lo largo de las clases. Se sugiere este tipo de gr√°ficos cuando se trabaje con DataFrames ya que poseen buena documentaci√≥n y una interfaz com√∫n para todos los gr√°ficos.\nOpciones:\n\nkind: Permite indicar mediante un string el tipo de gr√°fico a mostrar.\nfigsize = (w,h): Permite fijar el tama√±o de la figura. Notar que primero se entrega el ancho y luego el alto. Yo normalmente uso (20,6) ya que considero que queda bastante bien.\nedgecolor: Permite indicar el color del borde de las barras mediante un string. Tiene sentido para histogram√°s y bar plots.\ngrid = True/False: Permite mostrar o no una grilla.\nbins = n: Opci√≥n s√≥lo para histogramas que permite indicar en cu√°ntos bins se dividen los datos en el Histograma.\nalpha = 0.5: Corresponde al grado de transparecencia. Es un valor que va entre 0 y 1. Entre m√°s peque√±o el valor, m√°s transparente.\ntitle: Permite agregar un T√≠tulo como String.\nxlabel: Permite agregar un T√≠tulo al Eje X.\nylabel: Permite agregar un T√≠tulo al Eje Y.\n\n\n\n\niris_df.plot(\n    kind=\"hist\", alpha=0.5, bins=30, figsize=(20, 6), edgecolor=\"black\"\n)\n# Notar que este genera todos los histogramas superpuestos...\n\nPor alguna raz√≥n Pandas tiene el comando .hist(). Este comando es bastante √∫til porque a diferencia del anterior no superpone los histogramas, lo cual la mayor√≠a de las veces es lo que se busca.\n\niris_df.hist(figsize=(20, 6), bins=30, edgecolor=\"black\", grid=False)\n# tight_layout es opcional y a veces evita que hayan traslapes de t√≠tulos.\n# Usarlo si es que es necesario.\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nA diferencia de los Histogr√°mas, los Barplots son utilizados para aplicar una agregaci√≥n antes de gr√°ficar. Esta agregaci√≥n se puede utilizar mediante .value_counts() que permite contar valores, o mediante .groupby() el cu√°l permite aplicar otros tipos de agregaci√≥n.\n\n# Ac√° por ejemplos contamos la cantidad de pasajeros por Sexo\ntitanic_df[\"sex\"].value_counts()\n\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\n\n# Una vez que tenemos contados los elementos podemos graficar...\ntitanic_df[\"sex\"].value_counts().plot(\n    kind=\"bar\",\n    figsize=(5, 6),\n    title=\"N√∫mero de Pasajeros por Sexo...\",\n    edgecolor=\"black\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Otro ejemplo, en este caso calculando el promedio por de Edad y Tarifa por A√±o.\ntitanic_df.groupby(\"pclass\")[[\"age\", \"fare\"]].mean()\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\npclass\n\n\n\n\n\n\n1\n38.233441\n84.154687\n\n\n2\n29.877630\n20.662183\n\n\n3\n25.140620\n13.675550\n\n\n\n\n\n\n\n\n\n## En este caso, el √≠ndice Pclass ir√° al Eje X y los valores agregados de Age y Fare ir√°n como barras.\niris_df.groupby(\"species\").mean().plot(kind=\"bar\", edgecolor=\"black\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#boxplots",
    "href": "tics411/notebooks/02-EDA.html#boxplots",
    "title": "EDA",
    "section": "",
    "text": "iris_df.drop(columns=\"species\").plot(kind=\"box\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nNotar que a diferencia de los casos anteriores, el gr√°fico de puntos requiere que se definan qu√© columna ir√° en x y en y respectivamente.\n\n\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de P√©talo vs Ancho de P√©talo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#lineplot",
    "href": "tics411/notebooks/02-EDA.html#lineplot",
    "title": "EDA",
    "section": "",
    "text": "El lineplot es el gr√°fico por defecto de Pandas, por lo tanto no es necesario definir el par√°metro kind. Al igual que el gr√°fico de Puntos se debe definir las variables x e y. Se recomienda siempre que x sea una variable de tipo temporal.\n\nts_df.plot(x=\"Date\", y=\"Price\", title=\"Evoluci√≥n del Dow Jones\")\n\n\n## Este es un ejemplo de varias series de tiempo en conjunto.\n## Este c√≥digo s√≥lo genera datos sint√©ticos.\nfrom scipy.stats import norm\n\nts_df[\"AA\"] = ts_df[\"Price\"] + norm.rvs(size=649) * 55 + 1000\nts_df[\"BB\"] = -norm.rvs(size=649) * 55\n\nts_df.set_index(\"Date\").plot(title=\"Comparaci√≥n distintas Tendencias\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#mosaico",
    "href": "tics411/notebooks/02-EDA.html#mosaico",
    "title": "EDA",
    "section": "",
    "text": "En muchas ocaciones nosotros queremos mostrar una compilaci√≥n de todos nuestros gr√°ficos m√°s que cada uno por separado. Para eso Matplotlib cuenta con la opci√≥n Mosaico.\nMosaico permite generar una grilla definida como un String. Si se fijan nuestra grilla se define por el string:\n\"\"\"AAA\n   BCC\"\"\"\nEn este caso nuestro canvas se divide en 6 partes, el gr√°fico que asigne a A utilizar√° las 3 secciones superiores, B utilizar√° s√≥lo la secci√≥n de abajo a la izquierda y C utilizar√° las 2 restantes.\nPara asignar cada secci√≥n .plot() de pandas posee el par√°metro ax donde se debe generar la asignaci√≥n.\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.subplot_mosaic(\n    \"\"\"AAA\n       BCC\"\"\"\n)\n\n# Gr√°fico asignado a C\niris_df.drop(columns=\"species\").plot(\n    kind=\"box\", ax=ax[\"C\"], title=\"Distribuci√≥n de Datos por Variable\"\n)\n\n## Gr√°fico asignado a B\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de P√©talo vs Ancho de P√©talo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n    ax=ax[\"B\"],\n)\n\n## Gr√°fico asignado a A\niris_df.groupby(\"species\").mean().plot(\n    kind=\"bar\",\n    edgecolor=\"black\",\n    ax=ax[\"A\"],\n    rot=0,\n    title=\"Valores promedio por Especie\",\n)\n\n## Permite Agregar un t√≠tulo general a todo el Gr√°fico\nplt.suptitle(\"Este ser√° un t√≠tulo para todo el Gr√°fico\", fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#matplotlib",
    "href": "tics411/notebooks/02-EDA.html#matplotlib",
    "title": "EDA",
    "section": "",
    "text": "Los comandos mostrados anteriormente son una adaptaci√≥n de Matplotlib a Pandas. La gracia que tienen es que son f√°ciles de aprender y funcionar√°n directamente en Pandas que ser√° nuestra principal fuente de datos.\nEn el caso de trabajar con Numpy, estos comandos NO FUNCIONAR√ÅN. Por lo tanto es necesario utilizar la API de Matplotlib. La traducci√≥n no es 100% directa, pero normalmente todos los par√°metros de .plot() se cambiar√°n por comandos del tipo plt.---"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#ejemplo",
    "href": "tics411/notebooks/02-EDA.html#ejemplo",
    "title": "EDA",
    "section": "",
    "text": "plt.plot(x,y, c = \"red\") #Existe tambi√©n plt.bar, plt.hist, plt.scatter, plt.boxplot.\nplt.title(\"Este va a ser un t√≠tulo\")\nplt.xlabel(\"Este ser√° una etiqueta del Eje X\")\nAprender Matplotlib es bastante m√°s complicado pero tiene funcionalidades much√≠simo m√°s avanzadas que Pandas. Para este curso, no ser√° necesario especializarse en Matplotlib, pero s√≠ m√°s adelante utilizaremos algunos gr√°ficos que no se pueden hacer tan f√°cilmente en Pandas (pero ser√°n casos puntuales)."
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html",
    "href": "tics411/notebooks/08-proyecto_clustering.html",
    "title": "Preparaci√≥n de los Datos",
    "section": "",
    "text": "# En caso que de ejecutar esto en Colab, van a tener que instalar Scikit-Plot para poder ver la curva de Silhouette.\n#!pip install scikit-plot\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\nRANDOM_STATE = 0\nnp.random.seed(RANDOM_STATE)\nN = np.random.randint(5, 15, size=1)[0]\nn_samples = np.random.randint(100, 1000, size=N)\nX, _ = make_blobs(\n    n_samples=n_samples,\n    n_features=9,\n    cluster_std=2.5,\n    random_state=RANDOM_STATE,\n)\ndf = pd.DataFrame(X)\ndict_cat = {\n    0: \"Cat 1\",\n    1: \"Cat 2\",\n    2: \"Cat 3\",\n}\nrng = np.random.default_rng()\ndf[\"cat_var\"] = rng.choice(a=[0, 1, 2], size=len(df), p=[0.2, 0.3, 0.5])\ndf[\"cat_var\"] = df[\"cat_var\"].map(dict_cat)\n\ndf.columns = [f\"x{i}\" for i, _ in enumerate(df.columns, start=1)]\ndf[\"x1\"] += 100\ndf[\"x5\"] *= 327\ndf[\"x9\"] /= 15\n\ndf.to_csv(\"proyecto_clustering.csv\", index=False)\n## Ac√° comienza oficialmente el c√≥digo.\ndf = pd.read_csv(\"proyecto_clustering.csv\")\ndf.dtypes.value_counts().plot(\n    kind=\"bar\", title=\"Tipos de Datos en el Dataset\", edgecolor=\"k\"\n)\nplt.tight_layout()\ndf.hist(figsize=(20, 6), edgecolor=\"k\", grid=False)\nplt.tight_layout()\ndf[\"x10\"].value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Distribuci√≥n de las Variables Categ√≥ricas\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#variables-categ√≥ricas",
    "href": "tics411/notebooks/08-proyecto_clustering.html#variables-categ√≥ricas",
    "title": "Preparaci√≥n de los Datos",
    "section": "Variables Categ√≥ricas",
    "text": "Variables Categ√≥ricas\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\ndummy_vars = ohe.fit_transform(df[[\"x10\"]])\n\nX = pd.concat([df.drop(columns=\"x10\"), dummy_vars], axis=1)\nX\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10_Cat 1\nx10_Cat 2\nx10_Cat 3\n\n\n\n\n0\n105.576134\n4.823419\n3.409904\n-11.687494\n-1532.613468\n-4.589218\n-6.854641\n-8.877022\n-0.449964\n0.0\n0.0\n1.0\n\n\n1\n100.479786\n-4.876628\n-5.404970\n6.932649\n-4092.341900\n12.163845\n-6.502116\n10.874025\n0.348683\n0.0\n0.0\n1.0\n\n\n2\n97.357744\n8.467431\n-0.865210\n4.353712\n1444.577125\n-1.992772\n-12.223474\n-9.100414\n0.407230\n0.0\n0.0\n1.0\n\n\n3\n95.857842\n5.931475\n0.278352\n3.413013\n1959.773064\n-10.248761\n-8.136656\n-9.158037\n0.478212\n0.0\n0.0\n1.0\n\n\n4\n99.772427\n-2.876912\n4.499859\n1.308382\n-2318.502069\n2.062409\n-13.469304\n-0.236395\n0.478002\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6418\n98.951606\n6.649525\n1.869195\n1.765821\n4348.322822\n-6.866256\n-1.578755\n-12.749590\n0.454056\n0.0\n0.0\n1.0\n\n\n6419\n94.996949\n-6.638457\n3.999433\n0.989885\n-1811.824888\n-0.859185\n-7.422772\n3.293839\n0.558469\n0.0\n0.0\n1.0\n\n\n6420\n95.495497\n6.664764\n0.019823\n1.825686\n2845.172238\n-7.376139\n-8.056029\n-10.066078\n0.224961\n0.0\n0.0\n1.0\n\n\n6421\n99.435967\n5.469512\n6.342347\n-1.182801\n-358.410366\n-1.205160\n2.248149\n6.840680\n0.748647\n0.0\n1.0\n0.0\n\n\n6422\n109.218858\n-6.367392\n-0.857113\n-6.749834\n1913.311965\n-4.103422\n0.343194\n-5.460592\n0.121518\n0.0\n0.0\n1.0\n\n\n\n\n6423 rows √ó 12 columns\n\n\n\n\n\npca = PCA(n_components=2, random_state=42)\npca_X = pca.fit_transform(X)\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nplt.title(\"Visualizaci√≥n PCA del Dataset\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "href": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "title": "Preparaci√≥n de los Datos",
    "section": "K-Means",
    "text": "K-Means\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"N√∫mero de Cl√∫sters\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X, k_max=20, color=\"blue\", title=\"Curva del Codo para K-Means\"\n)\n\n\n\n\n\n\n\n\n\nmetricas = dict()\n\n\nK_KMEANS = 10\nkm = KMeans(n_clusters=K_KMEANS, n_init=10, random_state=RANDOM_STATE)\nlabels_km = km.fit_predict(X)\n\n\ns_km = silhouette_score(X, labels_km)\nmetricas[\"km_10\"] = s_km\nmetricas\n\n{'km_10': 0.39419687509752793}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#jer√°rquico",
    "href": "tics411/notebooks/08-proyecto_clustering.html#jer√°rquico",
    "title": "Preparaci√≥n de los Datos",
    "section": "Jer√°rquico",
    "text": "Jer√°rquico\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, M√©todo: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlinkage_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in linkage_list:\n    plot_dendogram(X, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef train_hierarchical(K_H, linkage):\n    hc = AgglomerativeClustering(n_clusters=K_H, linkage=linkage)\n    labels_h = hc.fit_predict(X)\n    s_h = silhouette_score(X, labels_h)\n    print(f\"El coeficiente de Silueta es {s_h}\")\n    return labels_h, s_h\n\n\nlabels_c9, s_c9 = train_hierarchical(K_H=9, linkage=\"complete\")\nlabels_a9, s_a9 = train_hierarchical(K_H=9, linkage=\"average\")\nlabels_w4, s_w4 = train_hierarchical(K_H=4, linkage=\"ward\")\n\nEl coeficiente de Silueta es 0.37657025597625804\nEl coeficiente de Silueta es 0.3812263959798965\nEl coeficiente de Silueta es 0.2852126845283154\n\n\n\nmetricas[\"s_c9\"] = s_c9\nmetricas[\"s_a9\"] = s_a9\nmetricas[\"s_w4\"] = s_w4\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "href": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "title": "Preparaci√≥n de los Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = X.shape[1] + 1\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino m√°s cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino m√°s cercano\")\n    plt.title(f\"B√∫squeda de EPS para DBSCAN con k={k}\")\n\n\ndbscan_elbow_plot(X, k=MIN_SAMPLES)\n\nEPS = 1.6\n\n\n\n\n\n\n\n\n\ndbs = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\nlabels_dbs = dbs.fit_predict(X)\ns_dbs = silhouette_score(X, labels_dbs)\nmetricas[\"s_dbs\"] = s_dbs\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154,\n 's_dbs': 0.1818560991479739}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#evaluaci√≥n",
    "href": "tics411/notebooks/08-proyecto_clustering.html#evaluaci√≥n",
    "title": "Preparaci√≥n de los Datos",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\n\npd.Series(metricas.values(), index=metricas.keys()).plot(\n    kind=\"bar\",\n    rot=0,\n    edgecolor=\"k\",\n    title=\"Silhouette Score para los modelos generados\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nlabels_km\n\narray([5, 9, 2, ..., 2, 0, 1], dtype=int32)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i])\n    plt.suptitle(title, fontsize=15)\n    plt.tight_layout()\n\n\ncenter_analysis_viz(\n    df,\n    n_clusters=10,\n    labels=labels_km,\n    columns=num_vars,\n    title=\"An√°lisis de Centro\",\n)\n\n\n\n\n\n\n\n\n\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"], c=labels_km)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X, labels_km)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/13-ex-DT.html",
    "href": "tics411/notebooks/13-ex-DT.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\n\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.survived\n\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\n\ndef make_pipeline(parameters):\n    scaler = SklearnTransformerWrapper(\n        StandardScaler(), variables=parameters[\"sc_variables\"]\n    )\n\n    print(\n        f\"Entrenamiento para Decision Tree y threshold = {parameters['threshold']}\"\n    )\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\n                \"num_imp\",\n                MeanMedianImputer(\n                    imputation_method=parameters[\"num_method\"]\n                ),\n            ),\n            (\n                \"cat_imp\",\n                CategoricalImputer(\n                    imputation_method=parameters[\"cat_method\"]\n                ),\n            ),\n            (\"ohe\", parameters[\"encoder\"]),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    DecisionTreeClassifier(\n                        random_state=42,\n                        min_samples_leaf=parameters[\"min_samples_leaf\"],\n                        min_samples_split=parameters[\"min_samples_split\"],\n                        max_depth=parameters[\"max_depth\"],\n                    ),\n                    threshold=parameters[\"threshold\"],\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\ndef show_tree(pipe, class_names=[\"No\", \"S√≠\"], figsize=(20, 6)):\n    feature_names = pipe[-2].feature_names_in_\n    plt.figure(figsize=(20, 6))\n    plot_tree(\n        pipe[-1].estimator_,\n        filled=True,\n        feature_names=feature_names,\n        class_names=class_names,\n    )\n    plt.show()\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.9805389221556886\nTest Accuracy 0.7533632286995515\n===================================\nTrain Precision 0.9918032786885246\nTest Precision 0.6888888888888889\n===================================\nTrain Recall 0.9565217391304348\nTest Recall 0.6966292134831461\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.8488023952095808\nTest Accuracy 0.8071748878923767\n===================================\nTrain Precision 0.8333333333333334\nTest Precision 0.8194444444444444\n===================================\nTrain Recall 0.7509881422924901\nTest Recall 0.6629213483146067\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.2,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.2\n===================================\nTrain Accuracy 0.8068862275449101\nTest Accuracy 0.7892376681614349\n===================================\nTrain Precision 0.6962025316455697\nTest Precision 0.6944444444444444\n===================================\nTrain Recall 0.8695652173913043\nTest Recall 0.8426966292134831\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.9,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.9\n===================================\nTrain Accuracy 0.8173652694610778\nTest Accuracy 0.7713004484304933\n===================================\nTrain Precision 0.9851851851851852\nTest Precision 0.8958333333333334\n===================================\nTrain Recall 0.525691699604743\nTest Recall 0.48314606741573035\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=0.1,\n    min_samples_split=2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.2,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.2\n===================================\nTrain Accuracy 0.6212574850299402\nTest Accuracy 0.6143497757847534\n===================================\nTrain Precision 0.5\nTest Precision 0.50920245398773\n===================================\nTrain Recall 0.924901185770751\nTest Recall 0.9325842696629213\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=0.2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.8023952095808383\nTest Accuracy 0.7757847533632287\n===================================\nTrain Precision 0.9290780141843972\nTest Precision 0.8679245283018868\n===================================\nTrain Recall 0.5177865612648221\nTest Recall 0.5168539325842697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/14-ex-NB.html",
    "href": "tics411/notebooks/14-ex-NB.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\n\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\"]]\ny = df.survived\n\n\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\n\n\ndef make_pipeline(parameters):\n    if parameters[\"sc_variables\"] == \"all\":\n        scaler = StandardScaler()\n    elif parameters[\"sc_variables\"] is not None:\n        scaler = SklearnTransformerWrapper(\n            StandardScaler(), variables=parameters[\"sc_variables\"]\n        )\n    else:\n        scaler = \"passthrough\"\n\n    if parameters[\"num_method\"] is None:\n        num_imp = \"passthrough\"\n    else:\n        num_imp = MeanMedianImputer(\n            imputation_method=parameters[\"num_method\"]\n        )\n\n    if parameters[\"cat_method\"] is None:\n        cat_imp = \"passthrough\"\n    else:\n        cat_imp = CategoricalImputer(\n            imputation_method=parameters[\"cat_method\"]\n        )\n\n    print(\n        f\"Entrenamiento para Naive Bayes y threshold = {parameters['threshold']}\"\n    )\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\"cat_imp\", cat_imp),\n            (\"num_imp\", num_imp),\n            (\"ohe\", parameters[\"encoder\"]),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    parameters[\"model\"],\n                    threshold=parameters[\"threshold\"],\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=MultinomialNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7844311377245509\nTest Accuracy 0.757847533632287\n===================================\nTrain Precision 0.7137254901960784\nTest Precision 0.6732673267326733\n===================================\nTrain Recall 0.7193675889328063\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7784431137724551\nTest Accuracy 0.7488789237668162\n===================================\nTrain Precision 0.6996197718631179\nTest Precision 0.6601941747572816\n===================================\nTrain Recall 0.7272727272727273\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=\"all\",\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7784431137724551\nTest Accuracy 0.7488789237668162\n===================================\nTrain Precision 0.6996197718631179\nTest Precision 0.6601941747572816\n===================================\nTrain Recall 0.7272727272727273\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"age\", \"fare\"]]\ny = df.survived\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=\"mean\",\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=MultinomialNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.6811377245508982\nTest Accuracy 0.7309417040358744\n===================================\nTrain Precision 0.6063829787234043\nTest Precision 0.7230769230769231\n===================================\nTrain Recall 0.4505928853754941\nTest Recall 0.5280898876404494\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=\"mean\",\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7844311377245509\nTest Accuracy 0.7623318385650224\n===================================\nTrain Precision 0.7056603773584905\nTest Precision 0.6730769230769231\n===================================\nTrain Recall 0.7391304347826086\nTest Recall 0.7865168539325843\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html",
    "href": "tics411/notebooks/pandas_basics.html",
    "title": "Seleccionar Filas, y columnas‚Ä¶",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df.shape, titanic_df.columns\n\n((891, 15),\n Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n        'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n        'alive', 'alone'],\n       dtype='object'))\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\ntitanic_df[\"survived_new\"] = titanic_df[\"survived\"].astype(\"float64\")\ntitanic_df\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\nsurvived_new\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n1.0\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n1.0\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n1.0\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n1.0\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n1.0\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n0.0\n\n\n\n\n891 rows √ó 16 columns"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html#seleccionar-filas-y-columnas",
    "href": "tics411/notebooks/pandas_basics.html#seleccionar-filas-y-columnas",
    "title": "Seleccionar Filas, y columnas‚Ä¶",
    "section": "Seleccionar Filas, y columnas‚Ä¶",
    "text": "Seleccionar Filas, y columnas‚Ä¶\n\n## Mostrar la diferencia entre una Serie y un DataFrame.\ntitanic_df.loc[10]\n\nsurvived                 1\npclass                   3\nsex                 female\nage                    4.0\nsibsp                    1\nparch                    1\nfare                  16.7\nembarked                 S\nclass                Third\nwho                  child\nadult_male           False\ndeck                     G\nembark_town    Southampton\nalive                  yes\nalone                False\nName: 10, dtype: object\n\n\n\ntitanic_df[\"embark_town\"].to_frame()\n\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\nSouthampton\n\n\n1\nCherbourg\n\n\n2\nSouthampton\n\n\n3\nSouthampton\n\n\n4\nSouthampton\n\n\n...\n...\n\n\n886\nSouthampton\n\n\n887\nSouthampton\n\n\n888\nSouthampton\n\n\n889\nCherbourg\n\n\n890\nQueenstown\n\n\n\n\n891 rows √ó 1 columns\n\n\n\n\n\n## Explicar que va una lista de elementos... no es un \"doble\" par√©ntesis.\ntitanic_df[[\"embark_town\", \"class\"]]\n\n\n\n\n\n\n\n\n\nembark_town\nclass\n\n\n\n\n0\nSouthampton\nThird\n\n\n1\nCherbourg\nFirst\n\n\n2\nSouthampton\nThird\n\n\n3\nSouthampton\nFirst\n\n\n4\nSouthampton\nThird\n\n\n...\n...\n...\n\n\n886\nSouthampton\nSecond\n\n\n887\nSouthampton\nFirst\n\n\n888\nSouthampton\nThird\n\n\n889\nCherbourg\nFirst\n\n\n890\nQueenstown\nThird\n\n\n\n\n891 rows √ó 2 columns\n\n\n\n\n\ntitanic_df.loc[[10, 15], [\"embark_town\", \"fare\", \"age\"]]\n\n\n\n\n\n\n\n\n\nembark_town\nfare\nage\n\n\n\n\n10\nSouthampton\n16.7\n4.0\n\n\n15\nSouthampton\n16.0\n55.0\n\n\n\n\n\n\n\n\n\ntitanic_df_shuffle = titanic_df.sample(frac=1)\ntitanic_df_shuffle\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n133\n1\n2\nfemale\n29.0\n1\n0\n26.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n748\n0\n1\nmale\n19.0\n1\n0\n53.1000\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n876\n0\n3\nmale\n20.0\n0\n0\n9.8458\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n226\n1\n2\nmale\n19.0\n0\n0\n10.5000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n342\n0\n2\nmale\n28.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283\n1\n3\nmale\n19.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n863\n0\n3\nfemale\nNaN\n8\n2\n69.5500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n124\n0\n1\nmale\n54.0\n0\n1\n77.2875\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n583\n0\n1\nmale\n36.0\n0\n0\n40.1250\nC\nFirst\nman\nTrue\nA\nCherbourg\nno\nTrue\n\n\n85\n1\n3\nfemale\n33.0\n3\n0\n15.8500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n\n\n891 rows √ó 15 columns\n\n\n\n\n\n# Esto es un error... si es que no se separa...\ntitanic_df_shuffle.iloc[3][[\"who\", \"adult_male\"]]\n\nwho            man\nadult_male    True\nName: 226, dtype: object\n\n\n\n## Algunos m√©todos importante...\ntitanic_df.describe(percentiles=[0.05, 0.25, 0.75, 0.95])\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n5%\n0.000000\n1.000000\n4.000000\n0.000000\n0.000000\n7.225000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\n95%\n1.000000\n3.000000\n56.000000\n3.000000\n2.000000\n112.079150\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n\ntitanic_df.mean(numeric_only=True)\n\nsurvived       0.383838\npclass         2.308642\nage           29.699118\nsibsp          0.523008\nparch          0.381594\nfare          32.204208\nadult_male     0.602694\nalone          0.602694\ndtype: float64\n\n\n\ntitanic_df.median(numeric_only=True)\n\nsurvived       0.0000\npclass         3.0000\nage           28.0000\nsibsp          0.0000\nparch          0.0000\nfare          14.4542\nadult_male     1.0000\nalone          1.0000\ndtype: float64\n\n\n\ntitanic_df.mode()\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n24.0\n0\n0\n8.05\nS\nThird\nman\nTrue\nC\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n\nMostrar que este tipo de m√©todos tambi√©n se pueden utilizar en Series."
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html#agrupar",
    "href": "tics411/notebooks/pandas_basics.html#agrupar",
    "title": "Seleccionar Filas, y columnas‚Ä¶",
    "section": "Agrupar",
    "text": "Agrupar\n\ndf = pd.DataFrame(\n    dict(a=[1, 1, 1, 1, 2, 2, 2, 2], b=[1, 2, 3, 4, 5, 6, 7, 8])\n)\n\ndf\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1\n\n\n1\n1\n2\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n2\n5\n\n\n5\n2\n6\n\n\n6\n2\n7\n\n\n7\n2\n8\n\n\n\n\n\n\n\n\n\nfor i in [df.shape, df.columns, df.index, df.dtypes]:\n    print(i)\n\n(8, 2)\nIndex(['a', 'b'], dtype='object')\nRangeIndex(start=0, stop=8, step=1)\na    int64\nb    int64\ndtype: object\n\n\n\ngroups = df.groupby(\"a\")\nfor id, g in groups:\n    print(f\"Este es el grupo: {id}\")\n    display(g)\n\nEste es el grupo: 1\nEste es el grupo: 2\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1.0\n\n\n1\n1\n2.0\n\n\n2\n1\n3.0\n\n\n3\n1\n4.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n4\n2\n5.0\n\n\n5\n2\n6.0\n\n\n6\n2\n7.0\n\n\n7\n2\n8.0\n\n\n\n\n\n\n\n\n\ndf.groupby(\"a\")[\"b\"].mean()\n\na\n1    2.5\n2    6.5\nName: b, dtype: float64\n\n\n\ntitanic_df.groupby(\"sex\")[\"fare\"].mean()\n\nsex\nfemale    44.479818\nmale      25.523893\nName: fare, dtype: float64\n\n\n\ntitanic_df.groupby([\"sex\", \"pclass\"])[[\"age\", \"fare\"]].median()\n\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\nsex\npclass\n\n\n\n\n\n\nfemale\n1\n35.0\n82.66455\n\n\n2\n28.0\n22.00000\n\n\n3\n21.5\n12.47500\n\n\nmale\n1\n40.0\n41.26250\n\n\n2\n30.0\n13.00000\n\n\n3\n25.0\n7.92500"
  },
  {
    "objectID": "tics411/clase-1.html#avisos-1",
    "href": "tics411/clase-1.html#avisos-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Avisos",
    "text": "Avisos\n\n\n\n\n\n\nAyudant√≠a\n\n\nAvisos\nTenemos (posible) ayudante, pero tenemos un problema de horario.\n\nHorario Actual: Viernes 20:00 a 21:10 hrs.\nHorario Propuesto: Lunes 11:45 a 12:55 hrs.\n\n\n\n\n\n\n\n\n\n\n\n\nTarea 1\n\n\n\nEntrega el 7 de Abril: Parejas inscribirse en Webcursos.\nPlazo para inscribir parejas: Este Domingo.\n\n\n\n\n\n\n\n\n\n\n\n\nFechas de Prueba\n\n\n\nPrueba 1: Martes 30 de Abril 18:30 a 21:00\nPrueba 2: Martes 11 de Julio 18:30 a 21:00"
  },
  {
    "objectID": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, registros, instancias. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos m√°s amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % baj√≠simo del total de datos existentes en el Mundo.\nDistintos tipos, por lo que normalmente requiere de alg√∫n tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-num√©ricos",
    "href": "tics411/clase-1.html#data-types-num√©ricos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Data Types: Num√©ricos",
    "text": "Data Types: Num√©ricos\n\nNum√©ricos\n\n\nValores a los que se les puede aplicar alguna operaci√≥n matem√°tica.\n\n\n\n\n\n\n\n\n\n\nDiscretas: N√∫mero finito o contable de valores. Integers (Enteros). Ej: N√∫mero de Hijos, Cantidad de Productos, Edad.\nContinuas: Existen infinitos puntos entre dos puntos. Floats (punto flotando o decimales). Ej. Temperatura, Peso."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-categ√≥ricos",
    "href": "tics411/clase-1.html#data-types-categ√≥ricos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Data Types: Categ√≥ricos",
    "text": "Data Types: Categ√≥ricos\n\nCateg√≥ricos\n\n\nDatos que representan una categor√≠a.\n\n\n\n\n\n\n\n\n\n\nNominales: S√≥lo nombres que no representan ning√∫n orden. Ej: Nacionalidad, g√©nero, ocupaci√≥n.\nOrdinales: Que tienen un orden o jerarqu√≠a inherente. Ej: Nivel de Escolaridad, tama√±o.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo todas las operaciones matem√°ticas son aplicables. Ej: Media, Mediana, Sumas, Restas, etc."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-otros",
    "href": "tics411/clase-1.html#data-types-otros",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Data Types: Otros",
    "text": "Data Types: Otros\n\nStrings\n\n\nDatos de texto, los cuales podr√≠an eventualmente ser tratados y representar algo. Ej: Rescatar comunas de una direcci√≥n, rescatar sexo desde el nombre, etc.\n\n\nFechas\n\n\nDatos tipo fecha, los cuales podr√≠an eventualmente ser tratados y representar variables de alg√∫n tipo. Ej: Rescatar A√±os, meses, d√≠as, semanas, trimestres (quarters), etc.\n\n\nDatos Geogr√°ficos\n\n\nDatos que representan la ubicaci√≥n geogr√°fica de un elemento. Ej: Latitud, Longitud, Coordenadas.\n\n\n\n\n\n\n\n\n\n\nSin importar el tipo de dato el mayor problema es su calidad."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "href": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Calidad de los Datos: Ruido",
    "text": "Calidad de los Datos: Ruido\n\nRuido\n\nCorresponde al error y extrema variabilidad en la medici√≥n en los datos. Este error puede ser aleatorio o sistem√°tico.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe le llama Se√±al a la tendencia principal y representa la informaci√≥n significativa y valiosa de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "href": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Calidad de los Datos: Outliers",
    "text": "Calidad de los Datos: Outliers\n\nOutliers\n\nSon datos considerablemente diferentes a la mayor√≠a del dataset. Dependiendo del caso pueden indicar casos \"interesantes\" o errores de medici√≥n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que dependiendo del caso puede ser una buena idea deshacerse de ellos. ¬øEn qu√© casos podr√≠a no ser necesario eliminarlos?"
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "href": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Calidad de los Datos: Valores Faltantes",
    "text": "Calidad de los Datos: Valores Faltantes\n\nMissing Values\n\n\nSon valores que por alguna raz√≥n no est√°n presentes.\n\n\n\n\n\nMissing at Random (MAR): Son valores que no est√°n presentes por causas que no se pueden controlar. Ej: No se registr√≥, no se pregunt√≥, fallas en el sistema de recolecci√≥n de datos, etc.\nInformative Missing: Es un valor no aplicable. Ej: Sueldo en ni√±os, Precio de la entrada de un concierto si es que NO compr√≥ entrada."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "href": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Calidad de los Datos: Datos Duplicados",
    "text": "Calidad de los Datos: Datos Duplicados\n\nDuplicates\n\nSe refiere a registros que pueden estar total o parcialmente duplicados.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsto genera problemas en la confiabilidad de los datos. ¬øCu√°l es el registro correcto?\nEj: Caso particular de una Jooycar (una startup de seguros)."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "href": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Calidad de los Datos: Dominio del Problema",
    "text": "Calidad de los Datos: Dominio del Problema\n\n\n\n\n\n\n\n\n\n\n\n\n\nPor lejos el problema de calidad m√°s dif√≠cil de encontrar.\nSe requiere experiencia y conocimiento profundo del negocio para detectarlo.\n\nEj: Caso de Super Avances en Cencosud."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-1",
    "href": "tics411/clase-1.html#feature-engineering-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nFeature Engineering\n\n\nTambi√©n conocida como Ingenier√≠a de Atributos, es el arte de trabajar las features existentes para limpiar o corregir variables existentes o crear nuevas variables.\n\n\nPreprocesamiento\n\n\nSe refiere al proceso de preparaci√≥n de los datos para su ingreso a un modelo. En una primera parte puede incluir limpieza de datos corruptos, redundantes y/o irrelevantes. Por otra parte, tambi√©n hace referencia a la transformaci√≥n de datos para que puedan ser consumidos por un algoritmo."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-2",
    "href": "tics411/clase-1.html#feature-engineering-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nNo existe un procedimiento est√°ndar.\nRevisar los datos y ver potenciales errores que puedan afectar el funcionamiento de un modelo."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "href": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Preprocesamiento: Valores Faltantes",
    "text": "Preprocesamiento: Valores Faltantes\n\nImputaci√≥n: Se refiere al proceso de rellenar datos faltantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependiendo del nivel de valores faltantes, es necesario evaluar la eliminaci√≥n de registros o atributos completos de ser necesario."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Preprocesamiento: Manejo de Outliers",
    "text": "Preprocesamiento: Manejo de Outliers\n\nCapping\n\nSe refiere al proceso de acotar un atributo eliminando los valores extremos o at√≠picos (outliers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAl igual que en el caso anterior, es necesario evaluar la eliminaci√≥n de registros si es que representan valores at√≠picos."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categ√≥ricas",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categ√≥ricas",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Preprocesamiento: Manejo de Variables Categ√≥ricas",
    "text": "Preprocesamiento: Manejo de Variables Categ√≥ricas\n\nLa mayor√≠a de los modelos no tienen la capacidad de poder lidiar con variables categ√≥ricas por lo que deben ser transformadas en una representaci√≥n num√©rica antes de ingresar a un modelo.\n\n\n\n\n\n\nOne Hot Encoder\n\n\n\n\n\n\nOrdinal Encoder\n\n\n\n\n\n\n\n\n\n\n\n\nOne Hot Encoder suele dar mejores resultados en modelos lineales modelos que dependan de distancias.\nOrdinal Encoder suele dar mejores resultados en modelos de √°rbol.\n\n\n\n\n\n\n\n¬øSon necesarias todas las columnas en un One Hot Encoder?"
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\nEl escalamiento se refiere al proceso de llevar distintas variables a una misma escala.\n\n\n\n\n\n\n\n\n\nEvitar que la escala de una ‚Äúsobre-importancia‚Äù a una cierta variable.\nPermitir una mejor convergencia de los algoritmos.\n\n\n\nStandardScaler (Normalizaci√≥n)\n\\[x_j=\\frac{x_j-\\mu_x}{\\sigma_x}\\]\n\n\n\n\n\n\n\nEste proceso fuerza (en la medida de lo posible) a tener media 0 y std 1.\nNotar que \\(\\sigma_x\\) hace referencia a la varianza poblacional.\n\n\n\n\nMinMax Scaler\n\\[x_j=\\frac{x_j-min(x_j)}{max(x_j)-min(x_j)}\\]\n\n\n\n\n\n\nEste proceso fuerza a los datos a distribuirse entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia: 0.75\nStd: 3.1875\nMin: -2\nMax: 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentering (Centrado): Se le llama a la diferencia entre la variable y su media.\nScaling (Escalado): Se le llama al cuociente entre la variable y su Desviaci√≥n Est√°ndar.\nStandardScaler (Normalizaci√≥n): Es Centrado y Escalado."
  },
  {
    "objectID": "tics411/clase-1.html#creaci√≥n-de-variables",
    "href": "tics411/clase-1.html#creaci√≥n-de-variables",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Creaci√≥n de Variables",
    "text": "Creaci√≥n de Variables\n\nCombinaci√≥n\n\n\nCombinar 2 o m√°s variables. Ej: Calcular el √°rea de un sitio a partir del ancho y largo.\n\n\nTransformaci√≥n\n\n\nAplicar una operaci√≥n a una variable. Ej: El logaritmo de las ganancias.\n\n\n\n\n\n\nDiscretizaci√≥n (Binning)\n\n\nGenerar categor√≠as a partir de una variable continua."
  },
  {
    "objectID": "tics411/clase-1.html#creaci√≥n-de-variables-1",
    "href": "tics411/clase-1.html#creaci√≥n-de-variables-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Creaci√≥n de Variables",
    "text": "Creaci√≥n de Variables\n\nRatios\n\nEs una medida que expresa la relaci√≥n entre dos cantidades. Ej: Puntos por partido, cantidad de transacciones por mes, etc.\n\nAgregaci√≥n\n\nAgregar o agrupar informaci√≥n resumida de ciertas variables. Ej: Promedio de tiempo en aprobar un tipo de cr√©dito."
  },
  {
    "objectID": "tics411/clase-1.html#selecci√≥n-de-variables",
    "href": "tics411/clase-1.html#selecci√≥n-de-variables",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Selecci√≥n de Variables",
    "text": "Selecci√≥n de Variables\n\nSe refiere al proceso de eliminar variables que pueden ser irrelevantes o poco significativas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcesos Manuales.\nProcesos Autom√°ticos:\n\nPCA (Principal Component Analysis).\nRecursive Feature Elimination.\nRecursive Feature Addition.\nEliminaci√≥n mediante alguna medida.\n\n\n\n\n\n\n\n\n\n\n\n\nObjetivo\n\n\n\nPuede ser una t√©cnica apropiada para combatir la Maldici√≥n de la Dimensionalidad (Curse of Dimensionality)."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-1",
    "href": "tics411/clase-1.html#medidas-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas",
    "text": "Medidas\n\nSon m√©tricas que permiten cuantificar la relaci√≥n existente entre dos o m√°s objetos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad",
    "href": "tics411/clase-1.html#medidas-similaridad",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad",
    "text": "Medidas: Similaridad"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-nominal",
    "href": "tics411/clase-1.html#medidas-similaridad-nominal",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Nominal",
    "text": "Medidas: Similaridad Nominal\n\n\n\nDisimilaridad: \\[D =\n\\begin{cases}\n0,  & \\text{if $p=q$} \\\\[2ex]\n1, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\nSimilaridad:\n\\[S =\n\\begin{cases}\n1,  & \\text{if $p=q$} \\\\[2ex]\n0, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 0\\] \\[D(p,q) = 1\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "href": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Ordinal",
    "text": "Medidas: Similaridad Ordinal\n\n\n\nDisimilaridad: \\[D = \\frac{|p-q|}{n}\\]\n\n\n\nSimilaridad:\n\\[S = 1 - \\frac{|p-q|}{n}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 1 - \\frac{5 - 4}{5} = 0.8\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "href": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Intervalo o Ratio",
    "text": "Medidas: Similaridad Intervalo o Ratio\n\n\n\nDisimilaridad: \\[D = |p-q|\\]\n\n\n\nSimilaridad:\n\\[S = -D\\] \\[S = \\frac{1}{1+D}\\]\n\n\n\n\nSea \\(p=35 ¬∞C\\) y \\(q = 40 ¬∞C\\). Luego:\n\\[ S(p,q) = -5\\] \\[S(p,q) = \\frac{1}{1 + 5} = 0.17\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categ√≥ricos",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categ√≥ricos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Datos Categ√≥ricos",
    "text": "Medidas: Similaridad Datos Categ√≥ricos\n\nSea p y q vectores de dimensi√≥n \\(m\\) con s√≥lo atributos categ√≥ricos. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\\[Sim(p,q) = \\sum_{i=1}^m S(p_i,q_i)\\]\n\n\n\n\nOverlap: \\[S(p_{a_i}, q_{a_i}) =\n\\begin{cases}\n1,  & \\text{if $p_{a_i} = q_{a_i}$} \\\\[2ex]\n0, & \\text{if $p_i\\neq q_i$}\n\\end{cases}\n\\]\n\n\n\nFrecuencia de Ocurrencia Inversa \\[S(p_i, q_i) = \\frac{1}{p_k(p_i)^2}\\]\n\n\n\nMedida de Goodall\n\n\\[S(p_i, q_i) = 1 - p_k(p_i)^2\\]\n\n\n\n\n\n\n\n\n\n\\(p_k()\\) se refiere a la probabilidad de ocurrencia del atributo k.\nTodas estas medidas son 0 si \\(p_i \\neq q_i\\)"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categ√≥ricos-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categ√≥ricos-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Datos Categ√≥ricos",
    "text": "Medidas: Similaridad Datos Categ√≥ricos\n\n\n\n\n\nEjercicio Propuesto: ¬øCu√°nto vale la similaridad entre los siguientes registros?\n\n1-4\n2-5\n7-8"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\nSea p y q vectores de dimensi√≥n \\(m\\) con s√≥lo atributos binarios. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}}\\]\n\nSimple Matching Coefficient = N√∫mero de Coincidencias / Total de Atributos\n\n\n\\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}}\\]\n\nJaccard Coefficient = N√∫mero de Coincidencias 11 / N√∫mero de Atributos distintos de Ceros."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\np_i\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nq_i\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}} = \\] \\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}} = \\]\n\n\n\\[\\frac{7 + 0}{7 + 2 + 1 + 0} = 0.7\\]\n\n\n\\[\\frac{0}{2 + 1 + 0} = 0\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\nSean \\(d_1\\) y \\(d_2\\) dos vectores. La distancia coseno se calcula como:\n\n\\[cos(d_1, d_2) = \\frac{d_1 \\cdot d_2}{||d_1||||d_2||}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\nEjercicio Propuesto: ¬øCu√°nto vale \\(cos(d_1,d_2)\\) y \\(cos(d_1,d_3)\\)?"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-1",
    "href": "tics411/clase-1.html#distancias-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Distancias",
    "text": "Distancias\n\nUna m√©trica o funci√≥n de distancia es una funci√≥n que define una distancia para cada par de elementos de un conjunto. Sean dos puntos x e y, una m√©trica o funci√≥n de distancia debe satisfacer las siguientes condiciones:\n\n\nNo Negatividad:\n\n\\(d(x,y) = \\ge 0\\)\n\nIdentidad:\n\n\\(d(x,y) = 0 \\Leftrightarrow x = y\\)\n\nSimetr√≠a:\n\n\\(d(x,y) = d(y,x)\\)\n\nDesigualdad Triangular:\n\n\\(d(x,z) \\le d(x,y) + d(y,z)\\)"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Distancias: Distancia Minkowski",
    "text": "Distancias: Distancia Minkowski\n\\[d(p,q) = \\left(\\sum_{k=1}^m |p_k - q_k|^r\\right)^{1/r}\\]\n\n\n\n\n\n\n\n\n\n\\(r=1 \\rightarrow\\) Distancia Manhattan (L1).\n\\(r=2 \\rightarrow\\) Distancia Euclideana (L2).\n\\(r=\\infty \\rightarrow\\) Distancia Chebyshev (L\\(\\infty\\)). \\[D_{ch}(p,q) = \\underset{k}{max} |p_k - q_k|\\]\n\n\n\n\n\n\n\nResolvamos en Colab\n\n\n\n\n\n\n\n\n\n\n\n\nSe denomina Matriz de Distancias a la Matriz que contiene la distancia \\(d(p_i,p_j)\\) en la coordenada \\(i,j\\)."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Distancias: Distancia Minkowski (Resultados)",
    "text": "Distancias: Distancia Minkowski (Resultados)"
  },
  {
    "objectID": "tics411/clase-1.html#ayudant√≠as",
    "href": "tics411/clase-1.html#ayudant√≠as",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ayudant√≠as",
    "text": "Ayudant√≠as\nAyudante: Sof√≠a Alvarez\nemail: sofalvarez@alumnos.uai.cl\n\n\n\n\n\n\n\nLas ayudant√≠as ser√°n en la manera que sean necesarias.\nEstar√°n enfocadas principalmente en aplicaciones, c√≥digo y dudas sobre Tarea."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Distancias: Distancia Mahalanobis",
    "text": "Distancias: Distancia Mahalanobis\n\\[d(p,q) = \\sqrt{(p-q)^T \\Sigma^{-1}(p-q)}\\]\ndonde \\(\\Sigma\\) es la Matriz de Covarianza de los datos de entrada.\n\\[cov(x,y) = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nPara 2 variables p y q:\n\n\\[\\Sigma = \\begin{bmatrix}\ncov(p,p) & cov(p,q) \\\\\ncov(q,p) & cov(q,q)\n\\end{bmatrix}\n\\]\nEjercicio: Supongamos las siguientes escalas de notas. Calcular la distancia entre la nota (1.0 y 7.0)\n\ntest #1: 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0\ntest #2: 1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Distancias: Distancia Mahalanobis (Resultados)",
    "text": "Distancias: Distancia Mahalanobis (Resultados)\n\n\n\n\n\n\ntest #1: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{3.79}(7-1)} = 3.08\\)\ntest #2: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{1.59}(7-1)} = 4.76\\)\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que la covarianza existente entre los datos influye en la distancia."
  },
  {
    "objectID": "tics411/clase-1.html#correlaci√≥n-1",
    "href": "tics411/clase-1.html#correlaci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Correlaci√≥n",
    "text": "Correlaci√≥n\n\nLa correlaci√≥n mide la relaci√≥n lineal entre 2 atributos.\n\n\n\n\nCorrelaci√≥n Poblacional\n\n\\[\\rho(X,Y) = corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}\\]\n\n\n\n\nCorrelaci√≥n Muestral o Pearson\n\n\\[r(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{S_xS_y}\\]"
  },
  {
    "objectID": "tics411/clase-1.html#correlaci√≥n-no-es-causalidad",
    "href": "tics411/clase-1.html#correlaci√≥n-no-es-causalidad",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Correlaci√≥n no es Causalidad",
    "text": "Correlaci√≥n no es Causalidad\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante recalcar que Causalidad no es igual a Correlaci√≥n. Ver video.\nLa Correlaci√≥n no se ve afectada por la escala de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categ√≥ricos-2",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categ√≥ricos-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad Datos Categ√≥ricos",
    "text": "Medidas: Similaridad Datos Categ√≥ricos"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\n\\[d_1 \\cdot d_2 = 5\\] \\[d_1 \\cdot d_3 = 84\\]\n\\[||d_1|| = \\sqrt{42} = 6.481\\] \\[||d_2|| = \\sqrt{6} = 2.449\\] \\[||d_3|| = \\sqrt{168} = 12.962\\]\n\\[cos(d_1, d_2) = 0.3150\\] \\[cos(d_1, d_3) = 0.9999\\]\n\n\n\n\n\n\n\n\nOne Hot Encoder\nOrdinal Encoder"
  },
  {
    "objectID": "tics411/clase-6.html#evaluaci√≥n",
    "href": "tics411/clase-6.html#evaluaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\n\nPensemos en la Evaluaci√≥n como una medida de desempe√±o el cu√°l ‚Äúeval√∫a‚Äù qu√© tan bien realizado est√° el clustering. El objetivo principal del Clustering debe ser siempre la generaci√≥n de clusters compactos que est√©n diferenciados los unos a los otros.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øCu√°l es el Clustering que mejor describe el problema."
  },
  {
    "objectID": "tics411/clase-6.html#objetivos-de-la-evaluaci√≥n",
    "href": "tics411/clase-6.html#objetivos-de-la-evaluaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Objetivos de la Evaluaci√≥n",
    "text": "Objetivos de la Evaluaci√≥n"
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins",
    "href": "tics411/clase-6.html#tendencia-hopkins",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins\n\nEstad√≠stico Hopkins\n\n\nPermite evaluar a priori si es que efectivamente existen clusters antes de aplicar un algoritmo.\n\n\n\n\n\n\\[H = \\frac{\\sum_{i = 1}^p w_i}{\\sum_{i = 1}^p u_i + \\sum_{i = 1}^p w_i}\\]\n\n\\(w_i\\): corresponde a la distancia de un punto aleatorio al vecino m√°s cercano en los datos originales.\n\\(u_i\\): corresponde a la distancia de un punto real del dataset al vecino m√°s cercano.\n\\(p\\): N√∫mero de puntos generados en el espacio del Dataset.\n\n\nfrom pyclustertend import hopkins\n\n1-hopkins(X, p)\n\n\n\n\n\n\n\nX: Dataset al cu√°l se le aplica el Estad√≠stico.\np: N√∫mero de Puntos para el c√°lculo.\n\n\n\n\n\n\n\n\n\n\npyclustertend entrega el valor 1-H."
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins-1",
    "href": "tics411/clase-6.html#tendencia-hopkins-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins"
  },
  {
    "objectID": "tics411/clase-6.html#c√°lculo-hopkins-ejemplo-p2",
    "href": "tics411/clase-6.html#c√°lculo-hopkins-ejemplo-p2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "C√°lculo Hopkins: Ejemplo p=2",
    "text": "C√°lculo Hopkins: Ejemplo p=2\n\n\n\n\n\n\n\nPuntos obtenidos de los Datos\n\\[u_1\\approx 0\\]\n\\[u_2\\approx 0\\]\n\nPuntos Aleatorios en el Espacio de los Datos\n\\[w_1\\approx 1.8\\]\n\\[w_2\\approx 1.12\\]\n\nC√°lculo Hopkins\n\\[ H = \\frac{w_1 + w_2}{u_1 + u_2 + w_1 + w_2}\\] \\[ H = \\frac{1.8 + 1.12}{0 + 0 + 1.8 + 1.8} = 1\\]"
  },
  {
    "objectID": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "href": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Visual Assesment of Tendency (VAT)",
    "text": "Visual Assesment of Tendency (VAT)\n\nCorresponde a una inspecci√≥n visual de la distancia entre los puntos (matriz de distancia). Colores m√°s oscuros indican menor distancias entre dichos puntos lo que indica mayor cohesi√≥n.\n\n\n\n\n\n\n\n\n\nSe pueden ver claramente dos bloques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo es posible ver bloques importantes.\n\n\n\n\n\n\n\n\n\n\nfrom pyclustertend import vat\n\nvat(X)"
  },
  {
    "objectID": "tics411/clase-6.html#correlaci√≥n",
    "href": "tics411/clase-6.html#correlaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Correlaci√≥n",
    "text": "Correlaci√≥n\nProcedimiento:\n\nConstruir una matriz de similaridad entre todos los puntos de la siguiente manera:\n\n\\[s(i,j) = \\frac{1}{d(i,j) + 1}\\]\n\nConstruir una matriz de similaridad \"ideal\" basada en la pertenencia a un Cluster.\nSi \\(i\\) y \\(j\\) pertenecen al mismo cluster entonces \\(s(i,j)=1\\), en otro caso \\(s(i,j) = 0\\)\nCalcular la Correlaci√≥n entre la matriz de similaridad y la matriz ideal (obtenidas en los pasos 1 y 2).\n\n\n\n\n\n\n\nUna correlaci√≥n alta indica que los puntos que est√°n en el mismo cluster son cercanos entre ellos."
  },
  {
    "objectID": "tics411/clase-6.html#cohesi√≥n",
    "href": "tics411/clase-6.html#cohesi√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Cohesi√≥n",
    "text": "Cohesi√≥n\n\nCohesi√≥n\n\n\nMide cu√°n cercanos est√°n los objetos dentro de un mismo cluster. Se utiliza la Suma de los Errores al Cuadrado, que es equivalente a la Inercia de K-Means (o Within Cluster).\n\n\n\n\\[ SSE_{total} = \\sum_{k = 1}^K\\sum_{x_i \\in C_k} (x_i - \\bar{C_k})^2\\]\n\n\\(C_k\\) corresponde al Centroide del Cluster \\(k\\). Dicho centroide puede ser calculado como la media/mediana de todos los puntos del Centroide.\n\\(K\\) corresponde al N√∫mero de Clusters.\n\n\n\n\n\n\n\n\nNo me gusta mucho este nombre, porque en realidad es como un inverso de la Cohesi√≥n."
  },
  {
    "objectID": "tics411/clase-6.html#separaci√≥n",
    "href": "tics411/clase-6.html#separaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Separaci√≥n",
    "text": "Separaci√≥n\n\nSeparaci√≥n\n\n\nMide cu√°n distinto es un cluster de otro. Se usa la suma de las distancias al cuadrado entre los centroides hacia el promedio de todos los puntos. (Between groups sum squares, SSB).\n\n\n\n\\[ SSB_{total} = \\sum_{k = 1}^K |C_k|(\\bar{X} - \\bar{C_k})^2\\]\n\n\\(|C_k|\\) corresponde al n√∫mero de elementos (Cardinalidad) del Cluster \\(i\\).\n\\(\\bar{X}\\) corresponde al promedio de todos los puntos."
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Coeficiente de Silhouette (Coeficiente de Silueta)",
    "text": "Coeficiente de Silhouette (Coeficiente de Silueta)\n\nEl coeficiente de Silhouette es otra medida que combina la cohesi√≥n y la separaci√≥n. Los valores var√≠an entre -1 y 1, donde valores cercanos a 1 representan una mejor agrupaci√≥n.\n\n\n\n\n\n\n\nValores cercanos a \\(-1\\) representan que el punto est√° incorrectamente asignado a un cluster.\n\n\n\n\\[S_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}\\]\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X, labels, sample_size = None, metric=\"euclidean\")"
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Coeficiente de Silhouette: Ejemplo",
    "text": "Coeficiente de Silhouette: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[C_{silueta} = \\frac{1}{n}\\sum_{i} s_i\\]\n\n\\(a_i\\): Distancia promedio del punto \\(i\\) a todos los otros puntos del mismo cluster. (Cohesi√≥n)\n\\(b_{ij}\\): Distancia promedio del punto \\(i\\) a todos los puntos del cluster \\(j\\) donde no pertenezca \\(i\\). (Separaci√≥n)\n\\(b_j\\): M√≠nimo de \\(b_{ij}\\) tal que el punto i no pertenezca al cluster \\(j\\). (Menor Separaci√≥n)"
  },
  {
    "objectID": "tics411/clase-6.html#ejercicio-propuesto",
    "href": "tics411/clase-6.html#ejercicio-propuesto",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejercicio Propuesto",
    "text": "Ejercicio Propuesto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio Propuesto\n\n\nCalcule el coeficiente de Silueta. Tabla de resultado al final de las Slides."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta",
    "href": "tics411/clase-6.html#curvas-de-silueta",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Curvas de Silueta",
    "text": "Curvas de Silueta\nEs com√∫n mostrar los resultados del coeficiente de silueta como gr√°ficos de este estilo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblemas\n\n\n\nSiluetas negativas.\nClusters bajo el promedio.\nMucha variabilidad de Silueta en un s√≥lo cluster."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta-implementaci√≥n",
    "href": "tics411/clase-6.html#curvas-de-silueta-implementaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Curvas de Silueta: Implementaci√≥n",
    "text": "Curvas de Silueta: Implementaci√≥n\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\nskplt.metrics.plot_silhouette(X, labels, metric=\"euclidean\", title=\"Silhouette Analysis\")\nplt.show()\n\nL1-2: Importaci√≥n de Librer√≠as Necesarias. Esta implementaci√≥n est√° en la librer√≠a Scikit-plot. (Para instalar pip install scikit-plot)\nX: Dataset usado para el clustering.\nlabels : etiquetas obtenidos de alg√∫n proceso de Clustering.\nmetric: M√©trica a utilizar, por defecto usa ‚Äúeuclidean‚Äù.\ntitle: Se puede agregar un T√≠tulo personalizado a la curva."
  },
  {
    "objectID": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "href": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Resultados Ejercicio Propuesto",
    "text": "Resultados Ejercicio Propuesto\n\n\n\n\n\nCoeficiente de Silhouette = 0.6148\n\n\n\n\n\n\nComprobar utilizando Scikit-Learn\n\n\n\n\n\nTics-411 Miner√≠a de Datos est√° licenciado bajo CC BY-NC-SA 4.0"
  },
  {
    "objectID": "tics411/lab-0.html#qu√©-es-scikit-learn",
    "href": "tics411/lab-0.html#qu√©-es-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øQu√© es Scikit-Learn?",
    "text": "¬øQu√© es Scikit-Learn?\n\n\n\n\n\nScikit-Learn (sklearn para los amigos) es una librer√≠a creada por David Cournapeau, como un Google Summer Code Project y luego Matthieu Brucher en su tesis.\nEn 2010 queda a cargo de INRIA y tiene un ciclo de actualizaci√≥n de 3 meses.\nEs la librer√≠a m√°s famosa y poderosa para hacer Machine Learning hoy en d√≠a.\nSu API es tan famosa, que hoy se sabe que una librer√≠a es de calidad si sigue los est√°ndares implementados por Scikit-Learn.\nPara que un algoritmo sea parte de Scikit-Learn debe poseer 3 a√±os desde su publicaci√≥n y 200+ citaciones mostrando su utilidad y amplio uso (ver ac√°).\nAdem√°s es una librer√≠a que obliga a que sus algoritmos tengan la capacidad de generalizar."
  },
  {
    "objectID": "tics411/lab-0.html#dise√±o",
    "href": "tics411/lab-0.html#dise√±o",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Dise√±o",
    "text": "Dise√±o\n\nScikit-Learn sigue un patr√≥n de Programaci√≥n Orientada a Objetos (POO) basado en clases.\n\n\n\n\n\n\n\n\nEn programaci√≥n, una clase es un objeto que internamente contiene estados que pueden ir cambiando en el tiempo.\n\nUna clase posee:\n\nM√©todos: Funciones que cambian el comportamiento de la clase.\nAtributos: Datos propios de la clase.\n\n\n\n\n\n\n\nScikit-Learn sigue el siguiente est√°ndar:\n\nTodas las Clases se escriben en CamelCase: Ej: KMeans,LogisticRegression, StandardScaler.\nLas clases en Scikit-Learn pueden representar algoritmos, o etapas de un preprocesamiento.\n\nLos algoritmos se denominan Estimators.\nLos preprocesamientos se denominan Transformers.\n\nLas funciones se escriben como snake_case y permiten realizar algunas operaciones b√°sicas en el proceso de modelamiento. Ej: train_test_split(), cross_val_score().\nNormalmente se utilizan letras may√∫sculas para denotar Matrices o DataFrames, mientras que las letras min√∫sculas denotan Vectores o Series."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-no-supervisados",
    "href": "tics411/lab-0.html#estimadores-no-supervisados",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Estimadores No supervisados",
    "text": "Estimadores No supervisados\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X) \n\ny_pred = model.predict(X) \n\n## Opcionalmente se puede entrenar y predecir a la vez.\nmodel.fit_predict(X) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperpar√°metros.\nL3. Entrenar o ajustar el modelo (Requiere s√≥lo de X).\nL5. Predecir. Los modelos de clasificaci√≥n tienen la capacidad de generar probabilidades.\nL7-8. Este tipo de modelos permite entrenar y predecir en un s√≥lo paso."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-predictivos",
    "href": "tics411/lab-0.html#estimadores-predictivos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Estimadores Predictivos",
    "text": "Estimadores Predictivos\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X_train, y_train) \n\ny_pred = model.predict(X_test) \ny_pred_proba = model.predict_proba(X_test)\n\nmodel.score(X_test,y_test) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperpar√°metros.\nL3. Entrenar o ajustar el modelo (Ojo, requiere de X e y).\nL5‚Äì6. Predecir en datos nuevos. (Algunos modelos pueden predecir probabilidades).\nL8. Evaluar el modelo en los datos nuevos."
  },
  {
    "objectID": "tics411/lab-0.html#output-de-un-modelo",
    "href": "tics411/lab-0.html#output-de-un-modelo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Output de un Modelo",
    "text": "Output de un Modelo\n\nLos modelos no entregan directamente un output sino que los dejan almacenados en su interior como un estado.\nLos Estimators tienen dos estados:\n\nNot Fitted: Modelo antes de ser entrenado\nFitted: Una vez que el modelo ya est√° entrenado. (Despu√©s de aplicar .fit())\n\n\n\n\n\n\n\n\n\nMuchos modelos pueden entregar informaci√≥n s√≥lo luego de ser entrenados (su atributo termina con un _).\nEj: model.coef_, model.intercept_.\n\n\n\n\n\n\n\n\n\n\n\nEl modelo es una herramienta a la cual le entregamos datos (Input), y nos devuelve datos (Predicciones)."
  },
  {
    "objectID": "tics411/lab-0.html#transformers",
    "href": "tics411/lab-0.html#transformers",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\n\n\nA diferencia de los Estimators, los Transformers no son modelos.\nSu input y su output son datos.\nAlgunos Transformers permiten escalar los datos, transformar categor√≠as en n√∫meros, rellenar valores faltantes. (Veremos m√°s acerca de esto en los Preprocesamiento).\n\n\n\n\n\n\nfrom sklearn.preprocessing import Transformer \ntr = Transformer(hp1=v1, hp2=v2,...) \ntr.fit(X) \n\nX_new = tr.transform(X) \n\n## Opcionalmente\nX_new = tr.fit_transform(X) \n\nL1. Importar la clase a utilizar (en este caso del submodulo preprocessing, aunque pueden haber otros como impute).\nL2. Instanciar el Transformer y sus hiperpar√°metros.\nL3. Entrenar o ajustar el Transformer.\nL5. Transformar los datos.\nL7-8. Adicionalmente se puede entrenar y transformar los datos en un s√≥lo paso."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines",
    "href": "tics411/lab-0.html#pipelines",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Pipelines",
    "text": "Pipelines\n\nEn ocasiones un Dataset requiere m√°s de un preprocesamiento.\nEstas Transformaciones normalmente se hacen en serie de manera consecutiva.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl Estimator es opcional, es decir, el Pipeline puede ser para combinar s√≥lo Transformers o Transformers + un Estimator.\n\n\n\n\n\n\n\n\n\n\nUn Pipeline puede tener s√≥lo un Estimator."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines-c√≥digo",
    "href": "tics411/lab-0.html#pipelines-c√≥digo",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Pipelines: C√≥digo",
    "text": "Pipelines: C√≥digo\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.pipeline import Pipeline \n\npipe = Pipeline(steps=[ \n    (\"ohe\", OneHotEncoder()),\n    (\"sc\", StandardScaler()),\n    (\"model\", DecisionTreeClassifier())\n])\n\npipe.fit(X_train, y_train) \ny_pred = pipe.predict(X_test) \n\npipe.score(X_test, y_test) \n\nL1-2. Importo mi modelo y mis preprocesamientos\nL3. Importo el Pipeline.\nL5-9. Instancio un Pipeline.\nL11. Entreno el Pipeline.\nL12. Predigo utilizando el Pipeline entrenado.\nL14. Eval√∫o el modelo en datos no vistos."
  },
  {
    "objectID": "tics411/lab-0.html#documentaci√≥n",
    "href": "tics411/lab-0.html#documentaci√≥n",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Documentaci√≥n",
    "text": "Documentaci√≥n\n\nProbablemente Scikit-Learn tenga una de las mejores documentaciones existentes.\n\n\nVeamos el caso de la Documentaci√≥n del One Hot Encoder"
  },
  {
    "objectID": "tics411/clase-5.html#clustering-densidad",
    "href": "tics411/clase-5.html#clustering-densidad",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (n√∫mero de objetos o puntos) en el vecindario sobrepase alg√∫n umbral.\n\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso utilizaremos DBSCAN (Density-Based Spatial Clustering Applications with Noise)."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-definiciones",
    "href": "tics411/clase-5.html#dbscan-definiciones",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "DBSCAN: Definiciones",
    "text": "DBSCAN: Definiciones\n\n\n\n\n\n\nHiperpar√°metros del Modelo\n\n\n\neps: Radio de an√°lisis\nMinPts: Corresponde al m√≠nimo de puntos necesarios en un Radio eps.\n\n\n\n\n\n\n\nDensidad\n\n\nDensidad es el n√∫mero de puntos dentro del radio eps.\n\n\nCore Point/Punto Central\n\n\nUn punto central/core es aquel que tiene al menos MinPts puntos dentro de la esfera definida por eps (se incluye √©l mismo).\n\n\n\n\n\nBorder Point/Punto Borde\n\n\nUn punto de borde tiene menos puntos que MinPts del eps, pero est√° dentro de la esfera de un punto central.\n\n\nNoise Point/Punto Ruido\n\n\nUn punto de ruido es todo aquel que no es punto central ni de borde."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-algoritmo-categorizaci√≥n-de-puntos",
    "href": "tics411/clase-5.html#dbscan-algoritmo-categorizaci√≥n-de-puntos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "DBSCAN: Algoritmo categorizaci√≥n de puntos",
    "text": "DBSCAN: Algoritmo categorizaci√≥n de puntos\n\nPrimeramente se aplica un algoritmo para categorizar cada punto de acuerdo a las definiciones anteriores.\n\n\nPara cada punto en el espacio:\n\nCalcular su densidad en EPS y aplicar el siguiente algoritmo:"
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteraci√≥n-1",
    "href": "tics411/clase-5.html#ejemplo-iteraci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo: Iteraci√≥n 1",
    "text": "Ejemplo: Iteraci√≥n 1\n\nSupongamos un ejemplo con \\(MinPts=4\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Core Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteraci√≥n-2",
    "href": "tics411/clase-5.html#ejemplo-iteraci√≥n-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo: Iteraci√≥n 2",
    "text": "Ejemplo: Iteraci√≥n 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Border Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteraci√≥n-3",
    "href": "tics411/clase-5.html#ejemplo-iteraci√≥n-3",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo: Iteraci√≥n 3",
    "text": "Ejemplo: Iteraci√≥n 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteraci√≥n-final",
    "href": "tics411/clase-5.html#ejemplo-iteraci√≥n-final",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Ejemplo: Iteraci√≥n Final",
    "text": "Ejemplo: Iteraci√≥n Final\n\n\n\n\n\n\n\n\n\n\n\nAhora, ¬øC√≥mo definimos que partes son clusters o no?"
  },
  {
    "objectID": "tics411/clase-5.html#algoritmo-de-clustering",
    "href": "tics411/clase-5.html#algoritmo-de-clustering",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Algoritmo de Clustering",
    "text": "Algoritmo de Clustering\nSe aplica el siguiente algoritmo para calcular clusterings.\n\n\n\n\n\n\nAntes de aplicar se desechan los Noise Points ya que no ser√°n considerados. (Veremos luego que ocurre con estos puntos).\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label"
  },
  {
    "objectID": "tics411/clase-5.html#iteraci√≥n-1",
    "href": "tics411/clase-5.html#iteraci√≥n-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Iteraci√≥n 1",
    "text": "Iteraci√≥n 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nTodos los puntos cercanos a un Core reciben la misma etiqueta."
  },
  {
    "objectID": "tics411/clase-5.html#iteraci√≥n-2",
    "href": "tics411/clase-5.html#iteraci√≥n-2",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Iteraci√≥n 2",
    "text": "Iteraci√≥n 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n## label ya est√° en 1\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nEn este caso obtuvimos 2 clusters, e indirectamente un 3er de puntos ruido."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan",
    "href": "tics411/clase-5.html#dbscan",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\n\n\n\n\n\n\n\n\n\n\n¬øSer√≠a posible replicar un proceso de Clustering similar utilizando K-Means? ¬øPor qu√©?"
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-detalles-t√©cnicos",
    "href": "tics411/clase-5.html#dbscan-detalles-t√©cnicos",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "DBSCAN: Detalles T√©cnicos",
    "text": "DBSCAN: Detalles T√©cnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nResistente al ruido.\nPuede lidiar con clusters de diferentes formas y tama√±os.\nNo es necesario especificar cu√°ntos clusters encontrar.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAlgoritmo de alta complejidad computacional que puede llegar \\(O(n^2)\\) en el peor caso.\nSe ve afectado por densidad de los datos y por datos con una alta dimensionalidad.\nSu √≥ptimo resultado depende espec√≠ficamente de sus Hiperpar√°metros.\nNo puede generalizar en datos no usados en entrenamiento."
  },
  {
    "objectID": "tics411/clase-5.html#c√≥mo-encontrar-los-hiperpar√°metros",
    "href": "tics411/clase-5.html#c√≥mo-encontrar-los-hiperpar√°metros",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øC√≥mo encontrar los Hiperpar√°metros?",
    "text": "¬øC√≥mo encontrar los Hiperpar√°metros?\n\n\n\n\n\n\n\n\nminPts\n\n\nPara datasets multidimensionales grandes, la regla es:\n\\[minPts \\ge dim + 1\\]\n\n\n\n\n\n\n\n\n\nOtras recomendaciones:\n\n\n\nPara dos dimensiones: \\(minPts=4\\) (Ester et al., 1996)\nPara m√°s de 2 dimensiones: \\(minPts = 2 \\cdot dim\\) (Sander et al., 1998)"
  },
  {
    "objectID": "tics411/clase-5.html#c√≥mo-encontrar-los-hiperpar√°metros-1",
    "href": "tics411/clase-5.html#c√≥mo-encontrar-los-hiperpar√°metros-1",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "¬øC√≥mo encontrar los Hiperpar√°metros?",
    "text": "¬øC√≥mo encontrar los Hiperpar√°metros?\n\nPara encontrar EPS se suele utilizar el m√©todo de Vecinos m√°s cercanos.\n\n\n\nIdea\n\nLa distancia de los puntos dentro de un cluster a su k-√©simo vecino deber√≠an ser similares.\nLuego, los puntos at√≠picos (o ruidosos) tienen el k-√©simo vecino a una mayor distancia.\n\n\n\n\n\n\n\nüí° Podemos plotear la distancia ordenada de cada punto a su k-√©simo vecino y seleccionar un eps cercano al crecimiento exponencial (codo)."
  },
  {
    "objectID": "tics411/clase-5.html#implementaci√≥n-en-scikit-learn",
    "href": "tics411/clase-5.html#implementaci√≥n-en-scikit-learn",
    "title": "TICS-411 Miner√≠a de Datos",
    "section": "Implementaci√≥n en Scikit-Learn",
    "text": "Implementaci√≥n en Scikit-Learn\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples = 5, eps = 0.5, metric = \"euclidean\")\n\n## Se entrena y se genera la predicci√≥n\ndbs.fit_predict(X)\n\n\nmin_samples: Corresponde a minPts. Por defecto 5.\neps: Corresponde al radio de la esfera en la que se buscan los puntos cercanos. Por defecto 0.5.\nmetric: Corresponde a la distancia utilizada para medir la distancia. Permite todas las distancias mencionadas ac√°.\n.fit_predict(): Entrenar√° el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento. Adicionalmente los puntos ruidosos se etiquetar√°n como -1.\n\n\nüëÄ Veamos un ejemplo."
  },
  {
    "objectID": "tics579.html",
    "href": "tics579.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentaci√≥n del Curso\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 1\n\n\nIntroducci√≥n a las Redes Neuronales\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  },
  {
    "objectID": "tics411-labs.html",
    "href": "tics411-labs.html",
    "title": "Pr√°cticos",
    "section": "",
    "text": "Pr√°ctico\nColab\n\n\n\n\nPreprocesamiento\n\n\n\nEDA\n\n\n\nK-Means\n\n\n\nAn√°lisis de Centros\n\n\n\nAglomerativo\n\n\n\nDBSCAN\n\n\n\nEvaluaci√≥n de Clusters\n\n\n\nEjemplos Hopkins\n\n\n\nProyecto Clustering\n\n\n\nApriori\n\n\n\nResoluci√≥n Gu√≠a\n\n\n\nKNN\n\n\n\nCross Validation\n\n\n\nDecision Tree\n\n\n\nNaive Bayes\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "tics579/clase-0.html#qui√©n-soy",
    "href": "tics579/clase-0.html#qui√©n-soy",
    "title": "TICS-579-Deep Learning",
    "section": "¬øQui√©n soy?",
    "text": "¬øQui√©n soy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia\nEstudi√© Ingenier√≠a Civil pero llevo 10 a√±os trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nSoy Msc. en Data Science y estoy cursando el PhD. en la UAI especificamente en Deep Learning.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqu√© mi primer paper el a√±o pasado sobre Hate Speech en Espa√±ol.\nJuego Tenis de Mesa, hago Agility con mi perrita Kira y escribo en mi Blog."
  },
  {
    "objectID": "tics579/clase-0.html#disclaimer",
    "href": "tics579/clase-0.html#disclaimer",
    "title": "TICS-579-Deep Learning",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\n\nMucho del contenido de este curso ser√° una mezcla entre ingl√©s y espa√±ol. Esto debido a que el contenido del curso est√° en constante desarrollo y casi no existen libros o art√≠culos en espa√±ol al respecto.\n\n\n\n\n\n\n\n\n\n\n\nEste curso se considera altamente te√≥rico y con una fuerte componente en programaci√≥n.\n\n\n\n\n\n\n\n\n\n\n\n¬°Est√°n advertidos!\n\n\n\n\n\n\n\n\n\n\n\nEst√° completamente prohibido copiar y pegar c√≥digo de alg√∫n modelo de IA. Hablaremos m√°s adelante c√≥mo trataremos de mitigar esto ya que gran parte del curso requiere programaci√≥n fuerte."
  },
  {
    "objectID": "tics579/clase-0.html#objetivos-del-curso",
    "href": "tics579/clase-0.html#objetivos-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\n\n\n\nSer el curso m√°s completo y exhaustivo de Deep Learning del pa√≠s.\n\n\n\n\n\n\nIdentificar elementos claves de las Redes Neuronales.\nEntender conceptos b√°sicos como el Training Loop, Gradient Propagation, Optimizaci√≥n, etc.\nIdentificar los distintos tipos de Redes Neuronales:\n\nFeed Fordward Networks,\nConvolutional Neural Networks,\nRecurrent Neural Networks,\nTransformers.\n\nEntender las Arquitecturas Estado del Arte en diferentes dominios: Datos Tabulares, Computer Vision, Natural Language Processing.\nImplementar, entrenar y evaluar Deep Neural Networks utilizando Pytorch."
  },
  {
    "objectID": "tics579/clase-0.html#t√≥picos-del-curso",
    "href": "tics579/clase-0.html#t√≥picos-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "T√≥picos del Curso",
    "text": "T√≥picos del Curso\n\n\n\nShallow Models\nEl Perceptron\nTensores\nEntrenamiento y Evaluaci√≥n de Modelos\nArquitecturas de Redes Neuronales\nAplicaci√≥n de estas redes a distintos dominios.\n\n\n\n\n\n\n\nImplementaci√≥n en librer√≠as SOTA como Pytorch y HuggingFace."
  },
  {
    "objectID": "tics579/clase-0.html#log√≠stica",
    "href": "tics579/clase-0.html#log√≠stica",
    "title": "TICS-579-Deep Learning",
    "section": "Log√≠stica",
    "text": "Log√≠stica\n\nClases todos los Jueves de 8:45 a 11:25 de manera presencial.\n\nSala: Por Confirmar.\n\nInstructor: Alfonso Tobar-Arancibia (alfonso.tobar.a@edu.uai.cl)\n\nOffice Hours: Miercoles por la ma√±ana en la A-220.\n\nAyudant√≠as Online, s√≥lo en algunas semanas.*\n\nAyudante: Mar√≠a Alejandra Bravo (mariaabravo@alumnos.uai.cl)\n\n\n\n\n\n\n\n\n\nHorarios Posibles:\n\n\n\nMartes de 17:00 a 18:10 hrs.\nMi√©rcoles de 8:45 a 09:55 hrs.\nMi√©rcoles de 10:15 a 11:25 hrs.\nMi√©rcoles de 17:00 a 18:10 hrs."
  },
  {
    "objectID": "tics579/clase-0.html#prerequisitos",
    "href": "tics579/clase-0.html#prerequisitos",
    "title": "TICS-579-Deep Learning",
    "section": "Prerequisitos",
    "text": "Prerequisitos\n\n\n\n\n\n\n\nHaber cursado Miner√≠a de Datos (no es excluyente pero es necesario saber de Machine Learning).\n\nEntrenamiento de un Modelo\nEvaluaci√≥n y Validaci√≥n\nRegresi√≥n Log√≠stica\n\n\n\n\n\n\n\n\n\n\n\n\nTener conocimientos de Algebra Lineal (poner mucha atenci√≥n al curso de Algebra Lineal del MSDS).\n\nNotaci√≥n Matricial\nMultiplicaciones Matriciales\nTransformaciones Lineales\n\n\n\n\n\n\n\n\n\n\n\n\nEntender Ingl√©s\n\nMucho del material adicional ser√°n lecturas o videos en ingl√©s."
  },
  {
    "objectID": "tics579/clase-0.html#recursos",
    "href": "tics579/clase-0.html#recursos",
    "title": "TICS-579-Deep Learning",
    "section": "Recursos",
    "text": "Recursos\n\nSlides\nNotebooks\nForo de Dudas\n\n\n\nDiapositivas Interactivas creadas en Quarto (links van a estar disponibles en Webcursos)\n\nContiene un √≠ndice de todas las slides.\nPermite copiar y pegar c√≥digo directamente.\nIm√°genes se pueden ver en tama√±o completo al clickearlas.\nSe puede buscar contenido espec√≠fico de cualquier Slide utilizando la Search Bar.\nSe puede obtener una copia en PDF presionando la tecla E para luego guardarlas para tomar notas."
  },
  {
    "objectID": "tics579/clase-0.html#herramientas",
    "href": "tics579/clase-0.html#herramientas",
    "title": "TICS-579-Deep Learning",
    "section": "Herramientas",
    "text": "Herramientas\n\n\n\n\n\n\nSe espera que los estudiantes dominen las siguientes herramientas:\n\nPython\nPandas/Numpy\nGoogle Colab\n\n\n\n\n\n\n\n\n\n\n\nA lo largo del curso utilizaremos otras librer√≠as que se ense√±aran a lo largo del curso:\n\nPytorch\nTransformers\nAlbumentations\netc."
  },
  {
    "objectID": "tics579/clase-0.html#material-complementario",
    "href": "tics579/clase-0.html#material-complementario",
    "title": "TICS-579-Deep Learning",
    "section": "Material Complementario",
    "text": "Material Complementario\n\nNo hay un texto gu√≠a para este curso. La mayor√≠a de las cosas aparecen d√≠a a d√≠a o las podemos encontrar en Papers, los cuales ir√°n siendo mencionados a medida que sea necesario.\n\nLectura Recomendada\n\n\n\n\n\n\n\n\n\nDocs Pytorch nn\nDocs Pytorch functional\nTutorial Colab\nAgregar Datos Externos a Colab"
  },
  {
    "objectID": "tics579/clase-0.html#reglas-del-curso",
    "href": "tics579/clase-0.html#reglas-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "Reglas del Curso",
    "text": "Reglas del Curso\n\n\n\n\n\n\nNota Final\n\n\n\\[NF = NT + 0.3 \\cdot NQ\\]\n\n\n\n\n\n\n\n\n\nTareas\n\n\n\nSe realizar√°n 5 Tareas. T5 es opcional y reemplaza la peor nota.\n\n\\[NT = 0.1 \\cdot T1 + 0.15 \\cdot T2 + 0.20 \\cdot T3 + 0.25 \\cdot T4\\]\n\n\n\n\n\n\n\n\n\nQuizes\n\n\n\nSe realizar√°n controles cortos al inicio de clases (previo aviso).\nSe realizar√°n suficientes controles para eliminar algunos al final del semestre.\nNo hay controles recuperativos.\n\n\\[NQ = \\frac{1}{n}\\sum_{i=1}^n Q_i\\]"
  },
  {
    "objectID": "tics579/clase-0.html#tareas-1",
    "href": "tics579/clase-0.html#tareas-1",
    "title": "TICS-579-Deep Learning",
    "section": "Tareas",
    "text": "Tareas\n\n\n\n\n\n\n\nReglas\n\n\n\nSe deben entregar en Jupyter Notebook.\nSe realizar√°n en parejas o un grupo de 3 (s√≥lo en caso de n√∫mero impar) previa inscripci√≥n.\nLas partes te√≥ricas que necesiten notaci√≥n matem√°tica se deben realizar dentro del Jupyter Notebook pero usando simbolog√≠a Latex (Dudas de c√≥mo hacerlo a la ayudante).\n\n\n\n\n\n\n\n\n\n\nDefensa de C√≥digo\n\n\n\nEl c√≥digo presentado en las tareas se defender√° mediante interrogaci√≥n oral en horarios a convenir (normalmente de ayudant√≠a).\n3 preguntas aleatorias a cualquier miembro del grupo.\nEn caso de no defender el c√≥digo correctamente se penalizar√° de la siguiente manera:\n\n3 preguntas buenas: 100% del puntaje.\n2 preguntas buenas: 70% del puntaje.\n1 preguntas buenas: 40% del puntaje.\n0 preguntas buenas: 20% del puntaje."
  },
  {
    "objectID": "tics579/clase-0.html#fechas-tareas",
    "href": "tics579/clase-0.html#fechas-tareas",
    "title": "TICS-579-Deep Learning",
    "section": "Fechas Tareas",
    "text": "Fechas Tareas\n\n\n\n\n\n\n\n\nTarea 1\n\n\n15 de Septiembre (23:59 hrs)\n\n\n\n\n\n\n\n\n\nTarea 2\n\n\n13 de Octubre (23:59 hrs)\n\n\n\n\n\n\n\n\n\n\nTarea 3\n\n\n3 de Noviembre (23:59 hrs)\n\n\n\n\n\n\n\n\n\nTarea 4\n\n\n24 de Noviembre (23:59 hrs)\n\n\n\n\n\n\n\n\n\n\n\nTarea 5 (Opcional, pero recomendada)\n\n\n15 de Diciembre (23:59 hrs)"
  }
]