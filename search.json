[
  {
    "objectID": "tics579.html",
    "href": "tics579.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentación del Curso\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 1\n\n\nPreliminares\n\n\n\nAug 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 2\n\n\nClase 2: Introducción a las Redes Neuronales y formalidades\n\n\n\nAug 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 3\n\n\nClase 3: Feed Forward Networks\n\n\n\nAug 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 4\n\n\nClase 4: Introducción a Pytorch\n\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 5\n\n\nClase 5: Model Training\n\n\n\nSep 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 6\n\n\nClase 6: Training Tips & Tricks\n\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  },
  {
    "objectID": "tics411/notebooks/14-ex-NB.html",
    "href": "tics411/notebooks/14-ex-NB.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\"]]\ny = df.survived\n\n\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\n\n\ndef make_pipeline(parameters):\n    if parameters[\"sc_variables\"] == \"all\":\n        scaler = StandardScaler()\n    elif parameters[\"sc_variables\"] is not None:\n        scaler = SklearnTransformerWrapper(\n            StandardScaler(), variables=parameters[\"sc_variables\"]\n        )\n    else:\n        scaler = \"passthrough\"\n\n    if parameters[\"num_method\"] is None:\n        num_imp = \"passthrough\"\n    else:\n        num_imp = MeanMedianImputer(\n            imputation_method=parameters[\"num_method\"]\n        )\n\n    if parameters[\"cat_method\"] is None:\n        cat_imp = \"passthrough\"\n    else:\n        cat_imp = CategoricalImputer(\n            imputation_method=parameters[\"cat_method\"]\n        )\n\n    print(\n        f\"Entrenamiento para Naive Bayes y threshold = {parameters['threshold']}\"\n    )\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\"cat_imp\", cat_imp),\n            (\"num_imp\", num_imp),\n            (\"ohe\", parameters[\"encoder\"]),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    parameters[\"model\"],\n                    threshold=parameters[\"threshold\"],\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=MultinomialNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7844311377245509\nTest Accuracy 0.757847533632287\n===================================\nTrain Precision 0.7137254901960784\nTest Precision 0.6732673267326733\n===================================\nTrain Recall 0.7193675889328063\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7784431137724551\nTest Accuracy 0.7488789237668162\n===================================\nTrain Precision 0.6996197718631179\nTest Precision 0.6601941747572816\n===================================\nTrain Recall 0.7272727272727273\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=\"all\",\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7784431137724551\nTest Accuracy 0.7488789237668162\n===================================\nTrain Precision 0.6996197718631179\nTest Precision 0.6601941747572816\n===================================\nTrain Recall 0.7272727272727273\nTest Recall 0.7640449438202247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"age\", \"fare\"]]\ny = df.survived\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=\"mean\",\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=MultinomialNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.6811377245508982\nTest Accuracy 0.7309417040358744\n===================================\nTrain Precision 0.6063829787234043\nTest Precision 0.7230769230769231\n===================================\nTrain Recall 0.4505928853754941\nTest Recall 0.5280898876404494\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    cat_method=\"frequent\",\n    num_method=\"mean\",\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n    sc_variables=None,\n    model=GaussianNB(),\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para Naive Bayes y threshold = 0.5\n===================================\nTrain Accuracy 0.7844311377245509\nTest Accuracy 0.7623318385650224\n===================================\nTrain Precision 0.7056603773584905\nTest Precision 0.6730769230769231\n===================================\nTrain Recall 0.7391304347826086\nTest Recall 0.7865168539325843\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/09-ex-apriori.html",
    "href": "tics411/notebooks/09-ex-apriori.html",
    "title": "Algoritmo Apriori",
    "section": "",
    "text": "%%capture\n!pip install mlxtend\n\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\nimport pandas as pd\n\n## Escribir acá las Transacciones\ntransactions = [\n    [\"Pan\", \"Mantequilla\", \"Leche\"],\n    [\"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Galletas\", \"Pañales\"],\n    [\"Leche\", \"Pañales\", \"Pan\", \"Mantequilla\"],\n    [\"Cerveza\", \"Pañales\"],\n]\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns=tre.columns_)\ndf_encoded\n\n\n\n\n\n\n\n\nCerveza\nGalletas\nLeche\nMantequilla\nPan\nPañales\n\n\n\n\n0\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n2\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\n\n\n4\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\ndef apriori_algorithm(\n    df,\n    min_supp=0.4,\n    min_conf=0.7,\n    variables=[\n        \"antecedents\",\n        \"consequents\",\n        \"support\",\n        \"confidence\",\n        \"lift\",\n    ],\n):\n\n    frequent_itemsets = apriori(\n        df, min_support=min_supp, use_colnames=True\n    )\n    rules = association_rules(\n        frequent_itemsets, metric=\"confidence\", min_threshold=min_conf\n    )\n\n    return frequent_itemsets, rules[variables]\n\n\nfrequent_itemsets, rules = apriori_algorithm(\n    df_encoded, min_supp=0.4, min_conf=0.7\n)\ndisplay(frequent_itemsets)\ndisplay(rules)\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.4\n(Cerveza)\n\n\n1\n0.4\n(Leche)\n\n\n2\n0.6\n(Mantequilla)\n\n\n3\n0.6\n(Pan)\n\n\n4\n0.6\n(Pañales)\n\n\n5\n0.4\n(Pañales, Cerveza)\n\n\n6\n0.4\n(Mantequilla, Leche)\n\n\n7\n0.4\n(Leche, Pan)\n\n\n8\n0.6\n(Mantequilla, Pan)\n\n\n9\n0.4\n(Mantequilla, Leche, Pan)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\n\n\n\n\n0\n(Cerveza)\n(Pañales)\n0.4\n1.0\n1.666667\n\n\n1\n(Leche)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n2\n(Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n3\n(Mantequilla)\n(Pan)\n0.6\n1.0\n1.666667\n\n\n4\n(Pan)\n(Mantequilla)\n0.6\n1.0\n1.666667\n\n\n5\n(Mantequilla, Leche)\n(Pan)\n0.4\n1.0\n1.666667\n\n\n6\n(Leche, Pan)\n(Mantequilla)\n0.4\n1.0\n1.666667\n\n\n7\n(Leche)\n(Mantequilla, Pan)\n0.4\n1.0\n1.666667\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/kmeans.html",
    "href": "tics411/notebooks/kmeans.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\n\n## Primera fila son coordenadas X\n## Segunda fila son coordenadas Y\n## Cada columna es el punto.\npuntos = np.array([[1, 2, 4, 5], [1, 1, 3, 4]])\npuntos\n\nc_1 = np.array([1, 1])\nc_2 = np.array([2, 1])\npuntos\n\narray([[1, 2, 4, 5],\n       [1, 1, 3, 4]])\n\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_clusters(puntos, c_1, c_2, c=None):\n    plt.scatter(puntos[0], puntos[1], s=500, c=c)\n    plt.scatter(\n        c_1[0],\n        c_1[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 1\",\n        edgecolors=\"k\",\n        s=200,\n        color=\"orange\",\n    )\n    plt.scatter(\n        c_2[0],\n        c_2[1],\n        marker=\"^\",\n        label=\"Centroide Cluster 2\",\n        edgecolors=\"k\",\n        s=200,\n        c=\"green\",\n    )\n    plt.grid(alpha=0.5)\n    plt.legend()\n\n\nplot_clusters(puntos, c_1, c_2)\n\n\n\n\n\n\n\n\n\ndef distancia(p0, p1):\n    x0 = p0[0]\n    x1 = p1[0]\n    y0 = p0[1]\n    y1 = p1[1]\n    return np.sqrt((x1 - x0) ** 2 + (y1 - y0) ** 2)\n\n\ndef calculate_distances(puntos, c_1, c_2):\n\n    distancia_mat = np.zeros((2, 4))\n    distancia_mat\n\n    for i in range(4):\n        p = puntos[:, i]\n        distancia_mat[0, i] = distancia(p, c_1)\n        distancia_mat[1, i] = distancia(p, c_2)\n\n    return distancia_mat\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\ndistancia_mat\n\narray([[0.        , 1.        , 3.60555128, 5.        ],\n       [1.        , 0.        , 2.82842712, 4.24264069]])\n\n\n\ndef calculate_clusters(distancia_mat):\n    min_distancia_mat = np.min(distancia_mat, axis=0)\n\n    clusters = distancia_mat == min_distancia_mat\n    return clusters\n\n\nclusters = calculate_clusters(distancia_mat)\nclusters.astype(\"int64\")\n\n## Solo el punto 1 pertenece al cluster 1, todo el resto al cluster 2\n\narray([[1, 0, 0, 0],\n       [0, 1, 1, 1]])\n\n\n\ndef calculate_centroids(clusters):\n    c_1 = puntos[:, clusters[0]].mean(axis=1)\n    c_2 = puntos[:, clusters[1]].mean(axis=1)\n\n    return c_1, c_2\n\n\nc_1, c_2 = calculate_centroids(clusters)\nc_1, c_2\n\n(array([1., 1.]), array([3.66666667, 2.66666667]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"blue\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\ndistancia_mat = calculate_distances(puntos, c_1, c_2)\nclusters = calculate_clusters(distancia_mat)\nc_1, c_2 = calculate_centroids(clusters)\nclusters\n\narray([[ True,  True, False, False],\n       [False, False,  True,  True]])\n\n\n\nc_1, c_2\n\n(array([1.5, 1. ]), array([4.5, 3.5]))\n\n\n\nplot_clusters(puntos, c_1, c_2, c=[\"red\", \"red\", \"blue\", \"blue\"])\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/Ejercicio-Matriz-Confusión.html",
    "href": "tics411/notebooks/Ejercicio-Matriz-Confusión.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\n\ndf = pd.DataFrame(\n    dict(\n        y=[1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2],\n        y_pred=[1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 2],\n    )\n)\n\nprint(classification_report(df.y, df.y_pred, digits=3))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0      0.500     0.333     0.400         3\n           1      0.800     0.800     0.800         5\n           2      0.500     0.667     0.571         3\n\n    accuracy                          0.636        11\n   macro avg      0.600     0.600     0.590        11\nweighted avg      0.636     0.636     0.629        11\n\n\n\n\n\n\n\n\n\n\n\nEs importante mencionar que para el caso de más de 2 clases, el Accuracy se calcula como:\n\n\\[Accuracy = \\frac{TP_{Clase\\,0} + TP_{Clase\\,1} + TP_{Clase\\,2}}{Total} = \\frac{1 + 4 + 2}{11} = \\frac{7}{11} = 0.636\\]\nEl accuracy es un sólo valor para todas las clases. Y en el caso que mostramos ayer, el concepto de TN se pierde, por lo que es necesario reducir la matriz a un problema binario para poder poder darle sentido.\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html",
    "href": "tics411/notebooks/06-ex-DBSCAN.html",
    "title": "DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\ndf = sns.load_dataset(\"iris\")\nX = df.drop(columns=\"species\")\nX\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#función-para-visualizar",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#función-para-visualizar",
    "title": "DBSCAN",
    "section": "Función para Visualizar",
    "text": "Función para Visualizar\n\nimport matplotlib.pyplot as plt\n\n\n## Función ligeramente modificada para no requerir centroides en caso que no sea aplicable.\ndef pca_viz(pca_X, labels, pca_centroids=None, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    if pca_centroids is not None:\n        plt.scatter(\n            pca_centroids[:, 0],\n            pca_centroids[:, 1],\n            marker=\"*\",\n            c=\"red\",\n            s=150,\n        )\n    plt.title(title)\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=13, eps=0.6)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\n\n\nfrom sklearn.decomposition import PCA\n\n## Probar minPts = 4 y eps = 0.2\n## Probar minPts = 10 y eps = 0.5\n## Probar minPts = 13 y eps = 0.6\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X_sc)\n\npca_viz(\n    pca_X,\n    labels=labels,\n    title=\"Visualización de DBSCAN para Iris en 2D\",\n)\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino más cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino más cercano\")\n    plt.title(f\"Búsqueda de EPS para DBSCAN con k={k}\")\n\n\n# k = 5 escogido ya que tenemos 4 dimensiones.\ndbscan_elbow_plot(X_sc, k=5)"
  },
  {
    "objectID": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperparámetros-óptimos",
    "href": "tics411/notebooks/06-ex-DBSCAN.html#modelo-entrenado-con-hiperparámetros-óptimos",
    "title": "DBSCAN",
    "section": "Modelo entrenado con Hiperparámetros Óptimos",
    "text": "Modelo entrenado con Hiperparámetros Óptimos\n\nMIN_PTS = 20\nEPS = 0.75\nsc = StandardScaler()\ndbs = DBSCAN(min_samples=MIN_PTS, eps=EPS)\nX_sc = sc.fit_transform(X)\nlabels = dbs.fit_predict(X_sc)\npca_viz(\n    pca_X,\n    labels=labels,\n    title=f\"Visualización de DBSCAN para Iris en 2D con los mejores Hiperparámetros: MinPts: {MIN_PTS} y eps = {EPS}\",\n)"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html",
    "href": "tics411/notebooks/11-ex-knn.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\ndf.dtypes.value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Tipos de Variable presente en Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "href": "tics411/notebooks/11-ex-knn.html#supongamos-que-utilizaremos-las-siguientes-variables",
    "title": "Clases UAI",
    "section": "Supongamos que utilizaremos las siguientes variables",
    "text": "Supongamos que utilizaremos las siguientes variables\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape\n\n((891, 5), (891,))"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#eda",
    "href": "tics411/notebooks/11-ex-knn.html#eda",
    "title": "Clases UAI",
    "section": "EDA",
    "text": "EDA\n\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nprint(f\"Variables Numéricas: {num_cols}\")\nprint(f\"Variables Categóricas: {cat_cols}\")\n\nVariables Numéricas: ['fare', 'age']\nVariables Categóricas: ['class', 'sex', 'embark_town']\n\n\n\nValores Faltantes (Nulos)\n\nX.isnull().mean().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Cantidad de Valores Nulos en el Titanic\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-numéricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-numéricas",
    "title": "Clases UAI",
    "section": "Variables Numéricas",
    "text": "Variables Numéricas\n\nX.hist(grid=False, edgecolor=\"k\")\nplt.suptitle(\"Distribución de Variables Numéricas\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#variables-categóricas",
    "href": "tics411/notebooks/11-ex-knn.html#variables-categóricas",
    "title": "Clases UAI",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\n\ncolor = [\"red\", \"blue\", \"green\"]\nfor cat, color in zip(cat_cols, color):\n    df[cat].value_counts().plot(\n        kind=\"bar\",\n        edgecolor=\"k\",\n        color=color,\n        title=f\"Categorías para '{cat}'\",\n    )\n    plt.show()"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "href": "tics411/notebooks/11-ex-knn.html#preprocesamiento",
    "title": "Clases UAI",
    "section": "Preprocesamiento",
    "text": "Preprocesamiento\n\nfrom feature_engine.imputation import CategoricalImputer\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X)\nX_imp\n\n\n\n\n\n\n\n\nclass\nsex\nembark_town\nfare\nage\n\n\n\n\n0\nThird\nmale\nSouthampton\n7.2500\n22.0\n\n\n1\nFirst\nfemale\nCherbourg\n71.2833\n38.0\n\n\n2\nThird\nfemale\nSouthampton\n7.9250\n26.0\n\n\n3\nFirst\nfemale\nSouthampton\n53.1000\n35.0\n\n\n4\nThird\nmale\nSouthampton\n8.0500\n35.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nSecond\nmale\nSouthampton\n13.0000\n27.0\n\n\n887\nFirst\nfemale\nSouthampton\n30.0000\n19.0\n\n\n888\nThird\nfemale\nSouthampton\n23.4500\nNaN\n\n\n889\nFirst\nmale\nCherbourg\n30.0000\n26.0\n\n\n890\nThird\nmale\nQueenstown\n7.7500\n32.0\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nfrom feature_engine.imputation import MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X_imp)\nX_imp\n\n\n\n\n\n\n\n\nclass\nsex\nembark_town\nfare\nage\n\n\n\n\n0\nThird\nmale\nSouthampton\n7.2500\n22.000000\n\n\n1\nFirst\nfemale\nCherbourg\n71.2833\n38.000000\n\n\n2\nThird\nfemale\nSouthampton\n7.9250\n26.000000\n\n\n3\nFirst\nfemale\nSouthampton\n53.1000\n35.000000\n\n\n4\nThird\nmale\nSouthampton\n8.0500\n35.000000\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nSecond\nmale\nSouthampton\n13.0000\n27.000000\n\n\n887\nFirst\nfemale\nSouthampton\n30.0000\n19.000000\n\n\n888\nThird\nfemale\nSouthampton\n23.4500\n29.699118\n\n\n889\nFirst\nmale\nCherbourg\n30.0000\n26.000000\n\n\n890\nThird\nmale\nQueenstown\n7.7500\n32.000000\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nfrom feature_engine.encoding import OneHotEncoder\n\nohe = OneHotEncoder()\nX_ohe = ohe.fit_transform(X_imp)\nX_ohe\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n7.2500\n22.000000\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n71.2833\n38.000000\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n2\n7.9250\n26.000000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n3\n53.1000\n35.000000\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n4\n8.0500\n35.000000\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n13.0000\n27.000000\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n887\n30.0000\n19.000000\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n888\n23.4500\n29.699118\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n889\n30.0000\n26.000000\n0\n1\n0\n1\n0\n0\n1\n0\n\n\n890\n7.7500\n32.000000\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 10 columns\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_ohe)\nX_sc_all\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.502445\n-0.592481\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n1\n0.786845\n0.638789\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n-1.623803\n2.074505\n-0.307562\n\n\n2\n-0.488854\n-0.284663\n0.902587\n-0.565685\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n3\n0.420730\n0.407926\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n4\n-0.486337\n0.407926\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.386671\n-0.207709\n-1.107926\n-0.565685\n1.960202\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n887\n-0.044381\n-0.823344\n-1.107926\n1.767767\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n888\n-0.176263\n0.000000\n0.902587\n-0.565685\n-0.510152\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n889\n-0.044381\n-0.284663\n-1.107926\n1.767767\n-0.510152\n0.737695\n-0.737695\n-1.623803\n2.074505\n-0.307562\n\n\n890\n-0.492378\n0.177063\n0.902587\n-0.565685\n-0.510152\n0.737695\n-0.737695\n-1.623803\n-0.482043\n3.251373\n\n\n\n\n891 rows × 10 columns\n\n\n\n\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"fare\", \"age\"])\nX_sc = sc.fit_transform(X_ohe)\nX_sc\n\n\n\n\n\n\n\n\nfare\nage\nclass_Third\nclass_First\nclass_Second\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.502445\n-0.592481\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n0.786845\n0.638789\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n2\n-0.488854\n-0.284663\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n3\n0.420730\n0.407926\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n4\n-0.486337\n0.407926\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.386671\n-0.207709\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n887\n-0.044381\n-0.823344\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n888\n-0.176263\n0.000000\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n889\n-0.044381\n-0.284663\n0\n1\n0\n1\n0\n0\n1\n0\n\n\n890\n-0.492378\n0.177063\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 10 columns"
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "href": "tics411/notebooks/11-ex-knn.html#entrenamiento-del-modelo",
    "title": "Clases UAI",
    "section": "Entrenamiento del Modelo",
    "text": "Entrenamiento del Modelo\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn_clf(X, y, k=5, prep=\"\"):\n    knn = KNeighborsClassifier(\n        n_neighbors=k, metric=\"euclidean\", n_jobs=-1\n    )\n    ## Notar que es posible utilizar Variables categóricas como Etiquetas...\n    knn.fit(X, y)\n    y_pred = knn.predict(X)\n    print(\n        f\"Score k = {k}, y Preprocesamiento: {prep}: {knn.score(X,y):.4f}\"\n    )\n    return y_pred\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    print(\n        \"=================================================================\"\n    )\n    y_pred_sc = knn_clf(X_sc, y, k=k, prep=\"StandardScaler Numérico\")\n    y_pred_sc_all = knn_clf(X_sc_all, y, k=k, prep=\"StandardScaler a todo\")\n    y_pred_ohe = knn_clf(X_ohe, y, k=k, prep=\"Sin Escalar\")\n\n=================================================================\nScore k = 3, y Preprocesamiento: StandardScaler Numérico: 0.8844\nScore k = 3, y Preprocesamiento: StandardScaler a todo: 0.8855\nScore k = 3, y Preprocesamiento: Sin Escalar: 0.8384\n=================================================================\nScore k = 5, y Preprocesamiento: StandardScaler Numérico: 0.8698\nScore k = 5, y Preprocesamiento: StandardScaler a todo: 0.8698\nScore k = 5, y Preprocesamiento: Sin Escalar: 0.8204\n=================================================================\nScore k = 7, y Preprocesamiento: StandardScaler Numérico: 0.8608\nScore k = 7, y Preprocesamiento: StandardScaler a todo: 0.8575\nScore k = 7, y Preprocesamiento: Sin Escalar: 0.7834\n=================================================================\nScore k = 9, y Preprocesamiento: StandardScaler Numérico: 0.8418\nScore k = 9, y Preprocesamiento: StandardScaler a todo: 0.8406\nScore k = 9, y Preprocesamiento: Sin Escalar: 0.7733\n=================================================================\nScore k = 11, y Preprocesamiento: StandardScaler Numérico: 0.8373\nScore k = 11, y Preprocesamiento: StandardScaler a todo: 0.8361\nScore k = 11, y Preprocesamiento: Sin Escalar: 0.7643\n=================================================================\nScore k = 13, y Preprocesamiento: StandardScaler Numérico: 0.8272\nScore k = 13, y Preprocesamiento: StandardScaler a todo: 0.8283\nScore k = 13, y Preprocesamiento: Sin Escalar: 0.7587\n=================================================================\nScore k = 15, y Preprocesamiento: StandardScaler Numérico: 0.8215\nScore k = 15, y Preprocesamiento: StandardScaler a todo: 0.8249\nScore k = 15, y Preprocesamiento: Sin Escalar: 0.7486\n\n\n\nConclusión: Los Preprocesamientos afectan de manera importante el entrenamiento de un modelo."
  },
  {
    "objectID": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "href": "tics411/notebooks/11-ex-knn.html#uso-de-pipelines",
    "title": "Clases UAI",
    "section": "Uso de Pipelines",
    "text": "Uso de Pipelines\n\nfrom sklearn.pipeline import Pipeline\n\n\ndef model_pipeline(num_method, cat_method, sc_variables, k=5):\n\n    sc = SklearnTransformerWrapper(\n        StandardScaler(), variables=sc_variables\n    )\n\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", OneHotEncoder()),\n            # (\"sc\", StandardScaler()),\n            (\"sc\", sc),\n            (\"model\", KNeighborsClassifier(n_neighbors=k, n_jobs=-1)),\n        ]\n    )\n\n    return pipe\n\n\npipe = model_pipeline(\n    num_method=\"mean\", cat_method=\"frequent\", sc_variables=[\"age\", \"fare\"]\n)\npipe\n\nPipeline(steps=[('num_imp', MeanMedianImputer(imputation_method='mean')),\n                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n                ('ohe', OneHotEncoder()), ('sc', StandardScaler()),\n                ('model', KNeighborsClassifier(n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('num_imp', MeanMedianImputer(imputation_method='mean')),\n                ('cat_imp', CategoricalImputer(imputation_method='frequent')),\n                ('ohe', OneHotEncoder()), ('sc', StandardScaler()),\n                ('model', KNeighborsClassifier(n_jobs=-1))]) MeanMedianImputerMeanMedianImputer(imputation_method='mean') CategoricalImputerCategoricalImputer(imputation_method='frequent') OneHotEncoderOneHotEncoder()  StandardScaler?Documentation for StandardScalerStandardScaler()  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_jobs=-1) \n\n\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\npipe.score(X, y)\n\n0.8698092031425365\n\n\n\ny_pred\n\narray(['no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes',\n       'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes',\n       'no', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no',\n       'yes', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no',\n       'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes',\n       'yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes',\n       'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes',\n       'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no',\n       'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes',\n       'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no',\n       'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no',\n       'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no',\n       'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no',\n       'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes',\n       'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no',\n       'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes',\n       'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes',\n       'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no',\n       'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no',\n       'yes', 'no', 'no', 'no'], dtype=object)"
  },
  {
    "objectID": "tics411/notebooks/pauta_guia2.html",
    "href": "tics411/notebooks/pauta_guia2.html",
    "title": "Pregunta 3:",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.DataFrame(\n    dict(x=[4, 5, 5, 6, 7, 7], y=[1, 1, 2, 7, 6, 7], c=[1, 1, 1, 2, 2, 2]),\n    index=[*range(1, 7)],\n)\ndf\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n1\n4\n1\n1\n\n\n2\n5\n1\n1\n\n\n3\n5\n2\n1\n\n\n4\n6\n7\n2\n\n\n5\n7\n6\n2\n\n\n6\n7\n7\n2\nfrom scipy.spatial import distance_matrix\nimport numpy as np\n\npoint = np.array([[8, 4]])\n\n## Distancias del Punto al resto de los puntos\npd.DataFrame(\n    distance_matrix(point, df[[\"x\", \"y\"]]), columns=[*range(1, 7)]\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n5.0\n4.242641\n3.605551\n3.605551\n2.236068\n3.162278\nfrom scipy.spatial.distance import cdist\n\npd.DataFrame(\n    cdist(point, df[[\"x\", \"y\"]], \"mahalanobis\"), columns=[*range(1, 7)]\n)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n3.07265\n2.182821\n2.357716\n3.24037\n1.855041\n2.338929"
  },
  {
    "objectID": "tics411/notebooks/pauta_guia2.html#pregunta-7",
    "href": "tics411/notebooks/pauta_guia2.html#pregunta-7",
    "title": "Pregunta 3:",
    "section": "Pregunta 7:",
    "text": "Pregunta 7:\n\nconf_mat = np.array([[8, 2, 0], [4, 13, 13], [5, 12, 43]])\n\n\n## Función para tomar una matriz de confusión y recrear las predicciones\ndef create_vectors(conf_mat):\n    rows, cols = conf_mat.shape\n\n    original_vals = np.empty((0, 2))\n    for i in range(rows):\n        for j in range(cols):\n            interim = np.repeat([[i, j]], conf_mat[i, j], axis=0)\n            original_vals = np.append(original_vals, interim, axis=0)\n\n    y = original_vals[:, 0]\n    y_pred = original_vals[:, 1]\n    return y, y_pred\n\n\ny, y_pred = create_vectors(conf_mat)\n\n\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n## Las predicciones recreadas generan esta matriz de confusión\n\n# OJO: ELIMINÉ un 1 para que hayan 100 valores, si no es lo que se busca,\n# se puede agregar el 1 en el objeto conf_mat de más arriba.\n\nConfusionMatrixDisplay.from_predictions(y, y_pred)\n\n## Esdto entrega el Accuracy único como mandé en un notebook y Precision, Recall y F1 por clases 0,1 y 2.\nprint(classification_report(y, y_pred, digits=3))\n\n              precision    recall  f1-score   support\n\n         0.0      0.471     0.800     0.593        10\n         1.0      0.481     0.433     0.456        30\n         2.0      0.768     0.717     0.741        60\n\n    accuracy                          0.640       100\n   macro avg      0.573     0.650     0.597       100\nweighted avg      0.652     0.640     0.641       100"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html",
    "href": "tics411/notebooks/02-EDA.html",
    "title": "EDA",
    "section": "",
    "text": "El siguiente notebook tiene por propósito mostrar algunos comandos básicos para poder realizar Exploración de Datos utilizando Pandas.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Vamos a cargar los siguientes datos para poder explorarlos.\niris_df = sns.load_dataset(\"iris\")\ntitanic_df = sns.load_dataset(\"titanic\")\nts_df = sns.load_dataset(\"dowjones\")\niris_df\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-tendencia-central",
    "title": "EDA",
    "section": "Medidas de Tendencia Central",
    "text": "Medidas de Tendencia Central\nLos comandos .mean() y .median() permiten calcular la media y la mediana en datos numéricos. Como se ve en los ejemplos permite llamar una Serie de Pandas y calcular un valor.\n\nTip: En caso de querer aplicar estos comandos a un DataFrame se recomienda utilizar el flag numeric_only = True para evitar calcular estos valores en Datos Categóricos donde no hacen sentido.\n\n\nprint(f\"Promedio de Ancho de Petalo {iris_df['sepal_width'].mean()}\")\nprint(f\"Mediana de Largo de Petalo {iris_df['sepal_length'].median()}\")\n\nPromedio de Ancho de Petalo 3.0573333333333337\nMediana de Largo de Petalo 5.8\n\n\nPandas también cuenta con el comando .mode() el cuál devuelve la moda. A diferencia de los comandos anteriores, .mode() puede utilizarse tanto para datos categóricos como datos numéricos.\n\nprint(f\"Moda de Especies: \")\niris_df[\"species\"].mode()\n\nModa de Especies: \n\n\n0        setosa\n1    versicolor\n2     virginica\nName: species, dtype: object\n\n\nEl comando .quantile() permite calcular algún percentil de interés. q es un valor que va entre 0 y 1 para indicar el percentil requerido. Recordar que la mediana es equivalente al Percentil 50.\n\np25 = iris_df[\"sepal_width\"].quantile(q=0.25)\np50 = iris_df[\"sepal_width\"].quantile(q=0.50)\np75 = iris_df[\"sepal_width\"].quantile(q=0.75)\niris_df[\"sepal_width\"].median(), p25, p50, p75\n\n(3.0, 2.8, 3.0, 3.3)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#medidas-de-dispersión",
    "href": "tics411/notebooks/02-EDA.html#medidas-de-dispersión",
    "title": "EDA",
    "section": "Medidas de Dispersión",
    "text": "Medidas de Dispersión\nPandas permite el cálculo de distintas medidas de dispersión. Al igual que los comandos anteriores contiene el flag numeric_only = True para evitar inconvenientes en DataFrames con distintos data types. Además contiene el comando ddof el cuál permitirá diferenciar si se quiere la medida poblacional (ddof = 0) o la muestral (ddof = 1).\n\n# Varianza Poblacional\niris_df.var(numeric_only=True, ddof=0)\n\nsepal_length    0.681122\nsepal_width     0.188713\npetal_length    3.095503\npetal_width     0.577133\ndtype: float64\n\n\n\n# Varianza Muestral\niris_df.var(numeric_only=True, ddof=1)\n\nsepal_length    0.685694\nsepal_width     0.189979\npetal_length    3.116278\npetal_width     0.581006\ndtype: float64\n\n\n\n# Desviación Estándar Muestral\niris_df.std(numeric_only=True, ddof=1)\n\nsepal_length    0.828066\nsepal_width     0.435866\npetal_length    1.765298\npetal_width     0.762238\ndtype: float64\n\n\n\n# Función para calcular el Rango Intercuartil...\ndef calculate_IQR(column):\n    quantiles = iris_df.quantile([0.25, 0.75], numeric_only=True)\n    iqr_sl = quantiles.loc[0.75, column] - quantiles.loc[0.25, column]\n    return iqr_sl\n\n\ncalculate_IQR(\"sepal_length\")\ncalculate_IQR(\"petal_width\")\n\n1.5\n\n\n\n# Coeficiente de Skewness o Asimetría.\niris_df.skew(numeric_only=True)\n\nsepal_length    0.314911\nsepal_width     0.318966\npetal_length   -0.274884\npetal_width    -0.102967\ndtype: float64"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#visualizaciones",
    "href": "tics411/notebooks/02-EDA.html#visualizaciones",
    "title": "EDA",
    "section": "Visualizaciones",
    "text": "Visualizaciones\nA continuación se mostrarán comandos propios de Pandas para poder generar los gráficos visto a lo largo de las clases. Se sugiere este tipo de gráficos cuando se trabaje con DataFrames ya que poseen buena documentación y una interfaz común para todos los gráficos.\nOpciones:\n\nkind: Permite indicar mediante un string el tipo de gráfico a mostrar.\nfigsize = (w,h): Permite fijar el tamaño de la figura. Notar que primero se entrega el ancho y luego el alto. Yo normalmente uso (20,6) ya que considero que queda bastante bien.\nedgecolor: Permite indicar el color del borde de las barras mediante un string. Tiene sentido para histogramás y bar plots.\ngrid = True/False: Permite mostrar o no una grilla.\nbins = n: Opción sólo para histogramas que permite indicar en cuántos bins se dividen los datos en el Histograma.\nalpha = 0.5: Corresponde al grado de transparecencia. Es un valor que va entre 0 y 1. Entre más pequeño el valor, más transparente.\ntitle: Permite agregar un Título como String.\nxlabel: Permite agregar un Título al Eje X.\nylabel: Permite agregar un Título al Eje Y.\n\n\nHistogramas\n\niris_df.plot(\n    kind=\"hist\", alpha=0.5, bins=30, figsize=(20, 6), edgecolor=\"black\"\n)\n# Notar que este genera todos los histogramas superpuestos...\n\nPor alguna razón Pandas tiene el comando .hist(). Este comando es bastante útil porque a diferencia del anterior no superpone los histogramas, lo cual la mayoría de las veces es lo que se busca.\n\niris_df.hist(figsize=(20, 6), bins=30, edgecolor=\"black\", grid=False)\n# tight_layout es opcional y a veces evita que hayan traslapes de títulos.\n# Usarlo si es que es necesario.\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nBarplots\nA diferencia de los Histográmas, los Barplots son utilizados para aplicar una agregación antes de gráficar. Esta agregación se puede utilizar mediante .value_counts() que permite contar valores, o mediante .groupby() el cuál permite aplicar otros tipos de agregación.\n\n# Acá por ejemplos contamos la cantidad de pasajeros por Sexo\ntitanic_df[\"sex\"].value_counts()\n\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\n\n# Una vez que tenemos contados los elementos podemos graficar...\ntitanic_df[\"sex\"].value_counts().plot(\n    kind=\"bar\",\n    figsize=(5, 6),\n    title=\"Número de Pasajeros por Sexo...\",\n    edgecolor=\"black\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n## Otro ejemplo, en este caso calculando el promedio por de Edad y Tarifa por Año.\ntitanic_df.groupby(\"pclass\")[[\"age\", \"fare\"]].mean()\n\n\n\n\n\n\n\n\nage\nfare\n\n\npclass\n\n\n\n\n\n\n1\n38.233441\n84.154687\n\n\n2\n29.877630\n20.662183\n\n\n3\n25.140620\n13.675550\n\n\n\n\n\n\n\n\n## En este caso, el índice Pclass irá al Eje X y los valores agregados de Age y Fare irán como barras.\niris_df.groupby(\"species\").mean().plot(kind=\"bar\", edgecolor=\"black\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#boxplots",
    "href": "tics411/notebooks/02-EDA.html#boxplots",
    "title": "EDA",
    "section": "Boxplots",
    "text": "Boxplots\n\niris_df.drop(columns=\"species\").plot(kind=\"box\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nPuntos\n\nNotar que a diferencia de los casos anteriores, el gráfico de puntos requiere que se definan qué columna irá en x y en y respectivamente.\n\n\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n)"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#lineplot",
    "href": "tics411/notebooks/02-EDA.html#lineplot",
    "title": "EDA",
    "section": "Lineplot",
    "text": "Lineplot\nEl lineplot es el gráfico por defecto de Pandas, por lo tanto no es necesario definir el parámetro kind. Al igual que el gráfico de Puntos se debe definir las variables x e y. Se recomienda siempre que x sea una variable de tipo temporal.\n\nts_df.plot(x=\"Date\", y=\"Price\", title=\"Evolución del Dow Jones\")\n\n\n## Este es un ejemplo de varias series de tiempo en conjunto.\n## Este código sólo genera datos sintéticos.\nfrom scipy.stats import norm\n\nts_df[\"AA\"] = ts_df[\"Price\"] + norm.rvs(size=649) * 55 + 1000\nts_df[\"BB\"] = -norm.rvs(size=649) * 55\n\nts_df.set_index(\"Date\").plot(title=\"Comparación distintas Tendencias\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#mosaico",
    "href": "tics411/notebooks/02-EDA.html#mosaico",
    "title": "EDA",
    "section": "Mosaico",
    "text": "Mosaico\nEn muchas ocaciones nosotros queremos mostrar una compilación de todos nuestros gráficos más que cada uno por separado. Para eso Matplotlib cuenta con la opción Mosaico.\nMosaico permite generar una grilla definida como un String. Si se fijan nuestra grilla se define por el string:\n\"\"\"AAA\n   BCC\"\"\"\nEn este caso nuestro canvas se divide en 6 partes, el gráfico que asigne a A utilizará las 3 secciones superiores, B utilizará sólo la sección de abajo a la izquierda y C utilizará las 2 restantes.\nPara asignar cada sección .plot() de pandas posee el parámetro ax donde se debe generar la asignación.\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.subplot_mosaic(\n    \"\"\"AAA\n       BCC\"\"\"\n)\n\n# Gráfico asignado a C\niris_df.drop(columns=\"species\").plot(\n    kind=\"box\", ax=ax[\"C\"], title=\"Distribución de Datos por Variable\"\n)\n\n## Gráfico asignado a B\niris_df.plot(\n    x=\"petal_length\",\n    y=\"petal_width\",\n    kind=\"scatter\",\n    title=\"Largo de Pétalo vs Ancho de Pétalo\",\n    xlabel=\"Largo\",\n    ylabel=\"Ancho\",\n    ax=ax[\"B\"],\n)\n\n## Gráfico asignado a A\niris_df.groupby(\"species\").mean().plot(\n    kind=\"bar\",\n    edgecolor=\"black\",\n    ax=ax[\"A\"],\n    rot=0,\n    title=\"Valores promedio por Especie\",\n)\n\n## Permite Agregar un título general a todo el Gráfico\nplt.suptitle(\"Este será un título para todo el Gráfico\", fontsize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#matplotlib",
    "href": "tics411/notebooks/02-EDA.html#matplotlib",
    "title": "EDA",
    "section": "Matplotlib",
    "text": "Matplotlib\nLos comandos mostrados anteriormente son una adaptación de Matplotlib a Pandas. La gracia que tienen es que son fáciles de aprender y funcionarán directamente en Pandas que será nuestra principal fuente de datos.\nEn el caso de trabajar con Numpy, estos comandos NO FUNCIONARÁN. Por lo tanto es necesario utilizar la API de Matplotlib. La traducción no es 100% directa, pero normalmente todos los parámetros de .plot() se cambiarán por comandos del tipo plt.---"
  },
  {
    "objectID": "tics411/notebooks/02-EDA.html#ejemplo",
    "href": "tics411/notebooks/02-EDA.html#ejemplo",
    "title": "EDA",
    "section": "Ejemplo",
    "text": "Ejemplo\nplt.plot(x,y, c = \"red\") #Existe también plt.bar, plt.hist, plt.scatter, plt.boxplot.\nplt.title(\"Este va a ser un título\")\nplt.xlabel(\"Este será una etiqueta del Eje X\")\nAprender Matplotlib es bastante más complicado pero tiene funcionalidades muchísimo más avanzadas que Pandas. Para este curso, no será necesario especializarse en Matplotlib, pero sí más adelante utilizaremos algunos gráficos que no se pueden hacer tan fácilmente en Pandas (pero serán casos puntuales)."
  },
  {
    "objectID": "tics411/notebooks/preguntas-prueba-2.html",
    "href": "tics411/notebooks/preguntas-prueba-2.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\ndf = pd.DataFrame(\n    dict(\n        y=[0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n        y_pred=[1, 1, 0, 0, 0, 1, 1, 1, 0, 0],\n    )\n)\nprint(classification_report(df.y, df.y_pred, digits=2))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0       0.40      0.40      0.40         5\n           1       0.40      0.40      0.40         5\n\n    accuracy                           0.40        10\n   macro avg       0.40      0.40      0.40        10\nweighted avg       0.40      0.40      0.40        10\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n4\nMucho\nMedio\n1\n\n\n5\nMucho\nBajo\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\nX_train\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n4\nMucho\nMedio\n1\n\n\n5\nMucho\nBajo\n1\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(X=[7, 5, 3, 5, 2], Y=[4, 7, 5, 7, 3], Clase=[1, 1, 1, -1, -1])\n)\nprint(df.to_latex())\n\n\\begin{tabular}{lrrr}\n\\toprule\n & X & Y & Clase \\\\\n\\midrule\n0 & 7 & 4 & 1 \\\\\n1 & 5 & 7 & 1 \\\\\n2 & 3 & 5 & 1 \\\\\n3 & 5 & 7 & -1 \\\\\n4 & 2 & 3 & -1 \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\ndf = pd.DataFrame(\n    {\n        \"Humedad\": [\n            85,\n            90,\n            86,\n            96,\n            80,\n            65,\n            70,\n            70,\n            95,\n            80,\n            91,\n            70,\n            90,\n            75,\n        ],\n        \"Se juega?\": [\n            \"No\",\n            \"No\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n            \"No\",\n            \"No\",\n            \"Sí\",\n            \"No\",\n            \"Sí\",\n            \"Sí\",\n            \"Sí\",\n        ],\n    }\n)\ndf.index = [*range(1, 15)]\nprint(df.to_latex())\n\n\\begin{tabular}{lrl}\n\\toprule\n & Humedad & Se juega? \\\\\n\\midrule\n1 & 85 & No \\\\\n2 & 90 & No \\\\\n3 & 86 & Sí \\\\\n4 & 96 & Sí \\\\\n5 & 80 & Sí \\\\\n6 & 65 & Sí \\\\\n7 & 70 & Sí \\\\\n8 & 70 & No \\\\\n9 & 95 & No \\\\\n10 & 80 & Sí \\\\\n11 & 91 & No \\\\\n12 & 70 & Sí \\\\\n13 & 90 & Sí \\\\\n14 & 75 & Sí \\\\\n\\bottomrule\n\\end{tabular}\n\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\n\ndf = pd.DataFrame(\n    dict(\n        y=[1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2],\n        y_pred=[1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 2],\n    )\n)\n\nprint(classification_report(df.y, df.y_pred, digits=3))\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n              precision    recall  f1-score   support\n\n           0      0.500     0.333     0.400         3\n           1      0.800     0.800     0.800         5\n           2      0.500     0.667     0.571         3\n\n    accuracy                          0.636        11\n   macro avg      0.600     0.600     0.590        11\nweighted avg      0.636     0.636     0.629        11\n\n\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay.from_predictions(df.y, df.y_pred)\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\nX_train\n\n\n\n\n\n\n\n\nX1\nX2\nc\n\n\n\n\n0\nMucho\nAlto\n1\n\n\n1\nPoco\nBajo\n1\n\n\n2\nMucho\nAlto\n0\n\n\n3\nMucho\nMedio\n1\n\n\n6\nMucho\nBajo\n1\n\n\n7\nPoco\nMedio\n0\n\n\n8\nMucho\nAlto\n0\n\n\n9\nMucho\nAlto\n1\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = pd.DataFrame(\n    dict(\n        X1=[\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Mucho\",\n            \"Poco\",\n            \"Mucho\",\n            \"Mucho\",\n        ],\n        X2=[\n            \"Alto\",\n            \"Bajo\",\n            \"Alto\",\n            \"Medio\",\n            \"Medio\",\n            \"Bajo\",\n            \"Bajo\",\n            \"Medio\",\n            \"Alto\",\n            \"Alto\",\n        ],\n        c=[1, 1, 0, 1, 1, 1, 1, 0, 0, 1],\n    )\n)\nX_train = df.drop(index=[4, 5])\nX_test = df.loc[[4, 5]]\n\ny_train = X_train.c\nX_train = X_train.drop(columns=\"c\")\n\noe = OrdinalEncoder()\nX_train_oe = oe.fit_transform(X_train)\nprint(oe.categories_)\ndt = DecisionTreeClassifier()\ndt.fit(X_train_oe, y_train)\n\nplt.figure(figsize=(20, 6))\nplot_tree(dt, filled=True, feature_names=X_train.columns)\nplt.tight_layout()\n\n[array(['Mucho', 'Poco'], dtype=object), array(['Alto', 'Bajo', 'Medio'], dtype=object)]\n\n\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        Brillo=[40, 50, 60, 10, 70, 60, 25],\n        Saturacion=[20, 50, 90, 25, 70, 10, 80],\n    )\n)\n\ndisplay(df)\ndf = pd.DataFrame(distance_matrix(df, df, p=2))\ndf.index = [*range(1, 8)]\ndf.columns = [*range(1, 8)]\ndf\n\n\n\n\n\n\n\n\nBrillo\nSaturacion\n\n\n\n\n0\n40\n20\n\n\n1\n50\n50\n\n\n2\n60\n90\n\n\n3\n10\n25\n\n\n4\n70\n70\n\n\n5\n60\n10\n\n\n6\n25\n80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n1\n0.000000\n31.622777\n72.801099\n30.413813\n58.309519\n22.360680\n61.846584\n\n\n2\n31.622777\n0.000000\n41.231056\n47.169906\n28.284271\n41.231056\n39.051248\n\n\n3\n72.801099\n41.231056\n0.000000\n82.006097\n22.360680\n80.000000\n36.400549\n\n\n4\n30.413813\n47.169906\n82.006097\n0.000000\n75.000000\n52.201533\n57.008771\n\n\n5\n58.309519\n28.284271\n22.360680\n75.000000\n0.000000\n60.827625\n46.097722\n\n\n6\n22.360680\n41.231056\n80.000000\n52.201533\n60.827625\n0.000000\n78.262379\n\n\n7\n61.846584\n39.051248\n36.400549\n57.008771\n46.097722\n78.262379\n0.000000\n\n\n\n\n\n\n\n\nimport numpy as np\n\nnp.sqrt((40 - 60) ** 2 + (20 - 10) ** 2)\n\n22.360679774997898\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/jerarquico.html",
    "href": "tics411/notebooks/jerarquico.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    dict(\n        alpha=[9, 10, 1, 6, 1],\n        beta=[3, 2, 9, 5, 10],\n        gamma=[7, 9, 4, 5, 3],\n    )\n)\n\nindices = [\"p53\", \"mdm2\", \"bcl2\", \"cylinE\", \"Caspade\"]\ndf.index = indices\ndf\n\n\n\n\n\n\n\n\nalpha\nbeta\ngamma\n\n\n\n\np53\n9\n3\n7\n\n\nmdm2\n10\n2\n9\n\n\nbcl2\n1\n9\n4\n\n\ncylinE\n6\n5\n5\n\n\nCaspade\n1\n10\n3\n\n\n\n\n\n\n\n\nfrom scipy.spatial import distance_matrix\n\n\ndef calculate_global_min(dm):\n    data = np.triu(dm)\n\n    min_val = np.nanmin(data[np.nonzero(data)])\n    position = [dm.index[val[0]] for val in np.where(data == min_val)]\n    return min_val, position\n\n\noriginal_dm = distance_matrix(df, df, p=2)\noriginal_dm = pd.DataFrame(\n    original_dm, index=df.index, columns=df.index\n).round(2)\noriginal_dm.index = np.arange(5).astype(str)\noriginal_dm.columns = np.arange(5).astype(str)\noriginal_dm\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\n1\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\n2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\n3\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\n4\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\ndata = original_dm.copy()\ndata.index = indices\ndata.columns = indices\ndata\n\n\n\n\n\n\n\n\np53\nmdm2\nbcl2\ncylinE\nCaspade\n\n\n\n\np53\n0.00\n2.45\n10.44\n4.12\n11.36\n\n\nmdm2\n2.45\n0.00\n12.45\n6.40\n13.45\n\n\nbcl2\n10.44\n12.45\n0.00\n6.48\n1.41\n\n\ncylinE\n4.12\n6.40\n6.48\n0.00\n7.35\n\n\nCaspade\n11.36\n13.45\n1.41\n7.35\n0.00\n\n\n\n\n\n\n\n\ndef clean_position(position):\n    pos = []\n    for p in position:\n        pos.extend(p.split(\",\"))\n    return pos\n\n\ndef new_iteration(dm, original_dm, linkage=np.nanmean):\n    min_val, position = calculate_global_min(dm)\n    print(f\"El valor mínimo encontrado es: {min_val}\")\n    print(f\"Clusters a fusionar: {position}\")\n    non_position = [col for col in dm.columns if col not in position]\n    print(f\"Clusters que no se fusionan: {non_position}\")\n    new_position = \",\".join(position)\n    new_dm = dm.copy()\n    values = []\n    clean_pos = clean_position(position)\n    for n_p in non_position:\n        n_p = n_p.split(\",\")\n        v = linkage(original_dm.loc[n_p, clean_pos])\n        values.append(v)\n\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    new_dm = new_dm.T\n    new_dm[new_position] = pd.Series(values, index=non_position)\n    return new_dm.drop(index=position, columns=position)\n\n\ndm_1 = new_iteration(original_dm, original_dm)\ndm_1\n\nEl valor mínimo encontrado es: 1.41\nClusters a fusionar: ['2', '4']\nClusters que no se fusionan: ['0', '1', '3']\n\n\n\n\n\n\n\n\n\n0\n1\n3\n2,4\n\n\n\n\n0\n0.00\n2.45\n4.120\n10.900\n\n\n1\n2.45\n0.00\n6.400\n12.950\n\n\n3\n4.12\n6.40\n0.000\n6.915\n\n\n2,4\n10.90\n12.95\n6.915\nNaN\n\n\n\n\n\n\n\n\ndm_2 = new_iteration(dm_1, original_dm)\ndm_2\n\nEl valor mínimo encontrado es: 2.45\nClusters a fusionar: ['0', '1']\nClusters que no se fusionan: ['3', '2,4']\n\n\n\n\n\n\n\n\n\n3\n2,4\n0,1\n\n\n\n\n3\n0.000\n6.915\n5.260\n\n\n2,4\n6.915\nNaN\n11.925\n\n\n0,1\n5.260\n11.925\nNaN\n\n\n\n\n\n\n\n\ndm_3 = new_iteration(dm_2, original_dm)\ndm_3\n\nEl valor mínimo encontrado es: 5.26\nClusters a fusionar: ['3', '0,1']\nClusters que no se fusionan: ['2,4']\n\n\n\n\n\n\n\n\n\n2,4\n3,0,1\n\n\n\n\n2,4\nNaN\n10.255\n\n\n3,0,1\n10.255\nNaN\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/hopkins.html",
    "href": "tics411/notebooks/hopkins.html",
    "title": "Ejemplos de Hopkins",
    "section": "",
    "text": "from pyclustertend import hopkins\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef blobs_examples(\n    n_centers, cluster_std, n_samples=4000, p=100, center_box=(-10, 10)\n):\n    df_spread, labels = make_blobs(\n        n_samples=n_samples,\n        centers=n_centers,\n        n_features=2,\n        random_state=42,\n        center_box=center_box,\n        cluster_std=cluster_std,\n    )\n    df_spread = pd.DataFrame(df_spread, columns=[\"x\", \"y\"])\n    plt.scatter(df_spread.x, df_spread.y, c=labels)\n    plt.title(f\"H = {1-hopkins(df_spread, p)}\")\n    plt.tight_layout()\n\n\n## Una sola nube, muy compacta...\nblobs_examples(n_centers=1, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchos nubes muy compactos...\nblobs_examples(n_centers=5, cluster_std=0.5, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\n## Muchas nubes extremadamente compactos\nblobs_examples(n_centers=5, cluster_std=0.001, n_samples=4000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso utilizamos la función make_blobs para simular clusters ficticios. Los clusters siempre son esféricos, es por eso que el Hopkins tiende a dar valores bastante buenos. Aunque, dependiendo de qué tan compacto sea la nube tiende a 1 de manera muy fuerte.\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\n\ndef random_examples(n_samples=4000, p=100):\n    df_random = pd.DataFrame(\n        np.random.rand(n_samples, 2), columns=[\"x\", \"y\"]\n    )\n    plt.scatter(df_random.x, df_random.y)\n    plt.title(f\"H = {1-hopkins(df_random, p)}\")\n    plt.tight_layout()\n\n\n## Puntos más dispersos\nrandom_examples(n_samples=100, p=10)\n\n\n\n\n\n\n\n\n\n## Más denso, pero aún aleatorio...\nrandom_examples(n_samples=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso estamos usando np.random.rand para simular sólo valores aleatorios. Se puede ver que entre más lleno está el espacio, Hopkins tiende a 0.5.\n\n\n## valores uniformemente distribuidos y con poca tendencia a agruparse (normalmente pocos puntos)\n## tienen H más pequeños, pero es díficil obtenerlos...\nnp.random.seed(0)\n\n\ndef uniform_example(n_samples=10, max_val=10, p=10):\n    df_uniform = pd.DataFrame(\n        dict(\n            x=np.random.randint(0, max_val + 1, size=n_samples),\n            y=np.random.randint(0, max_val, size=n_samples),\n        )\n    )\n\n    plt.scatter(df_uniform.x, df_uniform.y)\n    plt.title(f\"H = {1-hopkins(df_uniform, p)}\")\n\n\nuniform_example(n_samples=11, max_val=10, p=10)\n\n\n\n\n\n\n\n\n\nuniform_example(n_samples=4000, max_val=1000, p=100)\n\n\n\n\n\n\n\n\n\nEn este caso también estamos forzando aleatoriedad pero con uniformidad de distancia. Para eso simulamos usando np.random.randint para generar valores aleatorios pero más o menos equiespaciados uniformemente. Es bien interesante este caso, porque si usamos muchos datos, se tiende a valores completamente aleatorios, es decir H \\(\\sim\\) 0.5.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html",
    "href": "tics411/notebooks/07-ex-evaluation.html",
    "title": "Evaluación de Clusters",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom pyclustertend import vat, hopkins, ivat\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nnp.random.seed(0)\n\nset_config(transform_output=\"pandas\")\n\nX = sns.load_dataset(\"iris\").drop(columns=\"species\")\nX_random = np.random.rand(150, 4)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#visualización-de-ambos-datasets",
    "href": "tics411/notebooks/07-ex-evaluation.html#visualización-de-ambos-datasets",
    "title": "Evaluación de Clusters",
    "section": "Visualización de ambos Datasets",
    "text": "Visualización de ambos Datasets\n\n!pip install pyclustertend\n\nRequirement already satisfied: pyclustertend in /home/datacuber/miniconda3/lib/python3.9/site-packages (1.8.2)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.1.2 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.4.1.post1)\nRequirement already satisfied: numba&lt;0.55.0,&gt;=0.54.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (0.54.1)\nRequirement already satisfied: matplotlib&lt;4.0.0,&gt;=3.3.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (3.7.5)\nRequirement already satisfied: numpy==1.20.3 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.20.3)\nRequirement already satisfied: pandas&lt;2.0.0,&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pyclustertend) (1.5.3)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (9.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (23.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.0.9)\nRequirement already satisfied: cycler&gt;=0.10 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (0.11.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.2.0)\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (6.4.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (2.8.2)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (4.37.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.4.4)\nRequirement already satisfied: setuptools in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (67.5.1)\nRequirement already satisfied: llvmlite&lt;0.38,&gt;=0.37.0rc1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from numba&lt;0.55.0,&gt;=0.54.1-&gt;pyclustertend) (0.37.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from pandas&lt;2.0.0,&gt;=1.2.0-&gt;pyclustertend) (2022.2.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (3.1.0)\nRequirement already satisfied: scipy&gt;=1.6.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.2.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.1.2-&gt;pyclustertend) (1.3.2)\nRequirement already satisfied: zipp&gt;=3.1.0 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (3.11.0)\nRequirement already satisfied: six&gt;=1.5 in /home/datacuber/miniconda3/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&lt;4.0.0,&gt;=3.3.3-&gt;pyclustertend) (1.16.0)\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X)\npca = PCA(n_components=2)\npca_random = pca.fit_transform(X_random)\n\n\ndef compute_hopkins(X, p):\n    h_s = 1 - hopkins(X, p)\n    print(f\"Hopskins para p={p} es: {h_s}\")\n    return h_s\n\n\nhs_X = compute_hopkins(X, p=50)\nhs_random = compute_hopkins(X_random, p=50)\n\nfig, ax = plt.subplot_mosaic([[\"iris\", \"random\"]], figsize=(15, 6))\n\nax[\"iris\"].scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nax[\"random\"].scatter(pca_random[\"pca0\"], pca_random[\"pca1\"])\nax[\"random\"].set_title(\n    f\"Reducción a 2D de nuestros puntos aleatorios. H = {hs_random:.2f}\"\n)\nax[\"iris\"].set_title(f\"Reducción a 2D de Iris. H = {hs_X:.2f}\")\nplt.show()\n\nHopskins para p=50 es: 0.8241582644992403\nHopskins para p=50 es: 0.48048319214476964"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-iris",
    "title": "Evaluación de Clusters",
    "section": "VAT: Iris",
    "text": "VAT: Iris\n\nimport matplotlib.pyplot as plt\n\nvat(X_sc)\nplt.title(\"VAT para Iris Escalado\")\nivat(X_sc)\nplt.title(\"iVAT para Iris Escalado\")\n\nText(0.5, 1.0, 'iVAT para Iris Escalado')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "href": "tics411/notebooks/07-ex-evaluation.html#vat-random",
    "title": "Evaluación de Clusters",
    "section": "VAT: Random",
    "text": "VAT: Random\n\nvat(X_random)\nplt.title(\"VAT para Dataset Random\")\nivat(X_random)\nplt.title(\"iVAT para Dataset Random\")\n\nText(0.5, 1.0, 'iVAT para Dataset Random')"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#correlación",
    "href": "tics411/notebooks/07-ex-evaluation.html#correlación",
    "title": "Evaluación de Clusters",
    "section": "Correlación",
    "text": "Correlación\n\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance_matrix\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X_sc)\n\n\ndef cluster_correlation(X, labels, p=2):\n    \"\"\"p corresponde al nivel de la distancia de Minkowski\"\"\"\n    ideal_sim = (labels == labels.reshape(-1, 1)).astype(np.float32)\n\n    d_matrix = distance_matrix(X, X, p=p)\n    S = 1 / (d_matrix + 1)\n    return np.corrcoef(S.flatten(), ideal_sim.flatten()).min()\n\n\ncluster_correlation(X_sc, labels)\n\n0.6856891998862197"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#cohesión-y-separación",
    "href": "tics411/notebooks/07-ex-evaluation.html#cohesión-y-separación",
    "title": "Evaluación de Clusters",
    "section": "Cohesión y Separación",
    "text": "Cohesión y Separación\n\ncenters = km.cluster_centers_\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nsse, ssb = compute_clustering_metrics(X_sc, labels, centers, is_df=True)\nsse, ssb\n\n(222.36170496502297, 377.638295034977)"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "href": "tics411/notebooks/07-ex-evaluation.html#ejemplo-de-clases",
    "title": "Evaluación de Clusters",
    "section": "Ejemplo de Clases",
    "text": "Ejemplo de Clases\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(\n    dict(\n        x=[2, 3, 4, 8, 9, 10, 6, 7, 8],\n        y=[5, 4, 6, 3, 2, 5, 10, 8, 9],\n        c=[0, 0, 0, 1, 1, 1, 2, 2, 2],\n    )\n)\n\nd_matrix = distance_matrix(df[[\"x\", \"y\"]], df[[\"x\", \"y\"]], p=2)\nplt.scatter(df.x, df.y, c=df.c, s=200, edgecolors=\"k\")\n\ndf\n\n\n\n\n\n\n\n\nx\ny\nc\n\n\n\n\n0\n2\n5\n0\n\n\n1\n3\n4\n0\n\n\n2\n4\n6\n0\n\n\n3\n8\n3\n1\n\n\n4\n9\n2\n1\n\n\n5\n10\n5\n1\n\n\n6\n6\n10\n2\n\n\n7\n7\n8\n2\n\n\n8\n8\n9\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(df[[\"x\", \"y\"]], df.c)\n\n0.614855027897113\n\n\n\n## Esta función se hizo sólo para mostrar los pasos intermedios\n## Usen esta función para revisar sus resultados cuando estudien para la prueba.\n\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, -1]\n    return a, b, bj, n_clusters\n\n\na, b, bj, n_clusters = silhouette_score_m(d_matrix, df.c.values)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.6395238095238095"
  },
  {
    "objectID": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "href": "tics411/notebooks/07-ex-evaluation.html#silhouette-curve",
    "title": "Evaluación de Clusters",
    "section": "Silhouette Curve",
    "text": "Silhouette Curve\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X_sc, labels)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/viz.html",
    "href": "tics411/notebooks/viz.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\na = np.array([4.5, 4, 4.1, 1, 2.3, 2.2, 2.4, 5, 5.5, 6.2, 6, 6, 6, 6])\nb = np.append(a, a + 1)\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(b, bins=5)\nax[\"A\"].set_title(\"Bins 5\")\nax[\"A\"].set_xlabel(\"Notas\")\nax[\"A\"].set_ylabel(\"Número de Estudiantes\")\nax[\"B\"].hist(b, bins=15)\nax[\"B\"].set_title(\"Bins 15\")\nax[\"B\"].set_xlabel(\"Notas\")\nax[\"C\"].hist(b, bins=30)\nax[\"C\"].set_title(\"Bins 30\")\nax[\"C\"].set_xlabel(\"Notas\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import KernelDensity\n\n\nkd_gauss = KernelDensity(kernel=\"epanechnikov\")\nkd_gauss.fit(b[:, np.newaxis])\nx_grid = np.linspace(1, 6, 1000)\nb_gauss = np.exp(kd_gauss.score_samples(x_grid[:, np.newaxis]))\nplt.hist(b)\nplt.plot(b_gauss)\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nnp.random.seed(0)\nx_grid = np.linspace(-4.5, 3.5, 1000)\nx = np.concatenate([norm(-1, 1.0).rvs(400), norm(1, 0.3).rvs(100)])\n\n\ndef kde_sklearn(x, x_grid, bandwidth=0.2, **kwargs):\n    \"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n    kde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n    kde_skl.fit(x[:, np.newaxis])\n    # score_samples() returns the log-likelihood of the samples\n    log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n    return np.exp(log_pdf)\n\n\npdf = kde_sklearn(x, x_grid, bandwidth=0.2)\n\nplt.hist(x)\nplt.plot(x_grid, pdf)\n\n\n\n\n\n\n\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"tophat\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_tophat = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_gaussian = np.exp(log_densidad_pred)\n\n\nn = 100\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2)) + 3\n\nX_grid = np.linspace(0, 7, 200)\n\nmodelo_kde = KernelDensity(kernel=\"epanechnikov\", bandwidth=0.2)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred_epa = np.exp(log_densidad_pred)\n\n\nfig = plt.figure(figsize=(20, 6))\nax = fig.subplot_mosaic(\"ABC\")\nax[\"A\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"A\"].plot(X_grid, densidad_pred_tophat, color=\"red\", label=\"predicción\")\nax[\"A\"].set_title(\"Kernel Uniforme/Tophat h=0.2\")\n\nax[\"B\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"B\"].plot(X_grid, densidad_pred_gaussian, color=\"red\", label=\"predicción\")\nax[\"B\"].set_title(\"Kernel Gaussiano h=0.2\")\n\nax[\"C\"].hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax[\"C\"].plot(X_grid, densidad_pred_epa, color=\"red\", label=\"predicción\")\nax[\"C\"].set_title(\"Kernel Epanechnikov h=0.2\")\n\nText(0.5, 1.0, 'Kernel Epanechnikov h=0.2')\n\n\n\n\n\n\n\n\n\n\nn = 1000\nnp.random.seed(123)\nmuestra_1 = np.random.normal(loc=1, scale=0.5, size=int(n * 0.75))\nmuestra_2 = np.random.normal(loc=-1, scale=0.5, size=int(n * 0.25))\ndatos = np.hstack((muestra_1, muestra_2))\n\nX_grid = np.linspace(-3, 4, 1000)\n\nmodelo_kde = KernelDensity(kernel=\"linear\", bandwidth=1)\nmodelo_kde.fit(datos.reshape(-1, 1))\n\nlog_densidad_pred = modelo_kde.score_samples(X_grid.reshape((-1, 1)))\n# Se aplica el exponente para deshacer el logaritmo\ndensidad_pred = np.exp(log_densidad_pred)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.hist(datos, bins=30, density=True, color=\"#3182bd\", alpha=0.5)\nax.plot(X_grid, densidad_pred, color=\"red\", label=\"predicción\")\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df[\"embarked\"].value_counts().plot(\n    kind=\"bar\", rot=0, title=\"Personas embarcadas por puerto del Titanic\"\n)\n\n\n\n\n\n\n\n\n\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\n\n\n\ng = sns.catplot(\n    data=titanic_df, y=\"fare\", x=\"pclass\", kind=\"bar\", errorbar=None, hue=\"sex\"\n)\ng.figure.suptitle(\"Tarifa Promedio de Pasajeros del Titanic\\n por Clase y Sexo.\")\n\n\n\n\n\n\n\n\n\ndata = titanic_df[[\"age\", \"fare\"]].melt(value_vars=[\"age\", \"fare\"])\ndata\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nage\n22.00\n\n\n1\nage\n38.00\n\n\n2\nage\n26.00\n\n\n3\nage\n35.00\n\n\n4\nage\n35.00\n\n\n...\n...\n...\n\n\n1777\nfare\n13.00\n\n\n1778\nfare\n30.00\n\n\n1779\nfare\n23.45\n\n\n1780\nfare\n30.00\n\n\n1781\nfare\n7.75\n\n\n\n\n1782 rows × 2 columns\n\n\n\n\nsns.catplot(\n    kind=\"box\", x=\"variable\", y=\"value\", data=data, height=6, aspect=0.5, hue=\"variable\"\n)\n\n\n\n\n\n\n\n\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\ndata = iris_df.melt(\n    value_vars=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n)\ndata\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nsepal_length\n5.1\n\n\n1\nsepal_length\n4.9\n\n\n2\nsepal_length\n4.7\n\n\n3\nsepal_length\n4.6\n\n\n4\nsepal_length\n5.0\n\n\n...\n...\n...\n\n\n595\npetal_width\n2.3\n\n\n596\npetal_width\n1.9\n\n\n597\npetal_width\n2.0\n\n\n598\npetal_width\n2.3\n\n\n599\npetal_width\n1.8\n\n\n\n\n600 rows × 2 columns\n\n\n\n\ng = sns.catplot(x=\"variable\", y=\"value\", kind=\"box\", hue=\"variable\", data=data)\ng.figure.suptitle(\"Distribución de Medidas de Flores\")\ng._legend.remove()\ng.set(xlabel=None)\ng.set(ylabel=None)\n\n\n\n\n\n\n\n\n\nplt.scatter(iris_df.sepal_length, iris_df.petal_length)\nplt.title(\"Relación entre Largo del Sépalo y del Pétalo\")\nplt.xlabel(\"Largo del Sépalo\")\nplt.ylabel(\"Largo del Pétalo\")\n\nText(0, 0.5, 'Largo del Pétalo')\n\n\n\n\n\n\n\n\n\n\nanscombe = sns.load_dataset(\"anscombe\")\n\nfig = plt.figure(figsize=(10, 9))\nax = fig.subplot_mosaic(\n    \"\"\"AB\n                        CD\"\"\"\n)\nsns.regplot(data=anscombe.query(\"dataset == 'I'\"), x=\"x\", y=\"y\", ax=ax[\"A\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'II'\"), x=\"x\", y=\"y\", ax=ax[\"B\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'III'\"), x=\"x\", y=\"y\", ax=ax[\"C\"], ci=None)\nsns.regplot(data=anscombe.query(\"dataset == 'IV'\"), x=\"x\", y=\"y\", ax=ax[\"D\"], ci=None)\nplt.suptitle(\"Cuarteto de Anscombe\")\n\nText(0.5, 0.98, 'Cuarteto de Anscombe')\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\niris_df = sns.load_dataset(\"iris\")\niris_df\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\ndata = pca.fit_transform(iris_df.drop(columns=\"species\"))\nx, y = data[:,0], data[:,1]\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nc = iris_df.species.astype(\"category\").cat.codes\nplt.scatter(x,y, c = c)\nplt.title(\"3 Clusters\");\n\n\n\n\n\n\n\n\n\nc_prima = (c == 0).astype(\"int64\")\nplt.scatter(x,y, c= c_prima)\nplt.title(\"2 Clusters\")\n\nText(0.5, 1.0, '2 Clusters')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/clase-4.html#definiciones",
    "href": "tics411/clase-4.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nClustering Jerárquico\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender. Se basa en la construcción de Jerarquías para ir construyendo clusters.\n\n\nDendograma\n\n\nCorresponde a un diagrama en el que se muestran las distancias de atributos entre clases que son parte de un mismo cluster."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerarquía",
    "href": "tics411/clase-4.html#clustering-jerarquía",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Jerarquía",
    "text": "Clustering: Jerarquía\n\nLos algoritmos basados en jerarquía pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-algoritmo",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Algoritmo",
    "text": "Clustering Aglomerativo: Algoritmo\nAlgoritmo\n\nInicialmente se considera cada punto como un cluster.\nCalcula la matriz de proximidad/distancia entre cada cluster.\nRepetir (hasta que exista un solo cluster):\n\nUnir los cluster más cercanos.\nActualizar la matriz de proximidad/distancia.\n\n\n\n\n\n\n\n\nLo más importante de este proceso es el cálculo de la matriz de proximidad/distancia entre clusters\n\n\n\n\n\n\n\n\n\nDistintos enfoques de distancia entre clusters, segmentan los datos en forma distinta."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Ejemplo",
    "text": "Clustering Aglomerativo: Ejemplo\nSupongamos que tenemos cinco tipos de genes cuya expresión ha sido determinada por 3 caracteríticas. Las siguientes expresiones pueden ser vistas como la expresión dados los genes en tres experimentos. ​\n\nApliquemos un Clustering Jerárquico Aglomerativo utilizando como medida de similaridad la Distancia Euclideana.\n\n\n\n\n\n\n\nOtros tipos de distancia también son aplicables siguiendo un procedimiento análogo."
  },
  {
    "objectID": "tics411/clase-4.html#algoritmo-1era-iteración",
    "href": "tics411/clase-4.html#algoritmo-1era-iteración",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo: 1era Iteración",
    "text": "Algoritmo: 1era Iteración\n\n\n\n\n\n\nEl algoritmo considerará que todos los puntos inicialmente son un cluster. Por lo tanto, tratará de encontrar los 2 puntos más cercanos e intentará unirnos en un sólo cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblema: ¿Cómo actualizamos la matriz de Distancias?\n\n\n\n\n\n\n\n\n\n\n\nEntonces crearemos un nuevo cluster: bcl2-Caspade."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-single-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Single Linkage",
    "text": "Clustering Aglomerativo: Single Linkage\n\n\n\n\n\n\n\n\n\nDistancia entre clusters determinada por los puntos más similares entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = min\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nGenera Clusters largos y delgados.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nAfectado por Outliers"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-complete-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Complete Linkage",
    "text": "Clustering Aglomerativo: Complete Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por la distancia ente los puntos más disímiles entre los clusters.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = max\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a quebrar Clusters Grandes.\nTiene tendencia a generar Clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-average-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Average Linkage",
    "text": "Clustering Aglomerativo: Average Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el promedio de las distancias que componen los clusters.\nPunto intermedio entre Single y Complete.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = avg\\{d(x,y) | x \\in C_i, y \\in C_j\\}\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a datos atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "href": "tics411/clase-4.html#clustering-aglomerativo-ward-linkage",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Aglomerativo: Ward Linkage",
    "text": "Clustering Aglomerativo: Ward Linkage\n\n\n\n\n\n\n\n\n\nDistancia determinada por el incremento del Within cluster distance.\nMinimiza la distancia intra cluster y maximiza la distancia entre clusters.\n\n\n\n\n\n\n\n\n\n\n\n\\[D(C_i, C_j) = wc(Cij) - wc(C_i) - wc(C_j) = \\frac{n_i\\cdot n_j}{n_i + n_j}||\\bar{C_i} - \\bar{C_j}||^2\\]\n\n\n\n\n\n\n\n\nVentajas\n\n\n\nMenos suceptible a dato atípicos.\n\n\n\n\n\n\n\n\n\n\n\nLimitaciones\n\n\n\nTiende a generar clusters circulares."
  },
  {
    "objectID": "tics411/clase-4.html#hiperparámetros",
    "href": "tics411/clase-4.html#hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "Hiperparámetros",
    "text": "Hiperparámetros\nLos Hiperparámetros de este modelo serán:\n\n\n\n\n\n\nNote\n\n\n\nlinkage: La forma de calcular la distancia entre clusters.\ndistancia: La distancia utilizada como similaridad entre los clusters.\n\n\n\n\n\n\n\n\n\n\nA diferencia de K-Means, este método no requiere definir el número de Clusters a priori."
  },
  {
    "objectID": "tics411/clase-4.html#volvamos-a-la-iteración-1",
    "href": "tics411/clase-4.html#volvamos-a-la-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Volvamos a la Iteración 1",
    "text": "Volvamos a la Iteración 1\n\nSupongamos que por simplicidad utilizaremos Average Linkage. (El proceso para utilizar otro linkage es análogo).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos a extraer una Matriz entre los puntos a fusionar y los puntos de los clusters restantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 1era Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#iteración-2",
    "href": "tics411/clase-4.html#iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 2",
    "text": "Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 2da Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#iteración-3",
    "href": "tics411/clase-4.html#iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 3",
    "text": "Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendograma: 3ra Iteración"
  },
  {
    "objectID": "tics411/clase-4.html#dendograma-resultante",
    "href": "tics411/clase-4.html#dendograma-resultante",
    "title": "TICS-411 Minería de Datos",
    "section": "Dendograma Resultante",
    "text": "Dendograma Resultante\n\n\n\n\n\n\nNo es necesario realizar la última iteración ya que se entiende que ambos clusters se unen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo encontramos los clusters una vez que tenemos el Dendograma?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos escoger un umbral de distancia y ver cuántos clusters se forman.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComo regla general se deben escoger clusters más distanciados entre sí."
  },
  {
    "objectID": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "href": "tics411/clase-4.html#efecto-del-linkage-escogido",
    "title": "TICS-411 Minería de Datos",
    "section": "Efecto del Linkage Escogido",
    "text": "Efecto del Linkage Escogido"
  },
  {
    "objectID": "tics411/clase-4.html#clustering-jerárquico-detalles-técnicos",
    "href": "tics411/clase-4.html#clustering-jerárquico-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering Jerárquico: Detalles Técnicos",
    "text": "Clustering Jerárquico: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nNo requiere definir el número de Clusters a priori.\nAl tener distintas variantes es posible que los puntos sean agrupados de manera completamente distintas.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nMuy ineficiente computacionalmente debido a que genera una nueva matriz de distancia en cada iteración lo que entrega una complejidad \\(O(n^2)\\) o \\(O(n^3)\\) dependiendo del linkage.\nUna vez que se decide combinar 2 clusters no es posible revertir esta decisión.\nNo tiene capacidad de generalización, ya que no es posible aplicarlo a datos nuevos."
  },
  {
    "objectID": "tics411/clase-4.html#implementación-en-scikit-learn",
    "href": "tics411/clase-4.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=2, metric=\"euclidean\",linkage=\"ward\")\n\n## Se entrena y se genera la predicción\nac.fit_predict(X)\n\n\nn_clusters: Define el número de clusters a crear, por defecto 2.\nmetric: Permite distancias L1, L2 y coseno. Por defecto “euclidean”.\nlinkage: Permite single, complete, average y ward. Por defecto “ward”.\n.fit_predict(): Entrenará el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento.\n\n\n\n\n\n\n\n\n\nSi bien el método de Aglomeración no requiere el número de clusters a generar, Scikit-Learn lo exige de modo de poder etiquetar cada elemento.\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué no existen los métodos .fit() y .predict() por separado?"
  },
  {
    "objectID": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "href": "tics411/clase-4.html#otras-implementaciones-dendograma",
    "title": "TICS-411 Minería de Datos",
    "section": "Otras implementaciones (Dendograma)",
    "text": "Otras implementaciones (Dendograma)\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Genera los cálculos necesarios para construir el Histograma.\nZ = linkage(X, method='single', metric=\"euclidean\") \n\n# Graficar el Dendograma\nplt.figure(figsize=(10, 5)) # Define el tamaño del Gráfico\nplt.title('Dendograma Clustering Jerárquico') # Define un título para el dendograma\nplt.xlabel('Iris Samples')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\nplt.show()\n\n\nPrincipalmente este código permite graficar el Dendograma completo.\nL4: Genera una instancia del Dendograma. (Sería equivalente al .fit() de Scikit-Learn).\nL5-L12: Corresponde al código necesario para graficar el Dendograma."
  },
  {
    "objectID": "tics411/clase-4.html#sugerencias",
    "href": "tics411/clase-4.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que el clustering aglomerativo también es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala común.\n\n\n\n\n\n\n\n\n\n\nOtras técnicas como merge y split, no aplican a este tipo de clustering debido a las limitaciones del algoritmo."
  },
  {
    "objectID": "tics411/clase-4.html#variantes",
    "href": "tics411/clase-4.html#variantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes",
    "text": "Variantes\n\nEn casos en los que no es posible calcular distancias debido a la presencia de datos categóricos, es posible utilizar el Gower Dissimilarity como medida de similitud.\n\n\n\n\n\n\n\n\n\nGower\n\nSe define como la proporción de variables que tienen distinto valor con respecto al total sin considerar donde ambos son ceros.\n\n\n\n\\[Gower(p1,p2) = \\frac{3}{9}\\]"
  },
  {
    "objectID": "tics411/clase-1.html#avisos-1",
    "href": "tics411/clase-1.html#avisos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Avisos",
    "text": "Avisos\n\n\n\n\n\n\nAyudantía\n\n\nAvisos\nTenemos (posible) ayudante, pero tenemos un problema de horario.\n\nHorario Actual: Viernes 20:00 a 21:10 hrs.\nHorario Propuesto: Lunes 11:45 a 12:55 hrs.\n\n\n\n\n\n\n\n\n\n\n\n\nTarea 1\n\n\n\nEntrega el 7 de Abril: Parejas inscribirse en Webcursos.\nPlazo para inscribir parejas: Este Domingo.\n\n\n\n\n\n\n\n\n\n\n\n\nFechas de Prueba\n\n\n\nPrueba 1: Martes 30 de Abril 18:30 a 21:00\nPrueba 2: Martes 11 de Julio 18:30 a 21:00"
  },
  {
    "objectID": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-1.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, registros, instancias. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos más amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo.\nDistintos tipos, por lo que normalmente requiere de algún tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-numéricos",
    "href": "tics411/clase-1.html#data-types-numéricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Numéricos",
    "text": "Data Types: Numéricos\n\nNuméricos\n\n\nValores a los que se les puede aplicar alguna operación matemática.\n\n\n\n\n\n\n\n\n\n\nDiscretas: Número finito o contable de valores. Integers (Enteros). Ej: Número de Hijos, Cantidad de Productos, Edad.\nContinuas: Existen infinitos puntos entre dos puntos. Floats (punto flotando o decimales). Ej. Temperatura, Peso."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-categóricos",
    "href": "tics411/clase-1.html#data-types-categóricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Categóricos",
    "text": "Data Types: Categóricos\n\nCategóricos\n\n\nDatos que representan una categoría.\n\n\n\n\n\n\n\n\n\n\nNominales: Sólo nombres que no representan ningún orden. Ej: Nacionalidad, género, ocupación.\nOrdinales: Que tienen un orden o jerarquía inherente. Ej: Nivel de Escolaridad, tamaño.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo todas las operaciones matemáticas son aplicables. Ej: Media, Mediana, Sumas, Restas, etc."
  },
  {
    "objectID": "tics411/clase-1.html#data-types-otros",
    "href": "tics411/clase-1.html#data-types-otros",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Types: Otros",
    "text": "Data Types: Otros\n\nStrings\n\n\nDatos de texto, los cuales podrían eventualmente ser tratados y representar algo. Ej: Rescatar comunas de una dirección, rescatar sexo desde el nombre, etc.\n\n\nFechas\n\n\nDatos tipo fecha, los cuales podrían eventualmente ser tratados y representar variables de algún tipo. Ej: Rescatar Años, meses, días, semanas, trimestres (quarters), etc.\n\n\nDatos Geográficos\n\n\nDatos que representan la ubicación geográfica de un elemento. Ej: Latitud, Longitud, Coordenadas.\n\n\n\n\n\n\n\n\n\n\nSin importar el tipo de dato el mayor problema es su calidad."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "href": "tics411/clase-1.html#calidad-de-los-datos-ruido",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Ruido",
    "text": "Calidad de los Datos: Ruido\n\nRuido\n\nCorresponde al error y extrema variabilidad en la medición en los datos. Este error puede ser aleatorio o sistemático.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe le llama Señal a la tendencia principal y representa la información significativa y valiosa de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "href": "tics411/clase-1.html#calidad-de-los-datos-outliers",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Outliers",
    "text": "Calidad de los Datos: Outliers\n\nOutliers\n\nSon datos considerablemente diferentes a la mayoría del dataset. Dependiendo del caso pueden indicar casos \"interesantes\" o errores de medición.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante notar que dependiendo del caso puede ser una buena idea deshacerse de ellos. ¿En qué casos podría no ser necesario eliminarlos?"
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "href": "tics411/clase-1.html#calidad-de-los-datos-valores-faltantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Valores Faltantes",
    "text": "Calidad de los Datos: Valores Faltantes\n\nMissing Values\n\n\nSon valores que por alguna razón no están presentes.\n\n\n\n\n\nMissing at Random (MAR): Son valores que no están presentes por causas que no se pueden controlar. Ej: No se registró, no se preguntó, fallas en el sistema de recolección de datos, etc.\nInformative Missing: Es un valor no aplicable. Ej: Sueldo en niños, Precio de la entrada de un concierto si es que NO compró entrada."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "href": "tics411/clase-1.html#calidad-de-los-datos-datos-duplicados",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Datos Duplicados",
    "text": "Calidad de los Datos: Datos Duplicados\n\nDuplicates\n\nSe refiere a registros que pueden estar total o parcialmente duplicados.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsto genera problemas en la confiabilidad de los datos. ¿Cuál es el registro correcto?\nEj: Caso particular de una Jooycar (una startup de seguros)."
  },
  {
    "objectID": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "href": "tics411/clase-1.html#calidad-de-los-datos-dominio-del-problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Calidad de los Datos: Dominio del Problema",
    "text": "Calidad de los Datos: Dominio del Problema\n\n\n\n\n\n\n\n\n\n\n\n\n\nPor lejos el problema de calidad más difícil de encontrar.\nSe requiere experiencia y conocimiento profundo del negocio para detectarlo.\n\nEj: Caso de Super Avances en Cencosud."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-1",
    "href": "tics411/clase-1.html#feature-engineering-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nFeature Engineering\n\n\nTambién conocida como Ingeniería de Atributos, es el arte de trabajar las features existentes para limpiar o corregir variables existentes o crear nuevas variables.\n\n\nPreprocesamiento\n\n\nSe refiere al proceso de preparación de los datos para su ingreso a un modelo. En una primera parte puede incluir limpieza de datos corruptos, redundantes y/o irrelevantes. Por otra parte, también hace referencia a la transformación de datos para que puedan ser consumidos por un algoritmo."
  },
  {
    "objectID": "tics411/clase-1.html#feature-engineering-2",
    "href": "tics411/clase-1.html#feature-engineering-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nNo existe un procedimiento estándar.\nRevisar los datos y ver potenciales errores que puedan afectar el funcionamiento de un modelo."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "href": "tics411/clase-1.html#preprocesamiento-valores-faltantes",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Valores Faltantes",
    "text": "Preprocesamiento: Valores Faltantes\n\nImputación: Se refiere al proceso de rellenar datos faltantes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependiendo del nivel de valores faltantes, es necesario evaluar la eliminación de registros o atributos completos de ser necesario."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-outliers",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Manejo de Outliers",
    "text": "Preprocesamiento: Manejo de Outliers\n\nCapping\n\nSe refiere al proceso de acotar un atributo eliminando los valores extremos o atípicos (outliers).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAl igual que en el caso anterior, es necesario evaluar la eliminación de registros si es que representan valores atípicos."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categóricas",
    "href": "tics411/clase-1.html#preprocesamiento-manejo-de-variables-categóricas",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Manejo de Variables Categóricas",
    "text": "Preprocesamiento: Manejo de Variables Categóricas\n\nLa mayoría de los modelos no tienen la capacidad de poder lidiar con variables categóricas por lo que deben ser transformadas en una representación numérica antes de ingresar a un modelo.\n\n\n\n\n\n\nOne Hot Encoder\n\n\n\n\n\n\nOrdinal Encoder\n\n\n\n\n\n\n\n\n\n\n\nOne Hot Encoder suele dar mejores resultados en modelos lineales modelos que dependan de distancias.\nOrdinal Encoder suele dar mejores resultados en modelos de árbol.\n\n\n\n\n\n\n\n¿Son necesarias todas las columnas en un One Hot Encoder?"
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\nEl escalamiento se refiere al proceso de llevar distintas variables a una misma escala.\n\n\n\n\n\n\n\n\n\nEvitar que la escala de una “sobre-importancia” a una cierta variable.\nPermitir una mejor convergencia de los algoritmos.\n\n\n\nStandardScaler (Normalización)\n\\[x_j=\\frac{x_j-\\mu_x}{\\sigma_x}\\]\n\n\n\n\n\n\n\nEste proceso fuerza (en la medida de lo posible) a tener media 0 y std 1.\nNotar que \\(\\sigma_x\\) hace referencia a la varianza poblacional.\n\n\n\n\nMinMax Scaler\n\\[x_j=\\frac{x_j-min(x_j)}{max(x_j)-min(x_j)}\\]\n\n\n\n\n\n\nEste proceso fuerza a los datos a distribuirse entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "href": "tics411/clase-1.html#preprocesamiento-escalamiento-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Preprocesamiento: Escalamiento",
    "text": "Preprocesamiento: Escalamiento\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia: 0.75\nStd: 3.1875\nMin: -2\nMax: 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentering (Centrado): Se le llama a la diferencia entre la variable y su media.\nScaling (Escalado): Se le llama al cuociente entre la variable y su Desviación Estándar.\nStandardScaler (Normalización): Es Centrado y Escalado."
  },
  {
    "objectID": "tics411/clase-1.html#creación-de-variables",
    "href": "tics411/clase-1.html#creación-de-variables",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de Variables",
    "text": "Creación de Variables\n\nCombinación\n\n\nCombinar 2 o más variables. Ej: Calcular el área de un sitio a partir del ancho y largo.\n\n\nTransformación\n\n\nAplicar una operación a una variable. Ej: El logaritmo de las ganancias.\n\n\n\n\n\n\nDiscretización (Binning)\n\n\nGenerar categorías a partir de una variable continua."
  },
  {
    "objectID": "tics411/clase-1.html#creación-de-variables-1",
    "href": "tics411/clase-1.html#creación-de-variables-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de Variables",
    "text": "Creación de Variables\n\nRatios\n\nEs una medida que expresa la relación entre dos cantidades. Ej: Puntos por partido, cantidad de transacciones por mes, etc.\n\nAgregación\n\nAgregar o agrupar información resumida de ciertas variables. Ej: Promedio de tiempo en aprobar un tipo de crédito."
  },
  {
    "objectID": "tics411/clase-1.html#selección-de-variables",
    "href": "tics411/clase-1.html#selección-de-variables",
    "title": "TICS-411 Minería de Datos",
    "section": "Selección de Variables",
    "text": "Selección de Variables\n\nSe refiere al proceso de eliminar variables que pueden ser irrelevantes o poco significativas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcesos Manuales.\nProcesos Automáticos:\n\nPCA (Principal Component Analysis).\nRecursive Feature Elimination.\nRecursive Feature Addition.\nEliminación mediante alguna medida.\n\n\n\n\n\n\n\n\n\n\n\n\nObjetivo\n\n\n\nPuede ser una técnica apropiada para combatir la Maldición de la Dimensionalidad (Curse of Dimensionality)."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-1",
    "href": "tics411/clase-1.html#medidas-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas",
    "text": "Medidas\n\nSon métricas que permiten cuantificar la relación existente entre dos o más objetos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad",
    "href": "tics411/clase-1.html#medidas-similaridad",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad",
    "text": "Medidas: Similaridad"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-nominal",
    "href": "tics411/clase-1.html#medidas-similaridad-nominal",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Nominal",
    "text": "Medidas: Similaridad Nominal\n\n\n\nDisimilaridad: \\[D =\n\\begin{cases}\n0,  & \\text{if $p=q$} \\\\[2ex]\n1, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\nSimilaridad:\n\\[S =\n\\begin{cases}\n1,  & \\text{if $p=q$} \\\\[2ex]\n0, & \\text{if $p\\neq q$}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 0\\] \\[D(p,q) = 1\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "href": "tics411/clase-1.html#medidas-similaridad-ordinal",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Ordinal",
    "text": "Medidas: Similaridad Ordinal\n\n\n\nDisimilaridad: \\[D = \\frac{|p-q|}{n}\\]\n\n\n\nSimilaridad:\n\\[S = 1 - \\frac{|p-q|}{n}\\]\n\n\n\n\n\n\n\n\n\n\n\\[S(p,q) = 1 - \\frac{5 - 4}{5} = 0.8\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "href": "tics411/clase-1.html#medidas-similaridad-intervalo-o-ratio",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Intervalo o Ratio",
    "text": "Medidas: Similaridad Intervalo o Ratio\n\n\n\nDisimilaridad: \\[D = |p-q|\\]\n\n\n\nSimilaridad:\n\\[S = -D\\] \\[S = \\frac{1}{1+D}\\]\n\n\n\nSea \\(p=35 °C\\) y \\(q = 40 °C\\). Luego:\n\\[ S(p,q) = -5\\] \\[S(p,q) = \\frac{1}{1 + 5} = 0.17\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos\n\nSea p y q vectores de dimensión \\(m\\) con sólo atributos categóricos. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\\[Sim(p,q) = \\sum_{i=1}^m S(p_i,q_i)\\]\n\n\n\n\nOverlap: \\[S(p_{a_i}, q_{a_i}) =\n\\begin{cases}\n1,  & \\text{if $p_{a_i} = q_{a_i}$} \\\\[2ex]\n0, & \\text{if $p_i\\neq q_i$}\n\\end{cases}\n\\]\n\n\n\nFrecuencia de Ocurrencia Inversa \\[S(p_i, q_i) = \\frac{1}{p_k(p_i)^2}\\]\n\n\n\nMedida de Goodall\n\n\\[S(p_i, q_i) = 1 - p_k(p_i)^2\\]\n\n\n\n\n\n\n\n\n\\(p_k()\\) se refiere a la probabilidad de ocurrencia del atributo k.\nTodas estas medidas son 0 si \\(p_i \\neq q_i\\)"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos\n\n\n\n\n\nEjercicio Propuesto: ¿Cuánto vale la similaridad entre los siguientes registros?\n\n1-4\n2-5\n7-8"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\nSea p y q vectores de dimensión \\(m\\) con sólo atributos binarios. Para calcular la similaridad entre vectores se usa lo siguiente:\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}}\\]\n\nSimple Matching Coefficient = Número de Coincidencias / Total de Atributos\n\n\n\\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}}\\]\n\nJaccard Coefficient = Número de Coincidencias 11 / Número de Atributos distintos de Ceros."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-binarios-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Binarios",
    "text": "Medidas: Similaridad Datos Binarios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\np_i\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nq_i\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n\n\n\\[SMC = \\frac{M_{00} + M_{11}}{M_{00} + M_{01} + M_{10} + M_{11}} = \\] \\[JC = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}} = \\]\n\n\n\\[\\frac{7 + 0}{7 + 2 + 1 + 0} = 0.7\\]\n\n\n\\[\\frac{0}{2 + 1 + 0} = 0\\]"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\nSean \\(d_1\\) y \\(d_2\\) dos vectores. La distancia coseno se calcula como:\n\n\\[cos(d_1, d_2) = \\frac{d_1 \\cdot d_2}{||d_1||||d_2||}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\nEjercicio Propuesto: ¿Cuánto vale \\(cos(d_1,d_2)\\) y \\(cos(d_1,d_3)\\)?"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-1",
    "href": "tics411/clase-1.html#distancias-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias",
    "text": "Distancias\n\nUna métrica o función de distancia es una función que define una distancia para cada par de elementos de un conjunto. Sean dos puntos x e y, una métrica o función de distancia debe satisfacer las siguientes condiciones:\n\n\nNo Negatividad:\n\n\\(d(x,y) = \\ge 0\\)\n\nIdentidad:\n\n\\(d(x,y) = 0 \\Leftrightarrow x = y\\)\n\nSimetría:\n\n\\(d(x,y) = d(y,x)\\)\n\nDesigualdad Triangular:\n\n\\(d(x,z) \\le d(x,y) + d(y,z)\\)"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Minkowski",
    "text": "Distancias: Distancia Minkowski\n\\[d(p,q) = \\left(\\sum_{k=1}^m |p_k - q_k|^r\\right)^{1/r}\\]\n\n\n\n\n\n\n\n\n\n\\(r=1 \\rightarrow\\) Distancia Manhattan (L1).\n\\(r=2 \\rightarrow\\) Distancia Euclideana (L2).\n\\(r=\\infty \\rightarrow\\) Distancia Chebyshev (L\\(\\infty\\)). \\[D_{ch}(p,q) = \\underset{k}{max} |p_k - q_k|\\]\n\n\n\n\n\n\n\nResolvamos en Colab\n\n\n\n\n\n\n\n\n\n\n\nSe denomina Matriz de Distancias a la Matriz que contiene la distancia \\(d(p_i,p_j)\\) en la coordenada \\(i,j\\)."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-minkowski-resultados",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Minkowski (Resultados)",
    "text": "Distancias: Distancia Minkowski (Resultados)"
  },
  {
    "objectID": "tics411/clase-1.html#ayudantías",
    "href": "tics411/clase-1.html#ayudantías",
    "title": "TICS-411 Minería de Datos",
    "section": "Ayudantías",
    "text": "Ayudantías\nAyudante: Sofía Alvarez\nemail: sofalvarez@alumnos.uai.cl\n\n\n\n\n\n\n\nLas ayudantías serán en la manera que sean necesarias.\nEstarán enfocadas principalmente en aplicaciones, código y dudas sobre Tarea."
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Mahalanobis",
    "text": "Distancias: Distancia Mahalanobis\n\\[d(p,q) = \\sqrt{(p-q)^T \\Sigma^{-1}(p-q)}\\]\ndonde \\(\\Sigma\\) es la Matriz de Covarianza de los datos de entrada.\n\\[cov(x,y) = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\]\n\nPara 2 variables p y q:\n\n\\[\\Sigma = \\begin{bmatrix}\ncov(p,p) & cov(p,q) \\\\\ncov(q,p) & cov(q,q)\n\\end{bmatrix}\n\\]\nEjercicio: Supongamos las siguientes escalas de notas. Calcular la distancia entre la nota (1.0 y 7.0)\n\ntest #1: 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0\ntest #2: 1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0"
  },
  {
    "objectID": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "href": "tics411/clase-1.html#distancias-distancia-mahalanobis-resultados",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancias: Distancia Mahalanobis (Resultados)",
    "text": "Distancias: Distancia Mahalanobis (Resultados)\n\n\n\n\n\n\ntest #1: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{3.79}(7-1)} = 3.08\\)\ntest #2: \\(d(7.0,1.0) = \\sqrt{(7-1)\\frac{1}{1.59}(7-1)} = 4.76\\)\n\n\n\n\n\n\n\n\n\n\nEs importante notar que la covarianza existente entre los datos influye en la distancia."
  },
  {
    "objectID": "tics411/clase-1.html#correlación-1",
    "href": "tics411/clase-1.html#correlación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación",
    "text": "Correlación\n\nLa correlación mide la relación lineal entre 2 atributos.\n\n\n\n\nCorrelación Poblacional\n\n\\[\\rho(X,Y) = corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}\\]\n\n\n\n\nCorrelación Muestral o Pearson\n\n\\[r(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{S_xS_y}\\]"
  },
  {
    "objectID": "tics411/clase-1.html#correlación-no-es-causalidad",
    "href": "tics411/clase-1.html#correlación-no-es-causalidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación no es Causalidad",
    "text": "Correlación no es Causalidad\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante recalcar que Causalidad no es igual a Correlación. Ver video.\nLa Correlación no se ve afectada por la escala de los datos."
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-2",
    "href": "tics411/clase-1.html#medidas-similaridad-datos-categóricos-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad Datos Categóricos",
    "text": "Medidas: Similaridad Datos Categóricos"
  },
  {
    "objectID": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "href": "tics411/clase-1.html#medidas-similaridad-distancia-coseno-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas: Similaridad (Distancia Coseno)",
    "text": "Medidas: Similaridad (Distancia Coseno)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\\(a_1\\)\n\\(a_2\\)\n\\(a_3\\)\n\\(a_4\\)\n\\(a_5\\)\n\\(a_6\\)\n\\(a_7\\)\n\\(a_8\\)\n\\(a_9\\)\n\\(a_{10}\\)\n\n\n\n\nd_1\n3\n2\n0\n5\n0\n0\n0\n2\n0\n0\n\n\nd_2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n2\n\n\nd_3\n6\n4\n0\n10\n0\n0\n0\n4\n0\n0\n\n\n\n\\[d_1 \\cdot d_2 = 5\\] \\[d_1 \\cdot d_3 = 84\\]\n\\[||d_1|| = \\sqrt{42} = 6.481\\] \\[||d_2|| = \\sqrt{6} = 2.449\\] \\[||d_3|| = \\sqrt{168} = 12.962\\]\n\\[cos(d_1, d_2) = 0.3150\\] \\[cos(d_1, d_3) = 0.9999\\]"
  },
  {
    "objectID": "tics411/clase-12.html#intuición",
    "href": "tics411/clase-12.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\nSupongamos el siguiente dataset:\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo puedo separar ambas clases?"
  },
  {
    "objectID": "tics411/clase-12.html#intuición-1",
    "href": "tics411/clase-12.html#intuición-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición"
  },
  {
    "objectID": "tics411/clase-12.html#intuición-2",
    "href": "tics411/clase-12.html#intuición-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n\n\n\n\n\n\n\n\n\n\n\n\nLa frontera de decisión se puede caracterizar como la ecuación de una recta (en forma general).\n\n\n\n\n\n\n\n\n\n\n\nAdemás definiremos \\(h_\\theta(X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2\\)."
  },
  {
    "objectID": "tics411/clase-12.html#intuición-3",
    "href": "tics411/clase-12.html#intuición-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n\n\n\n\n\n\n\n\n\n\n\nPodríamos pensar que si \\(h_\\theta(X)\\) es positivo entonces pertenece a la clase 1 y si \\(h_\\theta(X)\\) es negativo pertenece a la clase 0."
  },
  {
    "objectID": "tics411/clase-12.html#la-función-sigmoide-o-logística",
    "href": "tics411/clase-12.html#la-función-sigmoide-o-logística",
    "title": "TICS-411 Minería de Datos",
    "section": "La Función Sigmoide o Logística",
    "text": "La Función Sigmoide o Logística\n\\[ g(z) = \\frac{1}{1 + e^{-z}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunción no lineal.\nFunción acotada entre 0 y 1.\n\\(g(\\varepsilon) = 0.5\\), \\(\\varepsilon = 0\\)\n\n\n\n\n\n\n\n\n\n\n\nDe acá sale la noción del umbral 0.5 que hemos visto en clases anteriores.\n\n\n\n\n\n\n\n\n\n\n\n¿Qué pasaría si ahora decimos que \\(z = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2\\)?"
  },
  {
    "objectID": "tics411/clase-12.html#la-regresión-logística",
    "href": "tics411/clase-12.html#la-regresión-logística",
    "title": "TICS-411 Minería de Datos",
    "section": "La Regresión Logística",
    "text": "La Regresión Logística\n\\[P[y = 1|X, \\theta] = g(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2)}}\\]\n\n\n\n\n\n\n\nRegla de Decisión:\n\n\n\nSi \\(g(z) \\ge 0.5 \\implies Clase \\, 1\\).\nSi \\(g(z) &lt; 0.5 \\implies Clase \\, 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(g(z)\\) se puede interpretar como una probabilidad de pertenecer a la Clase 1.\n\n\n\n\n\n\n\n\n\n\n\n\\(1 -g(z)\\) se puede interpretar como una probabilidad de NO pertenecer a la Clase 1, es decir, pertenecer a la Clase 0."
  },
  {
    "objectID": "tics411/clase-12.html#aprendizaje-del-modelo",
    "href": "tics411/clase-12.html#aprendizaje-del-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Aprendizaje del Modelo",
    "text": "Aprendizaje del Modelo\nSupongamos lo siguiente:\n\n\n\\[P(y = 1| X, \\theta) = g(z)\\]\n\n\\[P(y = 0| X, \\theta) = 1-g(z)\\]\n\nAmbas ecuaciones pueden comprimirse en una sola de la siguiente manera: \\[ P(y|X,\\theta) = g(z)^y (1 - g(z))^{1-y}\\]\n\n\n\n\n\n\nPara encontrar los parámetros \\(\\theta\\) podemos utilizar una técnica llamada Maximum Likelihood Estimation."
  },
  {
    "objectID": "tics411/clase-12.html#maximum-likelihood-estimation",
    "href": "tics411/clase-12.html#maximum-likelihood-estimation",
    "title": "TICS-411 Minería de Datos",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\\[\\mathcal{L}(\\theta) = \\prod_{i=1}^n P(y^{(i)} | x^{(i)}, \\theta)\\]\n\\[ \\underset{\\theta}{argmin} \\ -l(\\theta)\\] \\[l(\\theta) = log (\\mathcal{L(\\theta)}) = \\sum_{i=1}^n y^{(i)} \\cdot log(g(z)) + (1-y^{(i)})\\cdot log(1-g(z))\\]\n\n\n\n\n\n\nEsta ecuación se conoce como Entropía Cruzada o como Negative Log Loss (NLL) y tiene la gracia de que es una curva convexa lo que garantiza un valor único de los parámetros \\(\\theta\\)."
  },
  {
    "objectID": "tics411/clase-12.html#cálculo-de-coeficientes",
    "href": "tics411/clase-12.html#cálculo-de-coeficientes",
    "title": "TICS-411 Minería de Datos",
    "section": "Cálculo de Coeficientes",
    "text": "Cálculo de Coeficientes\n\n\n\n\n\n\nLa técnica más famosa para minimizar este tipo de problemas se conoce como Stochastic Gradient Descent. Lo que genera la siguiente solución:\n\n\n\n\\[\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{1}{n}\\sum_{i=1}^n\\left(g(z)-y^{(i)}\\right)x_j^{(i)}\\]\n\n\n\n\n\n\nA pesar de lo complicado que se ve la ecuación, implementarla en código es bastante sencillo."
  },
  {
    "objectID": "tics411/clase-12.html#frontera-de-decisión",
    "href": "tics411/clase-12.html#frontera-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Frontera de Decisión",
    "text": "Frontera de Decisión"
  },
  {
    "objectID": "tics411/clase-12.html#inference-time",
    "href": "tics411/clase-12.html#inference-time",
    "title": "TICS-411 Minería de Datos",
    "section": "Inference Time",
    "text": "Inference Time\nEn este caso se calcula: \\[g_\\theta(x^{(i)})=sigmoid(\\theta^t x^{(i)})\\]\n\n\\(\\theta\\): Corresponde a un vector con todos los parámetros calculados.\n\\(x^{(i)}\\): Corresponde a una instancia de \\(m\\) variables la cual generará una probabilidad.\n\n\\(\\theta^t x^{(i)}\\) corresponde al producto punto de dos vectores, que es equivalente a una “suma producto”.\n\n\\(g_\\theta(x^{(i)})\\): Generará un valor entre 0 y 1 al cuál se le aplica la Regla de Decisión."
  },
  {
    "objectID": "tics411/clase-12.html#implementación-en-python",
    "href": "tics411/clase-12.html#implementación-en-python",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python",
    "text": "Implementación en Python\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=1, penalty=\"l2\", random_state = 42)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\ny_proba = lr.predict_proba(X_test)\n\n## Visualizacion de los Parámetros \nlr.coef_\nlr.intercept_\n\n\nC: Corresponde a un parámetro de Regularización. Valores más pequeños implica mayor regularización. Por defecto 1.\npenalty: Corresponde al tipo de regularización. Por defecto “l2”.\n\n“l1”: Corresponde a la regularización Lasso. Genera que hayan parámetros cero, ayudando en la selección de variables.\n“l2”: Corresponde a la regularización Ridge. Genera que todos los parámetros sean pequeños, entregando estabilidad y buena interpretabilidad.\n“elasticnet”: Corresponde a la combinación de “l1” y “l2”.\nNone: No hay regularización.\n\n\n\n\n\n\n\n\n\nPara cambiar la regularización, consultar la documentación de Scikit-Learn."
  },
  {
    "objectID": "tics411/clase-12.html#interpretabilidad",
    "href": "tics411/clase-12.html#interpretabilidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Interpretabilidad",
    "text": "Interpretabilidad\n\nUna de las grandes ventajas que tiene la Regresión Logística es que sus predicciones son interpretables.\n\n\nTenemos un dataset de 2 variables:\n\nW: Corresponde al peso del Vehículo.\nqsec: Corresponde al tiempo en Segundos que lo toma en recorrer un cuarto de milla.\n\nQueremos predecir si el vehículo es Ecónomico o no (en términos de consumo de Bencina).\n\n\\[g_\\theta(x) = 0.5 - 3.5 \\cdot W + 1.5 \\cdot qsec \\]\n\n\n\n\n\n\n\n\nSi el vehículo se demora más en el cuarto de milla (qsec aumenta) entonces el vehículo es más económico.\n\nTiene menos potencia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi el vehículo es más pesado (W aumenta), entonces es menos económico.\n\nRequiere probablemente más combustible para mover dicho peso.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl valor del parámetro representa también la magnitud de la contribución."
  },
  {
    "objectID": "tics411/clase-12.html#sugerencias",
    "href": "tics411/clase-12.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\n\nEstandarización/Normalización de datos: Permite que la escala de los datos no afecte en la interpretabilidad.\nOne Hot Encoder: En general tiende a dar mejores resultados que el Ordinal.\nInteracciones: Combinación de variables.\nVariables no Lineales: Permite que la frontera de Decisión no sea necesariamente lineal (Regresión Polinómica)."
  },
  {
    "objectID": "tics411/clase-6.html#evaluación",
    "href": "tics411/clase-6.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\nPensemos en la Evaluación como una medida de desempeño el cuál “evalúa” qué tan bien realizado está el clustering. El objetivo principal del Clustering debe ser siempre la generación de clusters compactos que estén diferenciados los unos a los otros.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el Clustering que mejor describe el problema."
  },
  {
    "objectID": "tics411/clase-6.html#objetivos-de-la-evaluación",
    "href": "tics411/clase-6.html#objetivos-de-la-evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivos de la Evaluación",
    "text": "Objetivos de la Evaluación"
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins",
    "href": "tics411/clase-6.html#tendencia-hopkins",
    "title": "TICS-411 Minería de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins\n\nEstadístico Hopkins\n\n\nPermite evaluar a priori si es que efectivamente existen clusters antes de aplicar un algoritmo.\n\n\n\n\n\n\\[H = \\frac{\\sum_{i = 1}^p w_i}{\\sum_{i = 1}^p u_i + \\sum_{i = 1}^p w_i}\\]\n\n\\(w_i\\): corresponde a la distancia de un punto aleatorio al vecino más cercano en los datos originales.\n\\(u_i\\): corresponde a la distancia de un punto real del dataset al vecino más cercano.\n\\(p\\): Número de puntos generados en el espacio del Dataset.\n\n\nfrom pyclustertend import hopkins\n\n1-hopkins(X, p)\n\n\n\n\n\n\n\nX: Dataset al cuál se le aplica el Estadístico.\np: Número de Puntos para el cálculo.\n\n\n\n\n\n\n\n\n\n\npyclustertend entrega el valor 1-H."
  },
  {
    "objectID": "tics411/clase-6.html#tendencia-hopkins-1",
    "href": "tics411/clase-6.html#tendencia-hopkins-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Tendencia: Hopkins",
    "text": "Tendencia: Hopkins"
  },
  {
    "objectID": "tics411/clase-6.html#cálculo-hopkins-ejemplo-p2",
    "href": "tics411/clase-6.html#cálculo-hopkins-ejemplo-p2",
    "title": "TICS-411 Minería de Datos",
    "section": "Cálculo Hopkins: Ejemplo p=2",
    "text": "Cálculo Hopkins: Ejemplo p=2\n\n\n\n\n\n\n\nPuntos obtenidos de los Datos\n\\[u_1\\approx 0\\]\n\\[u_2\\approx 0\\]\n\nPuntos Aleatorios en el Espacio de los Datos\n\\[w_1\\approx 1.8\\]\n\\[w_2\\approx 1.12\\]\n\nCálculo Hopkins\n\\[ H = \\frac{w_1 + w_2}{u_1 + u_2 + w_1 + w_2}\\] \\[ H = \\frac{1.8 + 1.12}{0 + 0 + 1.8 + 1.8} = 1\\]"
  },
  {
    "objectID": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "href": "tics411/clase-6.html#visual-assesment-of-tendency-vat",
    "title": "TICS-411 Minería de Datos",
    "section": "Visual Assesment of Tendency (VAT)",
    "text": "Visual Assesment of Tendency (VAT)\n\nCorresponde a una inspección visual de la distancia entre los puntos (matriz de distancia). Colores más oscuros indican menor distancias entre dichos puntos lo que indica mayor cohesión.\n\n\n\n\n\n\n\n\n\nSe pueden ver claramente dos bloques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo es posible ver bloques importantes.\n\n\n\n\n\n\n\n\n\nfrom pyclustertend import vat\n\nvat(X)"
  },
  {
    "objectID": "tics411/clase-6.html#correlación",
    "href": "tics411/clase-6.html#correlación",
    "title": "TICS-411 Minería de Datos",
    "section": "Correlación",
    "text": "Correlación\nProcedimiento:\n\nConstruir una matriz de similaridad entre todos los puntos de la siguiente manera:\n\n\\[s(i,j) = \\frac{1}{d(i,j) + 1}\\]\n\nConstruir una matriz de similaridad \"ideal\" basada en la pertenencia a un Cluster.\nSi \\(i\\) y \\(j\\) pertenecen al mismo cluster entonces \\(s(i,j)=1\\), en otro caso \\(s(i,j) = 0\\)\nCalcular la Correlación entre la matriz de similaridad y la matriz ideal (obtenidas en los pasos 1 y 2).\n\n\n\n\n\n\n\nUna correlación alta indica que los puntos que están en el mismo cluster son cercanos entre ellos."
  },
  {
    "objectID": "tics411/clase-6.html#cohesión",
    "href": "tics411/clase-6.html#cohesión",
    "title": "TICS-411 Minería de Datos",
    "section": "Cohesión",
    "text": "Cohesión\n\nCohesión\n\n\nMide cuán cercanos están los objetos dentro de un mismo cluster. Se utiliza la Suma de los Errores al Cuadrado, que es equivalente a la Inercia de K-Means (o Within Cluster).\n\n\n\n\\[ SSE_{total} = \\sum_{k = 1}^K\\sum_{x_i \\in C_k} (x_i - \\bar{C_k})^2\\]\n\n\\(C_k\\) corresponde al Centroide del Cluster \\(k\\). Dicho centroide puede ser calculado como la media/mediana de todos los puntos del Centroide.\n\\(K\\) corresponde al Número de Clusters.\n\n\n\n\n\n\n\n\nNo me gusta mucho este nombre, porque en realidad es como un inverso de la Cohesión."
  },
  {
    "objectID": "tics411/clase-6.html#separación",
    "href": "tics411/clase-6.html#separación",
    "title": "TICS-411 Minería de Datos",
    "section": "Separación",
    "text": "Separación\n\nSeparación\n\n\nMide cuán distinto es un cluster de otro. Se usa la suma de las distancias al cuadrado entre los centroides hacia el promedio de todos los puntos. (Between groups sum squares, SSB).\n\n\n\n\\[ SSB_{total} = \\sum_{k = 1}^K |C_k|(\\bar{X} - \\bar{C_k})^2\\]\n\n\\(|C_k|\\) corresponde al número de elementos (Cardinalidad) del Cluster \\(i\\).\n\\(\\bar{X}\\) corresponde al promedio de todos los puntos."
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-coeficiente-de-silueta",
    "title": "TICS-411 Minería de Datos",
    "section": "Coeficiente de Silhouette (Coeficiente de Silueta)",
    "text": "Coeficiente de Silhouette (Coeficiente de Silueta)\n\nEl coeficiente de Silhouette es otra medida que combina la cohesión y la separación. Los valores varían entre -1 y 1, donde valores cercanos a 1 representan una mejor agrupación.\n\n\n\n\n\n\n\nValores cercanos a \\(-1\\) representan que el punto está incorrectamente asignado a un cluster.\n\n\n\n\\[S_i = \\frac{b_i - a_i}{max\\{a_i, b_i\\}}\\]\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X, labels, sample_size = None, metric=\"euclidean\")"
  },
  {
    "objectID": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "href": "tics411/clase-6.html#coeficiente-de-silhouette-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Coeficiente de Silhouette: Ejemplo",
    "text": "Coeficiente de Silhouette: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[C_{silueta} = \\frac{1}{n}\\sum_{i} s_i\\]\n\n\\(a_i\\): Distancia promedio del punto \\(i\\) a todos los otros puntos del mismo cluster. (Cohesión)\n\\(b_{ij}\\): Distancia promedio del punto \\(i\\) a todos los puntos del cluster \\(j\\) donde no pertenezca \\(i\\). (Separación)\n\\(b_j\\): Mínimo de \\(b_{ij}\\) tal que el punto i no pertenezca al cluster \\(j\\). (Menor Separación)"
  },
  {
    "objectID": "tics411/clase-6.html#ejercicio-propuesto",
    "href": "tics411/clase-6.html#ejercicio-propuesto",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejercicio Propuesto",
    "text": "Ejercicio Propuesto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio Propuesto\n\n\nCalcule el coeficiente de Silueta. Tabla de resultado al final de las Slides."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta",
    "href": "tics411/clase-6.html#curvas-de-silueta",
    "title": "TICS-411 Minería de Datos",
    "section": "Curvas de Silueta",
    "text": "Curvas de Silueta\nEs común mostrar los resultados del coeficiente de silueta como gráficos de este estilo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblemas\n\n\n\nSiluetas negativas.\nClusters bajo el promedio.\nMucha variabilidad de Silueta en un sólo cluster."
  },
  {
    "objectID": "tics411/clase-6.html#curvas-de-silueta-implementación",
    "href": "tics411/clase-6.html#curvas-de-silueta-implementación",
    "title": "TICS-411 Minería de Datos",
    "section": "Curvas de Silueta: Implementación",
    "text": "Curvas de Silueta: Implementación\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\nskplt.metrics.plot_silhouette(X, labels, metric=\"euclidean\", title=\"Silhouette Analysis\")\nplt.show()\n\nL1-2: Importación de Librerías Necesarias. Esta implementación está en la librería Scikit-plot. (Para instalar pip install scikit-plot)\nX: Dataset usado para el clustering.\nlabels : etiquetas obtenidos de algún proceso de Clustering.\nmetric: Métrica a utilizar, por defecto usa “euclidean”.\ntitle: Se puede agregar un Título personalizado a la curva."
  },
  {
    "objectID": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "href": "tics411/clase-6.html#resultados-ejercicio-propuesto",
    "title": "TICS-411 Minería de Datos",
    "section": "Resultados Ejercicio Propuesto",
    "text": "Resultados Ejercicio Propuesto\n\n\n\n\n\nCoeficiente de Silhouette = 0.6148\n\n\n\n\n\n\nComprobar utilizando Scikit-Learn\n\n\n\n\n\nTics-411 Minería de Datos está licenciado bajo CC BY-NC-SA 4.0"
  },
  {
    "objectID": "tics411/clase-3.html#definiciones",
    "href": "tics411/clase-3.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nAprendizaje No supervisado\n\n\nEs un tipo de aprendizaje que no requiere de etiquetas (las respuestas correctas) para poder aprender.\n\n\n\n\n\n\n\n\n\nEn nuestro caso nos enfocaremos en un caso particular de Modelación Descriptiva llamada Clustering.\n\n\n\n\nClustering\n\n\nConsiste en agrupar los datos en un menor número de entidades o grupos. A estos grupos se les conoce como clusters y pueden ser generados de manera global, o modelando las principales características de los datos."
  },
  {
    "objectID": "tics411/clase-3.html#intuición",
    "href": "tics411/clase-3.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\n¿Cuántos clusters se pueden apreciar?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-introducción",
    "href": "tics411/clase-3.html#clustering-introducción",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Introducción",
    "text": "Clustering: Introducción\n\n\n\n\n\n\nClustering: Consiste en buscar grupos de objetos tales que la similaridad intra-grupo sea alta, mientras que la similaridad inter-grupos sea baja. Normalmente la distancia es usada para determinar qué tan similares son estos grupos."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-evaluación",
    "href": "tics411/clase-3.html#clustering-evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Evaluación",
    "text": "Clustering: Evaluación\n\n\n\n\n\n\n\nEvaluar el nivel del éxito o logro del Clustering es complicado. ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-tipos",
    "href": "tics411/clase-3.html#clustering-tipos",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Tipos",
    "text": "Clustering: Tipos"
  },
  {
    "objectID": "tics411/clase-3.html#clustering-partición",
    "href": "tics411/clase-3.html#clustering-partición",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Partición",
    "text": "Clustering: Partición\n\nLos datos son separados en K clusters, donde cada punto pertenece exclusivamente a un único cluster."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-densidad",
    "href": "tics411/clase-3.html#clustering-densidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-jerarquía",
    "href": "tics411/clase-3.html#clustering-jerarquía",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Jerarquía",
    "text": "Clustering: Jerarquía\n\nLos algoritmos basados en jerarquía pueden seguir 2 estrategias:\n\n\nAglomerativos: Comienzan con cada objeto como un grupo (bottom-up). Estos grupos se van combinando sucesivamente a través de una métrica de similaridad. Para n objetos se realizan n-1 uniones.\nDivisionales: Comienzan con un solo gran cluster (bottom-down). Posteriormente este mega-cluster es dividido sucesivamente de acuerdo a una métrica de similaridad."
  },
  {
    "objectID": "tics411/clase-3.html#clustering-probabilístico",
    "href": "tics411/clase-3.html#clustering-probabilístico",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Probabilístico",
    "text": "Clustering: Probabilístico\nSe ajusta cada punto a una distribución de probabilidades que indica cuál es la probabilidad de pertenencia a dicho cluster."
  },
  {
    "objectID": "tics411/clase-3.html#partición",
    "href": "tics411/clase-3.html#partición",
    "title": "TICS-411 Minería de Datos",
    "section": "Partición",
    "text": "Partición\n\nLos datos son separados en K Clusters, donde cada punto pertenece exclusivamente a un único cluster. A K se le considera como un hiperparámetro.\n\n\n\n\n\n\n\n\nCluster Compactos: Minimizar la distancia intra-cluster (within cluster).\nClusters bien separados: Maximizar la distancia inter-cluster (between cluster).\n\n\n\n\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\nEl puntaje/score mide la calidad del clustering \\(C\\) para el Dataset \\(D\\)."
  },
  {
    "objectID": "tics411/clase-3.html#score",
    "href": "tics411/clase-3.html#score",
    "title": "TICS-411 Minería de Datos",
    "section": "Score",
    "text": "Score\n\\[ Score (C,D) = f(wc(C),bc(C))\\]\n\n\n\n\nDistancia Between-Cluster: \\[bc(C) = \\sum_{1 \\le j \\le k \\le K} d(r_j, r_k)\\]\n\ndonde \\(r_k\\) representa el centro del cluster \\(k\\): \\[r_k = \\frac{1}{n_k} \\sum_{x_i \\in C_k} x_i\\]\n\n\nDistancia Within-Cluster (Inercia): \\[wc(C) = \\sum_{k=1}^K \\sum_{x_i \\in C_k} d(x_i, r_k)\\]\n\n\n\n\n\n\n\n\n\n\nDistancia entre los centros de cada cluster.\n\n\n\n\n\n\n\n\n\n\nDistancia entre todos los puntos del cluster y su respectivo centro."
  },
  {
    "objectID": "tics411/clase-3.html#k-means",
    "href": "tics411/clase-3.html#k-means",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means",
    "text": "K-Means\n\nK-Means\n\n\nDado un número de clusters \\(K\\) (determinado por el usuario), cada cluster es asociado a un centro (centroide). Luego, cada punto es asociado al cluster con el centroide más cercano.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utiliza la Distancia Euclideana como medida de similaridad.\n\n\nSe seleccionan \\(K\\) puntos como centroides iniciales.\nRepite:\n\nForma K clusters asignando todos los puntos al centroide más cercano.\nRecalcula el centroide para cada clase como la media de todos los puntos de dicho cluster.\n\n\n\nSe repite este procedimiento por un número finito de iteraciones o hasta que los centroides no cambien."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo",
    "href": "tics411/clase-3.html#k-means-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\nResolvamos el siguiente ejemplo.\nSupongamos que tenemos tipos de manzana, y cada una de ellas tiene 2 atributos (features). Agrupemos estos objetos en 2 grupos de manzanas basados en sus características."
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-1",
    "href": "tics411/clase-3.html#k-means-ejemplo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n1era Iteración\n\n\n\n\nSupongamos los siguientes centroides iniciales: \\[C_1 = (1,1)\\] \\[C_2 = (2,1)\\]\n\n\n\n\n\n\n\n\n\nMatriz de Distancias al Centroide: (coordenada i,j representa distancia del punto j al centroide i)\n\n\n\n\n\n\\[D^1 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n1 & 0 & 2.83 & 4.24\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son: \\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-2",
    "href": "tics411/clase-3.html#k-means-ejemplo-2",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n2da Iteración\n\n\n\n\nLos nuevos centroides son:\n\n\\[C_1 = (1,1)\\] \\[C_2 = (\\frac{11}{3}, \\frac{8}{3})\\]\n\n\n\nCalculamos la Matriz de Distancias al Centroide:\n\n\n\n\\[D^2 = \\begin{bmatrix}\n0 & 1 & 3.61 & 5\\\\\n3.14 & 2.26 & 0.47 & 1.89\n\\end{bmatrix}\\]\n\n\n\nCalculemos la Matriz de Pertenencia \\(G\\):\n\n\n\n\\[G^2 = \\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLos nuevos centroides son:\n\\(C_1 = (\\frac{3}{2}, 1)\\) y \\(C_2 = (\\frac{9}{2}, \\frac{7}{2})\\)"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-ejemplo-3",
    "href": "tics411/clase-3.html#k-means-ejemplo-3",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Ejemplo",
    "text": "K-Means: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\nSi seguimos iterando notaremos que ya no hay cambios en los clusters. El algoritmo converge.\nEste es el resultado de usar \\(K=2\\). Utilizar otro valor de \\(K\\) entregará valores distintos.\n¿Es este el número de clusters óptimos?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-número-de-clusters-óptimos",
    "href": "tics411/clase-3.html#k-means-número-de-clusters-óptimos",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Número de Clusters Óptimos",
    "text": "K-Means: Número de Clusters Óptimos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSiempre es posible encontrar el número de clusters indicados.\nEntonces,\n\n¿Cómo debería escoger el valor de \\(K\\)?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-número-de-clusters-óptimos-1",
    "href": "tics411/clase-3.html#k-means-número-de-clusters-óptimos-1",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Número de Clusters Óptimos",
    "text": "K-Means: Número de Clusters Óptimos\n\nCurva del Codo\n\nEs una heurísitca en la cual gráfica el valor de una métrica de distancia (e.g. within distance) para distintos valores de \\(K\\). El valor óptimo de \\(K\\) será el codo de la curva, que es el valor donde se estabiliza la métrica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste valor del codo muchas veces es subjetivo y distintas apreciaciones pueden llegar a distintos \\(K\\) óptimos.\n\n\n\n\n\n\n\n\n\n\nEventualmente otras métricas distintas al within cluster distance podrían también ser usadas.\n\n\n\n\n\n\n\n\n\n¿Cuál es el efecto que está buscando la curva del codo? ¿Qué implica el valor de K escogido?"
  },
  {
    "objectID": "tics411/clase-3.html#k-means-detalles-técnicos",
    "href": "tics411/clase-3.html#k-means-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Means: Detalles Técnicos",
    "text": "K-Means: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nAlgoritmo relativamente eficiente (\\(O(k \\cdot n \\cdot i)\\)). Donde \\(k\\) es el número de clusters, \\(n\\) el número de puntos, e \\(i\\) el número de iteraciones.\nEncuentra “clusters esféricos”.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nSensible al punto de inicio.\nSolo se puede aplicar cuando el promedio es calculable.\nSe requiere definir K a priori (K es un hiperparámetro).\nSuceptible al ruido y a mínimos locales (podría no converger)."
  },
  {
    "objectID": "tics411/clase-3.html#implementación-en-scikit-learn",
    "href": "tics411/clase-3.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=8, n_init=10,random_state=None)\nkm.fit(X)\nkm.predict(X)\n\n## opcionalmente\nkm.fit_predict(X)\n\n\nn_clusters: Define el número de clusters a crear, por defecto 8.\nn_init: Cuántas veces se ejecuta el algoritmo, por defecto 10.\nrandom_state: Define la semilla aleatoria. Por defecto sin semilla.\ninit: Permite agregar centroides de manera manual.\n.fit(): Entrenará el modelo en los datos suministrados.\n.predict() Entregará las clusters asignados a cada dato suministrado.\n.clusters_centers_: Entregará las coordenadas de los centroides de cada Cluster.\n.inertia_: Entrega valores correspondiente a la within cluster distance.\n\n\n👀 Veamos un ejemplo en Colab."
  },
  {
    "objectID": "tics411/clase-3.html#sugerencias",
    "href": "tics411/clase-3.html#sugerencias",
    "title": "TICS-411 Minería de Datos",
    "section": "Sugerencias",
    "text": "Sugerencias\n\n\n\n\n\n\nPre-procesamientos\n\n\nEs importante recordar que K-Means es un Algoritmo basado en distancias, por lo tanto se ve afectado por Outliers y por Escala.\nSe recomienda preprocesar los datos con:\n\nWinsorizer() para eliminar Outliers.\nStandardScaler() o MinMaxScaler() para llevar a una escala común."
  },
  {
    "objectID": "tics411/clase-3.html#interpretación-clusters",
    "href": "tics411/clase-3.html#interpretación-clusters",
    "title": "TICS-411 Minería de Datos",
    "section": "Interpretación Clusters",
    "text": "Interpretación Clusters\n\n\n\n\n\n\nRecordar, que el clustering no clasifica. Por lo tanto, a pesar de que K-Means nos indica a qué cluster pertenece cierto punto, debemos interpretar cada cluster para entender qué es lo que se agrupó.\n\n\n\n\n\n\n\n\n\nLa interpretación del cluster es principalmente intuición y exploración, por lo tanto el EDA puede ser de utilidad para analizar clusters."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge",
    "href": "tics411/clase-3.html#post-procesamiento-merge",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\nPost-Procesamiento\n\n\nSe define como el tratamiento que podemos realizar al algoritmo luego de haber entregado ya sus predicciones.\n\n\n\nEs posible generar más clusters de los necesarios y luego ir agrupando los más cercanos."
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-merge-1",
    "href": "tics411/clase-3.html#post-procesamiento-merge-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Merge",
    "text": "Post-Procesamiento: Merge\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el problema con este caso de Post-Procesamiento?"
  },
  {
    "objectID": "tics411/clase-3.html#post-procesamiento-split",
    "href": "tics411/clase-3.html#post-procesamiento-split",
    "title": "TICS-411 Minería de Datos",
    "section": "Post-Procesamiento: Split",
    "text": "Post-Procesamiento: Split\n\n\n\n\n\n\n\n\n\n\n\nEn Scikit-Learn esto puede conseguirse utilizando el parámetro init. Se entregan los nuevos centroides para forzar a K-Means que separe ciertos clusters."
  },
  {
    "objectID": "tics411/clase-3.html#variantes-k-means",
    "href": "tics411/clase-3.html#variantes-k-means",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes K-Means",
    "text": "Variantes K-Means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SMD(p_1,p_2) = 4\\]\n\n\n\n\n\n\n\nAcá pueden encontrar una implementación de K-Modes en Python."
  },
  {
    "objectID": "tics411/lab-0.html#qué-es-scikit-learn",
    "href": "tics411/lab-0.html#qué-es-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Qué es Scikit-Learn?",
    "text": "¿Qué es Scikit-Learn?\n\n\n\n\n\nScikit-Learn (sklearn para los amigos) es una librería creada por David Cournapeau, como un Google Summer Code Project y luego Matthieu Brucher en su tesis.\nEn 2010 queda a cargo de INRIA y tiene un ciclo de actualización de 3 meses.\nEs la librería más famosa y poderosa para hacer Machine Learning hoy en día.\nSu API es tan famosa, que hoy se sabe que una librería es de calidad si sigue los estándares implementados por Scikit-Learn.\nPara que un algoritmo sea parte de Scikit-Learn debe poseer 3 años desde su publicación y 200+ citaciones mostrando su utilidad y amplio uso (ver acá).\nAdemás es una librería que obliga a que sus algoritmos tengan la capacidad de generalizar."
  },
  {
    "objectID": "tics411/lab-0.html#diseño",
    "href": "tics411/lab-0.html#diseño",
    "title": "TICS-411 Minería de Datos",
    "section": "Diseño",
    "text": "Diseño\n\nScikit-Learn sigue un patrón de Programación Orientada a Objetos (POO) basado en clases.\n\n\n\n\n\n\n\n\nEn programación, una clase es un objeto que internamente contiene estados que pueden ir cambiando en el tiempo.\n\nUna clase posee:\n\nMétodos: Funciones que cambian el comportamiento de la clase.\nAtributos: Datos propios de la clase.\n\n\n\n\n\n\n\nScikit-Learn sigue el siguiente estándar:\n\nTodas las Clases se escriben en CamelCase: Ej: KMeans,LogisticRegression, StandardScaler.\nLas clases en Scikit-Learn pueden representar algoritmos, o etapas de un preprocesamiento.\n\nLos algoritmos se denominan Estimators.\nLos preprocesamientos se denominan Transformers.\n\nLas funciones se escriben como snake_case y permiten realizar algunas operaciones básicas en el proceso de modelamiento. Ej: train_test_split(), cross_val_score().\nNormalmente se utilizan letras mayúsculas para denotar Matrices o DataFrames, mientras que las letras minúsculas denotan Vectores o Series."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-no-supervisados",
    "href": "tics411/lab-0.html#estimadores-no-supervisados",
    "title": "TICS-411 Minería de Datos",
    "section": "Estimadores No supervisados",
    "text": "Estimadores No supervisados\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X) \n\ny_pred = model.predict(X) \n\n## Opcionalmente se puede entrenar y predecir a la vez.\nmodel.fit_predict(X) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperparámetros.\nL3. Entrenar o ajustar el modelo (Requiere sólo de X).\nL5. Predecir. Los modelos de clasificación tienen la capacidad de generar probabilidades.\nL7-8. Este tipo de modelos permite entrenar y predecir en un sólo paso."
  },
  {
    "objectID": "tics411/lab-0.html#estimadores-predictivos",
    "href": "tics411/lab-0.html#estimadores-predictivos",
    "title": "TICS-411 Minería de Datos",
    "section": "Estimadores Predictivos",
    "text": "Estimadores Predictivos\nfrom sklearn.sub_modulo import Estimator \nmodel = Estimator(hp1=v1, hp2=v2,...) \nmodel.fit(X_train, y_train) \n\ny_pred = model.predict(X_test) \ny_pred_proba = model.predict_proba(X_test)\n\nmodel.score(X_test,y_test) \n\n\nL1. Importar la clase a utilizar.\nL2. Instanciar el modelo y sus hiperparámetros.\nL3. Entrenar o ajustar el modelo (Ojo, requiere de X e y).\nL5–6. Predecir en datos nuevos. (Algunos modelos pueden predecir probabilidades).\nL8. Evaluar el modelo en los datos nuevos."
  },
  {
    "objectID": "tics411/lab-0.html#output-de-un-modelo",
    "href": "tics411/lab-0.html#output-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Output de un Modelo",
    "text": "Output de un Modelo\n\nLos modelos no entregan directamente un output sino que los dejan almacenados en su interior como un estado.\nLos Estimators tienen dos estados:\n\nNot Fitted: Modelo antes de ser entrenado\nFitted: Una vez que el modelo ya está entrenado. (Después de aplicar .fit())\n\n\n\n\n\n\n\n\n\nMuchos modelos pueden entregar información sólo luego de ser entrenados (su atributo termina con un _).\nEj: model.coef_, model.intercept_.\n\n\n\n\n\n\n\n\n\n\n\nEl modelo es una herramienta a la cual le entregamos datos (Input), y nos devuelve datos (Predicciones)."
  },
  {
    "objectID": "tics411/lab-0.html#transformers",
    "href": "tics411/lab-0.html#transformers",
    "title": "TICS-411 Minería de Datos",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\n\n\nA diferencia de los Estimators, los Transformers no son modelos.\nSu input y su output son datos.\nAlgunos Transformers permiten escalar los datos, transformar categorías en números, rellenar valores faltantes. (Veremos más acerca de esto en los Preprocesamiento).\n\n\n\n\n\n\nfrom sklearn.preprocessing import Transformer \ntr = Transformer(hp1=v1, hp2=v2,...) \ntr.fit(X) \n\nX_new = tr.transform(X) \n\n## Opcionalmente\nX_new = tr.fit_transform(X) \n\nL1. Importar la clase a utilizar (en este caso del submodulo preprocessing, aunque pueden haber otros como impute).\nL2. Instanciar el Transformer y sus hiperparámetros.\nL3. Entrenar o ajustar el Transformer.\nL5. Transformar los datos.\nL7-8. Adicionalmente se puede entrenar y transformar los datos en un sólo paso."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines",
    "href": "tics411/lab-0.html#pipelines",
    "title": "TICS-411 Minería de Datos",
    "section": "Pipelines",
    "text": "Pipelines\n\nEn ocasiones un Dataset requiere más de un preprocesamiento.\nEstas Transformaciones normalmente se hacen en serie de manera consecutiva.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl Estimator es opcional, es decir, el Pipeline puede ser para combinar sólo Transformers o Transformers + un Estimator.\n\n\n\n\n\n\n\n\n\n\nUn Pipeline puede tener sólo un Estimator."
  },
  {
    "objectID": "tics411/lab-0.html#pipelines-código",
    "href": "tics411/lab-0.html#pipelines-código",
    "title": "TICS-411 Minería de Datos",
    "section": "Pipelines: Código",
    "text": "Pipelines: Código\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.pipeline import Pipeline \n\npipe = Pipeline(steps=[ \n    (\"ohe\", OneHotEncoder()),\n    (\"sc\", StandardScaler()),\n    (\"model\", DecisionTreeClassifier())\n])\n\npipe.fit(X_train, y_train) \ny_pred = pipe.predict(X_test) \n\npipe.score(X_test, y_test) \n\nL1-2. Importo mi modelo y mis preprocesamientos\nL3. Importo el Pipeline.\nL5-9. Instancio un Pipeline.\nL11. Entreno el Pipeline.\nL12. Predigo utilizando el Pipeline entrenado.\nL14. Evalúo el modelo en datos no vistos."
  },
  {
    "objectID": "tics411/lab-0.html#documentación",
    "href": "tics411/lab-0.html#documentación",
    "title": "TICS-411 Minería de Datos",
    "section": "Documentación",
    "text": "Documentación\n\nProbablemente Scikit-Learn tenga una de las mejores documentaciones existentes.\n\n\nVeamos el caso de la Documentación del One Hot Encoder"
  },
  {
    "objectID": "tics411/clase-5.html#clustering-densidad",
    "href": "tics411/clase-5.html#clustering-densidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering: Densidad",
    "text": "Clustering: Densidad\n\nSe basan en la idea de continuar el crecimiento de un cluster a medida que la densidad (número de objetos o puntos) en el vecindario sobrepase algún umbral.\n\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso utilizaremos DBSCAN (Density-Based Spatial Clustering Applications with Noise)."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-definiciones",
    "href": "tics411/clase-5.html#dbscan-definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Definiciones",
    "text": "DBSCAN: Definiciones\n\n\n\n\n\n\nHiperparámetros del Modelo\n\n\n\neps: Radio de análisis\nMinPts: Corresponde al mínimo de puntos necesarios en un Radio eps.\n\n\n\n\n\n\n\nDensidad\n\n\nDensidad es el número de puntos dentro del radio eps.\n\n\nCore Point/Punto Central\n\n\nUn punto central/core es aquel que tiene al menos MinPts puntos dentro de la esfera definida por eps (se incluye él mismo).\n\n\n\n\n\nBorder Point/Punto Borde\n\n\nUn punto de borde tiene menos puntos que MinPts del eps, pero está dentro de la esfera de un punto central.\n\n\nNoise Point/Punto Ruido\n\n\nUn punto de ruido es todo aquel que no es punto central ni de borde."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-algoritmo-categorización-de-puntos",
    "href": "tics411/clase-5.html#dbscan-algoritmo-categorización-de-puntos",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Algoritmo categorización de puntos",
    "text": "DBSCAN: Algoritmo categorización de puntos\n\nPrimeramente se aplica un algoritmo para categorizar cada punto de acuerdo a las definiciones anteriores.\n\n\nPara cada punto en el espacio:\n\nCalcular su densidad en EPS y aplicar el siguiente algoritmo:"
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-1",
    "href": "tics411/clase-5.html#ejemplo-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 1",
    "text": "Ejemplo: Iteración 1\n\nSupongamos un ejemplo con \\(MinPts=4\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Core Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-2",
    "href": "tics411/clase-5.html#ejemplo-iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 2",
    "text": "Ejemplo: Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Border Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-3",
    "href": "tics411/clase-5.html#ejemplo-iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración 3",
    "text": "Ejemplo: Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point."
  },
  {
    "objectID": "tics411/clase-5.html#ejemplo-iteración-final",
    "href": "tics411/clase-5.html#ejemplo-iteración-final",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Iteración Final",
    "text": "Ejemplo: Iteración Final\n\n\n\n\n\n\n\n\n\n\n\nAhora, ¿Cómo definimos que partes son clusters o no?"
  },
  {
    "objectID": "tics411/clase-5.html#algoritmo-de-clustering",
    "href": "tics411/clase-5.html#algoritmo-de-clustering",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo de Clustering",
    "text": "Algoritmo de Clustering\nSe aplica el siguiente algoritmo para calcular clusterings.\n\n\n\n\n\n\nAntes de aplicar se desechan los Noise Points ya que no serán considerados. (Veremos luego que ocurre con estos puntos).\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label"
  },
  {
    "objectID": "tics411/clase-5.html#iteración-1",
    "href": "tics411/clase-5.html#iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 1",
    "text": "Iteración 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nTodos los puntos cercanos a un Core reciben la misma etiqueta."
  },
  {
    "objectID": "tics411/clase-5.html#iteración-2",
    "href": "tics411/clase-5.html#iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Iteración 2",
    "text": "Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel=0\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n## label ya está en 1\nfor punto_c in corePoints:\n    if punto_c no tiene etiqueta:\n        label += 1\n        punto_c = label\n    for point_eps dentro de eps:\n        if punto_eps no tiene etiqueta:\n            punto_eps = label\n\n\n\n\n\n\n\n\n\nEn este caso obtuvimos 2 clusters, e indirectamente un 3er de puntos ruido."
  },
  {
    "objectID": "tics411/clase-5.html#dbscan",
    "href": "tics411/clase-5.html#dbscan",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\n\n\n\n\n\n\n\n\n\n\n¿Sería posible replicar un proceso de Clustering similar utilizando K-Means? ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-5.html#dbscan-detalles-técnicos",
    "href": "tics411/clase-5.html#dbscan-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN: Detalles Técnicos",
    "text": "DBSCAN: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nResistente al ruido.\nPuede lidiar con clusters de diferentes formas y tamaños.\nNo es necesario especificar cuántos clusters encontrar.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAlgoritmo de alta complejidad computacional que puede llegar \\(O(n^2)\\) en el peor caso.\nSe ve afectado por densidad de los datos y por datos con una alta dimensionalidad.\nSu óptimo resultado depende específicamente de sus Hiperparámetros.\nNo puede generalizar en datos no usados en entrenamiento."
  },
  {
    "objectID": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros",
    "href": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo encontrar los Hiperparámetros?",
    "text": "¿Cómo encontrar los Hiperparámetros?\n\n\n\n\n\n\n\n\nminPts\n\n\nPara datasets multidimensionales grandes, la regla es:\n\\[minPts \\ge dim + 1\\]\n\n\n\n\n\n\n\n\n\nOtras recomendaciones:\n\n\n\nPara dos dimensiones: \\(minPts=4\\) (Ester et al., 1996)\nPara más de 2 dimensiones: \\(minPts = 2 \\cdot dim\\) (Sander et al., 1998)"
  },
  {
    "objectID": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros-1",
    "href": "tics411/clase-5.html#cómo-encontrar-los-hiperparámetros-1",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo encontrar los Hiperparámetros?",
    "text": "¿Cómo encontrar los Hiperparámetros?\n\nPara encontrar EPS se suele utilizar el método de Vecinos más cercanos.\n\n\n\nIdea\n\nLa distancia de los puntos dentro de un cluster a su k-ésimo vecino deberían ser similares.\nLuego, los puntos atípicos (o ruidosos) tienen el k-ésimo vecino a una mayor distancia.\n\n\n\n\n\n\n\n💡 Podemos plotear la distancia ordenada de cada punto a su k-ésimo vecino y seleccionar un eps cercano al crecimiento exponencial (codo)."
  },
  {
    "objectID": "tics411/clase-5.html#implementación-en-scikit-learn",
    "href": "tics411/clase-5.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples = 5, eps = 0.5, metric = \"euclidean\")\n\n## Se entrena y se genera la predicción\ndbs.fit_predict(X)\n\n\nmin_samples: Corresponde a minPts. Por defecto 5.\neps: Corresponde al radio de la esfera en la que se buscan los puntos cercanos. Por defecto 0.5.\nmetric: Corresponde a la distancia utilizada para medir la distancia. Permite todas las distancias mencionadas acá.\n.fit_predict(): Entrenará el modelo en los datos suministrados e inmediatamente genera el cluster asociado a cada elemento. Adicionalmente los puntos ruidosos se etiquetarán como -1.\n\n\n👀 Veamos un ejemplo."
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión",
    "href": "tics411/clase-10.html#árboles-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión",
    "text": "Árboles de Decisión\n\nTécnica de clasificación supervisada que genera una decisión basada en árboles de decisión para clasificar instancias no conocidas."
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-ejemplo",
    "href": "tics411/clase-10.html#árboles-de-decisión-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Ejemplo",
    "text": "Árboles de Decisión: Ejemplo\n\nVisualmente, un árbol de decisión segmenta el espacio separando los datos en subgrupos.\n\n\n\n\n\n\n\nEsto permite la generacion de fronteras de decisión sumamente complejas.\n\n\n\nSupongamos el siguiente ejemplo:"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión",
    "href": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Frontera de Decisión",
    "text": "Árboles de Decisión: Frontera de Decisión"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión-1",
    "href": "tics411/clase-10.html#árboles-de-decisión-frontera-de-decisión-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Frontera de Decisión",
    "text": "Árboles de Decisión: Frontera de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál sería el Nivel de Ajuste de un modelo de este tipo?"
  },
  {
    "objectID": "tics411/clase-10.html#árboles-de-decisión-inferencia",
    "href": "tics411/clase-10.html#árboles-de-decisión-inferencia",
    "title": "TICS-411 Minería de Datos",
    "section": "Árboles de Decisión: Inferencia",
    "text": "Árboles de Decisión: Inferencia\nUna vez construido el árbol de decisión basta con recorrerlo para poder generar la predicción para una instancia dada:"
  },
  {
    "objectID": "tics411/clase-10.html#características-de-árboles",
    "href": "tics411/clase-10.html#características-de-árboles",
    "title": "TICS-411 Minería de Datos",
    "section": "Características de Árboles",
    "text": "Características de Árboles\n\nPueden trabajar con valores discretos o continuos. Además pueden ser usados como modelos de Clasificación o Regresíon.\nUna vez seleccionado un atributo no es posible devolverse (backtracking).\nDebido al poder de un árbol de Decisión la mayoría de las veces tienden al Overfitting. Una forma de evitar esto es usar técnicas de Pruning.\nEs preferible usar árboles cortos (Principio de Parsimonia o Occam's Razor).\n\n\n\n\n\n\n\nEl principio de Parsimonia recomienda encontrar soluciones a problemas utilizando la menor cantidad de elementos/parámetros."
  },
  {
    "objectID": "tics411/clase-10.html#tipos-de-árboles-de-decisión",
    "href": "tics411/clase-10.html#tipos-de-árboles-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Árboles de Decisión",
    "text": "Tipos de Árboles de Decisión\n\n\n\n\n\n\n\n\n\nBinary Split\n\n\n\n\n\n\n\n\n\n\nMulti-way Split\n\n\n\n\n\n\n\nHunt’s Algorithm \\(\\implies\\) Primer Método.\nID3 \\(\\implies\\) Sólo utiliza variables categóricas.\nC4.5 \\(\\implies\\) incluye variables continuas.\nC5.0 \\(\\implies\\) Permite separación en Múltiples Splits (No ha sido implementado en Sklearn).\nCART (Classification and Regression Trees) \\(\\implies\\) Permite que el output sea continuo pero solo utilizando Splits binarios.\n\n\n\n\n\n\n\n\n\n\nLos CARTs son por lejos los árboles más utilizados en las librerías más famosas y potentes: Scikit-Learn, XGboost, LightGBM, Catboost."
  },
  {
    "objectID": "tics411/clase-10.html#creación-de-un-árbol-de-decisión",
    "href": "tics411/clase-10.html#creación-de-un-árbol-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Creación de un Árbol de Decisión",
    "text": "Creación de un Árbol de Decisión\n\nPureza\n\nCorresponde a la probabilidad de no sacar dos registros de un Nodo que pertenezcan a la misma clase.\n\n\n\n\n\n\n\n\nEl árbol de Decisión busca crear Nodos lo más puro posible. Para ello puede utilizar las siguentes métricas:\n\n\n\n\n\nÍndice Gini\n\\[Gini(X) = 1 - \\sum_{x_i}p(x_i)^2\\]\n\nEntropía\n\\[H(X) = -\\sum_{x_i}p(x_i)log_2p(x_i)\\]\n\n\n\n\n\n\n\nA mayor valor, mayor nivel de impureza. 0 implica Nodo completamente puro."
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-ejemplo",
    "href": "tics411/clase-10.html#árbol-de-decisión-ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Ejemplo",
    "text": "Árbol de Decisión: Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCálculo de Impureza en Hoja\n\n\n\\[Gini_{(leaf)} = 1 - p(Yes)^2 - p(No)^2\\]\n\n\n\n\n\n\n\n\n\nCálculo de Impureza en Split\n\n\n\\[ Gini_{(split)} = \\frac{n_{(yes)}}{n} Gini_{(yes)} + \\frac{n_{(no)}}{n} Gini_{(no)}\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-popcorn",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-popcorn",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Popcorn",
    "text": "Árbol de Decisión: Raíz Popcorn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{4}\\right)^2 - \\left(\\frac{3}{4}\\right)^2 = 0.375\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.375 + \\frac{3}{7} \\cdot 0.444 = 0.405\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-soda",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-soda",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Soda",
    "text": "Árbol de Decisión: Raíz Soda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{4}\\right)^2 - \\left(\\frac{1}{4}\\right)^2 = 0.375\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{3}\\right)^2 - \\left(\\frac{3}{3}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.375 = 0.214\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5\\]\n\n\n\n\\[Gini_{(split)} = \\frac{6}{7}\\cdot 0.5 = 0.429\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-1",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{2}\\right)^2 - \\left(\\frac{2}{2}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{5}\\right)^2 - \\left(\\frac{2}{5}\\right)^2 = 0.48\\]\n\n\n\n\\[Gini_{(split)} = \\frac{5}{7}\\cdot 0.48 = 0.343\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-2",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{4}\\right)^2 - \\left(\\frac{2}{4}\\right)^2 = 0.5\\]\n\n\n\n\\[Gini_{(split)} = \\frac{3}{7}\\cdot 0.444 + \\frac{4}{7} \\cdot 0.5 = 0.476\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-3",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{2}{4}\\right)^2 - \\left(\\frac{2}{4}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{1}{3}\\right)^2 - \\left(\\frac{2}{3}\\right)^2 = 0.444\\]\n\n\n\n\\[Gini_{(split)} = \\frac{4}{7}\\cdot 0.5 + \\frac{3}{7} \\cdot 0.444 = 0.476\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-4",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-4",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{2}\\right)^2 - \\left(\\frac{2}{5}\\right)^2 = 0.48\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{2}\\right)^2 - \\left(\\frac{2}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{5}{7}\\cdot 0.48 = 0.343\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-raíz-age-5",
    "href": "tics411/clase-10.html#árbol-de-decisión-raíz-age-5",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: Raíz Age",
    "text": "Árbol de Decisión: Raíz Age\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos cortes de posibles Splits se calculan como el promedio de los valores adyacentes una vez que han sidos ordenados de mayor a menor.\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{6}{7}\\cdot 0.5 = 0.429\\]"
  },
  {
    "objectID": "tics411/clase-10.html#qué-split-elegiremos",
    "href": "tics411/clase-10.html#qué-split-elegiremos",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Qué Split elegiremos?",
    "text": "¿Qué Split elegiremos?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEscogeremos el Split más pequeño que representa el que genera más pureza.\n\n\n\n\n\n\n\n\n\n\n\nEl nodo que no le gusta la Soda quedó completamente puro. Por lo tanto, no puede seguir dividiéndose. Seguiremos trabajando sólo con aquellos que sí les gusta la Soda."
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{2}\\right)^2 - \\left(\\frac{0}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{2}{4}\\cdot 0.5 = 0.25\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-1",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{0}{1}\\right)^2 - \\left(\\frac{1}{1}\\right)^2 = 0\\] \\[Gini_{(no)} = 1 - \\left(\\frac{3}{3}\\right)^2 - \\left(\\frac{0}{3}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = 0\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-2",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{1}{2}\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0.5\\] \\[Gini_{(no)} = 1 - \\left(\\frac{2}{2}\\right)^2 - \\left(\\frac{0}{2}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{2}{4} \\cdot 0.5 = 0.25\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-3",
    "href": "tics411/clase-10.html#árbol-de-decisión-2do-nivel-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión: 2do Nivel",
    "text": "Árbol de Decisión: 2do Nivel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[Gini_{(yes)} = 1 - \\left(\\frac{2}{3}\\right)^2 - \\left(\\frac{1}{3}\\right)^2 = 0.444\\] \\[Gini_{(no)} = 1 - \\left(\\frac{1}{1}\\right)^2 - \\left(\\frac{0}{1}\\right)^2 = 0\\]\n\n\n\n\\[Gini_{(split)} = \\frac{3}{4} \\cdot 0.444 = 0.333\\]"
  },
  {
    "objectID": "tics411/clase-10.html#árbol-de-decisión",
    "href": "tics411/clase-10.html#árbol-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Árbol de Decisión",
    "text": "Árbol de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál sería la predicción?"
  },
  {
    "objectID": "tics411/clase-10.html#crecimiento-de-un-árbol",
    "href": "tics411/clase-10.html#crecimiento-de-un-árbol",
    "title": "TICS-411 Minería de Datos",
    "section": "Crecimiento de un Árbol",
    "text": "Crecimiento de un Árbol\n\nUn árbol sólo dejará de crecer si:\n\nNo hay más puntos a separar.\n\nTodas las muestras de un nodo pertenecen a la misma clase.\n\nNo hay más variables a separar.\n\n\n\n\n\n\n\n\n\nEsto normalmente termina en Overfitting.\n\n\n\n\n\n\n\n\n\n\nPara solucionar esto se aplica regularización. En el caso de Árboles esto se denomina prunning."
  },
  {
    "objectID": "tics411/clase-10.html#pruning",
    "href": "tics411/clase-10.html#pruning",
    "title": "TICS-411 Minería de Datos",
    "section": "Pruning",
    "text": "Pruning\nPrepruning: Define/Evita que el árbol crezca hasta:\n\nUn cierto nivel o número de hojas.\nAplicar un test estadístico (normalmente un proceso muy costoso).\nUsar medidas de complejidad para penalizar árboles de gran tamaño.\n\nPostpruning: Decide eliminar nodos, luego de que el árbol crezca.\n\nUsar un parámetro de Costo de Impureza."
  },
  {
    "objectID": "tics411/clase-10.html#hiperparámetros",
    "href": "tics411/clase-10.html#hiperparámetros",
    "title": "TICS-411 Minería de Datos",
    "section": "Hiperparámetros",
    "text": "Hiperparámetros\n\n\n\n\n\n\n\ncriterion: Elegir bajo qué criterio se mide la impureza.\nmax_depth: El nivel es la altura que tendrá el árbol. Niveles más bajos generan árboles más simples.\nmin_samples_split: Número de instancias necesarias para generar un split. Un mayor número o proporción generará árboles más simples.\nmin_samples_leaf: Número mínimo de instancias necesarias para que un nodo sea hoja. Un número o proporción más alta generará árboles más simples.\nccp_alpha: Está asociado a la pureza total del árbol. Para más información ver acá.\n\n\n\n\n\n\n\n\n\n\n¿Cómo se ve la complejidad/simplicidad en un árbol de Decisión?"
  },
  {
    "objectID": "tics411/clase-10.html#implementación-en-scikit-learn",
    "href": "tics411/clase-10.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndt = DecisionTreeClassifier(criterion=\"gini\", max_depth=None, min_sample_split=2, \n                            min_samples_leaf=1,min_impurity_decrease=0, \n                            ccp_alpha=0, random_state=42)\ndt.fit(X_train, y_train)\n\ny_pred = dt.predict(X_test)\ny_proba = dt.predict_proba(X_test)\n\n## Permite Visualizar el Árbol de Decisión\nplt_tree(dt, filled = True, feature_names=None, class_names=None)\n\n\ncriterion: Puede ser gini o entropía. Por defecto \"gini\".\nmax_depth: Número de niveles que se permita que crezca el nivel, por defecto None, significa todos los que pueda.\nmin_samples_split: El número mínimo de elementos dentro de un nodo para permitir el split. Por defecto 2.\nmin_samples_leaf: El número mínimo de elementos para que un nodo pueda ser considerado hoja. Por defecto 1.\nmin_impurity_decreased: Decrecimiento mínimo de la impureza. Si no se cumple, no hay Split. Por defecto 0.\nccp_alpha: Parámetro de Post-Pruning. Valores más altos genera la poda de más nodos."
  },
  {
    "objectID": "tics579/notebooks/Intro_pytorch.html",
    "href": "tics579/notebooks/Intro_pytorch.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nnp.random.seed(1)\n\n\nX_numpy = np.random.randn(1000, 10)\ny_numpy = np.random.randint(0, 2, 1000)\n\nX = torch.from_numpy(X_numpy).float()\ny = torch.from_numpy(y_numpy).float()\ny.shape\n\n((750, 10), (250, 10), (750,), (250,))\n\n\n\ntorch.__version__\n\n'2.4.0'\n\n\n\nw1 = nn.Linear(in_features=10, out_features=32)\nrelu_1 = nn.ReLU()\nw2 = nn.Linear(in_features=32, out_features=64)\nrelu_2 = nn.ReLU()\nw3 = nn.Linear(64, 1)\n\n\nout_w1 = w1(X)\nout_relu_1 = relu_1(out_w1)\nout_w2 = w2(out_relu_1)\nout_relu_2 = relu_2(out_w2)\nout_w3 = w3(out_relu_2)\nout_w3.shape\n\ntorch.Size([1000, 1])\n\n\n\nclass MyFFN(nn.Module):\n    def __init__(self, n_features, hidden_dim_1, hidden_dim_2, out_dim):\n        super().__init__()\n        self.w1 = nn.Linear(\n            in_features=n_features, out_features=hidden_dim_1\n        )\n        self.relu_1 = nn.ReLU()\n        self.w2 = nn.Linear(\n            in_features=hidden_dim_1, out_features=hidden_dim_2\n        )\n        self.relu_2 = nn.ReLU()\n        self.w3 = nn.Linear(hidden_dim_2, out_dim)\n\n    def forward(self, x):\n        x = self.w1(x)\n        x = self.relu_1(x)\n        x = self.w2(x)\n        x = self.relu_2(x)\n        x = self.w3(x)\n        return x\n\n\nmodel = MyFFN(n_features=10, hidden_dim_1=32, hidden_dim_2=64, out_dim=1)\nmodel\n\nMyFFN(\n  (w1): Linear(in_features=10, out_features=32, bias=True)\n  (relu_1): ReLU()\n  (w2): Linear(in_features=32, out_features=64, bias=True)\n  (relu_2): ReLU()\n  (w3): Linear(in_features=64, out_features=1, bias=True)\n)\n\n\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n\nimport matplotlib.pyplot as plt\n\n## Entrenamiento... (Training Loop)\nEPOCHS = 2000\n\nloss_list = []\nfor e in range(EPOCHS):\n    model.train()\n    optimizer.zero_grad()\n    preds = model(X)\n    ## predicciones primero, y luego el target\n    loss = criterion(preds, y.unsqueeze(1))\n    loss.backward()\n    optimizer.step()\n    loss_list.append(loss.item())\n\nplt.plot(range(EPOCHS), loss_list)\n\n\n\n\n\n\n\n\n\nmodel.w1\n\nLinear(in_features=10, out_features=32, bias=True)\n\n\n\nmodel\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/notebooks/control-2.html",
    "href": "tics579/notebooks/control-2.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import torch\n\ntorch.manual_seed(42)\ndevice = (\n    torch.device(\"cuda\")\n    if torch.cuda.is_available()\n    else torch.device(\"cpu\")\n)\n\nX = torch.randn((10, 5), requires_grad=True)\nW = torch.randn((5, 15), requires_grad=True)\n\nX2 = torch.sigmoid(X @ W)\n# a). Qué dimensiones devuelve el siguiente comando?\nprint(X2.shape)\n\n## b). Calcular las derivadas de X2\nX2._______(torch.ones_like(X2), retain_graph=________)\ndW = ________\ndX = ________\n\n## c). Cuál es el resultado de los siguientes dos comandos?\nprint(dW.shape)\nprint(dX.shape)\n\ntorch.Size([10, 15])\ntorch.Size([5, 15])\ntorch.Size([10, 5])\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1.html",
    "href": "tics579/notebooks/Tarea-1.html",
    "title": "Parte I: Optimizadores",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\nSEED = 10\ntorch.manual_seed(SEED)\n\n&lt;torch._C.Generator at 0x7f458820cf30&gt;\ndef plot_optimizers(theta_0, theta_1, title):\n    plt.plot(theta_0, theta_1)\n    plt.title(title)\n    plt.xlabel(r\"theta_0\")\n    plt.ylabel(r\"theta_1\")"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1.html#sgd",
    "href": "tics579/notebooks/Tarea-1.html#sgd",
    "title": "Parte I: Optimizadores",
    "section": "SGD",
    "text": "SGD\n\nalpha = 0.075\ntheta = torch.tensor([10.0, 1.0], requires_grad=True)\n\n\ndef f(theta):\n    return 0.5 * theta[0] ** 2 + 12 * theta[1] ** 2\n\n\nx = []\ny = []\n\nfor e in range(500):\n    output = f(theta)\n    output.backward(retain_graph=True)\n    df = theta.grad.data\n\n    theta.data = theta.data - alpha * df\n    theta.grad.data.zero_()\n    x.append(theta.detach().numpy()[0])\n    y.append(theta.detach().numpy()[1])\n\nplot_optimizers(x, y, title=\"Convergencia para SGD\")"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1.html#momentum",
    "href": "tics579/notebooks/Tarea-1.html#momentum",
    "title": "Parte I: Optimizadores",
    "section": "Momentum",
    "text": "Momentum\n\nbeta = 0.9\nu = torch.tensor([0, 0])\ntheta = torch.tensor([10.0, 1.0], requires_grad=True)\nx = []\ny = []\n\nfor e in range(500):\n    output = f(theta)\n    output.backward(retain_graph=True)\n    df = theta.grad.data\n    u = beta * u + (1 - beta) * df\n    theta.data = theta.data - alpha * u\n    theta.grad.data.zero_()\n    x.append(theta.detach().numpy()[0])\n    y.append(theta.detach().numpy()[1])\n\nplot_optimizers(x, y, title=\"Convergencia con SGD con Momentum\")"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1.html#adam",
    "href": "tics579/notebooks/Tarea-1.html#adam",
    "title": "Parte I: Optimizadores",
    "section": "Adam",
    "text": "Adam\n\noutput = f(theta)\noutput.backward(retain_graph=True)\ndf = theta.grad.data\n\n\nbeta_1 = 0.9\nbeta_2 = 0.999\nv = torch.tensor([0, 0])\ns = torch.tensor([0, 0])\ntheta = torch.tensor([10.0, 1.0], requires_grad=True)\nx = []\ny = []\nfor e in range(500):\n    output = f(theta)\n    output.backward(retain_graph=True)\n    df = theta.grad.data\n    v = beta_1 * v + (1 - beta_1) * df\n    s = beta_2 * s + (1 - beta_2) * df**2\n    v_p = v / (1 - beta_1 ** (e + 1))\n    s_p = s / (1 - beta_2 ** (e + 1))\n\n    theta.data = theta.data - alpha / s_p**0.5 * v_p\n    theta.grad.data.zero_()\n    x.append(theta.detach().numpy()[0])\n    y.append(theta.detach().numpy()[1])\n\nplot_optimizers(x, y, title=\"Convergencia con Adam\")"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1.html#parte-ii-red-softmax",
    "href": "tics579/notebooks/Tarea-1.html#parte-ii-red-softmax",
    "title": "Parte I: Optimizadores",
    "section": "Parte II: Red Softmax",
    "text": "Parte II: Red Softmax\n\ndef forward(X, y, W1, W2):\n    h = torch.relu(X @ W1) @ W2\n    S = torch.softmax(h, dim=1)\n    I_y = F.one_hot(y)\n    return h, S, I_y\n\n\ndef dReLU(x):\n    return torch.where(x &lt;= 0, 0, 1)\n\n\ndef dW2(X, W1, S, I_y):\n\n    return torch.relu(X @ W1).T @ (S - I_y)\n\n\ndef dW1(X, W1, W2, S, I_y):\n    return X.T @ (dReLU(X @ W1) * ((S - I_y) @ W2.T))\n\n\nX = torch.randn(100, 5)\ny = torch.randint(3, (100,))\nW1 = torch.randn(5, 32)\nW2 = torch.randn(32, 3)\n\nW1_pytorch = W1.clone()\nW2_pytorch = W2.clone()\nalpha = 0.1\nEPOCHS = 1000\nepoch_list = [10, 50, 100, 500, 1000]\n\n\ndef training_loop(X, y, W1, W2):\n    m = X.shape[0]\n    score = []\n    for e in range(1, EPOCHS + 1):\n        h, S, I_y = forward(X, y, W1, W2)\n        W1 -= alpha / m * dW1(X, W1, W2, S, I_y)\n        W2 -= alpha / m * dW2(X, W1, S, I_y)\n\n        if e in epoch_list:\n            h, _, _ = forward(X, y, W1, W2)\n            y_p = torch.argmax(h, dim=1).detach().numpy()\n            score.append(accuracy_score(y, y_p))\n\n    return W1, W2, score\n\n\nW1_t, W2_t, score = training_loop(X, y, W1, W2)\nplt.plot(epoch_list, score)\nscore"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1.html#parte-iii-entrenando-con-pytorch",
    "href": "tics579/notebooks/Tarea-1.html#parte-iii-entrenando-con-pytorch",
    "title": "Parte I: Optimizadores",
    "section": "Parte III: Entrenando con Pytorch",
    "text": "Parte III: Entrenando con Pytorch\n\nclass MySimpleMLP(nn.Module):\n    def __init__(\n        self, n_features, hidden_dim=32, output_dim=3, bias=False\n    ):\n        super().__init__()\n        self.w1 = nn.Linear(n_features, hidden_dim, bias=bias)\n        self.w2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n        self.activation = nn.ReLU(inplace=True)\n        self.w1.weight.data = W1_pytorch.T\n        self.w2.weight.data = W2_pytorch.T\n\n    def forward(self, x):\n        x = self.w1(x)\n        x = self.activation(x)\n        x = self.w2(x)\n        return x\n\n\nmodel = MySimpleMLP(n_features=5, bias=False)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=alpha)\n\n\ndef training_loop_pytorch(model, X, y, epochs=EPOCHS):\n    score = []\n    for e in range(1, epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        preds = model(X)\n        loss = criterion(preds, y)\n        loss.backward()\n        optimizer.step()\n\n        if e in epoch_list:\n            model.eval()\n            with torch.no_grad():\n                y_pred_pytorch = (\n                    torch.argmax(model(X), dim=1).detach().numpy()\n                )\n                score.append(accuracy_score(y, y_pred_pytorch))\n\n    return model, score\n\n\nmodel, score = training_loop_pytorch(model, X, y)\nplt.plot(epoch_list, score)\nscore"
  },
  {
    "objectID": "tics579/clase-1.html#por-qué-estudiar-deep-learning",
    "href": "tics579/clase-1.html#por-qué-estudiar-deep-learning",
    "title": "TICS-579-Deep Learning",
    "section": "¿Por qué estudiar Deep Learning?",
    "text": "¿Por qué estudiar Deep Learning?\n\nPrincipalmente porque es el Estado del Arte en las aplicaciones más Impresionantes en la Inteligencia Artificial.\n\n\n\n\nAlexnet (2012)\n\n\n\nAlphaGo (2016)\n\n\n\nTransformers (2017)\n\n\n\nGPT (2019)"
  },
  {
    "objectID": "tics579/clase-1.html#por-qué-estudiar-deep-learning-1",
    "href": "tics579/clase-1.html#por-qué-estudiar-deep-learning-1",
    "title": "TICS-579-Deep Learning",
    "section": "¿Por qué estudiar Deep Learning?",
    "text": "¿Por qué estudiar Deep Learning?\n\nPrincipalmente porque es el Estado del Arte en las aplicaciones más Impresionantes en la Inteligencia Artificial.\n\n\n\n\nGPT-3 (2021)\n\n\n\nAlphaFold (2021)\n\n\n\nStable Diffussion/Dalle (2022)\n\n\n\nLLMs (2023) (ChatGPT/Llama)"
  },
  {
    "objectID": "tics579/clase-1.html#por-qué-estudiar-deep-learning-2",
    "href": "tics579/clase-1.html#por-qué-estudiar-deep-learning-2",
    "title": "TICS-579-Deep Learning",
    "section": "¿Por qué estudiar Deep Learning?",
    "text": "¿Por qué estudiar Deep Learning?\n\n\n\n\n\n\n\nImágen tomada de la Clase de Zico Colter"
  },
  {
    "objectID": "tics579/clase-1.html#por-qué-estudiar-deep-learning-3",
    "href": "tics579/clase-1.html#por-qué-estudiar-deep-learning-3",
    "title": "TICS-579-Deep Learning",
    "section": "¿Por qué estudiar Deep Learning?",
    "text": "¿Por qué estudiar Deep Learning?\n\n\n\n\n\n\nFacilidad y Autograd\n\n\n\nFrameworks como Tensorflow, Pytorch o Jax permiten realizar esto de manera mucho más sencilla.\n\nFrameworks permiten calcular gradientes de manera automática.\nAntigua mente trabajar en Torch, Caffe o Theano podía tomar cerca de 50K líneas de código.\n\n\n\n\n\n\n\n\n\n\n\nCómputo\n\n\n\nProliferación de las GPUs, TPUs, HPUs, IPUs, como sistemas masivos de Cómputos.\n\nHow many computers to identify a cat? 16,000\n\n\n\n\n\n\n\n\n\n\n\nEstado del Arte\n\n\n\nModelos de Deep Learning pueden generar sistemas que entiendan imágenes, textos, audios, videos, grafos, etc."
  },
  {
    "objectID": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales",
    "href": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales",
    "title": "TICS-579-Deep Learning",
    "section": "El nacimiento de las Redes Neuronales",
    "text": "El nacimiento de las Redes Neuronales\n\nLas redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad más básica es una Neurona."
  },
  {
    "objectID": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales-1",
    "href": "tics579/clase-1.html#el-nacimiento-de-las-redes-neuronales-1",
    "title": "TICS-579-Deep Learning",
    "section": "El nacimiento de las Redes Neuronales",
    "text": "El nacimiento de las Redes Neuronales\n\nLas redes neuronales artificiales (ANN), son modelos inspirados en el mecanismo cerebral de sinapsis. Su unidad más básica es una Neurona.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste tipo de nomenclatura está sumamente pasada de moda.\n\n\n\n\n\nEste cálculo se puede representar como:\n\n\\[ y = \\phi(w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_5 \\cdot x_5)\\] \\[ y = \\phi(w^T \\cdot x)\\]\ndonde \\(w = [w_1, w_2, w_3, w_4, w_5]\\) y \\(x = [x_1, x_2, x_3, x_4, x_5]\\).\n\n\n\n\n\n\n\n\n¿Qué pasa si \\(\\phi(.)\\) vale la función identidad?\nTenemos una Regresión Lineal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué pasa si \\(\\phi(.)\\) vale la función sigmoide?\nTenemos una Regresión Logística."
  },
  {
    "objectID": "tics579/clase-1.html#arquitectura-de-una-red",
    "href": "tics579/clase-1.html#arquitectura-de-una-red",
    "title": "TICS-579-Deep Learning",
    "section": "Arquitectura de una Red",
    "text": "Arquitectura de una Red\n\n\n\n\n\n\n\n\nEstructura más común\n(Probablemente tampoco seguiremos esta nomenclatura)\n\nNodos o Neuronas\nEdges o Conexiones\nCapas\n\n\n\n\n\n\n\n\n¿Cuántas capas tiene esta red?\n\n\n\n\n\n\n\n\n\n\n\nDepende\n\n\n\n\n\n\nNormalmente todas las neuronas de una capa anterior se conectan con las de una capa posterior (Hay excepciones).\nDependiendo de la forma en la que se conecten, cada Arquitectura recibe un nombre."
  },
  {
    "objectID": "tics579/clase-1.html#los-ingredientes-de-un-algoritmo-de-aprendizaje",
    "href": "tics579/clase-1.html#los-ingredientes-de-un-algoritmo-de-aprendizaje",
    "title": "TICS-579-Deep Learning",
    "section": "Los Ingredientes de un Algoritmo de Aprendizaje",
    "text": "Los Ingredientes de un Algoritmo de Aprendizaje\n\nHipótesis\n\n\nUna función que describe como mapear inputs (features) con outputs (labels) por medio de parámetros.\n\n\nLoss Function\n\n\nUna función que especifica cuanta información se pierde. Mayor pérdida implica más error de estimación.\n\n\nMétodo de Optimización\n\n\nEs el responsable de combinar la hipótesis y la loss function. Corresponde a un procedimiento para determinar los parámetros de la hipótesis, minimizando la suma de las pérdidas en un set de entrenamiento."
  },
  {
    "objectID": "tics579/clase-1.html#ejemplo-softmax-regression",
    "href": "tics579/clase-1.html#ejemplo-softmax-regression",
    "title": "TICS-579-Deep Learning",
    "section": "Ejemplo: Softmax Regression",
    "text": "Ejemplo: Softmax Regression\n\nSoftmax Regression\n\n\nCorresponde la versión multiclase de una Regresión Logística. También se le llama una Shallow Network.\n\n\n\n\n\n\n\n\n\n\n\nConsideremos un problema de clasificación multiclase de \\(k\\) clases tal que:\n\n\n\nDatos de Entrenamiento: \\(x^{(i)}, y^{(i)} \\in {1,...,k}\\) para \\(i=1,...,m\\).\n\n\\(n\\): Es el número de Features.\n\\(m\\): Es el número de puntos en el training set.\n\\(k\\): Es el número de clases del problema.\n\n\n\n\n\n\n\n\n\n\n\nVamos a tener en total \\(n \\times k\\) parámetros o pesos que actualizar."
  },
  {
    "objectID": "tics579/clase-1.html#softmax-regression-hipótesis",
    "href": "tics579/clase-1.html#softmax-regression-hipótesis",
    "title": "TICS-579-Deep Learning",
    "section": "Softmax Regression: Hipótesis",
    "text": "Softmax Regression: Hipótesis\n\nVamos a definir una función que mapea valores de \\(x \\in \\mathbb{R}\\) a vectores de \\(k\\) dimensiones.\n\n\\[ h: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k\\] \\[ x \\rightarrow h_\\theta(x) = \\theta^T x\\]\n\ndonde \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(x \\in \\mathbb{R}^{n\\times 1}\\)\n\n\n\n\n\n\n\nEn este caso usamos una hipótesis lineal, ya que se usa una multiplicación matricial (o producto punto) para relacionar \\(\\theta\\) y \\(x\\).\n\n\n\n\n\n\n\n\n\nEn este caso el output de \\(h_i(x)\\) devolverá la probabilidad de pertenecer a una cierta clase \\(i\\).\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el tamaño/dimensión de \\(h_\\theta(x)\\)?"
  },
  {
    "objectID": "tics579/clase-1.html#notación-matricial",
    "href": "tics579/clase-1.html#notación-matricial",
    "title": "TICS-579-Deep Learning",
    "section": "Notación Matricial",
    "text": "Notación Matricial\n\nUna manera más conveniente de escribir estas operaciones es utilizar (Matrix Batch Form).\n\n\n\nDesign Matrix\n\\[X \\in \\mathbb{R}^{m \\times n} = \\begin{bmatrix}\n&-x^{(1)T}-\\\\\n& \\vdots & \\\\\n&-x^{(m)T}- &\\\\\n\\end{bmatrix}\\]\n\nLabels Vector\n\\[y \\in {1,...,k} = \\begin{bmatrix}\n&-y^{(1)}-\\\\\n& \\vdots & \\\\\n&-y^{(m)}- &\\\\\n\\end{bmatrix}\\]\n\nLa hipótesis también se puede reescribir de manera matricial como:\n\n\n\\[h_\\theta(X) = \\begin{bmatrix}\n&-h_\\theta(x^{(1)})^T-\\\\\n& \\vdots & \\\\\n&-h_\\theta(x^{(m)})^T-\\\\\n\\end{bmatrix}\\]\n\n\\[h_\\theta(X)= \\begin{bmatrix}\n&-x^{(1)T} \\theta-\\\\\n& \\vdots & \\\\\n&-x^{(m)T} \\theta-\\\\\n\\end{bmatrix} = X  \\theta\\]\n\n\n\n\n\n\n\n\nNormalmente este tipo de operaciones son las que utilizaremos para hacer nuestro código."
  },
  {
    "objectID": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss",
    "href": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function: Softmax/Cross-Entropy Loss",
    "text": "Loss Function: Softmax/Cross-Entropy Loss\n\n\n\n\n\n\n\nLa salida de nuestra Shallow Network retornará valores reales.\n\n\n\n\n\n\n\n\n\n\n\nPara poder tener una mejor interpretación del significado de cada una aplicaremos la función Softmax lo cual permitirá normalizar los resultados y llevará los resultados a una “distribución de probabilidad” (valores positivos que sumen 1).\n\n\n\n\n\n\n\n\n\n\n\n\nFormalmente definiremos la función Softmax como:\n\\[s_i = p(label = i) = \\frac{exp(h_i(x))}{\\sum_{j=1}^k exp(h_j(x))}\\]\n\\[s = \\begin{bmatrix}\n&s_1&\\\\\n& \\vdots & \\\\\n&s_k&\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss-1",
    "href": "tics579/clase-1.html#loss-function-softmaxcross-entropy-loss-1",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function: Softmax/Cross-Entropy Loss",
    "text": "Loss Function: Softmax/Cross-Entropy Loss\nPara medir el error/pérdida de información utilizaremos el Negative Log Loss o Cross Entropy Loss.\n\\[l_{ce}(h(x), y) = -log\\left(p(label = y)\\right)\\]\n\n\n\n\n\n\n\nPara garantizar el éxito de nuestro modelo, básicamente queremos maximizar la probabilidad de encontrar la etiqueta correcta, es decir, que \\(p(label = y)\\) sea lo más alto posible.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente en los problemas de optimización no se suele maximizar sino minimizar. Minimizar el valor negativo es equivalente a maximizar. Esto sería equivalente a minimizar el error del modelo.\n\n\n\n\n\n\n\n\n\n\n\nFinalmente por razones de estabilidad numérica, minimizamos el logaritmo de la probabilidad que es una técnica bien conocida en Estadística.\n\n\n\n\n\n\\[\\begin{align}\nl_{ce}(h(x), y) = -log\\left(p(label = y)\\right) &= -log \\left(\\frac{exp(h_{(i = y)}(x))}{\\sum_{j=1}^k exp(h_j(x))}\\right) \\\\\n&= - h_{(i=y)}(x) + log\\left(\\sum_{j = 1}^k exp(h_j(x))\\right)\\end{align}\\]"
  },
  {
    "objectID": "tics579/clase-1.html#método-de-optimización",
    "href": "tics579/clase-1.html#método-de-optimización",
    "title": "TICS-579-Deep Learning",
    "section": "Método de Optimización",
    "text": "Método de Optimización\n\nEl último ingrediente de un algoritmo de aprendizaje es el método de optimización. Es necesario minimizar la pérdida promedio asociada a todos los puntos de un cierto set de entrenamiento. Para ello definimos esto formalmente como:\n\n\\[\\underset{\\theta}{minimize} = \\frac{1}{m} \\sum_{i=1}^m l_{ce}(h_\\theta(x^{(i)}), y^{(i)})\\]\n\n\n\n\n\n\n¿Cómo encontramos los parámetros \\(\\theta\\) que minimizan la pérdida de información/error de estimación?\n\n\n\n\nGradient Descent\n\n\nEs un método numérico que permite minimizar funciones moviéndose en dirección contraria al Gradiente. Es computacionalmente muy eficiente y fácil de implementar en código."
  },
  {
    "objectID": "tics579/clase-1.html#gradient-descent",
    "href": "tics579/clase-1.html#gradient-descent",
    "title": "TICS-579-Deep Learning",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\nSe define el gradiente como la matriz que contiene las derivadas parciales de una función \\(f\\). Se denota como:\n\\[\\nabla_\\theta f(\\theta) \\in \\mathbb{R}^{n \\times k} =  \\begin{bmatrix}\n\\frac{\\partial f(\\theta)}{\\partial \\theta_{11}} & \\cdots & \\frac{\\partial f(\\theta)}{\\partial \\theta_{1k}} \\\\\n\\cdots & \\ddots & \\cdots \\\\\n\\frac{\\partial f(\\theta)}{\\partial \\theta_{n1}} & \\cdots & \\frac{\\partial f(\\theta)}{\\partial \\theta_{nk}}\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\\(\\theta_{ij}\\) corresponde al parámetro que une el nodo/feature \\(i\\) con el nodo/predicción \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl gradiente apunta a la dirección de máximo crecimiento de la función \\(f\\)."
  },
  {
    "objectID": "tics579/clase-1.html#gradient-descent-regla-de-actualización",
    "href": "tics579/clase-1.html#gradient-descent-regla-de-actualización",
    "title": "TICS-579-Deep Learning",
    "section": "Gradient Descent: Regla de Actualización",
    "text": "Gradient Descent: Regla de Actualización\nPara minimizar la función, la idea es descender iterativamente por el trayecto en contra del gradiente. La regla de actualización se define como:\n\\[\\theta := \\theta - \\alpha \\nabla_\\theta f(\\theta) = \\theta - \\frac{\\alpha}{m}\\nabla_\\theta l_{ce}(X\\theta,y)\\]\ncon \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(\\alpha &gt; 0\\) corresponde al step size o learning rate.\n\n\n\n\n\n\n\n\n\n\n\nEn nuestro caso \\(f\\) corresponderá a nuestro \\(l_{ce}\\) calculado anteriormente. El problema es, ¿cuánto vale el gradiente del Cross Entropy Loss?"
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\nSimplifiquemos el problema a calcular para un sólo vector \\(x\\).\n\\[\\theta := \\theta - \\alpha \\nabla_\\theta l_{ce}(\\theta^Tx,y) \\]\n\n\n\n\n\n\n\n\n¿Cuánto vale el Gradiente?\n\nNo es tan sencillo, ya que derivamos respecto a \\(\\theta\\) que es una matriz.\nPero derivamos a \\(\\theta^T x\\) que es un vector.\nPara ello, lo correcto es utilizar Calculo Diferencial Matricial, Jacobianos y Productos de Kroenecker (que probablemente no han visto en ningún curso).\n\nSPOILER: Yo tampoco lo he visto en ningún curso.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsaremos un truco (sumamente hacky 😱) que jamás deben revelar y que avergonzaría a cualquier profesor de Cálculo.\n\nPretenderemos que todos los valores son escalares y corregiremos las dimensiones al final."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-1",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-1",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\nSimplifiquemos el problema pensando que calcularemos el Gradiente para un sólo vector \\(x\\).\n\n\nEs decir, \\(x \\in \\mathbb{R}^{n\\times1}\\).\n\nAdemás sabemos que \\(\\nabla_\\theta l_{ce}(\\theta^Tx, y)\\) debe tener dimensiones \\(n \\times k\\).\n\n\n\n\n\n\n\n¿Por qué?\n\n\n\n\n\n\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = \\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^Tx}{\\partial \\theta}\\]\n\n\\[\\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} = \\frac{\\partial l_{ce}(h_\\theta(x), y)}{\\partial h_\\theta(x)} = \\begin{bmatrix}\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_1} \\\\\n\\vdots\\\\\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_k} \\\\\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nLuego el gradiente de \\(l_{ce}\\) respecto a \\(h\\) tiene dimensiones \\(k \\times 1\\)."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-2",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-2",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\\[\\begin{align}\n\\frac{\\partial l_{ce}(h,y)}{\\partial h_i} &= \\frac{\\partial }{\\partial h_i}\\left(-h_{(i = y)} + log \\sum_{j = 1}^k exp(h_j)\\right) \\\\\n&= -\\frac{\\partial h_{(i = y)}}{\\partial h_i}+ \\frac{1}{\\sum_{j = 1}^k exp(h_j)} \\cdot \\frac{\\partial}{\\partial h_i}\\left(\\sum_{j=1}^k exp(h_j)\\right) \\\\\n&= -\\frac{\\partial h_{(i = y)}}{\\partial h_i}+ \\frac{exp(h_i)}{\\sum_{j = 1}^k exp(h_j)} \\\\\n&= - 1\\{i=y\\} + s_i = s_i - 1\\{i=y\\}\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\\[1\\{i = y\\} = \\begin{cases}\n1,  & \\text{i = y} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\nFinalmente en forma vectorial quedaría como:\n\n\n\\[\\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} = s - e_y\\]\n\n\n\n\n\n\n\nDonde \\(z\\), es el vector de Softmax y \\(e_y\\) es un vector con un 1 en la posición \\(y\\) y 0 en el resto."
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-a-mano-3",
    "href": "tics579/clase-1.html#calculando-el-gradiente-a-mano-3",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente a mano",
    "text": "Calculando el Gradiente a mano\n\n\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = \\frac{\\partial l_{ce}(\\theta^T x,y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^Tx}{\\partial \\theta}\\] \\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = (s-e_y)\\cdot x \\]\n\n\n\n\n\n\n\nOjo con las dimensiones\n\n\n\n\\(s-e_y \\in \\mathbb{R}^{k \\times 1}\\)\n\\(x \\in \\mathbb{R}^{n \\times 1}\\)\n\n\n\n\n\n\nLuego:\n\\[\\nabla_\\theta l_{ce}(\\theta^T x,y) = x (s-e_y)^T\\]\n\n\n\n\n\n\n\n\n¿Cuál es el tamaño de \\(\\nabla_\\theta l_{ce}(\\theta^T x,y)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\\(n \\times k\\)\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué?"
  },
  {
    "objectID": "tics579/clase-1.html#calculando-el-gradiente-matrix-batch-form",
    "href": "tics579/clase-1.html#calculando-el-gradiente-matrix-batch-form",
    "title": "TICS-579-Deep Learning",
    "section": "Calculando el Gradiente Matrix Batch Form",
    "text": "Calculando el Gradiente Matrix Batch Form\nEsto sería equivalente a tomar en consideración todos los puntos del Training Set\n\n\n\\[\\begin{align}\\nabla_\\theta l_{ce}(X\\theta,y) &= \\frac{\\partial l_{ce}(X\\theta,y)}{\\partial X\\theta} \\cdot \\frac{\\partial X\\theta}{\\partial \\theta}\\\\\n&= (S - I_y) \\cdot X \\\\\n&= X^T \\cdot (S - I_y)\n\\end{align}\\]\n\n\n\n\n\n\n\n\\(S\\) corresponde al Softmax de \\(X\\theta\\) aplicado por filas.\n\\(I_y\\) corresponde al One Hot Encoder de las etiquetas. Filas con 1 en la etiqueta correcta y 0 en el resto.\n\n\n\n\n\n\n\n\n\n\n\n\nOjo con las dimensiones\n\n\n\n\\(S - I_y \\in \\mathbb{R}^{m \\times k}\\)\n\\(X \\in \\mathbb{R}^{m \\times n}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el tamaño de \\(\\nabla_\\theta l_{ce}(X\\theta,y)\\)?\n\n\n\n\n\n\nFinalmente la Regla de Actualización de parámetros usando Gradient Descent queda como:\n\\[\\theta := \\theta - \\frac{\\alpha}{m} X^T (S - I_y)\\]"
  },
  {
    "objectID": "tics579/clase-1.html#conclusiones",
    "href": "tics579/clase-1.html#conclusiones",
    "title": "TICS-579-Deep Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\n\n\n\n\n\n\n\n\n\nAcabamos de entrenar una Shallow Network, sin definir ningún concepto Fancy que es propio del área.\nNo hemos hablado ni de:\n\nForward Pass\nEpochs\nBackpropagation\nAdam\nActivation Functions\netc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAplicando esta simple regla se puede obtener cerca de un 8% de error clasificando dígitos en MNIST.\nSe puede programar en pocas líneas en Python.\n\n\n\n\n\n\n\n\n\n\n\n\nPero, ¿qué pasa con arquitecturas más complejas?"
  },
  {
    "objectID": "tics579/clase-1.html#multiplicación-matricial",
    "href": "tics579/clase-1.html#multiplicación-matricial",
    "title": "TICS-579-Deep Learning",
    "section": "Multiplicación Matricial",
    "text": "Multiplicación Matricial\n\n\n\n\n\n\n\n\n\nDonde \\(B_{*,i}\\) corresponde a la columna \\(i\\) de B.\nDonde \\(A_{i,*}\\) corresponde a la fila \\(i\\) de A."
  },
  {
    "objectID": "tics579/clase-3.html#feed-forward-networks-1",
    "href": "tics579/clase-3.html#feed-forward-networks-1",
    "title": "TICS-579-Deep Learning",
    "section": "Feed Forward Networks",
    "text": "Feed Forward Networks\n\nTeorema de aproximación Universal\n\n\nSe dice que una Red Neuronal puede aproximar cualquier función en una región cerrada.\n\n\n\nEs decir,\n\\[\\underset{x \\in \\mathbb{D}}{max}|f(x) - \\hat{f(x)}| \\le \\epsilon\\]\ncon \\(D \\subset \\mathbb{R}\\) y \\(\\epsilon &gt;0\\).\n\n\n\n\n\n\n\nExisten muchas otras funciones como Splines, KNN y otras que también tienen esta propiedad.\nSi se escogen puntos suficientemente cercanos, cumplir esta propiedad es trivial."
  },
  {
    "objectID": "tics579/clase-3.html#feed-forward-networks-ffn",
    "href": "tics579/clase-3.html#feed-forward-networks-ffn",
    "title": "TICS-579-Deep Learning",
    "section": "Feed Forward Networks (FFN)",
    "text": "Feed Forward Networks (FFN)\n\nEs un tipo de Arquitectura caracterizada por Nodos en un nivel que se conectan con todos los nodos del siguiente nivel. Este es probablemente el tipo de Arquitectura de Red Neuronal más común.\n\n\n\n\n\n\n\nEste tipo de Redes tiene distintos nombres que son usados de manera intercambiable:\n\nCapas Lineales: Probablemente por su denominación en Pytorch.\nCapas/Redes Densas: Probablemente por su denominación en Tensorflow.\nMultilayer Perceptron: O también conocido como MLP, debido a que es la generalización del Perceptrón, la primera propuesta de Redes Neuronales de Rosenblatt en 1958.\n\n\n\n\n\n\n\n\n\n\n\n\nComunmente\n\nCada suma-producto de parámetros e inputs corresponde a una capa. Nosotros llamamos capas de parámetros a cada grupo de conexiones.\nCada nodo corresponde a una Neurona. Nosotros consideramos que eso es la nueva dimensión a la que se mueve nuestro vector de entrada.\n\nSPOILER: El número de nodos/neuronas no es importante, nos importan más los parámetros necesarios.\n\nLa capa de salida corresponde a nuestra Hipótesis."
  },
  {
    "objectID": "tics579/clase-3.html#ffn-formalmente",
    "href": "tics579/clase-3.html#ffn-formalmente",
    "title": "TICS-579-Deep Learning",
    "section": "FFN: Formalmente",
    "text": "FFN: Formalmente\n\n\n\\[ Z_1 = X\\] \\[Z_{i +1} = \\sigma_i(Z_i W_i + b_i)\\] \\[h_\\theta(X) = Z_{L + 1}\\]\ncon \\(\\theta = \\{W_{1:L}, b_{1:L}\\}\\)\npara \\(i=1,...,L\\)\n\n\n\n\n\n\n\n\\(Z_{i}\\) corresponde a la salida de la capa \\(i\\), \\(W_i\\) corresponde al conjunto de parámetros de la capa \\(i\\), y \\(b_i\\) corresponde al bias de la capa \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\\(b_i\\) es otro set de parámetros, llamado bias (el cual se traduce como sesgo, pero lo vamos a mantener en inglés para evitar confusiones semánticas).\nAl agregar este componente, ya no se tiene una Transformación Lineal, sino que una Transformación Affine."
  },
  {
    "objectID": "tics579/clase-3.html#ffn---broadcasting",
    "href": "tics579/clase-3.html#ffn---broadcasting",
    "title": "TICS-579-Deep Learning",
    "section": "FFN - Broadcasting",
    "text": "FFN - Broadcasting\n\\[Z_{i + 1} = \\sigma_i(Z_i W_i + b_i^T)\\]\nSi chequeamos las dimensiones:\n\n\n\n\\(Z_i \\in \\mathbb{R}^{m \\times n_i}\\)\n\n\n\n\\(W_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}\\)\n\n\n\nPero, \\(b_i \\in \\mathbb{R}^{n_i+1}\\)\n\n\n\n\n\n\n\n\n\nTenemos un problema y es que esto hace que las dimensiones no calcen. Esto sería una operación no válida en términos matriciales. Sin embargo es posible realizarla aplicando Broadcasting.\n\n\n\n\n\n\n\n\n\n\n\nBroadcasting\n\n\nCorresponde a una replica de una dimensión de manera de permitir alguna operación que requiera que ciertas dimensiones calcen.\n\n\n\n\n\n\n\n\n\n\n\nBroadcasting Rules\n\n\n\nCada tensor debe tener al menos una dimensión.\nMoviéndose de derecha a izquierda por cada dimensión una vez alineadas a la derecha, las dimensiones deben:\n\nSer iguales,\niguales a 1,\no no debe existir."
  },
  {
    "objectID": "tics579/clase-3.html#ffn---broadcasting-1",
    "href": "tics579/clase-3.html#ffn---broadcasting-1",
    "title": "TICS-579-Deep Learning",
    "section": "FFN - Broadcasting",
    "text": "FFN - Broadcasting\n\nMatemáticamente el Broadcasting en este caso corresponde a:\n\n\\[ b_i^T = 1 b_i^T\\]\ndonde 1, es un vector de unos de \\(m \\times 1\\) y \\(b_i\\) es de dimensión \\(n_{i+1} \\times 1\\), al cuál se está aplicando el producto externo.\n\n\n\n\n\n\nEl Broadcasting permitirá que \\(b_i\\) tenga ahora dimensiones \\(m \\times n_{i+1}\\), lo cuál permitirá que la operación de suma se pueda realizar.\n\n\n\n\n\n\n\n\n\nEl Broadcasting evita que se tenga que almacenar información repetida, lo cual permite que las implementaciones sean más eficientes en términos de memoria. Siempre que se pueda se debe utilizar Broadcasting para simplificar un cálculo.\n\n\n\nMás info ver: Numpy Docs"
  },
  {
    "objectID": "tics579/clase-3.html#hiperparámetros-de-una-red-neuronal",
    "href": "tics579/clase-3.html#hiperparámetros-de-una-red-neuronal",
    "title": "TICS-579-Deep Learning",
    "section": "Hiperparámetros de una Red Neuronal",
    "text": "Hiperparámetros de una Red Neuronal\n\nHiperparámetros\n\n\nValores necesarios para el cómputo de una red neuronal que deben ser determinados por el modelador. Estos valores NO pueden ser aprendidos de manera autónoma por la red neuronal.\n\n\n\n\n\n\n\n\n\n\n\nLearning Rate (Karpathy Constant: 3e-4)\n¿Cuáles son las dimensiones de los Pesos (Weights) y de los sesgos (Biases)? (De qué tamaño es cada capa)\n¿Qué funciones de Activación se utilizarán?\n¿Qué funciones de perdida se utilizarán?\n¿Qué optimizadores se utilizarán?\n¿Cómo se inicializarán los parámetros de la Red Neuronal?\n¿Cuánto tiempo entrenanaremos nuestro modelo? ¿Cómo sabemos si es que convergió o no?"
  },
  {
    "objectID": "tics579/clase-3.html#tipos-de-hipótesis",
    "href": "tics579/clase-3.html#tipos-de-hipótesis",
    "title": "TICS-579-Deep Learning",
    "section": "Tipos de Hipótesis",
    "text": "Tipos de Hipótesis\n\nEn el aprendizaje supervisado contamos principalmente con la resolución de dos tipos de Problemas: Clasificación y Regresión. Dependiendo del tipo de Problema armaremos nuestra hipótesis.\n\n\\[h_\\theta(X) = Z_{L+1} \\in \\mathbb{R}^{m \\times k}\\]\n\n\n\nClasificación\n\nClasificación Binaria: Se requiere un \\(k=1\\). Se usa una Función Sigmoide para transformar el Output en la probabilidad de que ocurra la clase positiva.\nClasificación Multiclase: Se requiere un \\(k=C\\), donde C es el número de clases a clasificar. Se usa una función Softmax para transformar el output en una distribución de probabilidades.\nClasificación Multilabel: Se requiere un \\(k=C\\) donde C es el número de clases a clasificar. Se usa una función Sigmoide para transformar cada clase en probabilidades.\n\n\nRegresión\n\nRegresión Simple: Se requiere un \\(k=1\\). No requiere de funciones adicionales.\nRegresión Multiple: Se requiere un \\(k=V\\) con V el número de valores a predecir.\n\n\n\n\n\n\n\nAdicionalmente se pueden utilizar funciones como la sigmoide o ReLU para forzar salidas entre 0 y 1 o entre 0 e \\(\\infty\\) respectivamente.\n\n\n\n\n\n\n\n\n\n\n\nNormalmente las funciones necesarias en la capa de salida van embebidas en la Loss Function. Normalmente estas funciones sí deben aplicarse al momento de la Predicción del modelo."
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación",
    "href": "tics579/clase-3.html#funciones-de-activación",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\nActivation Functions\n\nCorresponden a las funciones que agregarán características no lineales a nuestra hipótesis, impidiendo la composición de transformaciones lineales (o Affine).\n\n\n\n\n\n\n\n\nComo convención, las funciones de activación sólo se aplicarán a las Hidden Layers. Es decir \\(\\sigma_{L+1}(x) = x\\).\n\n\n\n\n\n\n\n\n\nOtras convenciones utilizan funciones de activación para la capa de salida. Esto bajo el abánico de Pytorch no es correcto ya que la Activación de la última capa esta embebida en la Loss Function (Recordar como Softmax es parte del Cross Entropy).\nAhora, sí es posible utilizar funciones de Activación a la salida de una predicción, pero dichas funciones tienen otro propósito y no son del todo estrictamente necesarias.\n\n\n\n\n\n\n\n\n\n¿Puedo aplicar distintas Funciones de Activación a cada Neurona?\n\n\n\n\n\n\n\n\n\nPara ver más Activation Functions y detalles de su funcionamiento, ir directamente a la Documentación de Pytorch."
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-1",
    "href": "tics579/clase-3.html#funciones-de-activación-1",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\n\n\n\n\nDerivadas:\n\n\n\nSigmoide: \\(g'(z) = g(z)(1 - g(z))\\)\n\n\n\nTanh: \\(g'(z) = 1 - g^2(z)\\)\n\n\n\nReLU: \\(g'(z) =\n\\begin{cases}\n0,  & \\text{if $z \\le$ 0} \\\\[2ex]\n1, & \\text{if $z &gt; 0$}\n\\end{cases}\\)"
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-modernas",
    "href": "tics579/clase-3.html#funciones-de-activación-modernas",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación Modernas",
    "text": "Funciones de Activación Modernas\n\n\nLeaky ReLU\n\n\n\n\n\n\\[g(z) = max(0.1z, z)\\]\n\nParametrized ReLU (PReLU)\n\n\n\n\n\n\\[g(z) = max(az, z)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-2",
    "href": "tics579/clase-3.html#funciones-de-activación-2",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\n\nELU\n \\(g(z) =\n\\begin{cases}\nz,  & \\text{if $z \\ge$ 0} \\\\[2ex]\n\\alpha(e^{z}-1), & \\text{if $z &lt; 0$}\n\\end{cases}\\)\n\nGELU\n \\[\\begin{align} g(z) &= z \\cdot \\Phi(z) \\\\\ng(z)&= 0.5 \\cdot z \\cdot \\left(1 + Tanh\\left(\\sqrt{2/\\pi}\\right) \\cdot \\left(z + 0.044715 \\cdot z^3\\right)\\right)\\end{align}\\]"
  },
  {
    "objectID": "tics579/clase-3.html#funciones-de-activación-3",
    "href": "tics579/clase-3.html#funciones-de-activación-3",
    "title": "TICS-579-Deep Learning",
    "section": "Funciones de Activación",
    "text": "Funciones de Activación\n\n\nSELU\n \\[ g(z) = scale \\cdot (max(0,z) + min(0,\\alpha(e^z - 1)))\\]\ncon \\(\\alpha=1.6732632423543772848170429916717\\) y \\(scale = 1.0507009873554804934193349852946\\)\n\nSwish\n \\[g(z) = z \\cdot sigmoid(z)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#loss-functions",
    "href": "tics579/clase-3.html#loss-functions",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nAl igual que el caso de la Hipótesis, la Loss Function dependerá del tipo de problema a resolver. Existen muchas Loss Functions, pero los más comunes para problemas generales son las siguientes:\n\nClasificación Binaria: Binary Cross Entropy\n\\[BCE_i = - \\left[y_i \\cdot log(h(x_i)) + (1-y_i) log(1-h(x_i))\\right]\\]\ndonde \\(h(x)\\) corresponde a un valor de probabilidad de la clase positiva (debe ir entre 0 y 1).\n\n\n\n\n\n\nEn Pytorch se suele utilizar BCEWithLogitsLoss ya que aplica una función Sigmoide a la capa de salida además de ser una clase numericamente más estable. Esto garantiza que la salida de la Red tiene valores entre 0 y 1 como se necesita.\n\n\n\nClasificación Multiclase: CrossEntropy\n\\[CE_i = -log \\left(\\frac{exp(h_{(i=y)}(x_i))}{\\sum_{j=1}^k exp(h_j(x_i))}\\right)\\]\n\n\n\n\n\n\nEn Pytorch se suele utilizar CrossEntropyLoss ya que combina aplica una función Softmax a la capa de salida además de ser una clase numericamente más estable."
  },
  {
    "objectID": "tics579/clase-3.html#loss-functions-1",
    "href": "tics579/clase-3.html#loss-functions-1",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Functions",
    "text": "Loss Functions\nClasificación Multilabel: CrossEntropy\nPara este tipo de problema se debería aplicar un Negative LogLoss combinado con la salidas de una red que van entre 0 y 1 (es decir, que se aplica una Sigmoide)\n\n\n\n\n\n\nEn Pytorch se suele utilizar BCEWithLogitsLoss ya que combina aplica una función Softmax a la capa de salida y permite resultados de más de una dimensión.\n\n\n\nRegresión\n\n\n\nMean Absolute Error o L1Loss\n\n\\[L1_i = |y_i - h(x_i)|\\]\n\n\nMean Squared Error Loss o L2Loss\n\n\\[L2_i = (y_i - h(x_i))^2\\]\n\n\n\n\n\n\n\nEs importante recordar que en general se debe calcular un valor agregado de la Loss Function. En Pytorch a esto se le llama reduction. Donde el más utilizado es reduction=\"mean\". Es decir,\n\\[l = \\frac{1}{m}\\sum_{i=1}^m L_i\\]"
  },
  {
    "objectID": "tics579/clase-3.html#optimizers",
    "href": "tics579/clase-3.html#optimizers",
    "title": "TICS-579-Deep Learning",
    "section": "Optimizers",
    "text": "Optimizers\n\nGradient Descent corresponde al algoritmo de Optimización más popular, pero no necesariamente el más eficiente. Distintas variantes han ido apareciendo para ir mejorando eventuales deficiencias de la proposición inicial.\n\nNormal Gradient Descent\n\\[\\theta := \\theta - \\frac{\\alpha}{m}\\nabla_\\theta l(h_\\theta(X), y), \\text{donde $X \\in \\mathbb{R}^{m \\times n}$ e $y \\in \\mathbb{R}^{m \\times 1}$}\\]\n\n\n\n\n\n\nLa dirección del Gradiente utilizando menos puntos debería ser más o menos similar. Sin duda, más ruidoso, pero a la larga debería dirigir en casi la misma dirección. Por lo que podríamos hacer actualizaciones de parámetros utilizando B datos con B &lt;&lt; m.\n\n\n\n\n\n\n\n\n\nEsto entrega como beneficio, menos requerimientos de memoria, ya que operarían matrices más pequeñas, por lo tanto, requiere de menos RAM tanto en CPU como en GPU.\n\n\n\nStochastic Gradient Descent (MiniBatch)\n\n\n\\[\\theta := \\theta - \\frac{\\alpha}{B}\\nabla_\\theta l(h_\\theta(X), y), \\text{donde $X \\in \\mathbb{R}^{B \\times n}$ e $y \\in \\mathbb{R}^{B \\times 1}$}\\]\n\n\n\n\n\n\n\nSe van tomando \\(B\\) muestras de manera incremental hasta utilizar la totalidad de datos de entrenamiento"
  },
  {
    "objectID": "tics579/clase-3.html#sgd-with-momentum",
    "href": "tics579/clase-3.html#sgd-with-momentum",
    "title": "TICS-579-Deep Learning",
    "section": "SGD with Momentum",
    "text": "SGD with Momentum\nUpdate Rule\n\\[u_{t + 1} = \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t)\\] \\[\\theta_{t+1} = \\theta_t - \\alpha u_{t + 1}\\]\ndonde \\(0&lt;\\beta&lt;1\\), pero normalmente \\(\\beta=0.9\\).\n\n\n\n\n\n\nEste cálculo se denomina un Exponential Moving Average de los Gradientes.\n\n\n\n\\[\\begin{align} u_{t+1}&=(1-\\beta)\\nabla_\\theta f(\\theta_{t}) + \\beta u_t \\\\\nu_{t+1}&=(1-\\beta)\\nabla_\\theta f(\\theta_{t}) + \\beta \\left[(1-\\beta) \\nabla_\\theta f(\\theta_{t-1}) + \\beta u_{t-1}\\right] \\\\\nu_{t+1}&=(1-\\beta)\\nabla_\\theta f(\\theta_{t}) + \\beta (1-\\beta) \\nabla_\\theta f(\\theta_{t-1}) + \\beta^2 (1-\\beta) \\nabla_\\theta f(\\theta_{t-2})... \\\\\n\\end{align}\\]\n\n\n\n\n\n\nLa componente de momento, está tomando en consideración todos los otros Gradientes en pasos anteriores para escoger correctamente la dirección del Gradiente actual."
  },
  {
    "objectID": "tics579/clase-3.html#sgd-with-nesterov-momentum",
    "href": "tics579/clase-3.html#sgd-with-nesterov-momentum",
    "title": "TICS-579-Deep Learning",
    "section": "SGD with Nesterov Momentum",
    "text": "SGD with Nesterov Momentum\n\\[u_{t + 1} = \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t - \\alpha u_t)\\] \\[\\theta_{t+1} = \\theta_t - \\alpha u_{t + 1}\\]\n\n\n\n\n\n\nNotar que la lógica es casi la misma, sólo que el Gradiente se evalúa en un punto futuro. Es decir, \\(\\theta_t-\\alpha u_t\\) corresponde al punto siguiente utilizando SGD con Momentum.\n\n\n\n\n\nMomentum\n\n\n\n\n\n\nNesterov"
  },
  {
    "objectID": "tics579/clase-3.html#métodos-adaptativos-adagrad",
    "href": "tics579/clase-3.html#métodos-adaptativos-adagrad",
    "title": "TICS-579-Deep Learning",
    "section": "Métodos Adaptativos: Adagrad",
    "text": "Métodos Adaptativos: Adagrad\n\n¿Qué tal, si la tasa de aprendizaje se va adaptando en el tiempo y deja de ser estática?\n\n\n\n\n\n\n\nIdea\n\n\n\nNormalizar por la historia de los gradientes al cuadrado.\n\n\n\n\n\\[r_{t+1} = r_t + \\nabla_\\theta f(\\theta_t)^2\\] \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{r_{t+1}}}\\nabla_\\theta f(\\theta_t)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#métodos-adaptativos-rmsprop",
    "href": "tics579/clase-3.html#métodos-adaptativos-rmsprop",
    "title": "TICS-579-Deep Learning",
    "section": "Métodos Adaptativos: RMSProp",
    "text": "Métodos Adaptativos: RMSProp\n\n\n\n\n\n\nIdea\n\n\n\nNormalizar por el Exponential Moving Average de los Gradientes al cuadrado.\n\n\n\n\n\\[s_{t+1} = \\beta r_t + (1-\\beta) \\nabla_\\theta f(\\theta_t)^2\\] \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{s_{t+1}}}\\nabla_\\theta f(\\theta_t)\\]"
  },
  {
    "objectID": "tics579/clase-3.html#métodos-adaptativos-adam",
    "href": "tics579/clase-3.html#métodos-adaptativos-adam",
    "title": "TICS-579-Deep Learning",
    "section": "Métodos Adaptativos: Adam",
    "text": "Métodos Adaptativos: Adam\n\n\n\n\n\n\nIdea\n\n\n\nCombinar Descenso con Momentum y RMSProp.\n\n\n\n\n\n\n\\[v_{t+1} = \\beta_1 v_t + (1-\\beta_1) \\nabla_\\theta f(\\theta_t)\\] \\[s_{t+1} = \\beta_2 s_t + (1-\\beta_2) \\nabla_\\theta f(\\theta_t)^2\\] \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{s'_{t+1}}} v'_{t+1}\\]\n\nCorrecciones Iniciales\n\\[v'_{t+1} = \\frac{v_{t+1}}{1-\\beta_1^{t+1}}\\] \\[s'_{t+1} = \\frac{v_{t+1}}{1-\\beta_2^{t+1}}\\]\n\n\n\n\n\n\n\nPytorch utiliza 0.9 y 0.999 como valores de \\(\\beta_1\\) y \\(\\beta_2\\) respectivamente."
  },
  {
    "objectID": "tics579/clase-5.html#entrenamiento-de-la-red",
    "href": "tics579/clase-5.html#entrenamiento-de-la-red",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de la Red",
    "text": "Entrenamiento de la Red\n\nA diferencia de un Modelo de Machine Learning, las Redes Neuronales se entrenan de manera progresiva (se espera una mejora en cada Epoch). Si nuestra Arquitectura es apropiada nosotros deberíamos esperar que el Loss de nuestra red siempre disminuya. ¿Por qué?\n\n\n\n\n\n\n\n\n\n¿Siempre buscamos la Red que tenga el mejor Loss de Entrenamiento?\n\n\n\n\n\n\n\n\n\n\n\n\n\nAl igual que en los modelos de Machine Learning debemos evitar a toda costa el Overfitting. ¿Qué es el overfitting?"
  },
  {
    "objectID": "tics579/clase-5.html#entrenamiento-de-la-red-1",
    "href": "tics579/clase-5.html#entrenamiento-de-la-red-1",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de la Red",
    "text": "Entrenamiento de la Red\n\nBias-Variance Tradeoff (Dilema Sesgo-Varianza)\n\n\nProbablemente el concepto más importante para determinar si un modelo tiene potencial o no. Corresponden a dos tipos de errores que pueden sufrir los modelos de ML.\n\n\n\n\n\n\n\nBias\n\nCorresponde al sesgo, y tiene que ver con la diferencia entre el valor real y el valor predicho. Bajo sesgo implica una mejor predicción.\n\n\n\n\nVariance\n\nCorresponde a la varianza y tiene que ver con la dispersión dada por los valores predichos. Baja Varianza implica un modelo más estable pero menos flexible.\n\n\n\n\n\n\n\n\n\n\nEn general hay que buscar el equilibrio entre ambos tipos de errores:\n\nAlto Sesgo y baja Varianza: Underfitting.\nBajo Sesgo y Alta Varianza: Overfitting."
  },
  {
    "objectID": "tics579/clase-5.html#model-validation",
    "href": "tics579/clase-5.html#model-validation",
    "title": "TICS-579-Deep Learning",
    "section": "Model Validation",
    "text": "Model Validation\n\nValidación Cruzada\n\n\nSe refiere al proceso de entrenar un modelo en una cierta porción de los datos, pero validar sus rendimiento y capacidad de generalización en un set de datos no vistos por el modelo al momento de entrenar.\n\n\n\n\n\n\n\n\n\n\n¿Qué es la Generalización?\n\n\n\n\n\n\n\n\n\n\nLos dos métodos más populares que se usan en Machine Learning son Holdout y K-Fold. Más métodos se pueden encontrar en los docs de Scikit-Learn.\n\n\n\n\n\n\n\n\n\nDebido a los volúmenes de datos utilizados, el esquema de validación más utilizado es el Holdout."
  },
  {
    "objectID": "tics579/clase-5.html#model-validation-holdout",
    "href": "tics579/clase-5.html#model-validation-holdout",
    "title": "TICS-579-Deep Learning",
    "section": "Model Validation: Holdout",
    "text": "Model Validation: Holdout\n\n\n\n\n\n\n\n\n\n\n\nTrain\n\n\nSe utiliza para entrenar.\n\n\n\n\n\n\nValidation\n\n\nSe utiliza para medir el nivel de generalización del modelo.\n\n\n\n\n\n\nTest\n\n\nSe utiliza para evaluar reportando una métrica de diseño del Modelo.\n\n\n\n\n\n\n\n\n\nOJO\n\n\nLoss no es lo mismo que métrica. ¿Cuál es la diferencia?\n\n\n\n\n\n\n\n\n\nA diferencia de un modelo de Machine Learning el proceso de validación del modelo se realiza en paralelo con el entrenamiento. Es decir, se entrena y valida el modelo Epoch a Epoch."
  },
  {
    "objectID": "tics579/clase-5.html#model-validation-k-fold",
    "href": "tics579/clase-5.html#model-validation-k-fold",
    "title": "TICS-579-Deep Learning",
    "section": "Model Validation: K-Fold",
    "text": "Model Validation: K-Fold\n\n\n\n\n\n\n\n\n\n\n\nCorresponde al proceso de Holdout pero repetido \\(K\\) veces."
  },
  {
    "objectID": "tics579/clase-5.html#model-evaluation",
    "href": "tics579/clase-5.html#model-evaluation",
    "title": "TICS-579-Deep Learning",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nLa Evaluación del Modelo se hará en torno a una métrica definida a priori por el modelador. ¿Entonces es un Hiperparámetro?\n\n\n\n\n\n\n\nLa métrica a utilizar está íntimamente ligada al tipo de modelo.\n\n\n\n\n\nClasificación\n\n\\(Accuracy = \\frac{1}{m} \\sum_{i = 1}^m 1\\{y_i = \\hat{y_i}\\}\\)\n\\(Precision = \\frac{TP}{TP + FP}\\)\n\\(Recall = \\frac{TP}{TP + FN}\\)\n\\(F1-Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\n\n\nRegresión\n\n\\(RMSE = \\frac{1}{m} \\sum_{i=1}^m (y_i-\\hat{y_i})^2\\)\n\\(MAE = \\frac{1}{m} \\sum_{i=1}^m |y_i - \\hat{y_i}|\\)\n\\(MAPE = 100 \\cdot \\frac{1}{m} \\sum_{i=1}^m \\frac{|y_i-\\hat{y_i}|}{max(\\epsilon,y_i)}\\)\n\\(SMAPE = \\frac{2}{m} \\sum_{i=1}^2 \\frac{|y_i - \\hat{y_i}  |}{max(|y_i + \\hat{y_i}|,\\epsilon)}\\)\n\n\n\n\n\n\n\n\n\nLas métricas acá explicadas son métricas básicas de cualquier modelo general de Clasificación y Regresión. Existen muchas otras métricas que son específicas para campos específicos. IoU por ejemplo es una Métrica de Segmentación Semántica, Map@k es una métrica para modelos de Recomendación, Bleu o Rouge son métricas para NLP, etc. Para ver millones de métricas pueden ver las docs de Torchmetrics.\n\n\n\n\n\n\n\n\n\n\nEs posible utilizar métricas para ir monitoreando el progreso del modelo Epoch a Epoch."
  },
  {
    "objectID": "tics579/clase-5.html#training-validation-loop",
    "href": "tics579/clase-5.html#training-validation-loop",
    "title": "TICS-579-Deep Learning",
    "section": "Training-Validation Loop",
    "text": "Training-Validation Loop\n\nCorresponde a la modificación del Training Loop con el Objetivo de Entrenar y Validar de manera simultánea.\n\n\n\n\n\n\n\n\n\n\n\n\nSe realiza un Forward Pass con datos de Train y se calcula el Loss asociado. Internamente, Pytorch comienza a acumular Gradientes."
  },
  {
    "objectID": "tics579/clase-5.html#training-validation-loop-1",
    "href": "tics579/clase-5.html#training-validation-loop-1",
    "title": "TICS-579-Deep Learning",
    "section": "Training-Validation Loop",
    "text": "Training-Validation Loop\n\n\n\n\n\n\n\n\n\n\n\nSe realiza un Backward Pass, se aplican los gradientes y se aplica el Update Rule."
  },
  {
    "objectID": "tics579/clase-5.html#training-validation-loop-2",
    "href": "tics579/clase-5.html#training-validation-loop-2",
    "title": "TICS-579-Deep Learning",
    "section": "Training-Validation Loop",
    "text": "Training-Validation Loop\n\n\n\n\n\n\n\n\n\n\n\nSe realiza un nuevo Forward Pass, pero esta vez con los datos de Validación. En este caso Pytorch internamente sigue acumulando gradientes, lo cual no es correcto. Para ello se debe utilizar un with torch.no_grad(). Se calcula un Validation Loss."
  },
  {
    "objectID": "tics579/clase-5.html#monitoreo-de-un-modelo-validation-curve",
    "href": "tics579/clase-5.html#monitoreo-de-un-modelo-validation-curve",
    "title": "TICS-579-Deep Learning",
    "section": "Monitoreo de un Modelo: Validation Curve",
    "text": "Monitoreo de un Modelo: Validation Curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs importante ser capaz de identificar el momento exacto en el cual el momento comienza su overfitting. Para ello se utiliza el “Checkpointing”.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\nCorresponde a un snapshot del modelo a un cierto punto. En la práctica se almacenan los parámetros del mejor modelo y del último Epoch.\n\n\n\n\n\n\n\n\n\n\nEarlyStopping\n\n\n\nTeoricamente, una vez que la red Neuronal alcanza el punto de Overfitting ya no tiene sentido seguir el entrenamiento. Por lo tanto es posible detener el entrenamiento bajo una cierta condición."
  },
  {
    "objectID": "tics411.html",
    "href": "tics411.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Clase 0\n\n\nPresentación del Curso\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 1\n\n\nCalidad de los Datos y Feature Engineering\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 2\n\n\nExploratory Data Analysis (EDA)\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase Bonus\n\n\nIntroducción a Scikit-Learn\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 3\n\n\nModelación Descriptiva y K-Means\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 4\n\n\nClustering Jerárquico\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 5\n\n\nDBSCAN\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 6\n\n\nEvaluación de Clusters\n\n\n\nApr 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 7\n\n\nAlgoritmo Apriori\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 8\n\n\nIntroducción al Aprendizaje Supervisado\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 9\n\n\nEvaluación de Modelos\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 10\n\n\nÁrboles de Decisión\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 11\n\n\nNaive Bayes\n\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 12\n\n\nRegresión Logística\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClase 13\n\n\nDetección de Anomalías\n\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Diapositivas del Curso"
    ]
  },
  {
    "objectID": "tics411-labs.html",
    "href": "tics411-labs.html",
    "title": "Prácticos",
    "section": "",
    "text": "Práctico\nColab\n\n\n\n\nPreprocesamiento\n\n\n\nEDA\n\n\n\nK-Means\n\n\n\nAnálisis de Centros\n\n\n\nAglomerativo\n\n\n\nDBSCAN\n\n\n\nEvaluación de Clusters\n\n\n\nEjemplos Hopkins\n\n\n\nProyecto Clustering\n\n\n\nApriori\n\n\n\nResolución Guía\n\n\n\nKNN\n\n\n\nCross Validation\n\n\n\nDecision Tree\n\n\n\nNaive Bayes\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "tics579/clase-2.html#clase-anterior",
    "href": "tics579/clase-2.html#clase-anterior",
    "title": "TICS-579-Deep Learning",
    "section": "Clase anterior",
    "text": "Clase anterior\n\nLa Regresión Softmax es capaz de generar separaciones lineales para más de dos clases para cualquier punto \\(x \\in \\mathbb{R}^{1 \\times n}\\):\n\n\n\n\n\n\\(h_\\theta(x) = \\theta^T x\\), tal que \\(\\theta \\in \\mathbb{R}^{n \\times k}\\).\n\n\n\n\n\n\n\n\n\nEsta hipótesis es bastante limitada, y existen muchos problemas que no podrán solucionarse con este tipo de solución."
  },
  {
    "objectID": "tics579/clase-2.html#limitaciones-de-una-hipótesis-lineal",
    "href": "tics579/clase-2.html#limitaciones-de-una-hipótesis-lineal",
    "title": "TICS-579-Deep Learning",
    "section": "Limitaciones de una Hipótesis Lineal",
    "text": "Limitaciones de una Hipótesis Lineal\n\nEs claro que un problema como el que se muestra acá no podrá ser resuelto mediante un clasificador lineal (hipótesis lineal).\n\n\n\n\n\n\n\n\n\n\n¿Cómo se resuelve este tipo de problemas?\n\n\n\nCreando nuevas features que permitan predecir problemas no-lineales.\n\n\n\n\n\n\n\\[h_\\theta(x) = \\theta^T \\phi(x)\\]\n\ntal que \\(\\theta \\in \\mathbb{R}^{n \\times k}\\) y \\(\\phi(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^d\\) con \\(d &gt; n\\).\n\n\n\n\n\n\n\n\nBásicamente \\(\\phi(.)\\) es la manera matemática de denotar la creación de más features que permiten resolver el problema.\n\n\n\n\n\n\n\n\n\n\n\n\nSVM es un algoritmo que hace esto de manera automática utilizando el famoso Kernel Trick, donde \\(\\phi(.)\\) es conocido como el Kernel."
  },
  {
    "objectID": "tics579/clase-2.html#diferencias-entre-ml-y-dl",
    "href": "tics579/clase-2.html#diferencias-entre-ml-y-dl",
    "title": "TICS-579-Deep Learning",
    "section": "Diferencias entre ML y DL",
    "text": "Diferencias entre ML y DL\n\n\n\n\n\n\nLa diferencia principal entre el Machine Learning y el Deep Learning es la manera en la que se crean las features.\n\n\n\n\n\n\n\n\n\nNormalmente el Machine Learning está enfocado en que manualmente se generen features.\nDeep Learning busca que el Algoritmo busque esas features. El énfasis está en buscar la Arquitectura adecuada."
  },
  {
    "objectID": "tics579/clase-2.html#cómo-creamos-features-de-manera-automática",
    "href": "tics579/clase-2.html#cómo-creamos-features-de-manera-automática",
    "title": "TICS-579-Deep Learning",
    "section": "¿Cómo creamos features de manera automática?",
    "text": "¿Cómo creamos features de manera automática?\n\n\nUna primera idea sería crearlas de manera lineal:\n\\[\\phi(x) = W^T x\\]\ndonde \\(W \\in \\mathbb{R}^{n \\times d}\\).\n\n\n\n\n\n\n\n\n\nEn este caso nuestra hipótesis queda como: \\[ h_\\theta(x) = \\theta^T \\phi(x) = \\theta^T W^T x = \\tilde{\\theta}^T x\\]\n\n\n\n\n\n\n\nLamentablemente este approach no funciona, ya que \\(\\tilde{\\theta}^T\\) es sólo otra matriz que genera dos transformaciones simultáneas, pero que en este caso llevará de \\(n\\) a \\(k\\) de manera directa.\n\n\n\n\n\n\n\n\n\n\n\nOjo con las dimensiones.\n\n\n\n\\(W^t\\) tiene dimensión \\(d \\times n\\).\nSabemos que \\(h_\\theta(x)\\) tiene que devolver \\(k\\) outputs. Por lo tanto, \\(\\theta^T\\) tiene que tener dimensiones \\(k \\times d\\).\n\\(x\\) es un vector con \\(n\\) features por lo tanto es de dimensión \\(n \\times 1\\).\nEso hará que \\(h_\\theta(x)\\) sea de tamaño \\(k \\times 1\\)."
  },
  {
    "objectID": "tics579/clase-2.html#entonces-cómo",
    "href": "tics579/clase-2.html#entonces-cómo",
    "title": "TICS-579-Deep Learning",
    "section": "¿Entonces cómo?",
    "text": "¿Entonces cómo?\n\n\nVamos a utilizar funciones no lineales. Cualquiera sirve tal que:\n\\[\\phi(x) = \\sigma(W^Tx)\\]\ndonde \\(W \\in \\mathbb{R}^{n \\times d}\\) y \\(\\sigma: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\), es decir, \\(\\sigma\\) es una función escalar.\n\n\n\n\n\n\n\n\nDe este modo nuestra hipótesis quedaría como:\n\\[h_\\theta(x) = \\theta^T \\sigma(W^T x) \\neq \\tilde{\\theta}^T x\\]\n\n\n\n\n\n\n\n\nEstamos aplicando una transformación no-lineal a la transformación lineal de \\(x\\) con \\(W\\).\n\n\n\n\n\n\n\n\n\n\n\nNormalmente escogeremos funciones no-lineales que sean diferenciables para poder actualizar \\(\\theta\\) y \\(W\\).\nEsto es lo que llamaremos el entrenamiento de una red neuronal."
  },
  {
    "objectID": "tics579/clase-2.html#activation-functions",
    "href": "tics579/clase-2.html#activation-functions",
    "title": "TICS-579-Deep Learning",
    "section": "Activation Functions",
    "text": "Activation Functions\n\n\n\nDefiniremos las funciones de activación como funciones no-lineales que se aplican a la salida de cada capa para evitar la composición de dos trasnformaciones lineales consecutivas.\n\n\n\n\n\n\n\nEsta es la única manera de transformar hipótesis lineales en hipótesis no lineales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunciones Clásicas\n\n\n\nSigmoide\nReLU\nTanh\nSoftmax\n\n\n\n\n\n\n\n\n\n\n\nFunciones más modernas\n\n\n\nSwish\nGELU\nELU"
  },
  {
    "objectID": "tics579/clase-2.html#layer-non-linear-softmax-regression",
    "href": "tics579/clase-2.html#layer-non-linear-softmax-regression",
    "title": "TICS-579-Deep Learning",
    "section": "2-Layer non-linear Softmax Regression",
    "text": "2-Layer non-linear Softmax Regression\n\n\n\\[h_\\theta(x) = W_2^T \\phi(x) = W_2^T \\sigma(W_1^T x)\\]\ndonde \\(\\theta=\\{W_1 \\in \\mathbb{R}^{n \\times d}, W_2 \\in \\mathbb{R}^{d \\times k}\\}\\)\n\n\n\n\n\n\n\nPodemos pensar que \\(W_1 \\in \\mathbb{R}^{n \\times d}\\) es aquella matriz que lleva a un vector \\(x\\) de \\(n\\) a \\(d\\) dimensiones.\nDe la misma forma, \\(W_2 \\in \\mathbb{R}^{d \\times k}\\) es aquella matriz que lleva a un vector \\(x\\) de \\(d\\) a \\(k\\) dimensiones/salidas.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Batch Form\n\\[h_\\theta(X) = \\sigma(XW_1)W_2\\]\n\nUpdate Rule\n\\[W_1 := W_1 - \\frac{\\alpha}{m} \\nabla_{W_1} l_{ce}(h_\\theta(X),y)\\] \\[W_2 := W_2 - \\frac{\\alpha}{m} \\nabla_{W_2} l_{ce}(h_\\theta(X),y)\\]"
  },
  {
    "objectID": "tics579/clase-2.html#cálculo-de-gradientes",
    "href": "tics579/clase-2.html#cálculo-de-gradientes",
    "title": "TICS-579-Deep Learning",
    "section": "Cálculo de Gradientes",
    "text": "Cálculo de Gradientes\n\n\n\n\n\n\n\n\nGradiente de \\(W_1\\)\n\n\n\\[\\begin{align} \\nabla_{W_1} &= \\frac{\\partial l_{ce}(h_\\theta(X),y)}{\\partial \\sigma(XW_1)} \\cdot \\frac{\\partial h_\\theta(X)}{\\partial \\sigma(XW_1)} \\cdot \\frac{\\partial \\sigma(XW_1)}{\\partial XW_1} \\cdot \\frac{\\partial XW_1}{\\partial W_1} \\\\\n&= (Z-I_y)_{m \\times k} \\cdot (W_{2})_{d \\times k}  \\cdot \\sigma'(XW_1)_{m \\times d} \\cdot X_{m \\times n}\n\\end{align}\\]\nLuego, corrigiendo por dimensiones obtenemos que \\[\\nabla_{W_1} \\in \\mathbb{R}^{n \\times d} = X^T_{n \\times m} \\left[\\sigma'(XW_1) \\odot (Z-I_y)W_2^T \\right]_{m \\times d}\\]\n\n\n\n\n\n\n\n\n\n\nGradiente de \\(W_2\\)\n\n\n\\[\\begin{align} \\nabla_{W_2} &= \\frac{\\partial l_{ce}(h_\\theta(X),y)}{\\partial h_\\theta(X)} \\cdot \\frac{\\partial h_\\theta(X)}{\\partial W_2}\\\\\n&= (Z-I_y)_{m\\times k} \\cdot \\sigma(XW_1)_{m \\times d}\n\\end{align}\\]\nLuego, corrigiendo por dimensiones obtenemos que \\[\\nabla_{W_2} \\in \\mathbb{R}^{d \\times k} = \\sigma(XW_1)^T_{d \\times m}(Z - I_y)_{m \\times k}\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(\\odot\\) representa el producto Hadamard entre dos matrices. Esto es, multiplicación elemento a elemento.\n\\(\\sigma'(.)\\) representa la derivada de la función de activación \\(\\sigma(.)\\)"
  },
  {
    "objectID": "tics579/clase-2.html#definiciones",
    "href": "tics579/clase-2.html#definiciones",
    "title": "TICS-579-Deep Learning",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs\n\n\n\\[Z_1 = X\\]\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Outputs\n\n\n\\[Z_{i+1} = \\sigma_i(Z_iW_i), i=1,...,L\\] \\[Z_i \\in \\mathbb{R}^{m \\times n_i}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nOutput (Head)\n\n\n\\[h_\\theta(X) = Z_{L+1}\\]\n\n\n\n\n\n\n\n\n\nParámetros\n\n\n\\[\\theta = \\left[W_1,..., W_L\\right]\\] \\[ W_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nLas salidas intermedias (intermediate outputs) son las mal llamadas hidden layers. Esta red cuenta con \\(L\\) hidden layers \\(W\\)."
  },
  {
    "objectID": "tics579/clase-2.html#definiciones-1",
    "href": "tics579/clase-2.html#definiciones-1",
    "title": "TICS-579-Deep Learning",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\n\n\n\nRed Neuronal\n\n\nVamos a definir como Red Neuronal un tipo particular de hipótesis que consiste en:\n\nMultiples capas que permiten cambiar de dimensión.\nFunciones de activación no-lineales y diferenciables que permiten desacoplar transformaciones lineales.\nUn set de parámetros optimizables, que permiten reducir una Loss Function.\n\n\n\n\n\n\n\n\n\n\nSi bien estas redes toman inspiración de la biólogía, poco o nada tienen que ver con neuronas reales.\n\n\n\n\n\n\n\n\n\nTérminos como Neural Network, Deep Networks, Deep Learning, son ampliamente usados y algunas veces usados para diferenciar el tamaño de distintas arquitecturas.\nNosotros los vamos a usar prácticamente como sinónimos.\n\n\n\nUpdate Rule\n\\[W_i := W_i - \\frac{\\alpha}{m} \\nabla_{W_i} l(h_\\theta(X),y)\\]"
  },
  {
    "objectID": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal",
    "href": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal",
    "title": "TICS-579-Deep Learning",
    "section": "Cálculo de Gradientes de una Red Neuronal",
    "text": "Cálculo de Gradientes de una Red Neuronal\n\\[\\nabla_{W_i} l(Z_{L+1},y) = \\underbrace{\\frac{\\partial l(Z_{L+1},i)}{\\partial Z_{L+1}} \\cdot \\frac{\\partial Z_{{L+1}}}{\\partial Z_L} \\cdot \\frac{\\partial Z_L}{\\partial Z_{L-1}}...\\cdot \\frac{\\partial Z_{i+2}}{\\partial Z_{i+1}}}_{G_{i+1} = \\frac{\\partial l(Z_{L+1},y)}{\\partial Z_{i+1}}}\\cdot \\frac{\\partial Z_{i+1}}{\\partial W_i}\\]\n\n\n\n\n\n\nGradiente Entrante (Incoming Backward Gradient)\n\n\n\nVamos a definir el Gradiente Entrante hasta antes de la capa \\(i\\) (desde la salida en dirección a la entrada) como: \\[\\begin{align}G_i &= G_{i+1} \\cdot \\frac{\\partial Z_{i + 1}}{\\partial Z_i} \\\\\n&= G_{i+1} \\cdot \\frac{\\partial \\sigma_i(Z_i W_i)}{\\partial Z_i W_i} \\cdot \\frac{\\partial Z_i W_i}{\\partial Z_i}_{} \\\\\n&= (G_{i+1})_{m \\times n_{i+1}} \\cdot \\sigma'(Z_i W_i)_{m \\times n_{i + 1}} \\cdot (W_i)_{n_i \\times n_{i+1}}\n\\end{align}\\]\n\n\n\n\nLuego, \\[ G_i \\in \\mathbb{R}^{m \\times n_i} = \\left[ G_{i+1} \\odot \\sigma_i'(Z_i W_i)\\right] W_i^T\\]"
  },
  {
    "objectID": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal-1",
    "href": "tics579/clase-2.html#cálculo-de-gradientes-de-una-red-neuronal-1",
    "title": "TICS-579-Deep Learning",
    "section": "Cálculo de Gradientes de una Red Neuronal",
    "text": "Cálculo de Gradientes de una Red Neuronal\n\\[\\begin{align}\\nabla_{W_i} l(Z_{L+1},y) &= G_{i+1} \\cdot \\frac{\\partial Z_{i+1}}{\\partial W_i} \\\\\n&= G_{i+1} \\cdot \\frac{\\partial \\sigma_i'(Z_i W_i)}{\\partial Z_i W_i} \\cdot \\frac{\\partial Z_i W_i}{\\partial W_i} \\\\\n&= (G_{i+1})_{m \\times n_{i+1}} \\cdot \\sigma'(Z_i W_i)_{m \\times n_{i+1}} \\cdot (Z_i)_{m \\times n_i}\n\\end{align}\\]\n\n\n\n\n\n\n\n\nLuego el Gradiente de cualquier Loss Function con respecto a un set de parámetros \\(W_i\\) se escribe como:\n\\[\\nabla_{W_i}l(Z_{L+1}, y) = Z_i^T \\left[G_{i+1} \\odot \\sigma'(Z_i W_i)\\right]\\]"
  },
  {
    "objectID": "tics579/clase-2.html#forward-y-backward-passes",
    "href": "tics579/clase-2.html#forward-y-backward-passes",
    "title": "TICS-579-Deep Learning",
    "section": "Forward y Backward Passes",
    "text": "Forward y Backward Passes\n\nBackpropagation\n\nCorresponde al Algoritmo con el cuál calcularemos los Gradientes de una Red Neuronal. Es un nombre muy fancy para calcular la Regla de la Cadena de manera eficiente aplicando caching de los resultados intermedios.\n\n\n\nForward Pass\n\nInicializar \\(Z_1 = X\\).\nIterar calculando: \\(Z_i = \\sigma_i(Z_i W_i), i=1,...,L\\).\n\nBackward Pass\n\nInicializar \\(G_{L+1} = \\nabla_{Z_{L+1}}l(Z_{L+1},y) = S-I_y\\) (Este ejemplo es sólo el caso de Cross Entropy como Loss Function).\nIterar calculando: \\(G_i = \\left[G_{i+1} \\odot \\sigma_i'(Z_i W_i)\\right]W_i^T, i=L,...,1\\)\n\nUpdate Rule\n\nCalcular Gradientes para poder aplicar el Update Rule.\n\n\\[W_i := W_i - \\frac{\\alpha}{m}\\nabla_{W_i}l(Z_{L+1},y) = W_i - \\frac{\\alpha}{m} Z_i^T\\left[G_{i+1} \\odot \\sigma'(Z_i W_i)\\right]\\]"
  },
  {
    "objectID": "tics579/clase-2.html#conceptos-clásicos-del-entrenamiento-de-una-nn",
    "href": "tics579/clase-2.html#conceptos-clásicos-del-entrenamiento-de-una-nn",
    "title": "TICS-579-Deep Learning",
    "section": "Conceptos Clásicos del Entrenamiento de una NN",
    "text": "Conceptos Clásicos del Entrenamiento de una NN\n\n\n\n\n\n\n\n\n\n\n\n\nDefiniremos una Epoch como el número de veces que repetiremos el Algoritmo de Backpropagation con todos los datos de Entrenamiento. El número de epochs de entrenamiento será un hiperparámetro de un modelo.\nDefiniremos el learning rate como un hiperparámetro que controlará el aprendizaje del modelo.\nDefiniremos este tipo de redes neuronales como Feed Forward Networks o FFN aunque en la práctica tienen una pequeña modificación que veremos en la siguiente clase.\n\n\n\n\n\n\n\n\n\n\nEste tipo de redes es muy utilizada y recibe diversos nombres:\n\nFully Connected Layers\nDense Layers: Proviene de la nomenclatura utilizada por Tensorflow.\nLinear Layers: Proviene de la nomenclatura utilizada por Pytorch, pero no es del todo correcto.\nMLP o Multilayer Perceptron."
  },
  {
    "objectID": "tics579/clase-0.html#quién-soy",
    "href": "tics579/clase-0.html#quién-soy",
    "title": "TICS-579-Deep Learning",
    "section": "¿Quién soy?",
    "text": "¿Quién soy?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia\nEstudié Ingeniería Civil pero llevo 10 años trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nSoy Msc. en Data Science y estoy cursando el PhD. en la UAI especificamente en Deep Learning.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqué mi primer paper el año pasado sobre Hate Speech en Español.\nJuego Tenis de Mesa, hago Agility con mi perrita Kira y escribo en mi Blog."
  },
  {
    "objectID": "tics579/clase-0.html#disclaimer",
    "href": "tics579/clase-0.html#disclaimer",
    "title": "TICS-579-Deep Learning",
    "section": "Disclaimer",
    "text": "Disclaimer\n\n\n\n\n\n\n\nMucho del contenido de este curso será una mezcla entre inglés y español. Esto debido a que el contenido del curso está en constante desarrollo y casi no existen libros o artículos en español al respecto.\n\n\n\n\n\n\n\n\n\n\n\nEste curso se considera altamente teórico y con una fuerte componente en programación.\n\n\n\n\n\n\n\n\n\n\n\n¡Están advertidos!\n\n\n\n\n\n\n\n\n\n\n\nEstá completamente prohibido copiar y pegar código de algún modelo de IA. Hablaremos más adelante cómo trataremos de mitigar esto ya que gran parte del curso requiere programación fuerte."
  },
  {
    "objectID": "tics579/clase-0.html#objetivos-del-curso",
    "href": "tics579/clase-0.html#objetivos-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\n\n\n\nSer el curso más completo y exhaustivo de Deep Learning del país.\n\n\n\n\n\n\nIdentificar elementos claves de las Redes Neuronales.\nEntender conceptos básicos como el Training Loop, Gradient Propagation, Optimización, etc.\nIdentificar los distintos tipos de Redes Neuronales:\n\nFeed Fordward Networks,\nConvolutional Neural Networks,\nRecurrent Neural Networks,\nTransformers.\n\nEntender las Arquitecturas Estado del Arte en diferentes dominios: Datos Tabulares, Computer Vision, Natural Language Processing.\nImplementar, entrenar y evaluar Deep Neural Networks utilizando Pytorch."
  },
  {
    "objectID": "tics579/clase-0.html#tópicos-del-curso",
    "href": "tics579/clase-0.html#tópicos-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "Tópicos del Curso",
    "text": "Tópicos del Curso\n\n\n\nShallow Models\nEl Perceptron\nTensores\nEntrenamiento y Evaluación de Modelos\nArquitecturas de Redes Neuronales\nAplicación de estas redes a distintos dominios.\n\n\n\n\n\n\n\nImplementación en librerías SOTA como Pytorch y HuggingFace."
  },
  {
    "objectID": "tics579/clase-0.html#logística",
    "href": "tics579/clase-0.html#logística",
    "title": "TICS-579-Deep Learning",
    "section": "Logística",
    "text": "Logística\n\nClases todos los Jueves de 8:45 a 11:25 de manera presencial.\n\nSala: Por Confirmar.\n\nInstructor: Alfonso Tobar-Arancibia (alfonso.tobar.a@edu.uai.cl)\n\nOffice Hours: Miercoles por la mañana en la A-220.\n\nAyudantías Online, sólo en algunas semanas.*\n\nAyudante: María Alejandra Bravo (mariaabravo@alumnos.uai.cl)\n\n\n\n\n\n\n\n\n\nHorarios Posibles:\n\n\n\nMartes de 17:00 a 18:10 hrs.\nMiércoles de 8:45 a 09:55 hrs.\nMiércoles de 10:15 a 11:25 hrs.\nMiércoles de 17:00 a 18:10 hrs."
  },
  {
    "objectID": "tics579/clase-0.html#prerequisitos",
    "href": "tics579/clase-0.html#prerequisitos",
    "title": "TICS-579-Deep Learning",
    "section": "Prerequisitos",
    "text": "Prerequisitos\n\n\n\n\n\n\n\nHaber cursado Minería de Datos (no es excluyente pero es necesario saber de Machine Learning).\n\nEntrenamiento de un Modelo\nEvaluación y Validación\nRegresión Logística\n\n\n\n\n\n\n\n\n\n\n\n\nTener conocimientos de Algebra Lineal (poner mucha atención al curso de Algebra Lineal del MSDS).\n\nNotación Matricial\nMultiplicaciones Matriciales\nTransformaciones Lineales\n\n\n\n\n\n\n\n\n\n\n\n\nEntender Inglés\n\nMucho del material adicional serán lecturas o videos en inglés."
  },
  {
    "objectID": "tics579/clase-0.html#recursos",
    "href": "tics579/clase-0.html#recursos",
    "title": "TICS-579-Deep Learning",
    "section": "Recursos",
    "text": "Recursos\n\nSlides\nNotebooks\nForo de Dudas\n\n\n\nDiapositivas Interactivas creadas en Quarto (links van a estar disponibles en Webcursos)\n\nContiene un índice de todas las slides.\nPermite copiar y pegar código directamente.\nImágenes se pueden ver en tamaño completo al clickearlas.\nSe puede buscar contenido específico de cualquier Slide utilizando la Search Bar.\nSe puede obtener una copia en PDF presionando la tecla E para luego guardarlas para tomar notas."
  },
  {
    "objectID": "tics579/clase-0.html#herramientas",
    "href": "tics579/clase-0.html#herramientas",
    "title": "TICS-579-Deep Learning",
    "section": "Herramientas",
    "text": "Herramientas\n\n\n\n\n\n\nSe espera que los estudiantes dominen las siguientes herramientas:\n\nPython\nPandas/Numpy\nGoogle Colab\n\n\n\n\n\n\n\n\n\n\n\nA lo largo del curso utilizaremos otras librerías que se enseñaran a lo largo del curso:\n\nPytorch\nTransformers\nAlbumentations\netc."
  },
  {
    "objectID": "tics579/clase-0.html#material-complementario",
    "href": "tics579/clase-0.html#material-complementario",
    "title": "TICS-579-Deep Learning",
    "section": "Material Complementario",
    "text": "Material Complementario\n\nNo hay un texto guía para este curso. La mayoría de las cosas aparecen día a día o las podemos encontrar en Papers, los cuales irán siendo mencionados a medida que sea necesario.\n\nLectura Recomendada\n\n\n\n\n\n\n\n\n\nDocs Pytorch nn\nDocs Pytorch functional\nTutorial Colab\nAgregar Datos Externos a Colab"
  },
  {
    "objectID": "tics579/clase-0.html#reglas-del-curso",
    "href": "tics579/clase-0.html#reglas-del-curso",
    "title": "TICS-579-Deep Learning",
    "section": "Reglas del Curso",
    "text": "Reglas del Curso\n\n\n\n\n\n\nNota Final\n\n\n\\[NF = NT + 0.3 \\cdot NQ\\]\n\n\n\n\n\n\n\n\n\nTareas\n\n\n\nSe realizarán 5 Tareas. T5 es opcional y reemplaza la peor nota.\n\n\\[NT = 0.1 \\cdot T1 + 0.15 \\cdot T2 + 0.20 \\cdot T3 + 0.25 \\cdot T4\\]\n\n\n\n\n\n\n\n\n\nQuizes\n\n\n\nSe realizarán controles cortos al inicio de clases (previo aviso).\nSe realizarán suficientes controles para eliminar algunos al final del semestre.\nNo hay controles recuperativos.\n\n\\[NQ = \\frac{1}{n}\\sum_{i=1}^n Q_i\\]"
  },
  {
    "objectID": "tics579/clase-0.html#tareas-1",
    "href": "tics579/clase-0.html#tareas-1",
    "title": "TICS-579-Deep Learning",
    "section": "Tareas",
    "text": "Tareas\n\n\n\n\n\n\n\nReglas\n\n\n\nSe deben entregar en Jupyter Notebook.\nSe realizarán en parejas o un grupo de 3 (sólo en caso de número impar) previa inscripción.\nLas partes teóricas que necesiten notación matemática se deben realizar dentro del Jupyter Notebook pero usando simbología Latex (Dudas de cómo hacerlo a la ayudante).\n\n\n\n\n\n\n\n\n\n\nDefensa de Código\n\n\n\nEl código presentado en las tareas se defenderá mediante interrogación oral en horarios a convenir (normalmente de ayudantía).\n3 preguntas aleatorias a cualquier miembro del grupo.\nEn caso de no defender el código correctamente se penalizará de la siguiente manera:\n\n3 preguntas buenas: 100% del puntaje.\n2 preguntas buenas: 70% del puntaje.\n1 preguntas buenas: 40% del puntaje.\n0 preguntas buenas: 20% del puntaje."
  },
  {
    "objectID": "tics579/clase-0.html#fechas-tareas",
    "href": "tics579/clase-0.html#fechas-tareas",
    "title": "TICS-579-Deep Learning",
    "section": "Fechas Tareas",
    "text": "Fechas Tareas\n\n\n\n\n\n\n\n\nTarea 1\n\n\n15 de Septiembre (23:59 hrs)\n\n\n\n\n\n\n\n\n\nTarea 2\n\n\n13 de Octubre (23:59 hrs)\n\n\n\n\n\n\n\n\n\n\nTarea 3\n\n\n3 de Noviembre (23:59 hrs)\n\n\n\n\n\n\n\n\n\nTarea 4\n\n\n24 de Noviembre (23:59 hrs)\n\n\n\n\n\n\n\n\n\n\nTarea 5 (Opcional, pero recomendada)\n\n\n15 de Diciembre (23:59 hrs)"
  },
  {
    "objectID": "tics579/clase-0.html#garantías",
    "href": "tics579/clase-0.html#garantías",
    "title": "TICS-579-Deep Learning",
    "section": "Garantías",
    "text": "Garantías\n\n\n\n\n\n\nVamos a sufrir harto al menos las primeras semanas (yo al menos he sufrido harto preparando las clases), pero les aseguro que va a valer la pena.\nBare with me!!\n\n\n\n\n\n\n\n\n\nVamos a aprender conceptos muy avanzados que no muchos cursos consideran. Lo siento, pero van a ser mis conejillos de indias.\n\n\n\n\n\n\n\n\n\nVamos a pasarla bien mal, estudiando harto, demorándonos harto en las tareas, pero vamos a pasar todos.\n\n\n\n\n\n\n\n\n\nEso NO SIGNIFICA que las notas van a ser regaladas."
  },
  {
    "objectID": "tics579/clase-6.html#entrenamiento-de-un-modelo",
    "href": "tics579/clase-6.html#entrenamiento-de-un-modelo",
    "title": "TICS-579-Deep Learning",
    "section": "Entrenamiento de un Modelo",
    "text": "Entrenamiento de un Modelo\n\nEl entrenamiento de un modelo tiene demasiadas variables que pueden influir en el éxito del modelo. Algunos aspectos relevantes a los que hay que poner énfasis al momento de entrenar:\n\n\nOverfitting\nConvergencia/Tiempo de Convergencia\nGeneralización\nOptimización de Recursos Computacionales/Hardware.\nPrevenir problemas de Vanishing Gradient y Exploding Gradients.\n\n\n\n\n\n\n\nMuchas de las técnicas que veremos acá permiten abordar mejoras en nuestros modelos para uno o más aspectos de los mencionados anteriormente."
  },
  {
    "objectID": "tics579/clase-6.html#normalización",
    "href": "tics579/clase-6.html#normalización",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización",
    "text": "Normalización\n\nEn general el término Normalización está muy trillado y en la práctica se utiliza para referirse a muchos temas distintos. Algunas definiciones conocidas:\n\n\n\nNormalización\n\\[x_{i\\_norm} = \\frac{x_i-x_{min}}{x_{max} - x_{min}}\\] Esta operación se puede hacer mediante MinMaxScaler de Scikit-Learn.\n\nEstandarización\n\\[ x_{i\\_est} = \\frac{x_i - E[x]}{\\sqrt(Var[x])}\\]\nEsta operación se puede hacer mediante StandardScaler de Scikit-Learn."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm",
    "href": "tics579/clase-6.html#normalización-batch-norm",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)\nPaper 2015: Batch Normalization\n\n\n\n\n\n\n\n¿Por qué?\n\n\n\nAcelera el entrenamiento\nDisminuye la importancia de los Parámetros iniciales.\nRegulariza el modelo (un poquito)\nResuelve el problema de Internal Covariate Shift.\n\n\n\n\n\nEjemplo: Supongamos que dado la altura y la edad queremos predecir si será deportista de alto rendimiento."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-1",
    "href": "tics579/clase-6.html#normalización-batch-norm-1",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)\n\n\n\n\n\n\n\n\n\nCambios en Altura son mucho más pequeños que en Edad debido al rango.\nToma más tiempo optimizar (requiere parámetros más pequeños)\nSi el learning rate es alto puede diverger.\nSi el learning rate es bajo implica que demora mucho más en converger.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPros\n\n\n\nSin importar el punto inicial, el mínimo se encuentra casi a la misma distancia.\n\nEs posible utilizar un learning rate más grande sin miedo a diverger.\n\n\n\n\n\n\n\n\n\n\nCons\n\n\n\nMás cálculos y parámetros involucrados"
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-2",
    "href": "tics579/clase-6.html#normalización-batch-norm-2",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)"
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-3",
    "href": "tics579/clase-6.html#normalización-batch-norm-3",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm)",
    "text": "Normalización (Batch Norm)\n\n\n\n\n\n\n\n\nCálculo de Estadísticos\n\n\n\\[ \\mu_B = \\frac{1}{B} \\sum_{i=1}^B z^{(i)} = \\frac{1}{3}(4 + 7 + 5) = 5.33\\] \\[ \\sigma_B^2 = \\frac{1}{B} \\sum_{i=1}^B (z^{(i)} - \\mu_B)^2 = 1.555\\]\n\n\n\n\n\n\n\n\n\nNormalización\n\n\n\\[\\widehat{z^{(i)}} = \\frac{z^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\n\n\n\n\n\n\n\n\n\n\nScale and Shift: \\(\\gamma\\) y \\(\\beta\\) son parámetros.\n\n\n\\[BN_{\\gamma,\\beta}(z_i)= \\gamma \\widehat{z_i} + \\beta \\]\n\n\n\n\n\n\n\n\n\n\nZ2 norm\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nDonde \\(\\gamma\\) y \\(\\beta\\) son parámetros aprendidos durante el entrenamiento."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-test-time",
    "href": "tics579/clase-6.html#normalización-batch-norm-test-time",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm): Test Time",
    "text": "Normalización (Batch Norm): Test Time\n\n\n\n\n\n\nProblema\n\n\nLa predicción de una instancia \\(i\\) específica, ahora depende de otros elementos dentro del Batch. ¿Cómo funciona entonces el modelo en Test Time?\n\n\n\n\n\n\n\n\n\nSe estiman valores de \\(\\mu_B\\) y \\(\\sigma_B\\) para usar en inferencia basados en los valores obtenidos en entrenamiento.\n\n\n\n\n\n\n\n\n\n\n\nEstimación de Estadísticos\n\n\n\n\\(\\mu_B^{inf} = E[\\mu_B^{j}]\\), \\(j = 1,...,B\\)\n\\(\\sigma_B^{inf} = \\frac{m}{m-1}E[\\mu_B^{j}]\\), \\(j = 1,...,B\\)\n\n\n\n\n\n\n\n\n\n\n\nNormalización\n\n\n\\[\\widehat{z^{(i)}} = \\frac{z^{(i)} - \\mu_B^{inf}}{\\sqrt{(\\sigma_B^{inf})^2 + \\epsilon}}\\]\n\n\n\n\n\n\n\n\n\n\nScale and Shift: \\(\\gamma\\) y \\(\\beta\\) son parámetros.\n\n\n\\[BN_{\\gamma,\\beta}(z_i)= \\gamma \\widehat{z_i} + \\beta \\]\n\n\n\n\n\n\n\n\n\nLos parámetros \\(\\gamma\\) y \\(\\beta\\) son los aprendidos durante el proceso de entrenamiento."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-batch-norm-consejos",
    "href": "tics579/clase-6.html#normalización-batch-norm-consejos",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización (Batch Norm): Consejos",
    "text": "Normalización (Batch Norm): Consejos\n\nAndrew Ng propone utilizar BatchNorm justo antes de la función de Activacion.\nEl paper original también propone su uso justo antes de la activación.\nFrancoise Chollet, creador de Keras dice que los autores del paper en realidad lo utilizaron después de la función de activación.\nAdicionalmente existen benchmarks que muestran mejoras usando BatchNorm después de las funciones de activación.\n\n\n\n\n\n\n\nEntonces, la posición del BatchNorm termina siendo parte de la Arquitectura, y se debe comprobar donde tiene un mejor efecto.\n\n\n\n\n\n\n\n\n\nBatchnorm tiene efectos distintos al momento de entrenar o de evaluar/predecir en un modelo. Por lo tanto, de usar Batchnorm es imperativo utilizar los modos model.train() y model.eval() de manera apropiada."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-layer-norm",
    "href": "tics579/clase-6.html#normalización-layer-norm",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización: Layer Norm",
    "text": "Normalización: Layer Norm\n\n\n\n\n\n\nBatch Norm tiene algunos problemas:\n\n\n\nMuy difícil de calcular en datos secuenciales (lo veremos más adelante).\nInestable cuando el Batch Size es muy pequeño.\nDifícil de Paralelizar.\n\n\n\n\n\n\n\n\n\n\nBeneficios de Layer Norm\n\n\n\nPuede trabajar con secuencias.\nNo tiene problemas para trabajar con cualquier tipo de Batch Size.\nSe puede paralelizar, lo cuál es útil en redes como las RNN.\n\n\n\n\n\n\n\n\n\n\n\nEn este caso se realiza la normalización por capa o por Data Point (instancia).\nAdemás son el elementos cruciales en las Arquitecturas de Transformers."
  },
  {
    "objectID": "tics579/clase-6.html#normalización-layer-norm-1",
    "href": "tics579/clase-6.html#normalización-layer-norm-1",
    "title": "TICS-579-Deep Learning",
    "section": "Normalización: Layer Norm",
    "text": "Normalización: Layer Norm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\mu_{norm} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} z_j = \\frac{1}{4}(4 + 9 + 6 + 7) = 6.5\\] \\[ \\sigma_{norm}^2 = \\frac{1}{n_i} \\sum_{j=1}^{n_i} (z_j - \\mu_B)^2 = 3.25\\]\n\n\n\n\n\n\nNormalización\n\n\n\\[\\widehat{z_j} = \\frac{z_j - \\mu_{norm}}{\\sqrt{\\sigma_{norm}^2 + \\epsilon}}\\]"
  },
  {
    "objectID": "tics579/clase-6.html#regularización-l2-aka-weight-decay",
    "href": "tics579/clase-6.html#regularización-l2-aka-weight-decay",
    "title": "TICS-579-Deep Learning",
    "section": "Regularización L2 aka Weight Decay",
    "text": "Regularización L2 aka Weight Decay\n\n\nEn general el gran problema de las Redes Neuronales es el Overfitting. Esto porque las redes neuronales normalmente se denominan como Overparametrized Models. ¿Qué significa esto?\n\n\nWeight Decay\n\n\nCorresponde a una penalización que se da a los modelos para limitar su complejidad y asegurar que pueda generalizar correctamente en datos no vistos.\n\n\n\n\n\\[ \\underset{W_{i:L}}{minimize} \\frac{1}{m} \\sum_{i=1}^m l(h_\\theta(x^{(i)}),y^{(i)}) + \\frac{\\lambda}{2} \\sum_{i=1}^L ||W_i||_f^2\\]\nEso implica una transformación a nuestro Update Rule:\n\\[W_i := W_i - \\alpha \\nabla \\frac{1}{m} \\sum_{i=1}^m l(h_\\theta(x^{(i)}),y^{(i)}) - \\alpha \\lambda W_i = (1-\\alpha\\lambda)W_i - \\alpha \\nabla l(h_\\theta(x^{(i)}),y^{(i)})\\]\n\n\n\n\n\n\n\n\nSe puede ver que los pesos (weights) se contraen (decaen) antes de actualizarse en la dirección del gradiente.\n\n\n\n\n\n\n\n\n\n\nPor alguna razón Pytorch decidió implementarlo como una propiedad de los Optimizers cuando en realidad debió ser de la Loss Function."
  },
  {
    "objectID": "tics579/clase-6.html#dropout",
    "href": "tics579/clase-6.html#dropout",
    "title": "TICS-579-Deep Learning",
    "section": "Dropout",
    "text": "Dropout\n\nA diferencia de la estrategia anterior, este tipo de regularización se aplica a las activaciones de la red (resultados de la Transformación Affine, previo a la transformación no lineal).\n\nDefiniremos el Dropout como:\n\\[Z_{i+1} = \\sigma(W_i^T Z_i + b_i)\\] \\[\\widehat{Z_{i+1}} = D(Z_{i+1})\\]\ndonde \\(D\\) implica la aplicación de Dropout a la capa \\(i+1\\). El elemento \\(j\\) de la capa \\(\\widehat{Z_i}\\) se calcula como:\n\\[(\\widehat{Z_{i+1}})_j = \\begin{cases}\n\\frac{(Z_{i+1})_j}{1-p}  & \\text{with prob 1-p} \\\\\n0, & \\text{with prob p}\n\\end{cases}\\]\n\\(p\\) se conoce como el Dropout Rate. ::: {.callout-important} El factor \\(\\frac{1}{1-p}\\) se aplica para mantener la varianza estable luego de haber eliminado activaciones con probabilidad \\(p\\). :::\n\n\n\n\n\n\nDropout se aplica normalmente al momento de entrenar el modelo. Por lo tanto, de usar Dropout es imperativo cambiar al modo model.eval() al momento de predecir."
  },
  {
    "objectID": "tics579/clase-6.html#weights-initialization",
    "href": "tics579/clase-6.html#weights-initialization",
    "title": "TICS-579-Deep Learning",
    "section": "Weights Initialization",
    "text": "Weights Initialization\nHemos hablado que los métodos basados en SGD normalmente utilizan valores aleatorios para partir su entrenamiento, lo cual deja un poco al azar el éxito de un proceso de entrenamiento.\nExisten diversos estudios de cómo inicializar los parámetros para una convergencia óptima. Algunas de las inicializaciones son:\n\n\n\n\n\n\nActivaciones Triviales\n\n\n\nConstante\nSólo unos\nSólo Zeros"
  },
  {
    "objectID": "tics579/clase-6.html#weights-initialization-1",
    "href": "tics579/clase-6.html#weights-initialization-1",
    "title": "TICS-579-Deep Learning",
    "section": "Weights Initialization",
    "text": "Weights Initialization\n\n\nXavier o Glorot Uniforme\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{U}(-a,a)\\)\n\\[ a = gain \\cdot \\sqrt{\\frac{6}{fan_{in} + fan_{out}}}\\]\n\nXavier o Glorot Normal\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{N}(0,std^2)\\)\n\\[ std = gain \\cdot \\sqrt{\\frac{2}{fan_{in} + fan_{out}}}\\]\n\n\n\n\n\n\n\n\n\\(fan_{in}\\) corresponde al número de conexiones que entran a una neurona. Mientras que \\(fan_{out}\\) corresponde al número de neuronas que salen de dicha neurona.\n\\(fan\\_mode\\) corresponde a la elección de \\(fan_{in}\\) o \\(fan_{out}\\).\n\n\n\n\n\n\nKaiming (aka He) Uniforme\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{U}(-bound,bound)\\)\n\\[ bound = gain \\cdot \\sqrt{\\frac{3}{fan\\_mode}}\\]\n\nKaiming (aka He) Normal\nSe inicia con valores provenientes de una distribución uniforme: \\(\\mathcal{N}(0,std^2)\\)\n\\[std =\\sqrt{\\frac{gain}{fan\\_mode}}\\]"
  },
  {
    "objectID": "tics579/clase-6.html#training-control",
    "href": "tics579/clase-6.html#training-control",
    "title": "TICS-579-Deep Learning",
    "section": "Training Control",
    "text": "Training Control\nEl entrenamiento de una red neuronal puede tomar mucho tiempo. Es por eso que algunas buenas prácticas serían:\n\nDisponer de resultados preliminares aunque el entrenamiento no haya terminado.\nGuardar los pesos del mejor modelo obtenido en el proceso de entrenamiento.\nEvitar entrenar pasado el punto de Overfitting.\n\nAunque hay nuevas ideas de lo que se llama el grokking.\n\n\n\n\n\n\n\n\n\n\nEarly Stopping\n\n\n\nSe refiere al proceso de detener el entrenamiento luego de patience epochs sin mejorar el validation loss u otro criterio.\n\n\n\n\n\n\n\n\n\n\n\nCheckpointing\n\n\n\nCorresponde al proceso de guardar los parámetros obtenidos en un epoch en específico. Normalmente se guarda la mejor epoch y la última, pero se puede generar algún criterio."
  },
  {
    "objectID": "tics579/clase-6.html#categorical-variables",
    "href": "tics579/clase-6.html#categorical-variables",
    "title": "TICS-579-Deep Learning",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nEs importante mencionar que normalmente no se utilizan redes neuronales para poder entrenar datos tabulares. Pero de hacerlo, es muy probable que nos encontremos con variables categóricas. Para ello existen dos técnicas que son las más comunes en redes neuronales.\n\n\nOne Hot Encoder\n\nCorresponde a la representación mediante dummy variables. Normalmente se considera una representación Sparse de los datos.\n\n\n\n\n\n\n\n\nEn Pytorch se puede implementar como F.one_hot(), pero mi recomendación es utilizar las herramientas de Scikit-Learn para evitar Data Leakage."
  },
  {
    "objectID": "tics579/clase-6.html#categorical-variables-1",
    "href": "tics579/clase-6.html#categorical-variables-1",
    "title": "TICS-579-Deep Learning",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nEmbeddings\n\nEs una representación de Densa de los Datos. Corresponde a una representación a en un espacio dimensional definido que es aprendido por la misma red. La representación aprendida considera aspectos como la similaridad la cual se refleja como una medida de distancia.\n\n\nEn Pytorch esto se puede realizar mediante: nn.Embedding().\n\nnn.Embedding(num_embeddings, embedding_dim)\n\n\nnum_embeddings: Corresponde al número de categórías.\nembedding_dim: El número de dimensiones en el cual se quiere representar.\n\n\n\n\n\n\n\n\nEste proceso tiene parámetros entrenables asociados."
  },
  {
    "objectID": "tics579/clase-4.html#pytorch",
    "href": "tics579/clase-4.html#pytorch",
    "title": "TICS-579-Deep Learning",
    "section": "Pytorch",
    "text": "Pytorch\n\nEs una librería de manipulación de Tensores especializada en Deep Learning. Provee principalmente, manipulación de tensores (igual que Numpy, pero en GPU), además de Autograd (calcula derivadas de manera automática).\n\nPara poder comenzar a utilizarlo se requieren normalmente 3 imports:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\n\n\n\ntorch es donde se encuentran la mayoría de funciones básicas para manipular tensores.\ntorch.nn es donde se encuentran los módulos necesarios para poder crear redes neuronales (neural networks). Cada módulo es una clase en Python.\ntorch.nn.functional es donde se encontrarán las versiones funcionales de elementos de torch.nn."
  },
  {
    "objectID": "tics579/clase-4.html#gpu",
    "href": "tics579/clase-4.html#gpu",
    "title": "TICS-579-Deep Learning",
    "section": "GPU",
    "text": "GPU\n\n\n\n\n\n\n\nSu principal ventaja es que puede ejecutarse en GPU, lo cual entrega una ventaja comparativa enorme (Muchos más núcleos).\n\n\n\n\n\n\n\n\n\n\n\nLas GPUs están programadas en CUDA, una variante de C++ que es muy complicado de entender. Por lo que los mensajes de error son sumamente crípticos. Se recomienda desarrollar en CPU, y cambiar a GPU sólo cuando sea necesario ejecutar libre de errores.\n\n\n\n\n## Permite automáticamente reconocer si es que existe GPU en el sistema y de existir lo asigna.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n\n\n\nEl código de arriba es particularmente útil para Google Colab o plataformas que permitan activar o desactivar GPUs.\nTambién es posible definirlo de manera manual en caso de querer debuggear algo en particular."
  },
  {
    "objectID": "tics579/clase-4.html#mapeando-lo-aprendido-con-pytorch",
    "href": "tics579/clase-4.html#mapeando-lo-aprendido-con-pytorch",
    "title": "TICS-579-Deep Learning",
    "section": "Mapeando lo aprendido con Pytorch",
    "text": "Mapeando lo aprendido con Pytorch\n\n\n\n\n\n\n\nSupongamos el caso particular en el cual queremos resolver un problema de clasificación binaria. ¿Cuánto valdría \\(k\\) y cuál sería la Loss Function a utilizar?\nSupongamos que queremos transformar una Matriz \\(X\\) de 1000 registros y 10 variables. Además tenemos un vector \\(y\\) el cuál queremos predecir.\nSupongamos que queremos llevar a 32 variables, luego a 64 para luego generar nuestra predicción.\nSupongamos además que queremos usar como función de activación la función ReLU en ambas capas de transformación.\n\n\n\n\n\n\n\n\n\n¿Cómo definimos los 3 elementos principales de una red?\n\n\n\n\n\n\n\n\n\n\n\n(Hipótesis, Loss Function y Optimizador)"
  },
  {
    "objectID": "tics579/clase-4.html#nn.module",
    "href": "tics579/clase-4.html#nn.module",
    "title": "TICS-579-Deep Learning",
    "section": "nn.Module",
    "text": "nn.Module\n\n\nEn Pytorch, cada parte de una red es una clase.\n\n\n\n\n\n\n\n\nUna clase tiene la ventaja de que es un objeto mutable que puede almacenar estados en su interior. En el caso particular de una red neuronal, ¿qué estado será importante que guarde?\n\n\n\n\n\n\n\n\n\n\nUna vez que un módulo es instanciado, acepta tensores de entrada y devuelve tensores de salida.\n\n\n\n\nnn.Linear()\n\nCorresponde a la Red más básica de Pytorch y permite realizar Transformaciones Affine.\n\n\n\nfc = nn.Linear(in_features, out_features, bias=True)\n\n\nin_features es la dimensión inicial (\\(n_i\\)).\nout_features la dimensión a la que se quiere llevar (\\(n_{i+1}\\)).\n\n\n\n\n\n\n\nDe manera análoga, nn.ReLU() será el módulo que representará una función de activación ReLU.\n\n\n\n\n\n\n\n\n\n\n\nPero, ¿Cómo combinamos distintos módulos para crear una sóla arquitectura que represente nuestra Hipótesis?"
  },
  {
    "objectID": "tics579/clase-4.html#hipótesis",
    "href": "tics579/clase-4.html#hipótesis",
    "title": "TICS-579-Deep Learning",
    "section": "Hipótesis",
    "text": "Hipótesis\n\nPara poder crear una Hipótesis en Pytorch podemos combinar cada Módulo entra clase que herede desde nn.Module.\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self,):\n        pass\n    def forward(self,x):\n        pass\n\n\n\nLa red neuronal siempre debe heredar nn.Module. Esto permitirá que transformar la clase en Módulos que pueden combinarse para crear Arquitecturas cada vez más complejas.\n__init__() corresponde al constructor. Acá se deben definir todos los parámetros de entrada (similar a una función), con la que se instanciará la clase.\nforward() corresponde a la definición del *forward pass de la red en cuestión."
  },
  {
    "objectID": "tics579/clase-4.html#hipótesis-__init__",
    "href": "tics579/clase-4.html#hipótesis-__init__",
    "title": "TICS-579-Deep Learning",
    "section": "Hipótesis: __init__()",
    "text": "Hipótesis: __init__()\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self,*):\n        super().__init__()\n        self.w1 = nn.Linear(10,32)\n        self.w2 = nn.Linear(32,64)\n        self.w3 = nn.Linear(64,1)\n        self.relu_1= nn.ReLU()\n        self.relu_2= nn.ReLU()\n\n\n\nSiempre el primer elemento de una red neuronal la inicialización del nn.Module mediante el super().__init__().\nEs importante notar que todos los elementos dentro de la clase deben tener el prefijo self. Esto permite que estos elementos puedan estar disponibles en cualquier método de la clase.\nEs posible inicializar elementos mediante parámetros (representado por *) para que la red sea flexible y reutilizable. La convención es que todos los métodos tienen que tener como primer parámetro la palabra self y luego pueden tener otros parámetros."
  },
  {
    "objectID": "tics579/clase-4.html#hipótesis-forward",
    "href": "tics579/clase-4.html#hipótesis-forward",
    "title": "TICS-579-Deep Learning",
    "section": "Hipótesis: forward()",
    "text": "Hipótesis: forward()\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self,*):\n        super().__init__()\n        self.w1 = nn.Linear(10,32)\n        self.w2 = nn.Linear(32,64)\n        self.w3 = nn.Linear(64,1)\n        self.relu_1= nn.ReLU()\n        self.relu_2= nn.ReLU()\n    def forward(self,x):\n        x = self.w1(x)\n        x = self.relu_1(x)\n        x = self.w2(x)\n        x = self.relu_2(x)\n        x = self.w3(x)\n        return x\n\n\nLa método forward representa el *forward pass de la red e indica cómo están conectadas las distintas etapas de la red.\nEn este caso \\(x\\) representa una instancia/registro que va pasando por la red."
  },
  {
    "objectID": "tics579/clase-4.html#loss-function-y-optimizer",
    "href": "tics579/clase-4.html#loss-function-y-optimizer",
    "title": "TICS-579-Deep Learning",
    "section": "Loss Function y Optimizer",
    "text": "Loss Function y Optimizer\n\n\nLoss Function\n\n\nLa nomenclatura utilizada en Pytorch para referirse a la definición de la función de Pérdida es el criterion. Es decir, el criterio con el que se mide la pérdida. Más Loss Functions se pueden encontrar acá.\n\n\nOptimizador\n\n\nLa nomenclatura utilizada en Pytorch para referirse al optimizador a utilizar es optimizer. Éste se importa desde torch.optim y debe recibir como argumentos model.parameters() y al menos el learning_rate. Todos los optimizers pueden encontrarse acá.\n\n\n\n\nmodel = MyNeuralNetwork()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)"
  },
  {
    "objectID": "tics579/clase-4.html#training-loop",
    "href": "tics579/clase-4.html#training-loop",
    "title": "TICS-579-Deep Learning",
    "section": "Training Loop",
    "text": "Training Loop\n\nDefiniremos como Training Loop al proceso en el cual entrenaremos el modelo.\n\nfor e in range(EPOCHS):\n    ## Fijar el modelo en Modo Entrenamiento\n    model.train()\n\n    ## Fijar Gradientes en 0\n    optimizer.zero_grad()\n\n    ## Forward Pass\n    preds = model(X)\n\n    ## Cálculo del Loss (Ojo, primero va la predicción y luego el target). Ver Docs.\n    loss = criterion(preds, y)\n\n    ## Cálculo de Gradientes\n    loss.backward()\n\n    ## Update Rule\n    optimizer.step()\n\n\n\n\n\n\n\n\n.zero_grad() fijan los gradientes a cero, ya que Pytorch acumula gradientes siempre. Es importante que en cada epoch todos los gradientes acumulados vuelvan a cero para una siguiente optimización.\nEn el caso de querer dejar en zero los gradientes de un tensor, y no del optimizador, se puede usar .zero_()."
  },
  {
    "objectID": "tics579/clase-4.html#inferencia",
    "href": "tics579/clase-4.html#inferencia",
    "title": "TICS-579-Deep Learning",
    "section": "Inferencia",
    "text": "Inferencia\n\n\nPara generar predicciones basta con generar un Forward Pass con el modelo ya entrenado. Dependiendo del modelo, es posible que sea necesario aplicar un post-procesamiento.\n\n\n\n## Fijar el Modelo en Evaluación.\nmodel.eval()\n\n## Evita que Pytorch calcule Gradientes ya que no es necesario.\nwith torch.no_grad():\n    ## Cálculo de la salida del modelo (h)\n    h = model(X)\n\n## Cálculo de Probabilidades (si es que fuera necesario)\ny_proba = torch.sigmoid(h)\n\n## Clasificación propiamente tal\ny_preds = torch.where(y_proba&gt;=0.5, 1,0)"
  },
  {
    "objectID": "tics579/clase-4.html#mini-batching",
    "href": "tics579/clase-4.html#mini-batching",
    "title": "TICS-579-Deep Learning",
    "section": "Mini-Batching",
    "text": "Mini-Batching\n\n\n\n\n\n\n\n\nRara vez los datos vienen en formato de Tensor de Pytorch. Por lo tanto, el dataset (tablas, imágenes, videos, texto, audio, etc) debe ser llevado a formato Tensor, lo cual puede ser un proceso bastante costoso y que consume muchos recursos.\n\n\n\n\n\n\n\n\n\n\n\nAdemás, la cantidad de datos necesaria para poder entrenar un modelo de Deep Learning normalmente es alta. Lo cual limita el cierto Hardware al no contar con la capacidad necesaria.\n\n\n\n\n\nMini-Batching\n\nSe refiere a aplicar un proceso de Optimización Estocástica, con sólo una muestra de los datos. Se basa en que el gradiente de la suma de las muestras es equivalente al gradiente total.\n\n\nPara ello Pytorch introduce los conceptos de Dataset y DataLoader para implementar conversión y carga de datos on-the-fly.\n\n\nfrom torch.utils.data import Dataset, DataLoader"
  },
  {
    "objectID": "tics579/clase-4.html#mini-batching-dataset",
    "href": "tics579/clase-4.html#mini-batching-dataset",
    "title": "TICS-579-Deep Learning",
    "section": "Mini-Batching: Dataset",
    "text": "Mini-Batching: Dataset\n\nPytorch necesita crear una clase que herede de Dataset y que permita tomar elementos uno a uno y transformarlos en Tensores. Este clase debe tener al menos 3 métodos: __init__, __len__ y __getitem__.\n\n\n\n\n\n\n\nSupongamos que nuestros datos iniciales estaban en Numpy.\n\n\n\nclass MyDataSet(Dataset):\n    def __init__(self, X,y):\n        self.X = X\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self,idx):\n        features = torch.from_numpy(self.X[idx])\n        target = torch.from_numpy(self.y[idx])\n        return features, target"
  },
  {
    "objectID": "tics579/clase-4.html#model-registry",
    "href": "tics579/clase-4.html#model-registry",
    "title": "TICS-579-Deep Learning",
    "section": "Model Registry",
    "text": "Model Registry\n\nCada vez que nosotros llamamos un objeto modelo (que herede de nn.Module) este modelo mostrará el model registry. El registry permitirá ver todos los elementos que son parte del modelo. Para que un elemento sea parte del registro, debe haber sido definido como self.----.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi es que se define un elemento como self.--- debe definirse como un nn.Module y no como un F.---\nAdemás se puede acceder a cualquier elemento/atributo mediante el comando model.atributo."
  },
  {
    "objectID": "tics579/clase-4.html#model-registry-1",
    "href": "tics579/clase-4.html#model-registry-1",
    "title": "TICS-579-Deep Learning",
    "section": "Model Registry",
    "text": "Model Registry\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs posible acceder a los datos de Parámetros y Bias de una capa linear utilizando:\n\n\n\n\nmodel.w1.weights.data\nmodel.w1.bias.data\n\n\n\n\n\n\nIdea:\n\n\n\nPodría utilizarse esto para poder definir valores iniciales de capas de parámetros y de bias.\n\n\n\n\n\nclass MyNeuralNetwork(nn.Module):\n    def __init__(self, *):\n        self.w1 = ...\n        self.relu = ...\n        self.model.w1.weights.data = tensor([...])\n        self.model.w1.bias.data = tensor([...])\n    \n    def forward(self,x):\n        ..."
  },
  {
    "objectID": "tics579/clase-4.html#mini-batching-dataloader",
    "href": "tics579/clase-4.html#mini-batching-dataloader",
    "title": "TICS-579-Deep Learning",
    "section": "Mini-Batching: Dataloader",
    "text": "Mini-Batching: Dataloader\n\n\nEl Dataloader permitirá ir cargando los datos en memoria en un cierto batch_size. La idea es no generar cuellos de botella por falta de memoria disponible.\n\n\ndata = MyDataset(X,y)\ntrain_loader = DataLoader(data, batch_size=32, pin_memory=True,num_workers=12, shuffle=True)\n\n\n\n\n\n\n\nEsto implica que nuestro Training Loop deberá sufrir ciertas modificaciones para ir actualizandose por Batch y no sólo por Epoch.\n\n\n\n\nfor e in range(EPOCHS):\n    train_loss = []\n\n    model.train()\n    for batch in train_loader:\n        X, y = batch\n        optimizer.zero_grad()\n        preds = model(X)\n        loss = criterion(preds, y)\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n    print(f\"Loss para Epoch {e}: {np.mean(train_loss)}\")"
  },
  {
    "objectID": "tics579/notebooks/Tarea-1_template.html",
    "href": "tics579/notebooks/Tarea-1_template.html",
    "title": "Tarea 1",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\n\ndef plot_optimizers(theta_0, theta_1, title):\n    plt.plot(theta_0, theta_1)\n    plt.title(title)\n    plt.xlabel(r\"theta_0\")\n    plt.ylabel(r\"theta_1\")\n\n\nParte I\n\n\nParte II\n\n\nParte III\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics579/notebooks/Intro_pytorch_2.html",
    "href": "tics579/notebooks/Intro_pytorch_2.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\n\nSEED = 1\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_numpy = np.random.randn(1000, 10)\ny_numpy = np.random.randint(0, 2, 1000)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_numpy, y_numpy, test_size=0.25, random_state=42\n)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape\n\n((750, 10), (250, 10), (750,), (250,))\n\n\n\nclass MyData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return dict(\n            features=self.X[idx].float(), target=self.y[idx].float()\n        )\n\n\ndataset = MyData(X_numpy, y_numpy)\ndataset[0][\"target\"]\n\ntensor(1.)\n\n\n\nclass MyFFN(nn.Module):\n    def __init__(self, n_features, hidden_dim_1, hidden_dim_2, out_dim):\n        super().__init__()\n        self.w1 = nn.Linear(\n            in_features=n_features, out_features=hidden_dim_1\n        )\n        self.relu_1 = nn.ReLU()\n        self.w2 = nn.Linear(\n            in_features=hidden_dim_1, out_features=hidden_dim_2\n        )\n        self.relu_2 = nn.ReLU()\n        self.w3 = nn.Linear(hidden_dim_2, out_dim)\n\n    def forward(self, x):\n        x = self.w1(x)\n        x = self.relu_1(x)\n        x = self.w2(x)\n        x = self.relu_2(x)\n        x = self.w3(x)\n        return x\n\n\nmodel = MyFFN(n_features=10, hidden_dim_1=32, hidden_dim_2=32, out_dim=1)\n\n\nfrom tqdm.notebook import tqdm\n\n\ndef training_loop(\n    model, X_train, y_train, X_val, y_val, batch_size=32, epochs=50\n):\n    EPOCHS = epochs\n    model.to(device)\n\n    print(f\"Modelo entrenándose en {next(model.parameters()).device}\")\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n    train_data = MyData(X_train, y_train)\n    val_data = MyData(X_val, y_val)\n\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=10,\n        shuffle=True,\n        drop_last=True,  ## Opcional... se supone que genera más estabilidad en los Gradientes.\n    )\n    val_dataloader = DataLoader(\n        val_data,\n        batch_size=batch_size,\n        pin_memory=True,\n        num_workers=10,\n        shuffle=False,\n        drop_last=False,  # Debe ser falso para validar, sino no estamos calculando un Loss para todos los puntos.\n    )\n    train_loss = []\n    val_loss = []\n\n    for e in tqdm(range(1, EPOCHS + 1)):\n        train_batch_loss = []\n        val_batch_loss = []\n        model.train()\n        for batch in train_dataloader:\n            features, target = batch[\"features\"].to(device), batch[\n                \"target\"\n            ].to(device)\n\n            optimizer.zero_grad()\n            output = model(features)\n            ## Por qué no va acá una sigmoide a la salida?\n            loss = criterion(output, target.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            train_batch_loss.append(loss.item())\n        train_epoch_loss = np.mean(train_batch_loss)\n        if e % 50 == 0:\n            print(f\"Training Loss for Epoch {e}: {train_epoch_loss}\")\n        train_loss.append(train_epoch_loss)\n\n        model.eval()\n        with torch.no_grad():\n            for batch in val_dataloader:\n                features, target = batch[\"features\"].to(device), batch[\n                    \"target\"\n                ].to(device)\n\n                output = model(features)\n                loss = criterion(output, target.unsqueeze(1))\n                val_batch_loss.append(loss.item())\n        val_epoch_loss = np.mean(val_batch_loss)\n        if e % 50 == 0:\n            print(f\"Validation Loss for Epoch {e}: {val_epoch_loss}\")\n        val_loss.append(val_epoch_loss)\n\n    return model, train_loss, val_loss\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_validation_curve(train_loss, val_loss, epoch=50):\n    plt.plot(range(epoch), train_loss, label=\"Train Loss\")\n    plt.plot(range(epoch), val_loss, label=\"Validation Loss\")\n    plt.title(\"Validation Curve\")\n    plt.legend()\n    plt.show()\n\n\nEPOCHS = 50\nmodel = MyFFN(n_features=10, hidden_dim_1=64, hidden_dim_2=64, out_dim=1)\nmodel, train_loss, val_loss = training_loop(\n    model, X_train, y_train, X_val, y_val, batch_size=32, epochs=EPOCHS\n)\nplot_validation_curve(train_loss, val_loss)\nprint(f\"Min Loss: {min(val_loss)}\")\n\nModelo entrenándose en cuda:0\n\n\n\n\n\nTraining Loss for Epoch 50: 0.616158355837283\nValidation Loss for Epoch 50: 0.7319772690534592\n\n\n\n\n\n\n\n\n\nMin Loss: 0.6952822953462601\n\n\n\nEPOCHS = 100\nmodel = MyFFN(n_features=10, hidden_dim_1=64, hidden_dim_2=64, out_dim=1)\nmodel, train_loss, val_loss = training_loop(\n    model, X_train, y_train, batch_size=128, epochs=EPOCHS\n)\nplot_validation_curve(train_loss, val_loss, epoch=EPOCHS)\nprint(f\"Min Loss: {min(val_loss)}\")\n\n\n\n\nTraining Loss for Epoch 50: 0.6657268881797791\nValidation Loss for Epoch 50: 0.7142947316169739\nTraining Loss for Epoch 100: 0.6180922746658325\nValidation Loss for Epoch 100: 0.7341216802597046\n\n\n\n\n\n\n\n\n\nMin Loss: 0.6952072083950043\n\n\n\nfrom torchinfo import summary\n\nsummary(model)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nMyFFN                                    --\n├─Linear: 1-1                            704\n├─ReLU: 1-2                              --\n├─Linear: 1-3                            4,160\n├─ReLU: 1-4                              --\n├─Linear: 1-5                            65\n=================================================================\nTotal params: 4,929\nTrainable params: 4,929\nNon-trainable params: 0\n=================================================================\n\n\n\nEPOCHS = 20\nmodel = MyFFN(n_features=10, hidden_dim_1=32, hidden_dim_2=32, out_dim=1)\nmodel, train_loss, val_loss = training_loop(\n    model, X_train, y_train, batch_size=64, epochs=EPOCHS\n)\nplot_validation_curve(train_loss, val_loss, epoch=EPOCHS)\nprint(f\"Min Loss: {min(val_loss)}\")\n\n\n\n\n\n\n\n\n\n\n\nMin Loss: 0.6932080686092377\n\n\n\nsummary(model)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nMyFFN                                    --\n├─Linear: 1-1                            352\n├─ReLU: 1-2                              --\n├─Linear: 1-3                            1,056\n├─ReLU: 1-4                              --\n├─Linear: 1-5                            33\n=================================================================\nTotal params: 1,441\nTrainable params: 1,441\nNon-trainable params: 0\n=================================================================\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "charlas.html",
    "href": "charlas.html",
    "title": "Charlas",
    "section": "",
    "text": "Hate Speech UAI\n\n\nPresentación del HateStack\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike a Needle in the HateStack\n\n\nPresentación final para la Datatón 2022\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRFM-Superlag\n\n\nPresentación Final del Desafío Itaú-Binnario\n\n\n\nDec 5, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "tics411/clase-2.html#eda",
    "href": "tics411/clase-2.html#eda",
    "title": "TICS-411 Minería de Datos",
    "section": "EDA",
    "text": "EDA\n\nEl Analisis Exploratorio de Datos (EDA, por sus siglas en inglés) es procedimiento en el cual se analiza un dataset para explorar sus características principales.\n\n\nSu objetivo principal es poder familiarizarse con los datos además de encontrar potenciales problemas en su calidad.\nPrincipalmente hace uso de técnicas de manipulación de datos y visualizaciones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos hallazgos importantes dentro del proceso se les denomina insights.\n\n\n\n\n\n\n\n\n\nEl uso de visualizaciones inadecuadas podría llevar a conclusiones erróneas.\n\n\n\n\n\n\n\n\n\n\nSummary.\nVisualización."
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-tendencia-central",
    "href": "tics411/clase-2.html#medidas-de-tendencia-central",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas de Tendencia Central",
    "text": "Medidas de Tendencia Central"
  },
  {
    "objectID": "tics411/clase-2.html#medidas-de-dispersión-y-asimetría",
    "href": "tics411/clase-2.html#medidas-de-dispersión-y-asimetría",
    "title": "TICS-411 Minería de Datos",
    "section": "Medidas de Dispersión y Asimetría",
    "text": "Medidas de Dispersión y Asimetría"
  },
  {
    "objectID": "tics411/clase-2.html#eda-visualización",
    "href": "tics411/clase-2.html#eda-visualización",
    "title": "TICS-411 Minería de Datos",
    "section": "EDA: Visualización",
    "text": "EDA: Visualización\n\nLa visualización de datos es la presentación de datos en forma gráfica. Permite simplificar conceptos más complejos en especial a altos mandos.\n\n\nGracias a la evolución del cerebro humano somos capaces de detectar patrones complejos en la naturaleza a partir de la Visión.\n\n\n\n\n\n\n\n\nPuede ser difícil de aplicar si el tamaño de los datos es grande (sea en instancias o atributos). Por ejemplo, si los datos están en 4 dimensiones.\n\n\n\n\n\n\n\n\n\n\n\n\nSe suelen resumir los datos en estadísticas simples.\nGraficar datos en 1D, 2D y 3D (evitar dentro de lo posible).\nLa visualización debe ser comprensible ojalá sin ninguna explicación.\n\n\n\n\n\n\n\n\n\n\n\n\nEn caso de datos de alta dimensionalidad puede ser una buena idea reducir dimensiones mediante técnicas como:\n\nPCA\nUMAP\netc."
  },
  {
    "objectID": "tics411/clase-2.html#caso-de-visualización",
    "href": "tics411/clase-2.html#caso-de-visualización",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso de Visualización",
    "text": "Caso de Visualización\n\n\n\n\n\n\n\nFiguras\nEscala de Colores.\nTamaño de los puntos.\nDemasiada información en un sólo gráfico.\nNo se entiende el mensaje."
  },
  {
    "objectID": "tics411/clase-2.html#canales-visuales",
    "href": "tics411/clase-2.html#canales-visuales",
    "title": "TICS-411 Minería de Datos",
    "section": "Canales Visuales",
    "text": "Canales Visuales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe les llama canales visuales a elementos visuales que pueden utilizarse para expresar información (Clase Visualizacion Andreas Mueller).\nLa idea es poder mapear cada uno de estos canales a valores que queremos visualizar.\n\n\n\n\n\n\n\n\n\n\n\nNo todos los canales son igual de útiles ni fáciles de entender."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nHistograma\n\n\nEl histograma permite visualizar distribuciones univariadas acumulando los datos en rangos de igual tamaño (bins).\n\n\n\n\nPermite visualizar el centro, la extensión, la asimetría y outliers.\n\n\n\n\n\n\n\n\nEl histograma puede ser “engañoso” para conjuntos de datos pequeños.\nLa visualización puede resultar de manera muy distintas dependiendo del número de bins."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nKernel Density\n\n\nCorresponde a un suavizamiento de un Histograma en el cuál se usa un Kernel (función no negativa que suma 1 y tiene media 0) para agrupar los puntos vecinos.\n\n\n\n\n\nLa función estimada es:\n\\[f(x) = \\frac{1}{n} = \\sum_{i=1}^n K \\left(\\frac{x - x(i)}{h}\\right)\\]\n\n\\(K(u)\\) es el Kernel.\n\\(h\\) es el ancho de banda."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "href": "tics411/clase-2.html#visualizaciones-distribuciones-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Distribuciones",
    "text": "Visualizaciones: Distribuciones\n\nBoxplot (Caja y Bigotes)\n\nEs un tipo de gráfico que muestra la distribución de manera univariada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTiene la capacidad de mostrar varias distribuciones a la vez.\nAdemás presenta estadísticos de interés: Mediana, IQR y outliers.\nLos puntos fuera de los bigotes son considerados Outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos bigotes pueden representar:\n\nMínimo y Máximo. (En este caso no hay outliers).\n\\(\\mu \\pm 3\\sigma\\)\nPercentiles 5 y 95.\nOtros valores."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-barras",
    "href": "tics411/clase-2.html#visualizaciones-barras",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Barras",
    "text": "Visualizaciones: Barras\n\nBar Plot\n\n\nLa altura de la barra (normalmente Eje y) representa una agregación asociada a una categoría (normalmente Eje x).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOtras convenciones llaman a este gráfico Column Plot, mientras que el Bar Plot tiene las barras de manera horizontal."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-puntos",
    "href": "tics411/clase-2.html#visualizaciones-puntos",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Puntos",
    "text": "Visualizaciones: Puntos\n\nScatter\n\n\nGráfico empleado para mostrar distribución de datos bivariados\n\n\n\n\nMuestra la relación entre una variable independiente (Eje X) y una variable dependiente (Eje Y).\nPermite mostrar relaciones lineales o no-lineales (Correlaciones).\nOutliers.\nSimplemente ubicación de Puntos en el Espacio."
  },
  {
    "objectID": "tics411/clase-2.html#visualizaciones-líneas",
    "href": "tics411/clase-2.html#visualizaciones-líneas",
    "title": "TICS-411 Minería de Datos",
    "section": "Visualizaciones: Líneas",
    "text": "Visualizaciones: Líneas\n\nLineplot\n\n\nGráfico empleado para visualizar tendencias y su evolución de una medida (Eje Y) en el tiempo (Eje X).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi bien es posible utilizarlo para gráficar dos medidas continuas, las buenas prácticas indican que el eje X siempre debería contener una componente temporal."
  },
  {
    "objectID": "tics411/clase-2.html#estadísticos-vs-visualizaciones",
    "href": "tics411/clase-2.html#estadísticos-vs-visualizaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Estadísticos vs Visualizaciones",
    "text": "Estadísticos vs Visualizaciones"
  },
  {
    "objectID": "tics411/clase-2.html#otras-visualizaciones",
    "href": "tics411/clase-2.html#otras-visualizaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Otras Visualizaciones?",
    "text": "¿Otras Visualizaciones?"
  },
  {
    "objectID": "tics411/clase-0.html#quién-soy",
    "href": "tics411/clase-0.html#quién-soy",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Quién soy?",
    "text": "¿Quién soy?\n\n\n\n\n\n\n\nAlfonso Tobar-Arancibia, estudié Ingeniería Civil pero llevo 9 años trabajando como:\n\nData Analyst.\nData Scientist.\nML Engineer.\nData Engineer.\n\nTerminando mi Msc. y empezando mi PhD en la UAI.\nMe gusta mucho programar (en vivo).\nContribuyo a HuggingFace y Feature Engine.\nHe ganado 2 competencias de Machine Learning.\nPubliqué mi primer paper el año pasado sobre Hate Speech en Español.\nJuego Tenis de Mesa, hago Agility con mi perrita Kira y escribo en mi Blog."
  },
  {
    "objectID": "tics411/clase-0.html#objetivos-del-curso",
    "href": "tics411/clase-0.html#objetivos-del-curso",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivos del Curso",
    "text": "Objetivos del Curso\n\n\n\n\n\n\nIdentificar Elementos Claves del Machine Learning (Terminología, Nomenclatura, Intuición).\nEntender como interactúan los algoritmos más importantes.\nAprender a seleccionar el mejor Algoritmo para el Problema.\nEjecutar y aplicar algoritmos clásicos de Machine Learning.\nEvaluar el desempeño esperado del Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#tópicos",
    "href": "tics411/clase-0.html#tópicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tópicos",
    "text": "Tópicos\n\n\n\n\n\n\n\nIntroducción a la Minería de Datos\nAnálisis Exploratorio de Datos (EDA)\nModelos No Supervisados/Descriptivos\nModelos Supervisados/Predictivos\n\n\n\n\n\n\nModelos no Supervisados\n\nK-Means\nHierarchical Clustering\nDBScan\nApriori\n\n\nModelos Supervisados\n\nKNN\nÁrboles de Decisión\nNaive Bayes\nRegresión Logística"
  },
  {
    "objectID": "tics411/clase-0.html#sobre-las-clases",
    "href": "tics411/clase-0.html#sobre-las-clases",
    "title": "TICS-411 Minería de Datos",
    "section": "Sobre las clases",
    "text": "Sobre las clases\n\n\nClases presenciales, con participación activa de los estudiantes.\nEs un curso coordinado.\nCanal oficial será Webcursos.\nMucha terminología y material de estudio será en Inglés.\nHorario: Jueves.\n\n15:30 a 16:40 (Cátedra)\n17:00 a 18:10 (Práctico)\nIdealmente!!\n\nAsistencia es voluntaria, pero altamente recomendada."
  },
  {
    "objectID": "tics411/clase-0.html#materiales-de-clases",
    "href": "tics411/clase-0.html#materiales-de-clases",
    "title": "TICS-411 Minería de Datos",
    "section": "Materiales de Clases",
    "text": "Materiales de Clases\n\nDiapositivas\nPrácticos\n\n\n\n\n\n\n\n\nSlides interactivas (Código se puede copiar e imágenes se pueden ver en grande).\nSe puede buscar contenido en las diapositivas mediante un buscador.\nSe dejarán copias en PDF en Webcursos (levemente distintas).\n\n\n\n\n\n\n\n\n\n\nSe espera que los estudiantes dominen las siguientes tecnologías:\n\nPython\nGoogle Colab\nPandas/Numpy\nScikit-Learn (Se enseñará a lo largo del curso)."
  },
  {
    "objectID": "tics411/clase-0.html#material-complementario",
    "href": "tics411/clase-0.html#material-complementario",
    "title": "TICS-411 Minería de Datos",
    "section": "Material Complementario",
    "text": "Material Complementario\n\n\n\n\n\n\nCurso de Scikit-Learn \n\nTutorial Colab\nAgregar Datos Externos a Colab"
  },
  {
    "objectID": "tics411/clase-0.html#evaluación",
    "href": "tics411/clase-0.html#evaluación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\n\n\n\n\n\n\nDos Evaluaciones Escritas (P1, P2) coordinadas y cuatro tareas prácticas en parejas (T1, T2, T3, T4) \\[NP = 0.35 \\cdot P1 + 0.35 \\cdot P2 + 0.3 \\cdot \\bar{T}\\] \\[ \\bar{T} = (T1 + T2 + T3 + T4)/4 \\]\n\n\n\n\n\n\n\n\n\n\nSi NP &gt; 5\n\n\n\\[NF = NP\\]\n\n\n\n\n\n\n\n\n\nEn caso contrario:\n\n\n\\[NF = 0.7 \\cdot NP + 0.3 \\cdot E\\]"
  },
  {
    "objectID": "tics411/clase-0.html#ayudantías",
    "href": "tics411/clase-0.html#ayudantías",
    "title": "TICS-411 Minería de Datos",
    "section": "Ayudantías",
    "text": "Ayudantías\nAyudante: TBD\nemail: TBD\n\n\n\n\n\n\n\nLas ayudantías serán en la manera que sean necesarias.\nEstarán enfocadas principalmente en aplicaciones y código."
  },
  {
    "objectID": "tics411/clase-0.html#revolución-de-los-datos",
    "href": "tics411/clase-0.html#revolución-de-los-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Revolución de los Datos",
    "text": "Revolución de los Datos\n\n\n\n\n\n\n\nHablar de los distintos tipos de Datos.\nTodo es datos, y está lleno de ellos en Internet y el mundo."
  },
  {
    "objectID": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "href": "tics411/clase-0.html#nace-el-data-science-ciencia-de-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Nace el Data Science (Ciencia de Datos)",
    "text": "Nace el Data Science (Ciencia de Datos)\n\n\n\n\n\n\n\nExplicar las distintas etapas. Qué son cada uno de ellos.\nExplicar que no estoy de acuerdo con todas las definiciones."
  },
  {
    "objectID": "tics411/clase-0.html#cómo-aprovechar-la-información-que-tenemos",
    "href": "tics411/clase-0.html#cómo-aprovechar-la-información-que-tenemos",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo aprovechar la información que tenemos?",
    "text": "¿Cómo aprovechar la información que tenemos?\n\n\nData Mining (Minería de Datos)\n\n\n“The process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data.” (Fayyad, Piatetsky-Shapiro & Smith 1996)\n\n\n\n\n\n\nMachine Learning (Aprendizaje Automático)\n\n\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” (Mitchell, 2006)\n\n\n\n\n\n\nExplicar que estos son dos tipos de Approaches con el que hoy en día se enfrentan los datos.\nEl primero más enfocado en un análisis manual.\nEl segundo en un enfoque más automático."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos",
    "href": "tics411/clase-0.html#tipos-de-datos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos",
    "text": "Tipos de Datos\n\n\n\n\n\n\n\n\nDatos Estructurados\n\n\n\n\n\nDatos No Estructurados"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "href": "tics411/clase-0.html#tipos-de-datos-datos-tabulares",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Datos Tabulares",
    "text": "Tipos de Datos: Datos Tabulares\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Observaciones, instancias, registros. (Normalmente independientes).\nColumnas: Variables, Atributos, Features.\n\n\n\n\n\n\n\n\n\n\n\nProbablemente el tipo de datos más amigable.\nRequiere conocimiento de negocio (Domain Knowledge)\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo. También el que más disponible está en las empresas.\nDistintos data types, por lo que normalmente requiere de algún tipo de preprocesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "href": "tics411/clase-0.html#tipos-de-datos-series-de-tiempo",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Series de Tiempo",
    "text": "Tipos de Datos: Series de Tiempo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilas: Instancias temporales (Normalmente interdependientes).\nColumnas: Variables, Atributos, Features (Univariada o Multivariada).\n\n\n\n\n\n\n\n\n\n\n\nEs un % bajísimo del total de datos existentes en el Mundo.\nPropiedad temporal requiere preprocesamiento y modelos especiales."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-imágenes",
    "href": "tics411/clase-0.html#tipos-de-datos-imágenes",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Imágenes",
    "text": "Tipos de Datos: Imágenes\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste es el tipo de Datos que disparó la Inteligencia Artificial.\n¿Cuántos computadores para identificar un Gato? 16,000\n\n\n\n\n\n\n\n\n\n\n\n\nExplicar el concepto de Tensor, extensión de las matrices. Diferencia entre Grayscale y RGB."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "href": "tics411/clase-0.html#tipos-de-datos-texto-libre",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Texto Libre",
    "text": "Tipos de Datos: Texto Libre\n\n\n\n\n\n\n\n\n\n\n\n\nDatos Masivos.\nDificiles de lidiar ya que deben ser llevarse a una representación numérica.\nAlto nivel de Sesgo y Subjetividad.\n\n\n\n\n\n\n\n\n\n\n\nGracias a este tipo de datos se han producido los avances más increíbles del último tiempo: Transformers"
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-datos-videos",
    "href": "tics411/clase-0.html#tipos-de-datos-videos",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Datos: Videos",
    "text": "Tipos de Datos: Videos\n\n\n\n\n\n\n\n\nLos videos no son más que arreglos de imágenes.\nSon un tipo de dato muy pesado y difícil de lidiar.\nRequiere alto poder de Procesamiento."
  },
  {
    "objectID": "tics411/clase-0.html#tipos-de-aprendizaje",
    "href": "tics411/clase-0.html#tipos-de-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Aprendizaje",
    "text": "Tipos de Aprendizaje"
  },
  {
    "objectID": "tics411/clase-0.html#reinforcement-learning",
    "href": "tics411/clase-0.html#reinforcement-learning",
    "title": "TICS-411 Minería de Datos",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este tipo de aprendizaje se enseña por refuerzo. Es decir se da una recompensa si el sistema aprende lo que queremos.\n\n\n\n\n\n\n\n\n\n\n\nSi el premio es mayor, se pueden obtener aprendizajes mayores.\n\n\n\n\n\n\n\n\n\n\n\nUn ejemplo de esto es AlphaTensor en el cual un modelo aprendió una nueva manera de multiplicar matrices que es más eficiente.\n\n\n\n\n\n\n\n\n\n\n\nOtro ejemplo es AlphaFold donde el modelo aprendió/descubrió cómo se doblan las proteínas cuando se vuelven aminoácidos."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-supervisados-regresión-y-clasificación",
    "href": "tics411/clase-0.html#problemas-supervisados-regresión-y-clasificación",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas Supervisados: Regresión y Clasificación",
    "text": "Problemas Supervisados: Regresión y Clasificación\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión: Se busca estimar un valor continuo.\n\n(Estimar el valor de una casa).\n\nClasificación: Se busca encontrar una categoría o un valor discreto.\n\n(Clasificar una imagen como Perro o Gato).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara entrenar este tipo de modelos se necesitan etiquetas, es decir, la respuesta esperada del modelo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmbos ejemplos se pueden realizar utilizando Largo (Eje Y) y Peso (Eje X)."
  },
  {
    "objectID": "tics411/clase-0.html#clustering",
    "href": "tics411/clase-0.html#clustering",
    "title": "TICS-411 Minería de Datos",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\nClusters: Una categoría en la que sus componentes son similares. Los clusters normalmente no tienen un nombre propio, sino que uno les asigna uno.\nTambién se les llama segmentos. No usar la palabra clase.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo requiere de etiquetas, por lo tanto, no es posible evaluar su desempeño de manera 100% acertada."
  },
  {
    "objectID": "tics411/clase-0.html#reducción-de-dimensionalidad",
    "href": "tics411/clase-0.html#reducción-de-dimensionalidad",
    "title": "TICS-411 Minería de Datos",
    "section": "Reducción de Dimensionalidad",
    "text": "Reducción de Dimensionalidad\n\n\n\n\n\n\n\n\n\n\n\n\nReducción de la Dimensionalidad: Eliminar complejidad sin perder información clave para poder entender su comportamiento."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML",
    "text": "Nuestro Sistema de ML\nCreemos un Sistema de ML que sea capaz de ver una imágen y pronunciar correctamente el uso de la letra C.\n\n\n\n\n\n\nVamos a Entrenar un Modelo."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-entrenamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Entrenamiento",
    "text": "Nuestro Sistema de ML: Entrenamiento\n\n\n\n\n\n\n\n\n\nKasa\n\n\n\n\n\n\n\nKokodrilo\n\n\n\n\n\n\n\nKubo\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué patrones está aprendiendo el modelo?\n\n\n\n\n\nEntrenamiento\n\n\nEs el proceso en el cuál se permite al modelo aprender. En este proceso se le entregan ejemplos (Train Set) para que el modelo de manera autónoma pueda aprender patrones que le permitan resolver la tarea dada."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-inferencia",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Inferencia",
    "text": "Nuestro Sistema de ML: Inferencia\n\nInferencia/Predicción\n\n\nSe refiere al proceso en el que el modelo tiene que demostrar cuál sería su decisión de acuerdo a los patrones aprendidos en el proceso de entrenamiento. Los ejemplos en los que se prueba se le denomina Test Set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKukillo\n\n\nBikikleta\n\n\n\n\n\nGeneralización\n\n\nSe le llama generalización a la capacidad del modelo de aplicar lo aprendido de manera correcta en ejemplos no vistos."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-nuevas-instancias-de-entrenamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento",
    "text": "Nuestro Sistema de ML: Nuevas instancias de Entrenamiento\n\n\n\n\n\n\n\n\n\nKuchillo\n\n\n\n\n\n\n\nChokolate\n\n\n\n\n\n\n\nSinsel\n\n\n\n\n\n\n\n\n\n\n\n\nNo es bueno entrenar con las mismas instancias de de Test, es decir, con las cuales se evalúa el modelo. ¿Por qué?\n\n\n\n\n\nMencionar el caso de error de ImageNet."
  },
  {
    "objectID": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "href": "tics411/clase-0.html#nuestro-sistema-de-ml-reevaluemos-nuestro-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo",
    "text": "Nuestro Sistema de ML: Reevaluemos nuestro Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKollar\n\n\nKonejo\n\n\nKuchillo\n\n\nBisikleta\n\n\n\n\n\nEvaluación\n\n\nUtilizar una métrica que permita ponerle nota al modelo.\n\n\n\n\n\n\n1er Modelo: 2 correctas de 4, es decir 50%.\n\n\n\n\n2do Modelo: 4 correctas de 4, es decir 100%."
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas del Aprendizaje",
    "text": "Problemas del Aprendizaje\n\nSupongamos que queremos utilizar nuestro modelo para pronunciar palabras en otro idioma (otro Test Set).\n¿Qué problemas podemos encontrar?\n\n\n\n\nStomach \\(\\rightarrow\\) Stomak\nArcher \\(\\rightarrow\\) Archer\nChurch \\(\\rightarrow\\) Churk\n\nChurch.\n\nArcheology \\(\\rightarrow\\) Archeology\n\nArkeology.\n\nChicago \\(\\rightarrow\\) Chicago\n\nShicago.\n\nMuscle \\(\\rightarrow\\) Muskle\n\nMus_le.\n\nIch mag Schweinefleisch \\(\\rightarrow\\) Ich mag Schweinefleisk.\n\nIj mag Shvaineflaish.\n\n\n\n\n\n\n\n\n\n\nClaramente tenemos un problema. ¿A qué se debe esto?"
  },
  {
    "objectID": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "href": "tics411/clase-0.html#problemas-del-aprendizaje-definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Problemas del Aprendizaje: Definiciones",
    "text": "Problemas del Aprendizaje: Definiciones\n\nOverfitting (Sobreajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, porque se ajusta demasiado bien (llegando a memorizar) a los datos de entrenamiento. ¿Cómo se puede mitigar este problema?\n\n\n\n\n\n\n\n\n\n\nSe le tiende a llamar sobreentrenamiento, pero no es del todo correcto para el caso de modelos de Machine Learning. Lo más correcto es que el sobreentrenamiento provoca overfitting.\n\n\n\n\n\nMostrar ejemplos en Pizarra de manera gráfica. Ejemplos típicos de Excel.\n\n\n\nUnderfitting (Subajuste)\n\n\nSe refiere a cuando un modelo no es capaz de generalizar de manera correcta, pero a diferencia del overfitting no se ha ajustado correctamente a los datos. ¿Cómo se vería el underfitting en nuestro ejemplo?"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-crisp-dm",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Crisp-DM",
    "text": "Etapas del Modelamiento: Crisp-DM"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-kdd",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: KDD",
    "text": "Etapas del Modelamiento: KDD"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-semma",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Semma",
    "text": "Etapas del Modelamiento: Semma"
  },
  {
    "objectID": "tics411/clase-0.html#etapas-del-modelamiento-metodología-propia",
    "href": "tics411/clase-0.html#etapas-del-modelamiento-metodología-propia",
    "title": "TICS-411 Minería de Datos",
    "section": "Etapas del Modelamiento: Metodología Propia",
    "text": "Etapas del Modelamiento: Metodología Propia"
  },
  {
    "objectID": "tics411/clase-8.html#introducción-al-aprendizaje-supervisado",
    "href": "tics411/clase-8.html#introducción-al-aprendizaje-supervisado",
    "title": "TICS-411 Minería de Datos",
    "section": "Introducción al Aprendizaje Supervisado",
    "text": "Introducción al Aprendizaje Supervisado\nLos modelos Predictivos/Supervisados tienen la capacidad de predecir valores en datos no vistos.\n\nChip Huyen, Designing ML Systems (Profesora de Stanford)\n\n\n“Machine Learning Algorithms do not predict the future but encode the past, thus perpetuating the biases in the data and mode…”\n\n\n\nAprenden mediante un proceso de entrenamiento en un train set y evalúan su performance/rendimiento utilizando un test set."
  },
  {
    "objectID": "tics411/clase-8.html#definiciones",
    "href": "tics411/clase-8.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nFeatures\n\n\nTambién llamadas variables o atributos. Corresponden al input del Modelo y con el cuál el modelo aprende y predice. Normalmente es representado mediante una Matriz denominada \\(X\\).\n\n\nLabels o Etiquetas\n\n\nCorresponden a las respuestas que el modelo necesita mapear para poder descubrir patrones de manera automática. Normalmente se representa mediante un vector denominado \\(y\\)."
  },
  {
    "objectID": "tics411/clase-8.html#ejemplo",
    "href": "tics411/clase-8.html#ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nQueremos generar un algoritmo de aprendizaje tal que dado un cierto set de datos predigamos si es que a un niño se le dará o no permiso para jugar.\n\n\n\n\n\n\n\nProblema de Clasificación Binaria (Dos clases opuestas)."
  },
  {
    "objectID": "tics411/clase-8.html#definición-del-problema",
    "href": "tics411/clase-8.html#definición-del-problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Definición del Problema",
    "text": "Definición del Problema\n\\[h_\\theta(X) = f(X, \\theta)\\]\n\n\n\n\n\n\n\nA \\(h_\\theta(\\cdot)\\) la denominaremos hipótesis o simplemente modelo.\n\\(X\\) será nuestro set de features (\\(n\\times m\\) donde \\(n\\) es el número de observaciones y \\(m\\) el número de features).\n\nCada fila de \\(X\\) corresponde a un vector \\(x_i\\) que representa una observación de nuestro set de features.\n\\(\\theta\\) corresponde a los parámetros del modelo (existen modelos paramétricos y no paramétricos).\nCada algoritmo tendrá su propio mapeo \\(f(\\cdot)\\) para tratar de predecir una etiqueta.\n\n\n\n\n\n\n\n\n\n\nTipos de Hipótesis\n\n\n\nSi \\(h_\\theta(X)\\) devuelve valores discretos (o categóricos) hablaremos de un modelo de Clasificación.\nSi \\(h_\\theta(X)\\) devuelve valores continuos hablaremos de un modelo de Regresión."
  },
  {
    "objectID": "tics411/clase-8.html#tipos-de-problemas",
    "href": "tics411/clase-8.html#tipos-de-problemas",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Problemas",
    "text": "Tipos de Problemas\n\n\n\n\n\n\nClasificación:\n\n\n\nBinaria: La Clasificación es dicotómica, Perro o Gato, Sí o No, 1 o 0, Clase Positiva o Negativa.\nMulticlase: La clasificación puede tener más de 2 clases, pero sólo una es posible.\n\nEj: Perro, Gato o Canario; 0, 1, 2, 3, 4.\n\nMultilabel: La clasificación puede tener más de 2 clases, y más de una es posible a la vez.\n\nEj: Categorías de Libro: Puede ser Romance y Drama, Películas: Fantasía, Animación y Acción.\n\n\n\n\n\n\n\n\n\n\n\nRegresión:\n\n\n\nSimple: Predigo sólo un valor. Ej: Predecir la Temperatura.\nMultiple: Predigo varios valores continuos a la vez.\n\nEj: Modelo para intentar estimar Temperatura y Humedad a la vez.\n\nForecast: Donde se utilizan valores pasados para estimar valores futuros.\n\nDadas mis ganancias pasadas, estimar las futuras."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-intuición",
    "href": "tics411/clase-8.html#clasificación-intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Intuición",
    "text": "Clasificación: Intuición\n\n\n\n\n\n\nSupongamos el siguiente problema de clasificación. Tenemos un algoritmo, que dadas las variables Largo y Peso sean capaces de predecir si es que un Pez es una Reineta o una Sardina."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-intuición-1",
    "href": "tics411/clase-8.html#clasificación-intuición-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Intuición",
    "text": "Clasificación: Intuición\n\n\n\n\n\n\n\n\n\n\n\n\nQueremos encontrar una Regla de Decisión (Decision Rule) que permita clasificar correctamente un punto nuevo.\nDistintos modelos son capaces de encontrar distintas reglas de decisión. Por lo tanto, sus predicciones pueden ser completamente distintas."
  },
  {
    "objectID": "tics411/clase-8.html#clasificación-detalles",
    "href": "tics411/clase-8.html#clasificación-detalles",
    "title": "TICS-411 Minería de Datos",
    "section": "Clasificación: Detalles",
    "text": "Clasificación: Detalles\nEs importante mencionar que un modelo de clasificación puede generar:\n\nHard Predictions: Es decir, la instancia a predecir es clase 0 o clase 1.\nSoft Prediction: Es decir, la instancia a predecir tiene una probabilidad \\(p\\) de pertenecer a la clase 1 y de \\(1-p\\) de pertenecer a la clase 0.\n\n\n\n\n\n\n\n\nCuando se hace predicción binaria, lo común es usar un Threshold de 0.5 para elegir la clase. Es decir si \\(p&lt;0\\) entonces clase 0, si \\(p \\ge 0.5\\) entonces clase 1.\n\n\n\n\n\n\n\n\n\n\nEn el caso de predicción multiclase o multilabel. Se calcula la probabilidad para cada clase. Por lo tanto se se asigna la clase de mayor probabilidad."
  },
  {
    "objectID": "tics411/clase-8.html#k-nearest-neighbors",
    "href": "tics411/clase-8.html#k-nearest-neighbors",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\n\nEl modelo de vecinos más cercanos, o KNN por sus siglas en Inglés es un modelo basado en distancias. Su regla de decisión se basa en imitar el comportamiento de sus \\(K\\) vecinos más cercanos por votación (para clasificación) o la media (para regresión).\n\n\n\n\n\n\n\nK es un hiperparámetro de este modelo.\n\n\n\n\n\n\n\n\n\n\n\n\nSupongamos \\(K = 3\\).\nEs decir, tomaremos los 3 vecinos más cercanos.\n\n\n\n\n\n\n\nEn general es una buena idea elegir vecinos impares. ¿Por qué?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-1-training-time",
    "href": "tics411/clase-8.html#knn-paso-1-training-time",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Paso 1 (Training Time)",
    "text": "KNN: Paso 1 (Training Time)\n\nTraining Time\n\nCorresponde al periodo donde el modelo aprende de los datos. Toma un patrón y ese modelo es utilizado para predecir.\n\n\n\n\n\n\n\n\nEn el caso de un KNN NO HAY APRENDIZAJE en esta etapa.\n\n\n\n\n\n\n\n\n\nEs considerado un modelo no-paramétrico ya que no aprende parámetros para realizar su predicción."
  },
  {
    "objectID": "tics411/clase-8.html#knn-paso-2-test-time",
    "href": "tics411/clase-8.html#knn-paso-2-test-time",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Paso 2 (Test Time)",
    "text": "KNN: Paso 2 (Test Time)\n\nInference Time\n\nCorresponde al periodo donde el modelo debe emitir una predicción.\n\n\nEn este caso, KNN calcula las distancias del punto a predecir (en verde) a todos los otros puntos existentes (proceso caro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa predicción corresponderá a la etiqueta mayoritaria por votacioń\n\n\n\n\n\n\n\n\nLa predicción corresponderá a la etiqueta mayoritaria por votacioń.\n¿Cuál sería una buena estrategia de predicción para un modelo de Regresión?"
  },
  {
    "objectID": "tics411/clase-8.html#fronteras-de-decisión",
    "href": "tics411/clase-8.html#fronteras-de-decisión",
    "title": "TICS-411 Minería de Datos",
    "section": "Fronteras de Decisión",
    "text": "Fronteras de Decisión\n\n\n\n\n\n\n\n\n\n\n\n\nImplicitamente, todo modelo de Machine Learning generará lo que se llama una Frontera de Decisión.\nSi un punto no visto cae dentro de su frontera entonces se le asigna dicha etiqueta."
  },
  {
    "objectID": "tics411/clase-8.html#implementación-clasificación-en-scikit-learn",
    "href": "tics411/clase-8.html#implementación-clasificación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación Clasificación en Scikit-Learn",
    "text": "Implementación Clasificación en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClasifier(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicción...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) número de vecinos a utilizar. Por defecto 5.\nmetric: Métrica de distancia. Por defecto “Minkowski”.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un parámetro interno para poder paralelizar los cálculos. Se recomienda utilizar -1 para utilizar todos sus cores."
  },
  {
    "objectID": "tics411/clase-8.html#implementación-regresión-en-scikit-learn",
    "href": "tics411/clase-8.html#implementación-regresión-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación Regresión en Scikit-Learn",
    "text": "Implementación Regresión en Scikit-Learn\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn_clf = KNeighborsRegressor(n_neighbors = 5, metric=\"minkowski\", p=2, n_jobs=-1)\nknn_clf.fit(X, y)\n\n# Predicción...\ny_pred = knn_clf.predict(X)\n\n\nn_neighbors: \\(K\\) número de vecinos a utilizar. Por defecto 5.\nmetric: Métrica de distancia. Por defecto “Minkowski”.\np: Potencia de Minkowski: \\(p=1\\), Manhattan, \\(p=2\\) Euclideana. Por defecto \\(p=2\\).\nn_jobs: Corresponde a un parámetro interno para poder paralelizar los cálculos. Se recomienda utilizar -1 para utilizar todos sus cores.\n\n\n\n\n\n\n\n\n¿Cómo se encuentran las predicciones en un modelo de Regresión?"
  },
  {
    "objectID": "tics411/clase-8.html#knn-detalles-técnicos",
    "href": "tics411/clase-8.html#knn-detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "KNN: Detalles Técnicos",
    "text": "KNN: Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nModelo muy simple de implementar y entender.\nMuy eficiente en el aprendizaje.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nInferencia ineficiente: \\(O(mn^2)\\).\nCurse of Dimensionality: A medida que el número de dimensiones del problema crece, se requiere un incremento exponencial en la cantidad de datos para asegurar que existen suficientes vecinos cercanos para cualquier punto."
  },
  {
    "objectID": "tics411/clase-13.html#definición",
    "href": "tics411/clase-13.html#definición",
    "title": "TICS-411 Minería de Datos",
    "section": "Definición",
    "text": "Definición\n\nAnomalías\n\n\nConjunto de puntos que son considerablemente diferentes al resto.\n\n\n\n\n\n\n\n\n\nPor definición las Anomalías son relativamente raras. * Pueden ocurrir en proporciones extremadamente bajas en los datos. Ej: 1 entre mil. * El contexto es importante. Ej: Temperaturas bajo cero en Verano.\n\n\n\nEjemplos:\n\nTelecomunicaciones: Detección de Abusos de Roaming.\nBanca: Compras/Ventas inusualmente elevados.\nFinanzas y Seguros: Detectar y prevenir patrones de gastos fraudulentos.\nMantención: Predicción de comportamiento irregular/fallas.\nSmart Homes: Detecciones de fugas de Energía\netc."
  },
  {
    "objectID": "tics411/clase-13.html#más-ejemplos",
    "href": "tics411/clase-13.html#más-ejemplos",
    "title": "TICS-411 Minería de Datos",
    "section": "Más ejemplos",
    "text": "Más ejemplos\n\n\n\n\n\n\nLa definición de Anomalía es altamente subjetiva y depende mucho del Dominio en el cuál se está trabajando."
  },
  {
    "objectID": "tics411/clase-13.html#tipos-de-anomalías-series-de-tiempo",
    "href": "tics411/clase-13.html#tipos-de-anomalías-series-de-tiempo",
    "title": "TICS-411 Minería de Datos",
    "section": "Tipos de Anomalías (Series de Tiempo)",
    "text": "Tipos de Anomalías (Series de Tiempo)"
  },
  {
    "objectID": "tics411/clase-13.html#desafíos",
    "href": "tics411/clase-13.html#desafíos",
    "title": "TICS-411 Minería de Datos",
    "section": "Desafíos",
    "text": "Desafíos\n\n\n\n\n\n\nDesafíos\n\n\n\n¿Cuántos Atributos/Variables usamos para definir un Outlier?\n¿Cuántos Outliers existen?\nEste tipo de problema suele ser complicado de Etiquetar, por lo que es difícil resolverlo como un problema supervisado.\nPuede ser como “Encontrar una aguja en un pajar”."
  },
  {
    "objectID": "tics411/clase-13.html#enfoques",
    "href": "tics411/clase-13.html#enfoques",
    "title": "TICS-411 Minería de Datos",
    "section": "Enfoques",
    "text": "Enfoques"
  },
  {
    "objectID": "tics411/clase-13.html#técnicas-visuales",
    "href": "tics411/clase-13.html#técnicas-visuales",
    "title": "TICS-411 Minería de Datos",
    "section": "Técnicas Visuales",
    "text": "Técnicas Visuales\n\n\n\n\n\n\nEstas técnicas son muy subjetivas ya que dependen del criterio/apreciación del usuario.\n\n\n\n\n\nBox Plots\n\n\n\n\n\n\nScatter Plots"
  },
  {
    "objectID": "tics411/clase-13.html#técnicas-estadísticas-test-de-grubbs",
    "href": "tics411/clase-13.html#técnicas-estadísticas-test-de-grubbs",
    "title": "TICS-411 Minería de Datos",
    "section": "Técnicas Estadísticas: Test de Grubbs",
    "text": "Técnicas Estadísticas: Test de Grubbs\n\n\n\n\n\n\nEl test de Grubbs detecta si algún dato es un outlier sobre una variable asumiendo que se distribuyen de manera normal.\n\n\n\n\\[G = \\frac{\\underset{i = 1,2,...,n}{max}|x_i - \\bar{X}|}{S_x}\\]\ndonde \\(\\bar{X}\\) y \\(S_x\\) corresponden a la media y Desviación Estándar Muestral.\n\nEso implica que \\(G\\) se distribuye como una t-student de \\(n-2\\) grados de libertad, por lo tanto si:\n\n\\[ G_{critico} = \\frac{n-1}{\\sqrt{n}}\\sqrt{\\frac{t^2_{(\\alpha/n, n-2)}}{n-2+t_{(\\alpha/n, n-2)^2}}}\\]\n\n\n\n\n\n\nSi \\(G &gt; G_{critico}\\), \\(x_i\\) es considerado un outlier con una significancia \\(\\alpha/n\\) para una t-student con \\(n-2\\) grados de libertad."
  },
  {
    "objectID": "tics411/clase-13.html#test-de-grubs-en-python",
    "href": "tics411/clase-13.html#test-de-grubs-en-python",
    "title": "TICS-411 Minería de Datos",
    "section": "Test de Grubs en Python",
    "text": "Test de Grubs en Python\n\n\n\n\n\n\nEste código debiera entregar una lista de todos los puntos que son considerados outliers.\n\n\n\nfrom scipy import stats\nimport numpy as np\n\nn = 16 # Número de Datos\nalpha = 0.05 # nivel de confianza\n\nt_crit = stats.t.ppf(1-alpha/n, n-2)\nG_crit = (n-1)/np.sqrt(n)*np.sqrt(t_crit**2/(n-2 + t_crit**2))\n\ndata = np.array([5,14,15,15,19,17,16,20,22,8,21,28,11,9,29,40])\nG_test = np.abs(data-np.mean(data)/np.std(data))\n\ntest_grubbs = np.where(G_test&gt;G_crit)\nprint(f\"Outliers: {data[test_grubbs]}\")"
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado",
    "href": "tics411/clase-13.html#caso-multivariado",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\n\n\n\n\nLa idea es calcular la distancia de cada punto al centro tomando en consideración la covarianza."
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-1",
    "href": "tics411/clase-13.html#caso-multivariado-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado",
    "text": "Caso Multivariado\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 1\n\n\n\nCalcular el punto central de todos los puntos (Promedio) \\[\\mu = (3.16, 3.16)\\]\n\n\n\n\n\n\n\n\n\n\n\nPaso 2\n\n\n\nCalcular la Inversa de la Matriz de Covarianza:"
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-continuación",
    "href": "tics411/clase-13.html#caso-multivariado-continuación",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado: Continuación",
    "text": "Caso Multivariado: Continuación\n\n\n\n\n\n\nPaso 3\n\n\nCalcular la distancia de cada punto con respecto a la media y la inversa de la Covarianza.\n\n\n\n\\[d_i = (p_i - \\mu)^T \\Sigma^{-1}(p_i - \\mu)\\]\n\\[d_1 = (p_1 - \\mu)^T \\sigma^{-1}(p_1 - \\mu)\\]\n\\[d_1 = ([0,0] - [3.16, 3.16])^T \\begin{bmatrix}\n                            0.147 & -0.147  \\\\\n                            -0.147 & 1.911  \\\\\n                            \\end{bmatrix}\n                            ([0,0] - [3.16, 3.16])\\]\n\n\n\n\n\n\nSe debe repetir este procedimiento para cada punto."
  },
  {
    "objectID": "tics411/clase-13.html#caso-multivariado-continuación-1",
    "href": "tics411/clase-13.html#caso-multivariado-continuación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Caso Multivariado: Continuación",
    "text": "Caso Multivariado: Continuación\n\n\n\n\n\n\nPaso 4:\n\n\nSe debe calcular el punto crítico según t-student con 95% confianza, y orden de magnitud \\(m\\) dimensiones.\n\n\n\n\\[t_{(\\alpha = 0.95,2)} = 5.99\\]\n\n\n\n\n\n\nPaso 5\n\n\nComparar, Si \\(d_i&gt;t_{crit}\\) entonces \\(d_i\\) es Outlier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso ningún valor de \\(d_i\\) es mayor al $t_{(crit)}, por lo tanto, no hay outliers."
  },
  {
    "objectID": "tics411/clase-13.html#distancia-de-mahalanobis",
    "href": "tics411/clase-13.html#distancia-de-mahalanobis",
    "title": "TICS-411 Minería de Datos",
    "section": "Distancia de Mahalanobis",
    "text": "Distancia de Mahalanobis\nLa distancia de Mahalanobis corresponde a:\n\\[d_i = \\sqrt{(p_i - \\mu)^T \\Sigma^{-1}(p_i - \\mu)}\\]\n\n\n\n\n\n\nSe puede repetir el mismo procedimiento anterior, sólo que se define una Distancia de Mahalonobis umbral. Las que superen dicho umbral son considerados como Outliers."
  },
  {
    "objectID": "tics411/clase-13.html#dbscan",
    "href": "tics411/clase-13.html#dbscan",
    "title": "TICS-411 Minería de Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\nPodemos utilizar el procedimiento que aprendimos de DBSCAN. Todos los puntos Noise serán considerados como Anomalías.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEste punto corresponde a un Noise Point. Lo cuál en nuestro caso particular se considerará una Anomalía."
  },
  {
    "objectID": "tics411/clase-13.html#k-nearest-neighbor",
    "href": "tics411/clase-13.html#k-nearest-neighbor",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbor",
    "text": "K-Nearest Neighbor\n\n\n\n\n\n\nSe puede utilizar los modelos de vecinos más cercanos para determinar outliers siguiendo el siguiente procedimiento:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaso 1: Definir el valor de \\(k\\) para encontrar los vecinos más cercanos.\nEj: Sea \\(k=3\\)\nPaso 2: Calcular la Matriz de Distancias y determinar los vecinos más cercanos."
  },
  {
    "objectID": "tics411/clase-13.html#k-nearest-neighbor-continuación",
    "href": "tics411/clase-13.html#k-nearest-neighbor-continuación",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Nearest Neighbor: Continuación",
    "text": "K-Nearest Neighbor: Continuación\n\n\n\nPaso 3: Calcular la distancias Promedio.\n\n\n\n\n\n\nPaso 4: Escoger un Umbral. Si es que la distancia es mayor al Umbral entonces es un Outlier. Ej: \\(Dist_crit: 3\\)"
  },
  {
    "objectID": "tics411/clase-13.html#local-outlier-factor-lof",
    "href": "tics411/clase-13.html#local-outlier-factor-lof",
    "title": "TICS-411 Minería de Datos",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\n\nLocal Outlier Factor (LOF) detecta anomalías con sus vecindarios locales, en lugar de la distribución glocal de los datos.\n\n\n\n\n\n\n\nEn la Figura, \\(O1\\) y \\(O2\\) son anomalías locales en comparación con \\(C1\\), \\(O3\\) es una anomalía global, y \\(O4\\) no es una anomalía."
  },
  {
    "objectID": "tics411/clase-13.html#algoritmo",
    "href": "tics411/clase-13.html#algoritmo",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo",
    "text": "Algoritmo\n\nDeterminar \\(N(x,k)\\), los k-vecinos más cercanos de cada punto x.\nPara todo punto \\(y\\), calcular la distancia a su k-ésimo vecino más cercano.\nCalcular la reach-distance entre todos los puntos: \\[reach-distance_k(x,y) = max\\{k-distance(y), d(x,y)\\}\\]\nCalcule la densidad del vecindario local sobre sus \\(k\\) vecinos, donde \\(|N(x,k)| = k\\).\n\n\\[density(x,k) = lrd_k(x) = \\left(\\frac{\\sum_{y \\in N(x,k)} reach-distance_k(x,y)}{|N(x,k)|}\\right)^{-1}\\]\n\nCalcule el Local Outlier Factor para el punto x como la proporción de la densidad de sus \\(k\\) vecinos más cercanos, con respecto a la densidad del punto \\(x\\).\n\n\n\n\\[ LOF(x) = \\frac{\\sum_{y \\in N(x,k) density(y,k)}}{|N(x,k)|density(x,k)} \\]\n\n\n\n\n\n\n\nLOF(X) &gt;&gt; 1 implica anomalía."
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor",
    "text": "Ejemplo Local Outlier Factor\n\n\n\nConsideremos los siguientes 4 puntos de datos: a(0,0), b(0,1), c(1,1), d(3,0). Calcular el LOF para cada punto y mostrar la anomalía principal.\n\n\n\n\n\n\n\n\n\nUtilizar \\(K = 2\\) y Distancia Manhattan.\n\n\n\n\n\nPaso 1: Calcular Distancias\n\ndist(a,b) = 1\ndist(a,c) = 2\ndist(a,d) = 3\ndist(b,c) = 1\ndist(b,d) = 4\ndist(c,d) = 3"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 2: Para todo punto \\(y\\), calcule la distancia a su k-ésimo vecino más cercano.\n\n\\(dist_2(a) = dist(a,c) = 2\\) (c es el 2do vecino más cercano)\n\\(dist_2(b) = dist(b,a) = 1\\) (a/c es el 2do vecino más cercano)\n\\(dist_2(c) = dist(c,a) = 2\\) (a es el 2do vecino más cercano)\n\\(dist_2(d) = dist(d,a) = 3\\) (a/c es el 2do vecino más cercano)"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-1",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 3: Calcular la reach-distance entre todos los puntos, es decir, los puntos vecindarios a una distancia k.\n\n\n\n\\(N_k(o)\\): Vecindario de \\(k\\)-distancia de \\(o\\), \\(N_k(o)=\\{o'\\|o' \\in D, dist(o,o') \\le dist_k(o)\\}\\)\n\n\n\n\n\\(N_2(a) = \\{b,c\\}\\)\n\\(N_2(b) = \\{a,c\\}\\)\n\\(N_2(c) = \\{b,a\\}\\)\n\\(N_2(d) = \\{a,c\\}\\)"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-2",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\n\n\nPaso 4: Calcular la densidad del vecinadario local sobre sus \\(k\\) vecinos.\n\n\n\n\n\n\n\n\n\\(reach-dist_2(b \\leftarrow a) = max\\{dist_2(b), dist(b,a)\\} = max\\{1,1\\} = 1\\)\n\\(reach-dist_2(c \\leftarrow a) = max\\{dist_2(c), dist(c,a)\\} = max\\{2,2\\} = 2\\)\n\n\n\n\n\n\n\nCalcular el resto de manera análoga."
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-3",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nEntonces, \\(lrd_k(o)\\): Densidad de alcanzabilidad local de \\(o\\).\n\\[lrd_2(a) = \\frac{|\\mathcal{N}(a)|}{reach-dist_2(b\\leftarrow a) + reach-dist_2(c \\leftarrow a)} = \\frac{2}{1 + 2} = 0.667\\] \\[lrd_2(b) = \\frac{|\\mathcal{N}(b)|}{reach-dist_2(a\\leftarrow b) + reach-dist_2(b \\leftarrow b)} = \\frac{2}{2 + 2} = 0.5\\] \\[lrd_2(c) = \\frac{|\\mathcal{N}(c)|}{reach-dist_2(b\\leftarrow c) + reach-dist_2(a \\leftarrow c)} = \\frac{2}{1 + 2} = 0.667\\] \\[lrd_2(d) = \\frac{|\\mathcal{N}(d)|}{reach-dist_2(a\\leftarrow d) + reach-dist_2(c \\leftarrow d)} = \\frac{2}{1 + 2} = 0.33\\]"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-4",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-4",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 5: Calcular el Local Outlier Factor para el punto \\(x\\) como la proporción de la densidad de sus \\(k\\) vecinos más cercanos, con respecto a la densidad del punto \\(x\\).\n\n\n\\[LOF(x) = \\frac{\\sum_{y \\in N(x,k)} density(y,k)}{|N(x,k)|density(x,k)}\\]\n\\[LOF_2(a) = \\frac{lrd_2(b) + lrd_2(c)}{N_2(a) \\cdot lrd_2(a)} = \\frac{0.5 + 0.667}{2 \\cdot 0.667} = 0.87\\] \\[LOF_2(b) = \\frac{lrd_2(a) + lrd_2(c)}{N_2(b) \\cdot lrd_2(b)} = \\frac{0.667 + 0.667}{2 \\cdot 0.5} = 1.334\\] \\[LOF_2(c) = \\frac{lrd_2(b) + lrd_2(a)}{N_2(c) \\cdot lrd_2(c)} = \\frac{0.5 + 0.667}{2 \\cdot 0.667} = 0.87\\] \\[LOF_2(d) = \\frac{lrd_2(a) + lrd_2(c)}{N_2(d) \\cdot lrd_2(d)} = \\frac{0.667 + 0.667}{2 \\cdot 0.33} = 2\\]"
  },
  {
    "objectID": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-5",
    "href": "tics411/clase-13.html#ejemplo-local-outlier-factor-continuación-5",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Local Outlier Factor: Continuación",
    "text": "Ejemplo Local Outlier Factor: Continuación\nPaso 6: Ordena todas las LOF_k(o)\n\nLOF_2(d) = 2 \\(\\implies\\) el punto paraecer una anomalía (LOF &gt;&gt; 1)\nLOF_2(b) = 1.334\nLOF_2(a) = 0.87\nLOF_2(c) = 0.87\n\n\n\n\n\n\n\nDetalles Técnicos\n\n\n\nDado que esto sigue un enfoque local, la resolución depende de la elección del usuario para \\(k\\).\nGenera una puntuación (Anomaly Score) para cada punto.\nComo \\(LOF\\) es una razón, es difícil de interpretar. No existe un valor umbral específico por encima del cual un punto se define como un valor atípico. La identificación de un valor atípico depende del problema y del usuario\nComo \\(LOF\\) es una razón, es difícil de interpretar. No existe un valor umbral específico por encima del cual un punto se define como un valor atípico. La identificación de un valor atípico depende del problema y del usuario."
  },
  {
    "objectID": "tics411/clase-11.html#naive-bayes-preliminares",
    "href": "tics411/clase-11.html#naive-bayes-preliminares",
    "title": "TICS-411 Minería de Datos",
    "section": "Naive Bayes: Preliminares",
    "text": "Naive Bayes: Preliminares\n\nTambién conocido como Clasificador Inexperto de Bayes, es uno de los clasificadores más conocidos y sencillos.\n\n\n\n\n\n\n\nSe hizo particularmente conocido como uno de los primeros algoritmos en funcionar como Clasificador de Spam de manera efectiva.\n\n\n\n\nEs un modelo netamente probabilístico basado en el Teorema de Bayes.\n\nAprende una distribucional de Probabilidad Condicional.\nDado un punto \\(x_i\\), el modelo retorna la “probabilidad” de que \\(x_i\\) pertenezca a una clase específica."
  },
  {
    "objectID": "tics411/clase-11.html#definiciones",
    "href": "tics411/clase-11.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\nProbabilidad Condicional\n\n\\[P(X|C) = \\frac{P(X \\cap C)}{P(C)}\\]\n\nTeorema de Bayes\n\n\\[P(C|X) = \\frac{P(X|C)P(C)}{P(X)}\\]\n\nIndependencia Condicional\n\n\\[P(X_1, X_2, ..., X_k|C) = \\prod_{i=1}^k P(X_i|C)\\]\n\n\n\n\n\n\n\n\n\n\nSe lee como la Probabilidad de que Ocurra \\(X\\) dado que tenemos \\(C\\).\n\n\n\n\n\n\n\n\n\n\nLa probabilidad a posteriori (LHS), depende de el Likelihood, la probabilidad a priori y la evidencia (RHS).\n\n\n\n\n\n\n\n\n\n\nSi asumimos independencia, entonces la probabilidad conjunta de \\(k\\) eventos condicionados, se calcula como la productoria de las probabilidades condicionales independientes."
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo-básico",
    "href": "tics411/clase-11.html#ejemplo-básico",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo básico",
    "text": "Ejemplo básico\n\n\n\nSupongamos que:\n\nSabemos que la Meningitis produce Tortícolis el 50% de las veces.\nLa probabilidad de tener meningitis es: \\(1/50000\\).\nLa probabilidad de tener Tortícolis: \\(1/20\\).\n\n\n\n\nSi su paciente tiene tortícolis, ¿Cuál es la probabilidad de que tenga Meningitis?\n\\[P(M|T) = \\frac{P(T|M)P(M)}{P(T)}=\\frac{0.5 \\cdot 1/50000}{1/20} = 0.0002\\]"
  },
  {
    "objectID": "tics411/clase-11.html#modelo-naive-bayes-aprendizaje",
    "href": "tics411/clase-11.html#modelo-naive-bayes-aprendizaje",
    "title": "TICS-411 Minería de Datos",
    "section": "Modelo Naive Bayes: Aprendizaje",
    "text": "Modelo Naive Bayes: Aprendizaje\n\\[P(y = C_j|X_1, X_2, ..., X_k) = \\frac{P(X_1,X_2,..., X_k|y=C_j)P(y=C_j)}{P(X_1, X_2, ..., X_k)}\\]\n\n\n\n\\(P(y=C_j|X)\\) sería la probabilidad de que la predicción del modelo sea \\(C_j\\) dado que lo alimentamos con las variables \\(X\\).\nLuego \\(P(y=C_j)\\) es la probabilidad a priori de que la clase sea \\(C_j\\).\n\\(P(X|y=C_j)\\) es el likelihood (verosimilitud). Corresponde a la distribución de probabilidad de las variables X cuando la clase es \\(C_j\\).\n\\(P(X)\\) es la evidencia, y normalmente es muy complejo de calcular.\n\n\n\n\n\n\n\n\nPor simplicidad reduciremos \\(X_1, X_2, ..., X_k\\) a \\(X\\).\n\n\n\n\n\n\n\n\n\n\\(P(X)\\) tiene como única función la de normalizar la probabilidad para que vaya en un rango entre 0 y 1."
  },
  {
    "objectID": "tics411/clase-11.html#modelo-naive-bayes-predicción",
    "href": "tics411/clase-11.html#modelo-naive-bayes-predicción",
    "title": "TICS-411 Minería de Datos",
    "section": "Modelo Naive Bayes: Predicción",
    "text": "Modelo Naive Bayes: Predicción\n\\[\\hat{y_i} = \\underset{C_j}{argmax} \\: P(y=C_j|X) \\]\ndonde, \\[P(y = C_j|X) \\propto \\prod_{i=1}^k P(X|y=C_j)P(y=C_j)\\]\n\n\n\n\n\n\nLa predicción de Naive Bayes corresponde a la clase que entrega [un estimado de] la Probabilidad a Posteriori más grande."
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo",
    "href": "tics411/clase-11.html#ejemplo",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo clasificamos el siguiente punto?\n\\[ X = [C=Soleado,T=Media,H=Alta,V=Débil]\\]\n\n\n\n\n\n\nProbabilidad de Sí\n\n\n\\[P(y = Sí|X) = P(X|y=Sí)P(y=Sí)\\]\n\n\n\n\n\n\nProbabilidad de No\n\n\n\\[ P(y = No|X) = P(X|y=No)P(y=No)\\]"
  },
  {
    "objectID": "tics411/clase-11.html#ejemplo-1",
    "href": "tics411/clase-11.html#ejemplo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\n\\[ P(y = Sí|X) = P(C=Soleado|y=Sí)P(T=Media|y=Sí)P(H=Alta|y=Sí)P(V=Débil|y=Sí)P(y=Sí)\\]\n\n\n\n\n\n\n\\[ P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=Débil|y=No)P(y=No)\\]\n\n\n\n\n\nProbabilidad Condicional para clase Sí\n\\[\\small P(C = Soleado|y = Sí) = 2/9\\] \\[\\small P(T = Media|y = Sí) = 4/9\\]\n\\[\\small P(H = Alta|y = Sí) = 3/9\\] \\[\\small P(V=Débil|y = Sí) = 6/9\\]\n\nProbabilidad Condicional para clase No\n\\[\\small P(C = Soleado|y = No) = 3/5\\] \\[\\small P(T = Media|y = No) = 2/5\\]\n\\[\\small P(H = Alta|y = No) = 4/5\\] \\[\\small P(V=Débil|y = No) = 2/5\\]\n\nProbabilidad a priori\n\\[P(y = Sí) = \\frac{9}{14} = 0.642\\] \\[P(y = No) = \\frac{5}{14} = 0.357\\]"
  },
  {
    "objectID": "tics411/clase-11.html#predicción",
    "href": "tics411/clase-11.html#predicción",
    "title": "TICS-411 Minería de Datos",
    "section": "Predicción",
    "text": "Predicción\n\n\n\n\\[\\scriptsize P(y = Sí|X) = P(C=Soleado|y=Sí)P(T=Media|y=Sí)P(H=Alta|y=Sí)P(V=Débil|y=Sí)P(y=Sí)\\] \\[\\small P(y = Sí|X) = \\frac{2}{9} \\cdot \\frac{4}{9} \\cdot \\frac{3}{9} \\cdot \\frac{6}{9} \\cdot \\frac{9}{14} = 0.0141\\]\n\n\n\n\n\n\n\\[\\scriptsize P(y = No|X) = P(C=Soleado|y=No)P(T=Media|y=No)P(H=Alta|y=No)P(V=Débil|y=No)P(y=No)\\] \\[\\small P(y = No|X) = \\frac{3}{5} \\cdot \\frac{2}{5} \\cdot \\frac{4}{5} \\cdot \\frac{2}{5} \\cdot \\frac{5}{14} = 0.0274\\]\n\n\n\n\n\n\n\n\n\n\\[\\hat{y} = argmax \\{0.0141, 0.0274\\} = No\\]"
  },
  {
    "objectID": "tics411/clase-11.html#smoothing",
    "href": "tics411/clase-11.html#smoothing",
    "title": "TICS-411 Minería de Datos",
    "section": "Smoothing",
    "text": "Smoothing\nSupongamos otro dataset más pequeño:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDado que Naive Bayes se calcula como una Productoria, al tener probabilidades 0 inmediatamente la Probabilidad a Posteriori es 0.\n\n\n\n\\[ P(Clima = Soleado|y = Sí) = \\frac{0}{6}\\] \\[ P(Clima = Soleado|y = No) = \\frac{5}{8}\\]\n\n\\[P(X_j|C = i) = \\frac{N_{yj} + \\alpha}{N_y + M\\alpha}\\]\n\n\\(\\alpha\\): Es un Hiperparámetro. Si \\(\\alpha = 1\\) se le llama Laplace Smoothing, si \\(\\alpha &lt;1\\) entonces se le llama Lidstone Smoothing.\nM: Corresponde al número de posibles valores que puede tomar \\(X_j\\)\n\\(N_{yj}\\): Corresponde a la cantidad de registros que toman el valor de la variable \\(X_j\\) solicitado en la clase \\(y\\).\n\\(N_{y}\\): Corresponde a la cantidad de registros totales que tienen la clase \\(y\\)."
  },
  {
    "objectID": "tics411/clase-11.html#laplace-smoothing",
    "href": "tics411/clase-11.html#laplace-smoothing",
    "title": "TICS-411 Minería de Datos",
    "section": "Laplace Smoothing",
    "text": "Laplace Smoothing\n\n\nSin Laplace\n\n\n\n\n\n\nCon Laplace\n\n\n\n\n\n\n\n\n\n\n\n\nEn este caso \\(\\alpha = 1\\) y \\(M=3\\) ya que Clima puede tomar 3 valores: Soleado, Cubierto y Lluvia."
  },
  {
    "objectID": "tics411/clase-11.html#variables-continuas",
    "href": "tics411/clase-11.html#variables-continuas",
    "title": "TICS-411 Minería de Datos",
    "section": "Variables Continuas",
    "text": "Variables Continuas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos calcular el Likelihood como una PDF (Probability Density Function). La más común: Distribución Normal (Gaussian Naive Bayes).\n\n\n\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]"
  },
  {
    "objectID": "tics411/clase-11.html#variables-continuas-predicción",
    "href": "tics411/clase-11.html#variables-continuas-predicción",
    "title": "TICS-411 Minería de Datos",
    "section": "Variables Continuas: Predicción",
    "text": "Variables Continuas: Predicción\n\n\n\n\\[P(humedad=74|y = Sí) = \\frac{1}{\\sqrt{2\\pi \\cdot 10.2^2}}e^{-\\frac{(74-79.1)^2}{2\\cdot 10.2^2}} = 0.0345 \\]\n\n\n\n\n\n\n\\[P(humedad=74|y = No) = \\frac{1}{\\sqrt{2\\pi \\cdot 9.7^2}}e^{-\\frac{(74-86.2)^2}{2\\cdot 9.7^2}} = 0.01865 \\]\n\n\n\n\n\n\n\n\n\nLuego la predicción es Sí."
  },
  {
    "objectID": "tics411/clase-11.html#detalles-técnicos",
    "href": "tics411/clase-11.html#detalles-técnicos",
    "title": "TICS-411 Minería de Datos",
    "section": "Detalles Técnicos",
    "text": "Detalles Técnicos\n\n\n\n\n\n\nFortalezas\n\n\n\nFácil de Implementar\nA menudo tiene un rendimiento decente a pesar de que las variables pueden no ser independientes.\nPuede aprender de forma incremental.\nValores faltantes son ignorados en el proceso de Aprendizaje.\nModelo robusto frente a datos atípicos y/o irrelevantes.\n\n\n\n\n\n\n\n\n\n\nDebilidades\n\n\n\nAsumir clases condicionadas produce probabilidades sesgadas.\nDependencias entre las variables no pueden ser modeladas."
  },
  {
    "objectID": "tics411/clase-11.html#implementación-en-scikit-learn",
    "href": "tics411/clase-11.html#implementación-en-scikit-learn",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Scikit-Learn",
    "text": "Implementación en Scikit-Learn\nMultinomial Naive Bayes (Normal)\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB(alpha = 1)\nnb.fit(X_train, y_train)\n\ny_pred = nb.predict(X_test)\ny_proba = nb.predict_proba(X_test)\nGaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngb = GaussianNB()\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\ny_proba = gb.predict_proba(X_test)"
  },
  {
    "objectID": "tics411/clase-9.html#intuición",
    "href": "tics411/clase-9.html#intuición",
    "title": "TICS-411 Minería de Datos",
    "section": "Intuición",
    "text": "Intuición\nSupongamos que tengo que estudiar para la prueba de Minería de Datos y tengo que aprender a calcular el Coeficiente de Silueta.\n\n\nQué pasa si sólo les entrego una pregunta para estudiar y no tiene respuesta.\n¿Qué pasa si ahora les doy la respuesta?\n¿Qué pasa si te doy más ejercicios?\n¿Qué pasa luego de que haces muchos ejercicios?\n\n\n\n\n\n\n\n\n\nVoy aprendiendo mejor la tarea de calcular el coeficiente de Silueta. Lo mismo pasa con los modelos.\n\n\n\n\n\n\n\n\n\n\n\nPero no puedo medir qué tan bien aprendiste en los ejercicios que yo ya entregué para practicar. Tengo que hacer una prueba que tú no hayas visto, para ver si realmente aprendiste."
  },
  {
    "objectID": "tics411/clase-9.html#uso-de-un-modelo",
    "href": "tics411/clase-9.html#uso-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Uso de un Modelo",
    "text": "Uso de un Modelo\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo saber que el modelo está funcionando como esperamos?"
  },
  {
    "objectID": "tics411/clase-9.html#métricas",
    "href": "tics411/clase-9.html#métricas",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas",
    "text": "Métricas\nEl Rendimieto de un Modelo de Clasificación permite evaluar el error asociado al proceso de predicción.\n\n\n\n\nClase Positiva\n\nCorresponde a la clase/evento de interés. Ej: Tiene cancer, va a pagar su deuda, es un gato. Normalmente se denota como la Clase 1.\n\nClase Negativa\n\nCorresponde a la clase/evento contrario al de interés. Ej: No tiene cancer, no va a pagar su deuda, no es un gato. Normalmente se denota como la Clase 0.\n\n\n\n\n\n\n\n\n\n\n\nScikit-Learn usa la siguiente convención:\n\nSi se llama *_score un mayor puntaje es mejor.\nSi se llama *_error o *_loss un mejor puntaje es mejor."
  },
  {
    "objectID": "tics411/clase-9.html#métricas-matriz-de-confusión",
    "href": "tics411/clase-9.html#métricas-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas: Matriz de Confusión",
    "text": "Métricas: Matriz de Confusión\n\nLa Matriz de Confusión ordena los valores correctamente predichos y también los distintos errores que el modelo puede cometer.\n\n\n\n\n\n\n\n\n\n\nTP (Verdaderos Positivos)\n\nCorresponde a valores reales de la clase 1 que fueron correctamente predichos como clase 1.\n\nTN (Verdaderos Negativos)\n\nCorresponde a valores reales de la clase 0 que fueron correctamente predichos como clase 0.\n\nFP (Falsos Positivos)\n\nCorresponde a valores reales de la clase 0 que fueron incorrectamente predichos como clase 1.\n\nFN (Falsos Negativos)\n\nCorresponde a valores reales de la clase 1 que fueron incorrectamente predichos como clase 0."
  },
  {
    "objectID": "tics411/clase-9.html#métricas-a-partir-de-la-matriz-de-confusión",
    "href": "tics411/clase-9.html#métricas-a-partir-de-la-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Métricas: A partir de la Matriz de Confusión",
    "text": "Métricas: A partir de la Matriz de Confusión\n\n\n\n\n\nAccuracy\n\n\n\\[\\frac{TP + TN}{TP + TN + FP + FN}\\]\n\n\n\n\n\n\nPrecision\n\n\n\\[\\frac{TP}{TP + FP}\\]\n\n\n\n\n\n\n\nRecall\n\n\n\\[\\frac{TP}{TP + FN}\\]\n\n\n\n\n\n\nF1-Score\n\n\n\\[\\frac{2\\cdot Precision \\cdot Recall}{Precision + Recall} = \\frac{2 \\cdot TP}{2\\cdot TP + FP + FN}\\]\n\n\n\n\n\n\n\n\n\n\n\nAccuracy es probablemente la métrica más sencilla y más utilizada.\nPrecision y Recall ponderarán distintos errores (FP y FN respectivamente) con mayor severidad. Ambas métricas son Antagonistas.\nF1-Score corresponde a la media armónica del Precision y Recall, y tiende a ponderar los errores de manera más balanceada.\n\n\n\n\n¿Cuándo utilizar cada tipo de error?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-roc",
    "href": "tics411/clase-9.html#curva-roc",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva ROC",
    "text": "Curva ROC\nLa curva ROC fue desarrollada en 1950 para analizar señales ruidosas. La curva ROC permite al operador contrapesar la tasa de verdaderos positivos (Eje \\(y\\)) versus los falsos positivos (Eje x).\n\nEl área bajo la curva representa la calidad del modelo. Una manera de interpretarla es como la probabilidad de que una predicción de la clase positiva tenga mayor probabilidad que una de clase negativa. En otras palabras, mide que las probabilidades se encuentren correctamente ordenadas. Por lo tanto varía entre 0.5 y 1.\n\n\n\n\n\n\n\n\n\nROC \\(\\sim\\) 0.5\n\n\n\n\n\nROC \\(\\sim\\) 1"
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python",
    "href": "tics411/clase-9.html#implementación-en-python",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python",
    "text": "Implementación en Python\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\naccuracy_score(y_true, y_pred)\nprecision_score(y_true, y_pred)\nrecall_score(y_true, y_pred)\nf1_score(y_true, y_pred)\nroc_auc_score(y_true, y_proba)\n\ny_true: Corresponde a las etiquetas reales del Dataset.\ny_pred: Corresponde a las predicciones realizadas por el modelo.\ny_proba: Corresponden a las probabilidades predichas por el modelo (si es que el modelo lo permite)."
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python-matriz-de-confusión",
    "href": "tics411/clase-9.html#implementación-en-python-matriz-de-confusión",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Matriz de Confusión",
    "text": "Implementación en Python: Matriz de Confusión\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python-curva-roc",
    "href": "tics411/clase-9.html#implementación-en-python-curva-roc",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Curva ROC",
    "text": "Implementación en Python: Curva ROC\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_predictions(y_true, y_proba)"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-training",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-training",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva de Aprendizaje: Training",
    "text": "Curva de Aprendizaje: Training\n\n\n\n\n\n\n\n\n\n\n\n¿Qué sería la Complejidad del Modelo?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-validación",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-validación",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva de Aprendizaje: Validación",
    "text": "Curva de Aprendizaje: Validación\n\n\n\n\n\n\n\n\n\n\n\n¿Por qué el modelo pierde rendimiento cuando aumenta su Complejidad?"
  },
  {
    "objectID": "tics411/clase-9.html#curva-de-aprendizaje-mejor-ajuste",
    "href": "tics411/clase-9.html#curva-de-aprendizaje-mejor-ajuste",
    "title": "TICS-411 Minería de Datos",
    "section": "Curva de Aprendizaje: Mejor Ajuste",
    "text": "Curva de Aprendizaje: Mejor Ajuste\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting:\n\n\nGran diferencia entre Training y Validation Score.\n\n\n\n\n\n\n\n\n\nUnderfitting:\n\n\nPoca diferencia entre Training y Validation Score, pero con ambos puntajes “relativamente bajos”.\n\n\n\n\n\n\n\n\n\nProper fitting o Sweet Spot:\n\n\nCorresponde al mejor puntaje en el set de Validación. Donde también la distancia entre Train y Test es poca."
  },
  {
    "objectID": "tics411/clase-9.html#complejidad-de-un-modelo",
    "href": "tics411/clase-9.html#complejidad-de-un-modelo",
    "title": "TICS-411 Minería de Datos",
    "section": "Complejidad de un Modelo",
    "text": "Complejidad de un Modelo\n¿Qué modelo es un mejor clasificador?"
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff",
    "href": "tics411/clase-9.html#bias-variance-tradeoff",
    "title": "TICS-411 Minería de Datos",
    "section": "Bias Variance Tradeoff",
    "text": "Bias Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos puntos azules serán puntos que usaremos para entrenar.\nLos puntos verdes serán puntos que usaremos para validar."
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff-bias",
    "href": "tics411/clase-9.html#bias-variance-tradeoff-bias",
    "title": "TICS-411 Minería de Datos",
    "section": "Bias Variance Tradeoff: Bias",
    "text": "Bias Variance Tradeoff: Bias\n\nBias\n\n\nSe refiere a la incapacidad de un modelo de capturar la verdadera relación entre los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo está “sesgado” a tomar una cierta relación que no necesariamente existe."
  },
  {
    "objectID": "tics411/clase-9.html#bias-variance-tradeoff-variance",
    "href": "tics411/clase-9.html#bias-variance-tradeoff-variance",
    "title": "TICS-411 Minería de Datos",
    "section": "Bias Variance Tradeoff: Variance",
    "text": "Bias Variance Tradeoff: Variance\n\nVariance\n\n\nSe refiere a la diferencia de ajuste entre datasets (Train y Validación).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo varía demasiado su comportamiento entre Training y Testing Time."
  },
  {
    "objectID": "tics411/clase-9.html#complejidad-de-un-modelo-1",
    "href": "tics411/clase-9.html#complejidad-de-un-modelo-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Complejidad de un Modelo",
    "text": "Complejidad de un Modelo\n\n\nOverfitting\n\n\n\n\n\n\n\n\n\n\n\n\nRegularización: Se refiere a una penalización para disminuir su complejidad.\n\nModelos más simples: Utilizar modelos con una Frontera de Decisión más simple.\nMás datos!!! Más datos más dificil aprender, por lo tanto, modelos complejos se ven más beneficiados de esto.\n\n\n\n\n\n\nUnderfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuitar Regularización\nModelos más complejos\nMás variabilidad en los datos!!! Podría ser que los datos no permitan aprender patrones más complejos."
  },
  {
    "objectID": "tics411/clase-9.html#cómo-generamos-sets-de-validación",
    "href": "tics411/clase-9.html#cómo-generamos-sets-de-validación",
    "title": "TICS-411 Minería de Datos",
    "section": "¿Cómo generamos sets de Validación?",
    "text": "¿Cómo generamos sets de Validación?\n\nLa evaluación de modelos supervisados es fundamental. De no hacerlo de forma correcta podemos quedarnos con una idea muy equivocada del rendimiento del modelo.\n\n\nCross Validation (Validación Cruzada)\n\n\nSe debe evaluar el rendimiento de un modelo en un dataset diferente al que fue entrenado. Esta es la única manera en la que se puede medir el poder de generalización de un modelo.\n\n\nGeneralización\n\n\nCorresponde a la habilidad de un modelo de adaptarse apropiadamente a datos no vistos previamente.\n\n\n\n\n\n\n\n\n\nUtilizar una estrategia incorrecta de Validación puede llevar a problemas de generalización. La estrategia de Validación debe ser lo más parecida posible a cómo se utilizará el modelo en Producción.\n\n\n\n\n\n\n\n\n\nPara esto se asume que todos los datos son i.i.d (independent and identically distributed). De no lograr esto, lograr buenos rendimientos es más difícil."
  },
  {
    "objectID": "tics411/clase-9.html#validación-cruzada-holdout",
    "href": "tics411/clase-9.html#validación-cruzada-holdout",
    "title": "TICS-411 Minería de Datos",
    "section": "Validación Cruzada: Holdout",
    "text": "Validación Cruzada: Holdout\n\nTambién es conocido como Train Test Split o simplemente Split. Corresponde a la separacion de nuestra data cuando con el proposito de aislar observaciones que el modelo no vea para una correcta evaluación.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set es la porción de los datos que se utilizará exclusivamente para entrenar los datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl test set es la porción de los datos que se utilizará exclusivamente para validar los datos.\nEl test set simula los datos que eventualmente entrarán el modelo para obtener una predicción.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalmente se utilizan splits del tipo 70/30, 80/20 o 90/10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es el problema con este tipo de validación?"
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout",
    "href": "tics411/clase-9.html#variante-holdout",
    "title": "TICS-411 Minería de Datos",
    "section": "Variante Holdout",
    "text": "Variante Holdout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe agrega un validation set el cuál se utilizará para escoger los hiperparámetros que muestren un mejor poder de generalización.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl train set y el test set cumplen la misma función que tenían antes."
  },
  {
    "objectID": "tics411/clase-9.html#variante-holdout-procedimiento",
    "href": "tics411/clase-9.html#variante-holdout-procedimiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Variante Holdout: Procedimiento",
    "text": "Variante Holdout: Procedimiento\n\n\n\n\n\n\n\n\n\nProcedimiento\n\nRepetir para cada Modelo a probar.\n\n\n\n\n\n\n\n\nVamos a entender un modelo como la combinación de un Algoritmo de Aprendizaje + Hiperparámetros + Preprocesamiento.\n\n\n\n\n\n\nSe entrena cada Modelo en el train set. Se mide una métrica de Evaluación apropiada utilizando el Validation Set. La llamaremos métrica de Validación.\nSe escoge el mejor Modelo como el que tenga la mejor métrica de Validación.\nSe reentrena el modelo escogido pero ahora en un “nuevo set” compuesto por el Train set + el Validation set.\nSe reporta el rendimiento final del mejor modelo (al momento del diseño) utilizando métricas medidas en el Test Set."
  },
  {
    "objectID": "tics411/clase-9.html#k-fold-cv",
    "href": "tics411/clase-9.html#k-fold-cv",
    "title": "TICS-411 Minería de Datos",
    "section": "K-Fold CV",
    "text": "K-Fold CV\n\n\n\n\n\n\n\nEl proceso de Holdout podría llevar a un proceso de overfitting del Test Set si el modelo no es lo suficientemente robusto.\n\n\n\n\n\n\n\n\n\n\n\n\nEl K-Fold CV se aplica sólo al Train Set y la métrica final que se reporta utilizando el Test Set.\n\n\n\n\n\n\n\n\n\n\nFold\n\nEntenderemos Folds como divisiones que haremos a nuestro dataset. (En el ejemplo se divide el dataset en 5 Folds).\n\nSplit\n\nEntenderemos Splits, como iteraciones. En cada iteración utilizaremos un Fold como Validation Set y todos los Folds restantes como Train Set.\n\n\n\n\n\n\n\n\n\nLa métrica final se calculará como el promedio de las Métricas de Validación para cada Split.\nA veces la variabilidad (medido a través de la Desviación Estándar) también es usado como criterio para elegir el mejor modelo.\n\n\n\n\n\n\n\n\n\n\n\nEn la práctica se le llama incorrectamente Cross Validation al K-Fold."
  },
  {
    "objectID": "tics411/clase-9.html#bootstrap",
    "href": "tics411/clase-9.html#bootstrap",
    "title": "TICS-411 Minería de Datos",
    "section": "Bootstrap",
    "text": "Bootstrap\nConsiste en generar subgrupos aleatorios con repetición. Normalmente requiere específicar el tamaño de la muestra de entrenamiento. Y la cantidad de repeticiones que del proceso. Los sets de validación (en morado) acá se denominan out-of-bag samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa métrica final a reportar se mide como el promedio de los out-of-bag samples."
  },
  {
    "objectID": "tics411/clase-9.html#variantes-y-consejos",
    "href": "tics411/clase-9.html#variantes-y-consejos",
    "title": "TICS-411 Minería de Datos",
    "section": "Variantes y Consejos",
    "text": "Variantes y Consejos\n\nStratified K-Fold\n\nEs la variante más utilizada de K-Fold el cual genera los folds considerando que se mantenga la proporción de etiquetas en cada Fold.\n\nLeave One Out\n\nSería una variante con \\(K=n\\). Por lo tanto, el Validation Set tiene sólo una observación.\n\n\n\n\n\n\n\n\n\n¿Cuando usar cada uno?\n\n\n\nSi se tiene una cantidad de datos suficiente (normalmente tamaños muy grandes se prefiere) el Holdout.\n\nEntre más registros, menos % de Validation Set se deja.\n\nSi se requiere robustez, o hay Test sets que son muy variables se prefiere K-Fold.\n\nSi es que hay desbalance de clases, se prefiere la versión Stratified.\n\n\nSi se tienen muy pocos datos, entonces utilizar Leave-One-Out.\nBootstrap también es utilizado cuando se tengan pocos datos. Aunque suele ser un approach más estadístico."
  },
  {
    "objectID": "tics411/clase-9.html#baseline",
    "href": "tics411/clase-9.html#baseline",
    "title": "TICS-411 Minería de Datos",
    "section": "Baseline",
    "text": "Baseline\n\nUn modelo Baseline es un modelo simple, normalmente sin aprendizaje asociado o con poder de aprendizaje más limitado, el cuál será utilizado como medida de referencia para ver si algoritmos más complejos efectivamente están aprendiendo.\n\n\n\n\n\n\n\nSi estamos probando un nuevo modelo y éste es capaz de superar el rendimiento de un Baseline, se considera como que estamos aprendiendo algo nuevo.\n\n\n\n\n\n\n\n\n\nModelos que no superaron el puntaje de un modelo Baseline normalmente son deshechados."
  },
  {
    "objectID": "tics411/clase-9.html#implementación-en-python-baselines",
    "href": "tics411/clase-9.html#implementación-en-python-baselines",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Baselines",
    "text": "Implementación en Python: Baselines\nfrom sklearn.dummy import DummyClassifier\n\ndc = DummyClassifier(strategy=\"prior\", random_state = 42, constant=None)\ndc.fit(X_train,y_train)\ny_pred = dc.predict(X_test)\n\n\n\nstrategy: Corresponde a estrategias “dummy” con las cuales generar predicciones.\n\n“prior”: predice siempre la clase más frecuente observada en el entrenamiento. Si se predice la probabilidad, se devuelve la probabilidad empírica.\n“constant”: Devuelve un valor constante provisto por el usuario.\n“uniform”: Predice probabilidades aleatorios obtenidas mediante una distribución uniforme."
  },
  {
    "objectID": "tics411/clase-9.html#data-leakage",
    "href": "tics411/clase-9.html#data-leakage",
    "title": "TICS-411 Minería de Datos",
    "section": "Data Leakage",
    "text": "Data Leakage\n\nFuga de Datos\n\n\nSe refiere al proceso donde el modelo por alguna razón conoce información que no debería conocer. Puede ser información del Test Set o variables que revelan información primordial sobre la etiqueta.\n\n\n\n\n\n\n\n\n\nCuando existe Data Leakage es posible que los resultados del modelo no reflejen correctamente su rendimiento dando una falsa sensación de optimismo.\n\n\n\nEjemplos\n\nEstandarizar o aplicar preprocesamientos antes del Split de la Data.\nUtilizar variables que tienen directa relación con el Target.\n\n\n\n\n\n\n\n\nSe recomienda siempre que sea posible utilizar Pipelines para poder evitar el Data Leakage."
  },
  {
    "objectID": "tics411/clase-7.html#introducción",
    "href": "tics411/clase-7.html#introducción",
    "title": "TICS-411 Minería de Datos",
    "section": "Introducción",
    "text": "Introducción\n\nGracias a los planes de fidelización (juntar puntos, dar RUT, acumular millas, etc.) las empresas son capaces de detectar patrones:\n\n\nQué nos gusta,\nQué compramos,\nCon qué frecuencia lo compramos,\nJunto con qué lo compramos\netc.\n\n\n\n\n\n\n\nMarket Basket Analysis\n\n\nCorresponde al estudio de nuestra canasta de compras. De modo que podamos entender qué cosas son las que como clientes preferimos y una empresa pueda Recomendar de manera más apropiadas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones",
    "href": "tics411/clase-7.html#definiciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones",
    "text": "Definiciones\n\nPatrón\n\n\nPredicado (output True/False) para verificar si una estructura buscada ocurre o no.\n\n\nTarea\n\n\nEncontrar reglas de asociación basado en patrones.\n\n\n\nEjemplos\n\nDatasets de supermercados:\n\n10% de los clientes totales compran vino y quedo (patrón: si compro vino, también llevo queso).\n\nDatasets de Alarmas:\n\nSi la alarma A y B suenan en un intervalo de 30 segundos, entonces la alarma C sonará dentro de un intervalo de 60 segundos con 50% de probabilidad."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado\n\nDatos Transaccionales\n\n\nUna transacción involucra un conjunto de elementos. Una boleta de supermercado muestra el conjunto de elementos comprados por un cliente. Los productos involucrados en una transacción se denominan items."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "href": "tics411/clase-7.html#ejemplo-datos-supermercado-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo: Datos Supermercado",
    "text": "Ejemplo: Datos Supermercado"
  },
  {
    "objectID": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "href": "tics411/clase-7.html#objetivo-y-aplicaciones",
    "title": "TICS-411 Minería de Datos",
    "section": "Objetivo y Aplicaciones",
    "text": "Objetivo y Aplicaciones\n\n\n\n\n\n\nObjetivo\n\n\nEncontrar asociaciones entre elementos u objetos de bases de datos transaccionales.\n\n\n\n\n\n\n\n\n\nAplicaciones\n\n\n\nApoyo a toma de decisiones.\nAnálisis de Información de Ventas.\nDistribución y ubicación de Mercaderías.\nSegmentación de Clientes en base de patrones de compra.\nDiágnostico y predicción de alarmas."
  },
  {
    "objectID": "tics411/clase-7.html#definiciones-medidas",
    "href": "tics411/clase-7.html#definiciones-medidas",
    "title": "TICS-411 Minería de Datos",
    "section": "Definiciones: Medidas",
    "text": "Definiciones: Medidas\n\n\n\n\n\n\nSupport (Soporte)\n\nFracción de Transacciones que contienen a \\(X\\). Probabilidad de que una transacción contenga a \\(X\\).\n\n\n\\[Supp(X) = P(X)\\]\n\n\n\n\n\n\n\nSupport Count\n\nNúmero de Transacciones que contienen a \\(X\\).\n\n\n\\[SuppCount(X) = Count(X)\\]\n\n\n\n\n\n\n\n\nConfidence (Confianza o Eficiencia)\n\nFracción de las Transacciones en las que aparece \\(X\\) que también incluyen \\(Z\\).\n\n\n\\[Conf(X \\implies Z) = \\frac{Supp(X \\cup Z)}{Supp(X)}\\] \\[Conf(X \\implies Z) = \\frac{SuppCount(X \\cup Z)}{SuppCount(X)}\\]\n\n\n\n\n\n\n\n\n\n\nOjo con la Notación \\(\\cup\\). En este caso significa que tanto el producto X como el Producto Z sean parte de la transacción."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "href": "tics411/clase-7.html#ejemplos-support-y-confidence",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplos: Support y Confidence",
    "text": "Ejemplos: Support y Confidence\n\n\n\n\n\n\n\n\n\\[ Supp({Pan}) = 4/7\\] \\[ Supp({Leche}) = 3/7\\] \\[ Supp({Pan, Huevo}) = 2/7\\]\n\\[ Conf({Pan} \\implies {Huevo}) = \\frac{Supp({Pan, Huevo})}{Supp(Pan)} = \\frac{2/7}{4/7}\\]\n\\[ Conf({Pan} \\implies {Leche}) = \\frac{Supp({Pan, Leche})}{Supp(Pan)} = \\frac{1/7}{4/7}\\] \\[ Conf({Leche} \\implies {Pan}) = \\frac{Supp({Pan, Leche})}{Supp(Leche)} = \\frac{1/7}{3/7}\\]"
  },
  {
    "objectID": "tics411/clase-7.html#problema",
    "href": "tics411/clase-7.html#problema",
    "title": "TICS-411 Minería de Datos",
    "section": "Problema",
    "text": "Problema\n\nEn un dataset transaccional de n productos totales y \\(|U_i|\\) elementos para la Transacción \\(i\\).\n\nSe pueden generar un total de \\(N_{reglas}\\) de asociación:\n\\[N_{reglas} = \\sum_{i=1}^{2^{n}} \\sum_{j=0}^{|U_i|}\\binom{|U_i|}{j}\\]\n\n\n\n\n\n\n\n\n\nSi suponemos un supermercado que tiene 1000 productos, y transacciones que pueden ir entre 1 y 50 productos. El problema es muy costoso, y se podrían eventualmente generar demasiadas combinaciones."
  },
  {
    "objectID": "tics411/clase-7.html#algoritmo-apriori",
    "href": "tics411/clase-7.html#algoritmo-apriori",
    "title": "TICS-411 Minería de Datos",
    "section": "Algoritmo Apriori",
    "text": "Algoritmo Apriori\n\nApriori\n\n\nEs un algoritmo para aprender reglas de asociación que utiliza el principio Apriori para buscar de forma eficiente las reglas que satisfacen los límites de soporte y confianza.\n\n\n\n\nAlgoritmo\n\nFijar \\(k=1\\) y determinar lista de candidatos de tamaño \\(k\\).\n\nCalcular la frecuencia del conjunto.\nEliminar conjuntos con baja frecuencia (utilizando un umbral de soporte).\nUnir los conjuntos frecuentes para generar conjuntos de tamaño \\(k+1\\).\nSi existe la posibilidad de seguir creando combinaciones volver al paso a y repetir.\n\nUsar todos los conjuntos frecuentes para generar reglas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori",
    "href": "tics411/clase-7.html#ejemplo-apriori",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori",
    "text": "Ejemplo Apriori\n\nSupongamos el siguiente dataset transaccional:\n\nSupongamos que queremos calcular las reglas de asociación que tengan un MinSupp=40% y un MinConf=70%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodríamos pensar que MinSupp y MinConf son los hiperparámetros de este algoritmo."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 1",
    "text": "Ejemplo Apriori: Iteración 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGalletas NO CUMPLE con el Soporte Mínimo solicitado. Por lo tanto, lo elimino y genero relaciones de 2 productos sin considerar Galletas."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-2",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-2",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 2",
    "text": "Ejemplo Apriori: Iteración 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcá NO SE ELIMINA ningún producto, ya que en los itemsets que sobrevivieron hay Pan, Mantequilla, Leche, Pañales y Cerveza."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-iteración-3",
    "href": "tics411/clase-7.html#ejemplo-apriori-iteración-3",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Iteración 3",
    "text": "Ejemplo Apriori: Iteración 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nSe puede apreciar que los únicos 3 productos que sobreviven son Pan, Mantequilla y Leche. Por lo tanto, NO ES POSIBLE generar reglas con 4 productos."
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas",
    "href": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Generación de Reglas",
    "text": "Ejemplo Apriori: Generación de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pan, Mantequilla}:\n\n\\(Conf(Pan \\implies Mantequilla) = \\frac{Supp(Pan, Mantequilla)}{Supp(Pan)} = \\frac{3}{3}\\)✅ \\(Conf(Mantequilla \\implies Pan) = \\frac{Supp(Pan, Mantequilla)}{Supp(Mantequilla)} = \\frac{3}{3}\\)✅\n\n\n\nPara {Pan, Leche}:\n\n\\(Conf(Pan \\implies Leche) = \\frac{Supp(Pan, Leche)}{Supp(Pan)} = \\frac{2}{3}\\) ❌ \\(Conf(Leche \\implies Pan) = \\frac{Supp(Pan, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ✅\n\n\n\nPara {Mantequilla, Leche}:\n\n\\(Conf(Mantequilla \\implies Leche) = \\frac{Supp(Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\) ❌ \\(Conf(Leche \\implies Mantequilla) = \\frac{Supp(Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\) ✅"
  },
  {
    "objectID": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas-1",
    "href": "tics411/clase-7.html#ejemplo-apriori-generación-de-reglas-1",
    "title": "TICS-411 Minería de Datos",
    "section": "Ejemplo Apriori: Generación de Reglas",
    "text": "Ejemplo Apriori: Generación de Reglas\n\n\n\n\n\n\n\n\n\n\nPara {Pañales, Cerveza}:\n\n\\(Conf(Pañales \\implies Cerveza) = \\frac{Supp(Pañales, Cerveza)}{Supp(Pañales)} = \\frac{2}{3}\\)❌ \\(Conf(Cerveza \\implies Pañales) = \\frac{Supp(Pañales, Cerveza)}{Supp(Cerveza)} = \\frac{2}{2}\\)✅\n\n\n\nPara {Pan, Mantequilla, Leche}:\n\n\\(Conf({Pan, Mantequilla} \\implies {Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Mantequilla)} = \\frac{2}{3}\\)❌ \\(Conf({Pan, Leche} \\implies {Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan, Leche)} = \\frac{2}{2}\\)✅ \\(Conf({Mantequilla, Leche} \\implies {Pan}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla, Leche)} = \\frac{2}{2}\\)✅\n\n\\(Conf({Leche} \\implies {Pan, Mantequilla}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Leche)} = \\frac{2}{2}\\)✅ \\(Conf({Mantequilla} \\implies {Pan, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Mantequilla)} = \\frac{2}{3}\\)❌ \\(Conf({Pan} \\implies {Mantequilla, Leche}) = \\frac{Supp(Pan, Mantequilla, Leche)}{Supp(Pan)} = \\frac{2}{3}\\)❌"
  },
  {
    "objectID": "tics411/clase-7.html#resultado-final",
    "href": "tics411/clase-7.html#resultado-final",
    "title": "TICS-411 Minería de Datos",
    "section": "Resultado Final",
    "text": "Resultado Final\n\n\nItemset MinSupp = 40%\n\n\n\n\n\n\nReglas Finales MinConf = 70%\n\\[Pan \\implies Mantequilla\\] \\[Mantequilla \\implies Pan\\] \\[Leche \\implies Pan\\] \\[Leche \\implies Mantequilla\\] \\[Cerveza \\implies Pañales\\] \\[\\{Pan, Leche\\} \\implies Mantequilla\\]\n\\[\\{Mantequilla, Leche\\} \\implies Pan\\] \\[Leche \\implies \\{Pan, Mantequilla\\}\\]\n\n\n\n\n\n\nInsights:\n\n\n\nEl Pan, la Leche y la Mantequilla están relacionados.\nParece ser que si llevo Cervezas también llevo Pañales."
  },
  {
    "objectID": "tics411/clase-7.html#evaluación-de-reglas-de-asociación",
    "href": "tics411/clase-7.html#evaluación-de-reglas-de-asociación",
    "title": "TICS-411 Minería de Datos",
    "section": "Evaluación de Reglas de Asociación",
    "text": "Evaluación de Reglas de Asociación\n\nLift\n\nMide qué tan lejos de la independencia están \\(X\\) e \\(Y\\). Lift varía entre 0 y \\(\\infty\\).\n\n\n\\[Lift(X,Y) = \\frac{Conf(X \\implies Y)}{s(Y)}\\]\n\n\\(Lift(X,Y) \\sim 1\\) implica independencia y la regla no es importante.\n\\(Lift(X,Y) &lt; 1\\) implica una asociación negativa de la regla.\n\\(Lift(X,Y) &gt; 1\\) implica una asociativa de la regla. Un mayor Lift implica que la regla es potencialmente útil para el futuro.\n\nEjemplo:\n\\[Lift(Cerveza, Pañales) = \\frac{Conf(Cerveza \\implies Pañales)}{Supp(Pañales)} = \\frac{1}{0.6} = 1.67\\]\n\n\n\n\n\n\nUna persona que compra Cerveza tiene 1.67 más chances de comprar Pañales."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-preprocesamiento",
    "href": "tics411/clase-7.html#implementación-en-python-preprocesamiento",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Preprocesamiento",
    "text": "Implementación en Python: Preprocesamiento\nPre-procesamiento\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\ntre = TransactionEncoder()\ndf = tre.fit_transform(transactions)\ndf_encoded = pd.DataFrame(df, columns = tre.columns_)\nL4: transactions debe ser una lista de listas. Cada fila, son distintas transacciones. Cada transaccion puede tener distinto número de elementos. L5: tre.columns_ extrae los nombres de los productos para que el DataFrame sea más entendible.\n\n\n\n\n\n\ndf_encoded es un DataFrame tipo OneHotEncoder pero con valores Booleanos (Esto es solicitado por la documentación)."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-itemsets",
    "href": "tics411/clase-7.html#implementación-en-python-itemsets",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Itemsets",
    "text": "Implementación en Python: Itemsets\nfrom mlxtend.frequent_patterns import apriori \n\nitemset = apriori(df_encoded, min_support=0.5, use_colnames = True)\nL3: df_encoded es el DataFrame preprocesado.\n\nmin_support: Corresponde al Soporte Mínimo para generar itemsets. Por defecto 0.5.\nuse_colnames: Permite que las reglas usen los nombres de las columnas para referirse a los productos. Por defecto es False, pero conviene usarlo como True.\nitemset será un DataFrame con los itemsets generados."
  },
  {
    "objectID": "tics411/clase-7.html#implementación-en-python-reglas",
    "href": "tics411/clase-7.html#implementación-en-python-reglas",
    "title": "TICS-411 Minería de Datos",
    "section": "Implementación en Python: Reglas",
    "text": "Implementación en Python: Reglas\nfrom mlxtend.frequent_patterns import association_rules\n\nrules = association_rules(itemsets, metric=\"confidence\", min_threshold=0.8)\nL3: itemset es el dataframe generado en el paso anterior.\n\nmetric: Métrica para definir reglas, puede ser “confidence” y otras definidas acá\nmin_threshold: Corresponde al umbral de la métrica a utilizar. Por defecto 0.8.\nrules corresponde a un Dataset que tiene las Reglas de Asociación detectadas y muchas métricas asociadas."
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html",
    "href": "tics411/notebooks/01-Preprocesamiento.html",
    "title": "Clases UAI",
    "section": "",
    "text": "%%capture\n## Ejecutar esta celda para instalar o actualizar Feature_Engine\n!pip install -U feature_engine\n## Chequear que la versión de Feature Engine sea al menos 1.7\nimport feature_engine\n\nfeature_engine.__version__\n\n'1.7.0'\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import set_config\n\n## (Opcional) Este comando permite que el output de Scikit-Learn sean Pandas DataFrames.\n## Por dejecto, Scikit-Learn transforma todo a Numpy, ya que es más eficiente computacionalmente.\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "href": "tics411/notebooks/01-Preprocesamiento.html#valores-faltantes",
    "title": "Clases UAI",
    "section": "Valores Faltantes",
    "text": "Valores Faltantes\n\n## Para detectar valores faltantes se utiliza el siguiente comando.\ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\n\n## Opcionalmente se puede obtener el % o la fracción de nulos utilizando la siguiente variante.\ndf.isnull().mean()\n\nsurvived       0.000000\npclass         0.000000\nsex            0.000000\nage            0.198653\nsibsp          0.000000\nparch          0.000000\nfare           0.000000\nembarked       0.002245\nclass          0.000000\nwho            0.000000\nadult_male     0.000000\ndeck           0.772166\nembark_town    0.002245\nalive          0.000000\nalone          0.000000\ndtype: float64\n\n\nPandas: Es posible imputar valores usando Pandas con el comando .fillna().\n\nmedia = df[\"age\"].mean()\nmediana = df[\"age\"].median()\nprint(f\"Promedio de Edad: {media}\")\nprint(\n    f'Promedio de Edad con Imputación con Ceros: {df[\"age\"].fillna(0).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputación por Media: {df[\"age\"].fillna(media).mean()}'\n)\nprint(\n    f'Promedio de Edad con Imputación por Mediana: {df[\"age\"].fillna(mediana).mean()}'\n)\n\nPromedio de Edad: 29.69911764705882\nPromedio de Edad con Imputación con Ceros: 23.79929292929293\nPromedio de Edad con Imputación por Media: 29.69911764705882\nPromedio de Edad con Imputación por Mediana: 29.36158249158249\n\n\nScikit-Learn: Utiliza la clase SimpleImputer, el cual permite distintas estrategias de Imputación: \"mean\", \"median\", \"most_frequent\", \"constant\".\n\nfrom sklearn.impute import SimpleImputer\n\nsc = SimpleImputer(strategy=\"mean\")\n## En este caso uso [[]] ya que Scikit Learn espera Matrices o DataFrames.\n## Utilizar [[]] fuerza a que AGE sea un DataFrame de una Columna y no una Serie.\n\ndata_imputed = sc.fit_transform(df[[\"age\"]])\n## Se puede ver que los nuevos datos ya no poseen valores Perdidos.\ndata_imputed.isnull().sum()\n\nage    0\ndtype: int64"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "href": "tics411/notebooks/01-Preprocesamiento.html#outliers",
    "title": "Clases UAI",
    "section": "Outliers",
    "text": "Outliers\npandas: En Pandas se pueden acotar los outliers utilizando .clip()\n\nprint(f\"Promedio de Tarifas: {df.fare.mean()}\")\ndf[\"fare\"].agg([\"min\", \"max\"])\n\nPromedio de Tarifas: 32.204207968574636\n\n\nmin      0.0000\nmax    512.3292\nName: fare, dtype: float64\n\n\n\nlower: Define la cota inferior.\nupper: Define la cota superior.\n\n\nclipped_data = df[[\"fare\"]].clip(lower=10, upper=50)\nclipped_data.agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n10.0\n\n\nmax\n50.0\n\n\n\n\n\n\n\n\ndf[[\"fare\"]]\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows × 1 columns\n\n\n\n\n## Los valores menores a 10 fueron reemplazados por 10.\n## Los valores mayores a 50 fueron reemplazados por 50.\nclipped_data\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows × 1 columns\n\n\n\nsklearn: Para este caso nos apoyaremos de la librería feature_engine la cual posee herramientas para acotar. feature_engine sigue exactamente la misma lógica de Scikit-Learn.\n\nfrom feature_engine.outliers import ArbitraryOutlierCapper, Winsorizer\n\ncapper = ArbitraryOutlierCapper(\n    max_capping_dict=dict(fare=50), min_capping_dict=dict(fare=10)\n)\ncapper.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n10.00\n\n\n1\n50.00\n\n\n2\n10.00\n\n\n3\n50.00\n\n\n4\n10.00\n\n\n...\n...\n\n\n886\n13.00\n\n\n887\n30.00\n\n\n888\n23.45\n\n\n889\n30.00\n\n\n890\n10.00\n\n\n\n\n891 rows × 1 columns\n\n\n\n\ncapping_method: Define la Estragegia a utilizar para el Winsorizer. Ver Docs.\n\n\n## \"gaussian\" permite acotar por mu +/- 3*std\n## \"iqr\" permite rellenar por Q1 - 3*iqr y Q3 + 3*iqr\nwin = Winsorizer(capping_method=\"gaussian\")\nwin.fit_transform(df[[\"fare\"]])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\n0\n7.2500\n\n\n1\n71.2833\n\n\n2\n7.9250\n\n\n3\n53.1000\n\n\n4\n8.0500\n\n\n...\n...\n\n\n886\n13.0000\n\n\n887\n30.0000\n\n\n888\n23.4500\n\n\n889\n30.0000\n\n\n890\n7.7500\n\n\n\n\n891 rows × 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#variables-categóricas",
    "href": "tics411/notebooks/01-Preprocesamiento.html#variables-categóricas",
    "title": "Clases UAI",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\npandas:\n\nOne Hot Encoding\nPara la conversión de variables categóricas utilizamos pd.get_dummies(). * drop_first: Si es True se elimina la primera categoría.\n\npd.get_dummies(df[\"embark_town\"], drop_first=False)\n\n\n\n\n\n\n\n\nCherbourg\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nFalse\nTrue\n\n\n1\nTrue\nFalse\nFalse\n\n\n2\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n\n\n886\nFalse\nFalse\nTrue\n\n\n887\nFalse\nFalse\nTrue\n\n\n888\nFalse\nFalse\nTrue\n\n\n889\nTrue\nFalse\nFalse\n\n\n890\nFalse\nTrue\nFalse\n\n\n\n\n891 rows × 3 columns\n\n\n\n\npd.get_dummies(df[\"embark_town\"], drop_first=True)\n# Una ventaja de este procedimiento es que no considera los Nulos como otra categoría...\n\n\n\n\n\n\n\n\nQueenstown\nSouthampton\n\n\n\n\n0\nFalse\nTrue\n\n\n1\nFalse\nFalse\n\n\n2\nFalse\nTrue\n\n\n3\nFalse\nTrue\n\n\n4\nFalse\nTrue\n\n\n...\n...\n...\n\n\n886\nFalse\nTrue\n\n\n887\nFalse\nTrue\n\n\n888\nFalse\nTrue\n\n\n889\nFalse\nFalse\n\n\n890\nTrue\nFalse\n\n\n\n\n891 rows × 2 columns\n\n\n\n\n\nOrdinal Encoder\nSe utiliza pd.factorize(). * sort: Usar True ya que coloca las categorías en orden. Además de esta manera se comporta igual que OrdinalEncoder de Scikit-Learn.\n\npd.DataFrame(\n    pd.factorize(df[\"embark_town\"], sort=True)[0], columns=[\"new_column\"]\n)\n\n\n\n\n\n\n\n\nnew_column\n\n\n\n\n0\n2\n\n\n1\n0\n\n\n2\n2\n\n\n3\n2\n\n\n4\n2\n\n\n...\n...\n\n\n886\n2\n\n\n887\n2\n\n\n888\n2\n\n\n889\n0\n\n\n890\n1\n\n\n\n\n891 rows × 1 columns\n\n\n\nScikit-Learn:\n\n\nOne Hot Encoding\n\nsparse_output: Se debe fijar como False para poder ver el output como Pandas\ndrop: Se debe colocar \"first\" o el nombre de una sóla categoría a eliminar.\n\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nohe = OneHotEncoder(drop=\"first\", sparse_output=False)\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n\n\n890\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 3 columns\n\n\n\n\n\nOrdinal Encoder\n\nohe = OneHotEncoder(\n    drop=[\"Queenstown\"], sparse_output=False\n)  # También se puede colocar np.nan.\nohe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\nembark_town_Cherbourg\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n1.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n\n\n886\n0.0\n1.0\n0.0\n\n\n887\n0.0\n1.0\n0.0\n\n\n888\n0.0\n1.0\n0.0\n\n\n889\n1.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n\n\n\n\n891 rows × 3 columns\n\n\n\n\noe = OrdinalEncoder()\noe.fit_transform(df[[\"embark_town\"]])\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\n2.0\n\n\n1\n0.0\n\n\n2\n2.0\n\n\n3\n2.0\n\n\n4\n2.0\n\n\n...\n...\n\n\n886\n2.0\n\n\n887\n2.0\n\n\n888\n2.0\n\n\n889\n0.0\n\n\n890\n1.0\n\n\n\n\n891 rows × 1 columns"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "href": "tics411/notebooks/01-Preprocesamiento.html#escalamiento",
    "title": "Clases UAI",
    "section": "Escalamiento",
    "text": "Escalamiento\nEl escalamiento normalmente se realiza sólo en Scikit-Learn. Se mostrarán la Estandarización y Normalización.\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n## Llamaremos esto Estandarización... (sólo por convención del curso)\nsc = StandardScaler()\ndata = sc.fit_transform(df[[\"fare\"]])\ndata.agg([\"mean\", \"std\"])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmean\n3.987333e-18\n\n\nstd\n1.000562e+00\n\n\n\n\n\n\n\n\n## Llamaremos esto Normalización... (sólo por convención del curso)\nmms = MinMaxScaler()\nmms.fit_transform(df[[\"fare\"]]).agg([\"min\", \"max\"])\n\n\n\n\n\n\n\n\nfare\n\n\n\n\nmin\n0.0\n\n\nmax\n1.0"
  },
  {
    "objectID": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-sólo-a-algunas-variables.",
    "href": "tics411/notebooks/01-Preprocesamiento.html#aplicar-preprocesamientos-sólo-a-algunas-variables.",
    "title": "Clases UAI",
    "section": "Aplicar Preprocesamientos sólo a algunas variables.",
    "text": "Aplicar Preprocesamientos sólo a algunas variables.\nScikit-Learn fue diseñado para el entrenamiento eficiente de modelos. Para ello, se basó en Numpy, el cuál no cuenta con nombre de columnas, por lo que para poder aplicar pre-procesamientos a ciertas partes del Dataset utiliza lo que se llama el ColumnTransformer(), el cuál va más allá del alcance del curso.\nPara simplificar el proceso de elegir ciertas columnas, feature_engine posee una el SklearnTransformerWrapper que permite elegir qué variables queremos pasar por cierta transformación.\n\n## Sin SklearnTransformerWrapper\n\nohe = OneHotEncoder(sparse_output=False)\nohe.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea columnas dummies incluso para las variables numéricas.\n\n\n\n\n\n\n\n\nage_0.42\nage_0.67\nage_0.75\nage_0.83\nage_0.92\nage_1.0\nage_2.0\nage_3.0\nage_4.0\nage_5.0\n...\nage_70.0\nage_70.5\nage_71.0\nage_74.0\nage_80.0\nage_nan\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n889\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 93 columns\n\n\n\n\n## Aplicar preprocesamientos a ciertas variables...\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nohe_w = SklearnTransformerWrapper(\n    OneHotEncoder(sparse_output=False), variables=\"embark_town\"\n)\nohe_w.fit_transform(df[[\"age\", \"embark_town\"]])\n## Crea dummies sólo para la variable embark_town y deja age como estaba.\n\n\n\n\n\n\n\n\nage\nembark_town_Cherbourg\nembark_town_Queenstown\nembark_town_Southampton\nembark_town_nan\n\n\n\n\n0\n22.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n38.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26.0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n35.0\n0.0\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\n27.0\n0.0\n0.0\n1.0\n0.0\n\n\n887\n19.0\n0.0\n0.0\n1.0\n0.0\n\n\n888\nNaN\n0.0\n0.0\n1.0\n0.0\n\n\n889\n26.0\n1.0\n0.0\n0.0\n0.0\n\n\n890\n32.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n891 rows × 5 columns"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html",
    "href": "tics411/notebooks/pandas_basics.html",
    "title": "Seleccionar Filas, y columnas…",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ntitanic_df = sns.load_dataset(\"titanic\")\ntitanic_df.shape, titanic_df.columns\n\n((891, 15),\n Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n        'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n        'alive', 'alone'],\n       dtype='object'))\ntitanic_df.dtypes\n\nsurvived          int64\npclass            int64\nsex              object\nage             float64\nsibsp             int64\nparch             int64\nfare            float64\nembarked         object\nclass          category\nwho              object\nadult_male         bool\ndeck           category\nembark_town      object\nalive            object\nalone              bool\ndtype: object\ntitanic_df[\"survived_new\"] = titanic_df[\"survived\"].astype(\"float64\")\ntitanic_df\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\nsurvived_new\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n1.0\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n1.0\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n1.0\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n0.0\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n1.0\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n0.0\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n1.0\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n0.0\n\n\n\n\n891 rows × 16 columns\n## Mostrar la diferencia entre una Serie y un DataFrame.\ntitanic_df.loc[10]\n\nsurvived                 1\npclass                   3\nsex                 female\nage                    4.0\nsibsp                    1\nparch                    1\nfare                  16.7\nembarked                 S\nclass                Third\nwho                  child\nadult_male           False\ndeck                     G\nembark_town    Southampton\nalive                  yes\nalone                False\nName: 10, dtype: object\ntitanic_df[\"embark_town\"].to_frame()\n\n\n\n\n\n\n\n\nembark_town\n\n\n\n\n0\nSouthampton\n\n\n1\nCherbourg\n\n\n2\nSouthampton\n\n\n3\nSouthampton\n\n\n4\nSouthampton\n\n\n...\n...\n\n\n886\nSouthampton\n\n\n887\nSouthampton\n\n\n888\nSouthampton\n\n\n889\nCherbourg\n\n\n890\nQueenstown\n\n\n\n\n891 rows × 1 columns\n## Explicar que va una lista de elementos... no es un \"doble\" paréntesis.\ntitanic_df[[\"embark_town\", \"class\"]]\n\n\n\n\n\n\n\n\nembark_town\nclass\n\n\n\n\n0\nSouthampton\nThird\n\n\n1\nCherbourg\nFirst\n\n\n2\nSouthampton\nThird\n\n\n3\nSouthampton\nFirst\n\n\n4\nSouthampton\nThird\n\n\n...\n...\n...\n\n\n886\nSouthampton\nSecond\n\n\n887\nSouthampton\nFirst\n\n\n888\nSouthampton\nThird\n\n\n889\nCherbourg\nFirst\n\n\n890\nQueenstown\nThird\n\n\n\n\n891 rows × 2 columns\ntitanic_df.loc[[10, 15], [\"embark_town\", \"fare\", \"age\"]]\n\n\n\n\n\n\n\n\nembark_town\nfare\nage\n\n\n\n\n10\nSouthampton\n16.7\n4.0\n\n\n15\nSouthampton\n16.0\n55.0\ntitanic_df_shuffle = titanic_df.sample(frac=1)\ntitanic_df_shuffle\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n133\n1\n2\nfemale\n29.0\n1\n0\n26.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n748\n0\n1\nmale\n19.0\n1\n0\n53.1000\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n876\n0\n3\nmale\n20.0\n0\n0\n9.8458\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n226\n1\n2\nmale\n19.0\n0\n0\n10.5000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n342\n0\n2\nmale\n28.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n283\n1\n3\nmale\n19.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n863\n0\n3\nfemale\nNaN\n8\n2\n69.5500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n124\n0\n1\nmale\n54.0\n0\n1\n77.2875\nS\nFirst\nman\nTrue\nD\nSouthampton\nno\nFalse\n\n\n583\n0\n1\nmale\n36.0\n0\n0\n40.1250\nC\nFirst\nman\nTrue\nA\nCherbourg\nno\nTrue\n\n\n85\n1\n3\nfemale\n33.0\n3\n0\n15.8500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n\n\n891 rows × 15 columns\n# Esto es un error... si es que no se separa...\ntitanic_df_shuffle.iloc[3][[\"who\", \"adult_male\"]]\n\nwho            man\nadult_male    True\nName: 226, dtype: object\n## Algunos métodos importante...\ntitanic_df.describe(percentiles=[0.05, 0.25, 0.75, 0.95])\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n5%\n0.000000\n1.000000\n4.000000\n0.000000\n0.000000\n7.225000\n\n\n25%\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\n95%\n1.000000\n3.000000\n56.000000\n3.000000\n2.000000\n112.079150\n\n\nmax\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\ntitanic_df.mean(numeric_only=True)\n\nsurvived       0.383838\npclass         2.308642\nage           29.699118\nsibsp          0.523008\nparch          0.381594\nfare          32.204208\nadult_male     0.602694\nalone          0.602694\ndtype: float64\ntitanic_df.median(numeric_only=True)\n\nsurvived       0.0000\npclass         3.0000\nage           28.0000\nsibsp          0.0000\nparch          0.0000\nfare          14.4542\nadult_male     1.0000\nalone          1.0000\ndtype: float64\ntitanic_df.mode()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n24.0\n0\n0\n8.05\nS\nThird\nman\nTrue\nC\nSouthampton\nno\nTrue"
  },
  {
    "objectID": "tics411/notebooks/pandas_basics.html#agrupar",
    "href": "tics411/notebooks/pandas_basics.html#agrupar",
    "title": "Seleccionar Filas, y columnas…",
    "section": "Agrupar",
    "text": "Agrupar\n\ndf = pd.DataFrame(\n    dict(a=[1, 1, 1, 1, 2, 2, 2, 2], b=[1, 2, 3, 4, 5, 6, 7, 8])\n)\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1\n\n\n1\n1\n2\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n2\n5\n\n\n5\n2\n6\n\n\n6\n2\n7\n\n\n7\n2\n8\n\n\n\n\n\n\n\n\nfor i in [df.shape, df.columns, df.index, df.dtypes]:\n    print(i)\n\n(8, 2)\nIndex(['a', 'b'], dtype='object')\nRangeIndex(start=0, stop=8, step=1)\na    int64\nb    int64\ndtype: object\n\n\n\ngroups = df.groupby(\"a\")\nfor id, g in groups:\n    print(f\"Este es el grupo: {id}\")\n    display(g)\n\nEste es el grupo: 1\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1.0\n\n\n1\n1\n2.0\n\n\n2\n1\n3.0\n\n\n3\n1\n4.0\n\n\n\n\n\n\n\nEste es el grupo: 2\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n4\n2\n5.0\n\n\n5\n2\n6.0\n\n\n6\n2\n7.0\n\n\n7\n2\n8.0\n\n\n\n\n\n\n\n\ndf.groupby(\"a\")[\"b\"].mean()\n\na\n1    2.5\n2    6.5\nName: b, dtype: float64\n\n\n\ntitanic_df.groupby(\"sex\")[\"fare\"].mean()\n\nsex\nfemale    44.479818\nmale      25.523893\nName: fare, dtype: float64\n\n\n\ntitanic_df.groupby([\"sex\", \"pclass\"])[[\"age\", \"fare\"]].median()\n\n\n\n\n\n\n\n\n\nage\nfare\n\n\nsex\npclass\n\n\n\n\n\n\nfemale\n1\n35.0\n82.66455\n\n\n2\n28.0\n22.00000\n\n\n3\n21.5\n12.47500\n\n\nmale\n1\n40.0\n41.26250\n\n\n2\n30.0\n13.00000\n\n\n3\n25.0\n7.92500"
  },
  {
    "objectID": "tics411/notebooks/13-ex-DT.html",
    "href": "tics411/notebooks/13-ex-DT.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.survived\n\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\n\ndef make_pipeline(parameters):\n    scaler = SklearnTransformerWrapper(\n        StandardScaler(), variables=parameters[\"sc_variables\"]\n    )\n\n    print(\n        f\"Entrenamiento para Decision Tree y threshold = {parameters['threshold']}\"\n    )\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\n                \"num_imp\",\n                MeanMedianImputer(\n                    imputation_method=parameters[\"num_method\"]\n                ),\n            ),\n            (\n                \"cat_imp\",\n                CategoricalImputer(\n                    imputation_method=parameters[\"cat_method\"]\n                ),\n            ),\n            (\"ohe\", parameters[\"encoder\"]),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    DecisionTreeClassifier(\n                        random_state=42,\n                        min_samples_leaf=parameters[\"min_samples_leaf\"],\n                        min_samples_split=parameters[\"min_samples_split\"],\n                        max_depth=parameters[\"max_depth\"],\n                    ),\n                    threshold=parameters[\"threshold\"],\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\ndef show_tree(pipe, class_names=[\"No\", \"Sí\"], figsize=(20, 6)):\n    feature_names = pipe[-2].feature_names_in_\n    plt.figure(figsize=(20, 6))\n    plot_tree(\n        pipe[-1].estimator_,\n        filled=True,\n        feature_names=feature_names,\n        class_names=class_names,\n    )\n    plt.show()\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.9805389221556886\nTest Accuracy 0.7533632286995515\n===================================\nTrain Precision 0.9918032786885246\nTest Precision 0.6888888888888889\n===================================\nTrain Recall 0.9565217391304348\nTest Recall 0.6966292134831461\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.8488023952095808\nTest Accuracy 0.8071748878923767\n===================================\nTrain Precision 0.8333333333333334\nTest Precision 0.8194444444444444\n===================================\nTrain Recall 0.7509881422924901\nTest Recall 0.6629213483146067\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.2,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.2\n===================================\nTrain Accuracy 0.8068862275449101\nTest Accuracy 0.7892376681614349\n===================================\nTrain Precision 0.6962025316455697\nTest Precision 0.6944444444444444\n===================================\nTrain Recall 0.8695652173913043\nTest Recall 0.8426966292134831\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=2,\n    max_depth=5,\n    encoder=OneHotEncoder(),\n    threshold=0.9,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.9\n===================================\nTrain Accuracy 0.8173652694610778\nTest Accuracy 0.7713004484304933\n===================================\nTrain Precision 0.9851851851851852\nTest Precision 0.8958333333333334\n===================================\nTrain Recall 0.525691699604743\nTest Recall 0.48314606741573035\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=0.1,\n    min_samples_split=2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.2,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.2\n===================================\nTrain Accuracy 0.6212574850299402\nTest Accuracy 0.6143497757847534\n===================================\nTrain Precision 0.5\nTest Precision 0.50920245398773\n===================================\nTrain Recall 0.924901185770751\nTest Recall 0.9325842696629213\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparameters = dict(\n    num_method=\"mean\",\n    cat_method=\"frequent\",\n    sc_variables=[\"fare\", \"age\"],\n    min_samples_leaf=1,\n    min_samples_split=0.2,\n    max_depth=None,\n    encoder=OneHotEncoder(),\n    threshold=0.5,\n)\npipe = make_pipeline(parameters)\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\nshow_tree(pipe)\n\nEntrenamiento para Decision Tree y threshold = 0.5\n===================================\nTrain Accuracy 0.8023952095808383\nTest Accuracy 0.7757847533632287\n===================================\nTrain Precision 0.9290780141843972\nTest Precision 0.8679245283018868\n===================================\nTrain Recall 0.5177865612648221\nTest Recall 0.5168539325842697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/15-ex-LR.html",
    "href": "tics411/notebooks/15-ex-LR.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\n\n\n\n\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.survived\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nX_train.shape, X_test.shape\n\n((668, 5), (223, 5))\n\n\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\nfrom sklego.meta import Thresholder\n\n\ndef make_pipeline(\n    num_method, cat_method, encoder, sc_variables, C, threshold=0.5\n):\n    scaler = SklearnTransformerWrapper(\n        StandardScaler(), variables=sc_variables\n    )\n\n    print(f\"Entrenamiento para C={C} y threshold = {threshold}\")\n    print(\"===================================\")\n    pipe = Pipeline(\n        steps=[\n            (\"num_imp\", MeanMedianImputer(imputation_method=num_method)),\n            (\"cat_imp\", CategoricalImputer(imputation_method=cat_method)),\n            (\"ohe\", encoder),\n            (\"sc\", scaler),\n            (\n                \"model\",\n                Thresholder(\n                    LogisticRegression(\n                        C=C, random_state=42, max_iter=10000\n                    ),\n                    threshold=threshold,\n                ),\n            ),\n        ]\n    )\n    return pipe\n\n\ndef make_evaluation(\n    model,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)\n\n    train_acc = accuracy_score(y_train, y_pred_train)\n    test_acc = accuracy_score(y_test, y_pred)\n    train_precision = precision_score(y_train, y_pred_train)\n    test_precision = precision_score(y_test, y_pred)\n    train_recall = recall_score(y_train, y_pred_train)\n    test_recall = recall_score(y_test, y_pred)\n\n    print(f\"Train Accuracy {train_acc}\")\n    print(f\"Test Accuracy {test_acc}\")\n    print(\"===================================\")\n    print(f\"Train Precision {train_precision}\")\n    print(f\"Test Precision {test_precision}\")\n    print(\"===================================\")\n    print(f\"Train Recall {train_recall}\")\n    print(f\"Test Recall {test_recall}\")\n\n    print(\"===================================\")\n    print(f\"Coeficientes: {model[-1].estimator_.coef_}\")\n    print(f\"Coeficientes: {model[-1].estimator_.intercept_}\")\n\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n    RocCurveDisplay.from_predictions(y_test, y_pred_proba[:, 1])\n\n\npipe = make_pipeline(\n    num_method=\"mean\",\n    cat_method=\"missing\",\n    encoder=OneHotEncoder(),\n    sc_variables=[\"age\", \"fare\"],\n    C=10,\n    threshold=0.6,\n)\n\nmake_evaluation(pipe, X_train, X_test, y_train, y_test)\n\nEntrenamiento para C=10 y threshold = 0.6\n===================================\nTrain Accuracy 0.8068862275449101\nTest Accuracy 0.8026905829596412\n===================================\nTrain Precision 0.8444444444444444\nTest Precision 0.8082191780821918\n===================================\nTrain Recall 0.6007905138339921\nTest Recall 0.6629213483146067\n===================================\nCoeficientes: [[ 0.08468769 -0.35930833  0.91502373 -1.02074655  0.36843929 -1.13894327\n   1.40165975 -0.60754774  0.03083727 -0.10686131  0.94628825]]\nCoeficientes: [0.3325951]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportante, normalemente es buena idea escoger sólo una métrica. O darle más preponderancia a una métrica, ya que el mejor modelo puede ser muy distinto dependiendo de la métrica a utilizar.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html",
    "href": "tics411/notebooks/10-resolucion_guia.html",
    "title": "K-Means",
    "section": "",
    "text": "import pandas as pd\nfrom scipy.spatial import distance_matrix\n\ndf = pd.DataFrame(dict(x=[0, 0, 1, 4, 5, 6], y=[1, 0, 0, 4, 4, 6]))\ndisplay(df)\nd_matrix = pd.DataFrame(distance_matrix(df, df))\nd_matrix\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n1\n\n\n1\n0\n0\n\n\n2\n1\n0\n\n\n3\n4\n4\n\n\n4\n5\n4\n\n\n5\n6\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.000000\n1.000000\n1.414214\n5.000000\n5.830952\n7.810250\n\n\n1\n1.000000\n0.000000\n1.000000\n5.656854\n6.403124\n8.485281\n\n\n2\n1.414214\n1.000000\n0.000000\n5.000000\n5.656854\n7.810250\n\n\n3\n5.000000\n5.656854\n5.000000\n0.000000\n1.000000\n2.828427\n\n\n4\n5.830952\n6.403124\n5.656854\n1.000000\n0.000000\n2.236068\n\n\n5\n7.810250\n8.485281\n7.810250\n2.828427\n2.236068\n0.000000\ncentroides = pd.DataFrame(dict(x=[1, 5], y=[1, 6]))\ncentroides_2 = pd.DataFrame(dict(x=[1 / 3, 5], y=[1 / 3, 14 / 3]))\nimport matplotlib.pyplot as plt\n\nplt.scatter(df.x, df.y)\nplt.scatter(centroides.x, centroides.y, c=\"red\")\nplt.scatter(centroides_2.x, centroides_2.y, c=\"green\")\nplt.title(\"Centroides Iter 1: Rojo, Iter 2: Verde\")\nplt.tight_layout()\n## Distancia Centroides 1 a Puntos\npd.DataFrame(distance_matrix(centroides, df))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1.000000\n1.414214\n1.000000\n4.242641\n5.0\n7.071068\n\n\n1\n7.071068\n7.810250\n7.211103\n2.236068\n2.0\n1.000000\n## Distancia Centroides 2 a Puntos\npd.DataFrame(distance_matrix(centroides_2, df))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.745356\n0.471405\n0.745356\n5.18545\n5.934831\n8.013877\n\n\n1\n6.200358\n6.839428\n6.146363\n1.20185\n0.666667\n1.666667"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "href": "tics411/notebooks/10-resolucion_guia.html#dbscan",
    "title": "K-Means",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=2, eps=2)\ndbs.fit_predict(df)\n\narray([ 0,  0,  0,  1,  1, -1])\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(min_samples=1, eps=1)\ndbs.fit_predict(df)\n\narray([0, 0, 0, 1, 1, 2])"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "href": "tics411/notebooks/10-resolucion_guia.html#jerarquico-linkage-complete",
    "title": "K-Means",
    "section": "Jerarquico Linkage Complete",
    "text": "Jerarquico Linkage Complete\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nplot_dendogram(df, link=\"complete\")"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#cohesión-y-separación",
    "href": "tics411/notebooks/10-resolucion_guia.html#cohesión-y-separación",
    "title": "K-Means",
    "section": "Cohesión y Separación",
    "text": "Cohesión y Separación\n\nimport numpy as np\n\n\ndef compute_clustering_metrics(X, labels, centers, is_df=True):\n    if is_df:\n        X = X.to_numpy()\n    sse = np.square(X - centers[labels]).sum()\n    count = np.bincount(labels)\n    ssb = (\n        np.square(X.mean(axis=0) - centers) * count.reshape(-1, 1)\n    ).sum()\n    return sse, ssb\n\n\nlabels = np.array([0, 0, 0, 1, 1, 1])\ncenters = centroides_2.values\nsse, ssb = compute_clustering_metrics(df, labels, centers, is_df=True)\nsse, ssb\n\n(6.0, 60.833333333333336)"
  },
  {
    "objectID": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "href": "tics411/notebooks/10-resolucion_guia.html#silhouette",
    "title": "K-Means",
    "section": "Silhouette",
    "text": "Silhouette\n\ndef silhouette_score_m(d_matrix, clust_labels):\n    n_clusters = len(np.unique(clust_labels))\n    clusters = clust_labels\n    idx_cohesion = clusters == np.arange(n_clusters).reshape(-1, 1)\n    a = np.zeros_like(clusters, dtype=np.float32)\n    bj = np.zeros((len(clusters), n_clusters))\n    for i, (row, c) in enumerate(zip(d_matrix, clusters)):\n        val = row[idx_cohesion[c] & (row != 0)]\n        a[i] = val.mean() if len(val) else 0\n        for cl in range(n_clusters):\n            if cl != c:\n                val = row[idx_cohesion[cl]]\n                bj[i, cl] = val.mean() if len(val) else 0\n\n    b = np.sort(bj, axis=1)[:, 1]\n    return a, b, bj, n_clusters\n\n\nd_matrix = distance_matrix(df, df)\na, b, bj, n_clusters = silhouette_score_m(d_matrix, labels)\n\n\ndef create_table_for_silhouette(a, b, bj, n_clusters):\n    s_score = (b - a) / np.max((a, b), axis=0)\n    columns = (\n        [\"a\"] + [\"b\" + str(i) for i in range(n_clusters)] + [\"b\", \"s\"]\n    )\n\n    s_table = pd.DataFrame(\n        np.hstack(\n            [\n                a.reshape(-1, 1),\n                bj,\n                b.reshape(-1, 1),\n                s_score.reshape(-1, 1),\n            ]\n        ),\n        columns=columns,\n    )\n    return s_table\n\n\ns_score_table = create_table_for_silhouette(a, b, bj, n_clusters)\ns_score_table[\"s\"].mean()\n\n0.7517302154855591\n\n\n\ns_score_table\n\n\n\n\n\n\n\n\na\nb0\nb1\nb\ns\n\n\n\n\n0\n1.207107\n0.000000\n6.213734\n6.213734\n0.805736\n\n\n1\n1.000000\n0.000000\n6.848420\n6.848420\n0.853981\n\n\n2\n1.207107\n0.000000\n6.155701\n6.155701\n0.803904\n\n\n3\n1.914214\n5.218951\n0.000000\n5.218951\n0.633219\n\n\n4\n1.618034\n5.963643\n0.000000\n5.963643\n0.728684\n\n\n5\n2.532248\n8.035260\n0.000000\n8.035260\n0.684858"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html",
    "href": "tics411/notebooks/04-analisis_centros.html",
    "title": "Análisis de Centros",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\ndf = sns.load_dataset(\"iris\")\npca = PCA(n_components=2)\npca_coords = pca.fit_transform(df.drop(columns=\"species\"))\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nlabels = km.fit_predict(df.drop(columns=\"species\"))\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\nmean_table, std_table = create_tables(\n    df,\n    labels,\n    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n)\n## Corresponde a los valores promedios de cada variable por Cluster (los Centroides)\nmean_table\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n5.901613\n2.748387\n4.393548\n1.433871\n\n\n1\n5.006000\n3.428000\n1.462000\n0.246000\n\n\n2\n6.850000\n3.073684\n5.742105\n2.071053\n## Corresponde a la Desviación Estándar de cada variable por Cluster\nstd_table\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nlabels\n\n\n\n\n\n\n\n\n0\n0.466410\n0.296284\n0.508895\n0.297500\n\n\n1\n0.352490\n0.379064\n0.173664\n0.105386\n\n\n2\n0.494155\n0.290092\n0.488590\n0.279872"
  },
  {
    "objectID": "tics411/notebooks/04-analisis_centros.html#representación-gráfica",
    "href": "tics411/notebooks/04-analisis_centros.html#representación-gráfica",
    "title": "Análisis de Centros",
    "section": "Representación Gráfica",
    "text": "Representación Gráfica\nAcá les dejo una Función con la cual pueden realizar el Análisis de Centros. Para ello requieren un DataFrame que contenga las variables a analizar y su etiqueta.\nSe debe indicar, el df, el número de Clusters creados, la columna de la etiqueta, y las columnas a analizar. Adicionalmente se puede agregar un título y cambiar las dimensiones del gráfico.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i].title())\n    plt.suptitle(title, fontsize=15)\n    plt.show()\n\n\ncolumns = df.drop(columns=[\"species\", \"labels\"]).columns.tolist()\ncenter_analysis_viz(\n    df,\n    n_clusters=3,\n    labels=labels,\n    columns=columns,\n    title=\"Análisis de Centros para Iris\",\n)"
  },
  {
    "objectID": "tics411/notebooks/knn_desarrollo.html",
    "href": "tics411/notebooks/knn_desarrollo.html",
    "title": "Preprocesamiento",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf.dtypes.value_counts().plot(kind=\"bar\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nX = df[[\"sex\", \"age\", \"class\", \"embark_town\", \"fare\"]]\ny = df.alive\ny\n\n0       no\n1      yes\n2      yes\n3      yes\n4       no\n      ... \n886     no\n887    yes\n888     no\n889    yes\n890     no\nName: alive, Length: 891, dtype: object\n\n\n\ny.value_counts(normalize=True).plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nnum_vars = X.select_dtypes(np.number).columns.tolist()\ncat_vars = [col for col in X.columns if col not in num_vars]\ncat_vars\n\n['sex', 'class', 'embark_town']\n\n\n\nX[num_vars].hist(figsize=(20, 6))\n\narray([[&lt;Axes: title={'center': 'age'}&gt;,\n        &lt;Axes: title={'center': 'fare'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\nX.groupby(\"age\").fare.mean()\n\nage\n0.42       8.5167\n0.67      14.5000\n0.75      19.2583\n0.83      23.8750\n0.92     151.5500\n           ...   \n70.00     40.7500\n70.50      7.7500\n71.00     42.0792\n74.00      7.7750\n80.00     30.0000\nName: fare, Length: 88, dtype: float64\n\n\n\ndf[\"rango_edad\"] = pd.cut(X[\"age\"], 5)\ndf.groupby(\"rango_edad\").fare.median()\n\nrango_edad\n(0.34, 16.336]      26.00000\n(16.336, 32.252]    10.50000\n(32.252, 48.168]    24.86875\n(48.168, 64.084]    29.70000\n(64.084, 80.0]      26.55000\nName: fare, dtype: float64\n\n\n\ndf.groupby(\"rango_edad\").fare.mean()\n\nrango_edad\n(0.34, 16.336]      31.588877\n(16.336, 32.252]    28.260499\n(32.252, 48.168]    42.788940\n(48.168, 64.084]    50.327235\n(64.084, 80.0]      28.905691\nName: fare, dtype: float64\n\n\n\ndf.fare.plot(kind=\"box\")\n\n\n\n\n\n\n\n\n\nfor cat in cat_vars:\n    X[cat].value_counts().plot(\n        kind=\"bar\", title=f\"Gráfico de variable {cat}\"\n    )\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX.groupby(\"class\").fare.mean()\n\nclass\nFirst     84.154687\nSecond    20.662183\nThird     13.675550\nName: fare, dtype: float64\n\n\n\nX\n\n\n\n\n\n\n\n\nsex\nage\nclass\nembark_town\nfare\n\n\n\n\n0\nmale\n22.0\nThird\nSouthampton\n7.2500\n\n\n1\nfemale\n38.0\nFirst\nCherbourg\n71.2833\n\n\n2\nfemale\n26.0\nThird\nSouthampton\n7.9250\n\n\n3\nfemale\n35.0\nFirst\nSouthampton\n53.1000\n\n\n4\nmale\n35.0\nThird\nSouthampton\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nmale\n27.0\nSecond\nSouthampton\n13.0000\n\n\n887\nfemale\n19.0\nFirst\nSouthampton\n30.0000\n\n\n888\nfemale\nNaN\nThird\nSouthampton\n23.4500\n\n\n889\nmale\n26.0\nFirst\nCherbourg\n30.0000\n\n\n890\nmale\n32.0\nThird\nQueenstown\n7.7500\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nX.isnull().mean().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n\nmmi = MeanMedianImputer(imputation_method=\"mean\")\nX_imp = mmi.fit_transform(X)\n\nci = CategoricalImputer(imputation_method=\"frequent\")\nX_imp = ci.fit_transform(X_imp)\nX_imp\n\n\n\n\n\n\n\n\nsex\nage\nclass\nembark_town\nfare\n\n\n\n\n0\nmale\n22.000000\nThird\nSouthampton\n7.2500\n\n\n1\nfemale\n38.000000\nFirst\nCherbourg\n71.2833\n\n\n2\nfemale\n26.000000\nThird\nSouthampton\n7.9250\n\n\n3\nfemale\n35.000000\nFirst\nSouthampton\n53.1000\n\n\n4\nmale\n35.000000\nThird\nSouthampton\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nmale\n27.000000\nSecond\nSouthampton\n13.0000\n\n\n887\nfemale\n19.000000\nFirst\nSouthampton\n30.0000\n\n\n888\nfemale\n29.699118\nThird\nSouthampton\n23.4500\n\n\n889\nmale\n26.000000\nFirst\nCherbourg\n30.0000\n\n\n890\nmale\n32.000000\nThird\nQueenstown\n7.7500\n\n\n\n\n891 rows × 5 columns\n\n\n\n\nohe = OneHotEncoder(variables=[\"sex\", \"embark_town\"])\nX_enc = ohe.fit_transform(X_imp)\nod = OrdinalEncoder(encoding_method=\"arbitrary\")\nX_enc = od.fit_transform(X_enc)\nX_enc\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n22.000000\n0\n7.2500\n1\n0\n1\n0\n0\n\n\n1\n38.000000\n1\n71.2833\n0\n1\n0\n1\n0\n\n\n2\n26.000000\n0\n7.9250\n0\n1\n1\n0\n0\n\n\n3\n35.000000\n1\n53.1000\n0\n1\n1\n0\n0\n\n\n4\n35.000000\n0\n8.0500\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n27.000000\n2\n13.0000\n1\n0\n1\n0\n0\n\n\n887\n19.000000\n1\n30.0000\n0\n1\n1\n0\n0\n\n\n888\n29.699118\n0\n23.4500\n0\n1\n1\n0\n0\n\n\n889\n26.000000\n1\n30.0000\n1\n0\n0\n1\n0\n\n\n890\n32.000000\n0\n7.7500\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 8 columns\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\n\nsc = SklearnTransformerWrapper(StandardScaler(), variables=[\"age\", \"fare\"])\nX_sc = sc.fit_transform(X_enc)\n\n\nsc_all = StandardScaler()\nX_sc_all = sc_all.fit_transform(X_enc)\nX_sc_all\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n-0.592481\n-0.820037\n-0.502445\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n1\n0.638789\n0.431081\n0.786845\n-1.355574\n1.355574\n-1.623803\n2.074505\n-0.307562\n\n\n2\n-0.284663\n-0.820037\n-0.488854\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n3\n0.407926\n0.431081\n0.420730\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n4\n0.407926\n-0.820037\n-0.486337\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.207709\n1.682199\n-0.386671\n0.737695\n-0.737695\n0.615838\n-0.482043\n-0.307562\n\n\n887\n-0.823344\n0.431081\n-0.044381\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n888\n0.000000\n-0.820037\n-0.176263\n-1.355574\n1.355574\n0.615838\n-0.482043\n-0.307562\n\n\n889\n-0.284663\n0.431081\n-0.044381\n0.737695\n-0.737695\n-1.623803\n2.074505\n-0.307562\n\n\n890\n0.177063\n-0.820037\n-0.492378\n0.737695\n-0.737695\n-1.623803\n-0.482043\n3.251373\n\n\n\n\n891 rows × 8 columns\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef knn(X, y, k=3):\n    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n    knn.fit(X, y)\n    print(f\"Puntaje para k = {k}: {knn.score(X, y)}\")\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_sc, y, k=k)\n\nPuntaje para k = 3: 0.8843995510662177\nPuntaje para k = 5: 0.8686868686868687\nPuntaje para k = 7: 0.8608305274971941\nPuntaje para k = 9: 0.8428731762065096\nPuntaje para k = 11: 0.835016835016835\nPuntaje para k = 13: 0.8249158249158249\nPuntaje para k = 15: 0.819304152637486\n\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_sc_all, y, k=k)\n\nPuntaje para k = 3: 0.8866442199775533\nPuntaje para k = 5: 0.8698092031425365\nPuntaje para k = 7: 0.8552188552188552\nPuntaje para k = 9: 0.8383838383838383\nPuntaje para k = 11: 0.835016835016835\nPuntaje para k = 13: 0.8282828282828283\nPuntaje para k = 15: 0.8237934904601572\n\n\n\nfor k in [3, 5, 7, 9, 11, 13, 15]:\n    knn(X_enc, y, k=k)\n\nPuntaje para k = 3: 0.8372615039281706\nPuntaje para k = 5: 0.8204264870931538\nPuntaje para k = 7: 0.7867564534231201\nPuntaje para k = 9: 0.7721661054994389\nPuntaje para k = 11: 0.7676767676767676\nPuntaje para k = 13: 0.7575757575757576\nPuntaje para k = 15: 0.7508417508417509\n\n\n\nX_enc\n\n\n\n\n\n\n\n\nage\nclass\nfare\nsex_male\nsex_female\nembark_town_Southampton\nembark_town_Cherbourg\nembark_town_Queenstown\n\n\n\n\n0\n22.000000\n0\n7.2500\n1\n0\n1\n0\n0\n\n\n1\n38.000000\n1\n71.2833\n0\n1\n0\n1\n0\n\n\n2\n26.000000\n0\n7.9250\n0\n1\n1\n0\n0\n\n\n3\n35.000000\n1\n53.1000\n0\n1\n1\n0\n0\n\n\n4\n35.000000\n0\n8.0500\n1\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n27.000000\n2\n13.0000\n1\n0\n1\n0\n0\n\n\n887\n19.000000\n1\n30.0000\n0\n1\n1\n0\n0\n\n\n888\n29.699118\n0\n23.4500\n0\n1\n1\n0\n0\n\n\n889\n26.000000\n1\n30.0000\n1\n0\n0\n1\n0\n\n\n890\n32.000000\n0\n7.7500\n1\n0\n0\n0\n1\n\n\n\n\n891 rows × 8 columns\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html",
    "href": "tics411/notebooks/12-ex-CV.html",
    "title": "Holdout (Train Test Split)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"titanic\")\ndf\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows × 15 columns\nX = df[[\"class\", \"sex\", \"embark_town\", \"fare\", \"age\"]]\ny = df.alive\n\nX.shape, y.shape\n\n((891, 5), (891,))\nnum_cols = X.select_dtypes(np.number).columns.tolist()\ncat_cols = [col for col in X.columns if col not in num_cols]\nnum_cols, cat_cols\n\n(['fare', 'age'], ['class', 'sex', 'embark_town'])\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\nX_train.shape, X_test.shape\n\n((668, 5), (223, 5))\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import CategoricalImputer, MeanMedianImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ndef make_pipeline(k, scale=None):\n    if scale is not None:\n        scaler = SklearnTransformerWrapper(\n            StandardScaler(), variables=num_cols\n        )\n    else:\n        scaler = StandardScaler()\n\n    pipe = Pipeline(\n        steps=[\n            (\"ci\", CategoricalImputer(imputation_method=\"frequent\")),\n            (\"mmi\", MeanMedianImputer(imputation_method=\"mean\")),\n            (\"ohe\", OneHotEncoder()),\n            (\"sc\", scaler),\n            (\"model\", KNeighborsClassifier(n_neighbors=k)),\n        ]\n    )\n    return pipe\n\n\npipe = make_pipeline(k=5)\n# pipe = make_pipeline(k=5, scale = num_cols)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\npipe.score(X_test, y_test)\n\n0.7757847533632287"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#way-holdout-train-validation-test-split",
    "href": "tics411/notebooks/12-ex-CV.html#way-holdout-train-validation-test-split",
    "title": "Holdout (Train Test Split)",
    "section": "3-way Holdout (Train Validation Test Split)",
    "text": "3-way Holdout (Train Validation Test Split)\n\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\nfor k in [3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    pipe.fit(X_train, y_train)\n    metric = pipe.score(X_val, y_val)\n    print(f\"Puntaje del Modelo, k = {k}: {metric}\")\n    print(\"=============================================\")\n\nPuntaje del Modelo, k = 3: 0.7982062780269058\n=============================================\nPuntaje del Modelo, k = 5: 0.7757847533632287\n=============================================\nPuntaje del Modelo, k = 7: 0.7892376681614349\n=============================================\nPuntaje del Modelo, k = 9: 0.8026905829596412\n=============================================\nPuntaje del Modelo, k = 11: 0.7982062780269058\n============================================="
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#k-fold",
    "href": "tics411/notebooks/12-ex-CV.html#k-fold",
    "title": "Holdout (Train Test Split)",
    "section": "K-Fold",
    "text": "K-Fold\n\nimport numpy as np\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n\ndef make_kfold(X, y, k, kfold=5):\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=42)\n    score = []\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), start=1):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        pipe = make_pipeline(k=k)\n        pipe.fit(X_train, y_train)\n        metric = pipe.score(X_val, y_val)\n        score.append(metric)\n        print(f\"Metric for Fold {fold}: {metric:.3f}\")\n    return score\n\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    print(f\"k = {k}\")\n    kfold_score = make_kfold(X_trainval, y_trainval, k, kfold=5)\n    mean = np.mean(kfold_score)\n    std = np.std(kfold_score)\n    print(f\"K-Fold Score: {mean:.3f} +/- {std:.3f}\")\n    print(\"=========================================\")\n\nk = 1\nMetric for Fold 1: 0.748\nMetric for Fold 2: 0.811\nMetric for Fold 3: 0.725\nMetric for Fold 4: 0.803\nMetric for Fold 5: 0.746\nK-Fold Score: 0.767 +/- 0.034\n=========================================\nk = 3\nMetric for Fold 1: 0.769\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.789\nMetric for Fold 4: 0.789\nMetric for Fold 5: 0.810\nK-Fold Score: 0.802 +/- 0.029\n=========================================\nk = 5\nMetric for Fold 1: 0.790\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.803\nMetric for Fold 4: 0.754\nMetric for Fold 5: 0.789\nK-Fold Score: 0.798 +/- 0.032\n=========================================\nk = 7\nMetric for Fold 1: 0.783\nMetric for Fold 2: 0.853\nMetric for Fold 3: 0.803\nMetric for Fold 4: 0.761\nMetric for Fold 5: 0.817\nK-Fold Score: 0.803 +/- 0.031\n=========================================\nk = 9\nMetric for Fold 1: 0.790\nMetric for Fold 2: 0.846\nMetric for Fold 3: 0.789\nMetric for Fold 4: 0.761\nMetric for Fold 5: 0.810\nK-Fold Score: 0.799 +/- 0.028\n=========================================\nk = 11\nMetric for Fold 1: 0.776\nMetric for Fold 2: 0.839\nMetric for Fold 3: 0.796\nMetric for Fold 4: 0.754\nMetric for Fold 5: 0.789\nK-Fold Score: 0.791 +/- 0.028\n========================================="
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#versión-reducida",
    "href": "tics411/notebooks/12-ex-CV.html#versión-reducida",
    "title": "Holdout (Train Test Split)",
    "section": "Versión Reducida",
    "text": "Versión Reducida\n\nfrom sklearn.model_selection import cross_val_score\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor k in [1, 3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    vals = cross_val_score(pipe, X_trainval, y_trainval, cv=kf)\n    print(f\"k = {k}\")\n    print(f\"Metric: {np.mean(vals):.3f} +/- {np.std(vals):.3f}\")\n\nk = 1\nMetric: 0.767 +/- 0.034\nk = 3\nMetric: 0.802 +/- 0.029\nk = 5\nMetric: 0.798 +/- 0.032\nk = 7\nMetric: 0.803 +/- 0.031\nk = 9\nMetric: 0.799 +/- 0.028\nk = 11\nMetric: 0.791 +/- 0.028"
  },
  {
    "objectID": "tics411/notebooks/12-ex-CV.html#calcular-test-scores",
    "href": "tics411/notebooks/12-ex-CV.html#calcular-test-scores",
    "title": "Holdout (Train Test Split)",
    "section": "Calcular Test Scores",
    "text": "Calcular Test Scores\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    pipe = make_pipeline(k=k)\n    pipe.fit(X_trainval, y_trainval)\n    metric = pipe.score(X_test, y_test)\n    print(f\"Score for k = {k}: {metric}\")\n\nScore for k = 1: 0.7821229050279329\nScore for k = 3: 0.8156424581005587\nScore for k = 5: 0.770949720670391\nScore for k = 7: 0.8044692737430168\nScore for k = 9: 0.8212290502793296\nScore for k = 11: 0.8156424581005587\n\n\n\nPero, qué está ocurriendo acá?"
  },
  {
    "objectID": "tics411/notebooks/05-ex-jerarquico.html",
    "href": "tics411/notebooks/05-ex-jerarquico.html",
    "title": "Ejemplo Clustering Aglomerativo",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\ndf = sns.load_dataset(\"iris\")\ndf\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nX = df.drop(columns=\"species\")\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\nX_sc\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n3\n-1.506521\n0.098217\n-1.283389\n-1.315444\n\n\n4\n-1.021849\n1.249201\n-1.340227\n-1.315444\n\n\n...\n...\n...\n...\n...\n\n\n145\n1.038005\n-0.131979\n0.819596\n1.448832\n\n\n146\n0.553333\n-1.282963\n0.705921\n0.922303\n\n\n147\n0.795669\n-0.131979\n0.819596\n1.053935\n\n\n148\n0.432165\n0.788808\n0.933271\n1.448832\n\n\n149\n0.068662\n-0.131979\n0.762758\n0.790671\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_iris = pca.fit_transform(X_sc)\n\n\ndef pca_viz(pca, color=None, title=\"\"):\n    plt.scatter(pca_iris[\"pca0\"], pca_iris[\"pca1\"], c=color)\n    plt.title(title)\n    plt.show()\n\n\npca_viz(pca_iris, title=\"Visualización de Iris en 2 dimensiones\")\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlink_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in link_list:\n    plot_dendogram(X_sc, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nagc = AgglomerativeClustering(\n    n_clusters=3, metric=\"euclidean\", linkage=\"ward\"\n)\nlabels = agc.fit_predict(X_sc)\npca_viz(\n    pca_iris,\n    color=labels,\n    title=\"Clustering Iris. Método Average, 3 Clusters.\",\n)\n\n## Transformarlo en función para probar muchas combinaciones...\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html",
    "href": "tics411/notebooks/08-proyecto_clustering.html",
    "title": "Preparación de los Datos",
    "section": "",
    "text": "# En caso que de ejecutar esto en Colab, van a tener que instalar Scikit-Plot para poder ver la curva de Silhouette.\n#!pip install scikit-plot\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import set_config\n\nset_config(transform_output=\"pandas\")\n\nRANDOM_STATE = 0\nnp.random.seed(RANDOM_STATE)\nN = np.random.randint(5, 15, size=1)[0]\nn_samples = np.random.randint(100, 1000, size=N)\nX, _ = make_blobs(\n    n_samples=n_samples,\n    n_features=9,\n    cluster_std=2.5,\n    random_state=RANDOM_STATE,\n)\ndf = pd.DataFrame(X)\ndict_cat = {\n    0: \"Cat 1\",\n    1: \"Cat 2\",\n    2: \"Cat 3\",\n}\nrng = np.random.default_rng()\ndf[\"cat_var\"] = rng.choice(a=[0, 1, 2], size=len(df), p=[0.2, 0.3, 0.5])\ndf[\"cat_var\"] = df[\"cat_var\"].map(dict_cat)\n\ndf.columns = [f\"x{i}\" for i, _ in enumerate(df.columns, start=1)]\ndf[\"x1\"] += 100\ndf[\"x5\"] *= 327\ndf[\"x9\"] /= 15\n\ndf.to_csv(\"proyecto_clustering.csv\", index=False)\n## Acá comienza oficialmente el código.\ndf = pd.read_csv(\"proyecto_clustering.csv\")\ndf.dtypes.value_counts().plot(\n    kind=\"bar\", title=\"Tipos de Datos en el Dataset\", edgecolor=\"k\"\n)\nplt.tight_layout()\ndf.hist(figsize=(20, 6), edgecolor=\"k\", grid=False)\nplt.tight_layout()\ndf[\"x10\"].value_counts().plot(\n    kind=\"bar\",\n    edgecolor=\"k\",\n    title=\"Distribución de las Variables Categóricas\",\n)\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#variables-categóricas",
    "href": "tics411/notebooks/08-proyecto_clustering.html#variables-categóricas",
    "title": "Preparación de los Datos",
    "section": "Variables Categóricas",
    "text": "Variables Categóricas\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\ndummy_vars = ohe.fit_transform(df[[\"x10\"]])\n\nX = pd.concat([df.drop(columns=\"x10\"), dummy_vars], axis=1)\nX\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10_Cat 1\nx10_Cat 2\nx10_Cat 3\n\n\n\n\n0\n105.576134\n4.823419\n3.409904\n-11.687494\n-1532.613468\n-4.589218\n-6.854641\n-8.877022\n-0.449964\n0.0\n0.0\n1.0\n\n\n1\n100.479786\n-4.876628\n-5.404970\n6.932649\n-4092.341900\n12.163845\n-6.502116\n10.874025\n0.348683\n0.0\n0.0\n1.0\n\n\n2\n97.357744\n8.467431\n-0.865210\n4.353712\n1444.577125\n-1.992772\n-12.223474\n-9.100414\n0.407230\n0.0\n0.0\n1.0\n\n\n3\n95.857842\n5.931475\n0.278352\n3.413013\n1959.773064\n-10.248761\n-8.136656\n-9.158037\n0.478212\n0.0\n0.0\n1.0\n\n\n4\n99.772427\n-2.876912\n4.499859\n1.308382\n-2318.502069\n2.062409\n-13.469304\n-0.236395\n0.478002\n0.0\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6418\n98.951606\n6.649525\n1.869195\n1.765821\n4348.322822\n-6.866256\n-1.578755\n-12.749590\n0.454056\n0.0\n0.0\n1.0\n\n\n6419\n94.996949\n-6.638457\n3.999433\n0.989885\n-1811.824888\n-0.859185\n-7.422772\n3.293839\n0.558469\n0.0\n0.0\n1.0\n\n\n6420\n95.495497\n6.664764\n0.019823\n1.825686\n2845.172238\n-7.376139\n-8.056029\n-10.066078\n0.224961\n0.0\n0.0\n1.0\n\n\n6421\n99.435967\n5.469512\n6.342347\n-1.182801\n-358.410366\n-1.205160\n2.248149\n6.840680\n0.748647\n0.0\n1.0\n0.0\n\n\n6422\n109.218858\n-6.367392\n-0.857113\n-6.749834\n1913.311965\n-4.103422\n0.343194\n-5.460592\n0.121518\n0.0\n0.0\n1.0\n\n\n\n\n6423 rows × 12 columns\n\n\n\n\npca = PCA(n_components=2, random_state=42)\npca_X = pca.fit_transform(X)\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"])\nplt.title(\"Visualización PCA del Dataset\")\nplt.tight_layout()"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "href": "tics411/notebooks/08-proyecto_clustering.html#k-means",
    "title": "Preparación de los Datos",
    "section": "K-Means",
    "text": "K-Means\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"Número de Clústers\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X, k_max=20, color=\"blue\", title=\"Curva del Codo para K-Means\"\n)\n\n\n\n\n\n\n\n\n\nmetricas = dict()\n\n\nK_KMEANS = 10\nkm = KMeans(n_clusters=K_KMEANS, n_init=10, random_state=RANDOM_STATE)\nlabels_km = km.fit_predict(X)\n\n\ns_km = silhouette_score(X, labels_km)\nmetricas[\"km_10\"] = s_km\nmetricas\n\n{'km_10': 0.39419687509752793}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#jerárquico",
    "href": "tics411/notebooks/08-proyecto_clustering.html#jerárquico",
    "title": "Preparación de los Datos",
    "section": "Jerárquico",
    "text": "Jerárquico\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\ndef plot_dendogram(X, link=\"ward\"):\n    Z = linkage(X, method=link)\n\n    plt.figure(figsize=(10, 5))\n    plt.title(f\"Clustering Utilizando Iris, Método: {link}\")\n    plt.xlabel(\"Iris Samples\")\n    plt.ylabel(\"Distance\")\n    dendrogram(Z, leaf_rotation=90.0, leaf_font_size=8.0)\n    plt.show()\n\n\nlinkage_list = [\"single\", \"complete\", \"average\", \"ward\"]\nfor l in linkage_list:\n    plot_dendogram(X, link=l)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef train_hierarchical(K_H, linkage):\n    hc = AgglomerativeClustering(n_clusters=K_H, linkage=linkage)\n    labels_h = hc.fit_predict(X)\n    s_h = silhouette_score(X, labels_h)\n    print(f\"El coeficiente de Silueta es {s_h}\")\n    return labels_h, s_h\n\n\nlabels_c9, s_c9 = train_hierarchical(K_H=9, linkage=\"complete\")\nlabels_a9, s_a9 = train_hierarchical(K_H=9, linkage=\"average\")\nlabels_w4, s_w4 = train_hierarchical(K_H=4, linkage=\"ward\")\n\nEl coeficiente de Silueta es 0.37657025597625804\nEl coeficiente de Silueta es 0.3812263959798965\nEl coeficiente de Silueta es 0.2852126845283154\n\n\n\nmetricas[\"s_c9\"] = s_c9\nmetricas[\"s_a9\"] = s_a9\nmetricas[\"s_w4\"] = s_w4\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "href": "tics411/notebooks/08-proyecto_clustering.html#dbscan",
    "title": "Preparación de los Datos",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = X.shape[1] + 1\n\n\ndef dbscan_elbow_plot(X, k=5):\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    distances, _ = knn.kneighbors(X)\n    distances = np.sort(distances[:, -1])\n    n_pts = distances.shape[0]\n\n    plt.plot(range(1, n_pts + 1), distances)\n    plt.xlabel(\n        f\"Puntos ordenados por Distancia al {k} vecino más cercano.\"\n    )\n    plt.ylabel(f\"Distancia al {k} vecino más cercano\")\n    plt.title(f\"Búsqueda de EPS para DBSCAN con k={k}\")\n\n\ndbscan_elbow_plot(X, k=MIN_SAMPLES)\n\nEPS = 1.6\n\n\n\n\n\n\n\n\n\ndbs = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\nlabels_dbs = dbs.fit_predict(X)\ns_dbs = silhouette_score(X, labels_dbs)\nmetricas[\"s_dbs\"] = s_dbs\nmetricas\n\n{'km_10': 0.39419687509752793,\n 's_c9': 0.37657025597625804,\n 's_a9': 0.3812263959798965,\n 's_w4': 0.2852126845283154,\n 's_dbs': 0.1818560991479739}"
  },
  {
    "objectID": "tics411/notebooks/08-proyecto_clustering.html#evaluación",
    "href": "tics411/notebooks/08-proyecto_clustering.html#evaluación",
    "title": "Preparación de los Datos",
    "section": "Evaluación",
    "text": "Evaluación\n\npd.Series(metricas.values(), index=metricas.keys()).plot(\n    kind=\"bar\",\n    rot=0,\n    edgecolor=\"k\",\n    title=\"Silhouette Score para los modelos generados\",\n)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nlabels_km\n\narray([5, 9, 2, ..., 2, 0, 1], dtype=int32)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ndef create_tables(df, labels, columns):\n    df[\"labels\"] = labels\n    std = df.groupby(\"labels\")[columns].std(numeric_only=True)\n    mean = df.groupby(\"labels\")[columns].mean(numeric_only=True)\n    return mean, std\n\n\ndef center_analysis_viz(\n    df, n_clusters, labels, columns, title=\"\", figsize=(20, 20)\n):\n    clusters_axis = [f\"Cluster {i}\" for i in range(1, n_clusters + 1)]\n\n    n_columns = len(columns)\n    colors = list(mcolors.TABLEAU_COLORS.values())[:n_columns]\n    fig, ax = plt.subplots(n_columns, figsize=figsize)\n\n    mean_table, std_table = create_tables(df, labels, columns)\n\n    for i in range(n_columns):\n        ax[i].errorbar(\n            clusters_axis,\n            mean_table[columns[i]],\n            yerr=std_table[columns[i]],\n            capsize=20,\n            linestyle=\"none\",\n            marker=\"o\",\n            lw=3,\n            capthick=3,\n            ms=10,\n            c=colors[i],\n        )\n        ax[i].set_title(columns[i])\n    plt.suptitle(title, fontsize=15)\n    plt.tight_layout()\n\n\ncenter_analysis_viz(\n    df,\n    n_clusters=10,\n    labels=labels_km,\n    columns=num_vars,\n    title=\"Análisis de Centro\",\n)\n\n\n\n\n\n\n\n\n\nplt.scatter(pca_X[\"pca0\"], pca_X[\"pca1\"], c=labels_km)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_silhouette(X, labels_km)\nplt.show()"
  },
  {
    "objectID": "tics411/notebooks/distancia.html",
    "href": "tics411/notebooks/distancia.html",
    "title": "Clases UAI",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(dict(x=[0, 2, 3, 5], y=[2, 0, 1, 1]))\ndf.index = [1, 2, 3, 4]\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n1\n0\n2\n\n\n2\n2\n0\n\n\n3\n3\n1\n\n\n4\n5\n1\n\n\n\n\n\n\n\n\nnp.zeros((4, 4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\ndef distancia_l1(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n\n    return np.abs(x1 - x2) + np.abs(y1 - y2)\n\n\ndef distancia_l2(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n\n\ndef distancia_linf(p, q):\n    x1 = p[\"x\"]\n    x2 = q[\"x\"]\n    y1 = p[\"y\"]\n    y2 = q[\"y\"]\n    d_x = np.abs(x1 - x2)\n    d_y = np.abs(y1 - y2)\n    return np.max([d_x, d_y])\n\n\ndef calculate_matrix(distance, n_puntos):\n    m = np.zeros((n_puntos, n_puntos))\n    for i in range(n_puntos):\n        for j in range(n_puntos):\n            p = df.iloc[i]\n            q = df.iloc[j]\n            m[i, j] = distance(p, q)\n\n    return m\n\n\nm_m = calculate_matrix(distancia_l1, 4)\nm_e = calculate_matrix(distancia_l2, 4)\nm_c = calculate_matrix(distancia_linf, 4)\n\n\nm_m\n\narray([[0., 4., 4., 6.],\n       [4., 0., 2., 4.],\n       [4., 2., 0., 2.],\n       [6., 4., 2., 0.]])\n\n\n\nm_e\n\narray([[0.        , 2.82842712, 3.16227766, 5.09901951],\n       [2.82842712, 0.        , 1.41421356, 3.16227766],\n       [3.16227766, 1.41421356, 0.        , 2.        ],\n       [5.09901951, 3.16227766, 2.        , 0.        ]])\n\n\n\nm_c\n\narray([[0., 2., 3., 5.],\n       [2., 0., 1., 3.],\n       [3., 1., 0., 2.],\n       [5., 3., 2., 0.]])\n\n\n\n\na = pd.Series(\n    [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0]\n)\nb = pd.Series(\n    [1.0, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 7.0]\n)\n\na.var(ddof=0)  # Varianza Poblacional\n\n3.5\n\n\n\na.var(ddof=1)  # Varianza Muestral\n\n3.7916666666666665\n\n\n\nb.var(ddof=1)\n\n1.5916666666666668\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/legacy_code.html",
    "href": "tics411/notebooks/legacy_code.html",
    "title": "Clases UAI",
    "section": "",
    "text": "## Otra forma de calcular lo mismo pero mucho más ineficiente. No usar!!\n# def compute_ideal_sim(labels):\n#     labels = pd.Series(labels, name=\"labels\")\n#     labels_df = labels.to_frame().reset_index()\n#     return (\n#         labels_df.merge(labels_df, how=\"outer\", on=\"labels\")\n#         .add(1)\n#         .set_index([\"index_x\", \"index_y\"])\n#         .unstack(level=1)\n#         .fillna(0)\n#         .astype(bool)\n#         .astype(int)\n#     )\n\n\n# ideal_sim_pd = compute_ideal_sim(labels).to_numpy()\n\n\n## Es para demostrar que dan lo mismo.\n# np.array_equal(ideal_sim_np, ideal_sim_pd)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html",
    "href": "tics411/notebooks/03-ex_kmeans.html",
    "title": "Ejemplo K-Means",
    "section": "",
    "text": "import seaborn as sns\n\n# Importamos el Dataset Iris\ndf = sns.load_dataset(\"iris\")\ndf\ndf[\"species\"].value_counts()\n# Definimos X como una Matriz sin la variable Species.\nX = df.drop(columns=\"species\")\nX"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "href": "tics411/notebooks/03-ex_kmeans.html#ayuda-visual",
    "title": "Ejemplo K-Means",
    "section": "Ayuda Visual",
    "text": "Ayuda Visual\nVamos a utilizar PCA para poder reducir las dimensiones a un tamaño el cual podamos visualizar: 2D.\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n## Esto es sólo una ayuda para poder visualizar datos\n# que están en más dimensiones de las que podemos ver.\npca = PCA(n_components=2, random_state=1)\npca_X = pca.fit_transform(X)\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(pca_X[:, 0], pca_X[:, 1])\nplt.title(\"Visualización de Iris en 2D.\")\nplt.tight_layout()\n\n\n## Esta es una función que nos permitirá visualizar nuestras etiquetas en un espacio reducido por PCA.\n## Además permite la visualización de los centroides de nuestro proceso...\n\n\ndef pca_viz(pca_X, pca_centroids, labels, title=None, cmap=\"viridis\"):\n    plt.scatter(pca_X[:, 0], pca_X[:, 1], c=labels, cmap=cmap)\n    plt.scatter(\n        pca_centroids[:, 0],\n        pca_centroids[:, 1],\n        marker=\"*\",\n        c=\"red\",\n        s=150,\n    )\n    plt.title(title)\n\n\nImplementación de K-Means\n\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nlabels = km.fit_predict(X)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\n\npca_viz(\n    pca_X,\n    pca_centroids,\n    labels=labels,\n    title=\"Visualización de K-Means en Iris 2D\",\n)\n\n\n\nEfecto del Escalamiento en K-Means\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_sc = sc.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_sc = pca.fit_transform(X_sc)\nkm = KMeans(n_clusters=2, n_init=10, random_state=1)\nsc_labels = km.fit_predict(X_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\npca_viz(\n    pca_X_sc,\n    pca_centroids,\n    sc_labels,\n    title=\"K-Means de Iris en 2D luego de Estandarizar los datos. \",\n)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\nX_mm = mm.fit_transform(X)\npca = PCA(n_components=2, random_state=1)\npca_X_mm = pca.fit_transform(X_mm)\nkm = KMeans(n_clusters=3, n_init=10, random_state=1)\nmm_labels = km.fit_predict(X_mm)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(\n    pca_X_mm,\n    pca_centroids,\n    mm_labels,\n    title=\"K-Means de Iris en 2D luego de Normalizar los datos.\",\n)"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#ejemplo-más-avanzado-sin-entrenar-con-todos-los-datos",
    "href": "tics411/notebooks/03-ex_kmeans.html#ejemplo-más-avanzado-sin-entrenar-con-todos-los-datos",
    "title": "Ejemplo K-Means",
    "section": "Ejemplo más avanzado sin entrenar con todos los datos…",
    "text": "Ejemplo más avanzado sin entrenar con todos los datos…\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(X, test_size=0.25, random_state=1)\n\n\nEstamos dejando un 25% de los datos fuera para poder chequear cuál sería la predicción que se le dan a dichos datos.\n\n\npca = PCA(n_components=2)\nkm = KMeans(n_clusters=2, n_init=10)\nsc = StandardScaler()\n## Fit siempre se hace con datos de `Entrenamiento`.\n\n## Escalamos los datos...\nsc.fit(X_train)\nX_train_sc = sc.transform(X_train)\nX_test_sc = sc.transform(X_test)\n\n# Generamos las coordenadas del PCA para visualizar\npca.fit(X_train_sc)\npca_train = pca.transform(X_train_sc)\npca_test = pca.transform(X_test_sc)\n\ntrain_labels = km.fit_predict(X_train_sc)\ntest_labels = km.predict(X_test_sc)\ncentroids = km.cluster_centers_\npca_centroids = pca.transform(centroids)\n\npca_viz(pca_train, pca_centroids, train_labels)\npca_viz(pca_test, pca_centroids, test_labels, cmap=\"tab20b\")"
  },
  {
    "objectID": "tics411/notebooks/03-ex_kmeans.html#cuál-es-el-k-óptimo",
    "href": "tics411/notebooks/03-ex_kmeans.html#cuál-es-el-k-óptimo",
    "title": "Ejemplo K-Means",
    "section": "Cuál es el K óptimo?",
    "text": "Cuál es el K óptimo?\n\ndef elbow_curve(X, k_max=10, color=\"blue\", title=None):\n    wc = []\n    for k in range(1, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=1)\n        km.fit(X)\n        wc.append(km.inertia_)\n\n    k = [*range(1, k_max + 1)]\n    plt.plot(k, wc, c=color, marker=\"*\")\n    plt.title(title)\n    plt.xlabel(\"Número de Clústers\")\n    plt.ylabel(\"Within Distance\")\n    return wc\n\n\nwc = elbow_curve(\n    X_train,\n    k_max=15,\n    color=\"red\",\n    title=\"Curva del Codo para el Dataset Iris, sólo con Train Set.\",\n)\n\n\nwc"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     datacuber.cl\n  \n\n  \n  \nSoy Alfonso y he trabajado como Científico de Datos por los últimos 10 años. Además me gusta el Machine Learning Competitivo y hasta el momento he ganado 2 competencias.\nActualmente me encuentro cursando mi PhD. en Data Science. Mis intereses de investigación tienen que ver con Machine Learning y Deep Learning enfocándome principalmente en la aplicación de Transformers.\nEn mi tiempo libre practico Tenis de Mesa y escribo sobre Machine Learning en mi blog: datacuber.cl.\n\n\nUniversidad Adolfo Ibañez, Viña del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Ibañez, Viña del Mar | Msc. in Data Science | 2022 - 2024\nUniversidad Técnica Federico Santa María | Ingeniería Civil | 2005 - 2013\nPuedes ver más detalles de mi carrera acá.\n\n\n\n\nHate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "index.html#educación",
    "href": "index.html#educación",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "Universidad Adolfo Ibañez, Viña del Mar | PhD. in Data Science | 2023 - 2026\nUniversidad Adolfo Ibañez, Viña del Mar | Msc. in Data Science | 2022 - 2024\nUniversidad Técnica Federico Santa María | Ingeniería Civil | 2005 - 2013\nPuedes ver más detalles de mi carrera acá."
  },
  {
    "objectID": "index.html#publicaciones",
    "href": "index.html#publicaciones",
    "title": "Alfonso Tobar, Msc.",
    "section": "",
    "text": "Hate Speech Recognition in Chilean Tweets"
  },
  {
    "objectID": "tics579-labs.html",
    "href": "tics579-labs.html",
    "title": "Prácticos",
    "section": "",
    "text": "Práctico\nColab\n\n\n\n\nIntro Pytorch\n\n\n\nIntro Pytorch 2\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notebooks"
    ]
  }
]